{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "from autoencoder import Autoencoder\n",
    "import torchvision\n",
    "from model2 import classification_model\n",
    "import copy\n",
    "import partition\n",
    "from pca import PCADigitReducer\n",
    "from autoencoder import reduce_dimensions\n",
    "from training import train,test, train_fashion,test_fashion\n",
    "from federated_learning import distribute_global_model, federated_averaging\n",
    "from model4 import MultilayerPerceptron\n",
    "import cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x1bd2f7d3e10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predefined stuff\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size_train = 100\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "num_clusters = 2\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.2860,), (0.3204,))  \n",
    "])\n",
    "\n",
    "fashion_mnist_train_loader = DataLoader(\n",
    "    datasets.FashionMNIST('/files/', train=True, download=True, transform=fashion_mnist_transform),\n",
    "    batch_size=batch_size_train, shuffle=True\n",
    ")\n",
    "\n",
    "fashion_mnist_test_loader = DataLoader(\n",
    "    datasets.FashionMNIST('/files/', train=False, download=True, transform=fashion_mnist_transform),\n",
    "    batch_size=batch_size_test, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_pca = copy.copy(fashion_mnist_train_loader)\n",
    "test_loader_pca = copy.copy(fashion_mnist_test_loader)\n",
    "\n",
    "train_loader_auto = copy.copy(fashion_mnist_train_loader)\n",
    "test_loader_auto = copy.copy(fashion_mnist_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, *tensors):\n",
    "        super().__init__(*tensors)\n",
    "        self.data = tensors[0]\n",
    "        self.targets = tensors[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for data, labels in train_loader_pca:\n",
    "    train_data.append(data.view(data.size(0), -1))  \n",
    "    train_labels.append(labels)\n",
    "train_data = torch.cat(train_data, dim=0)  \n",
    "train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "train_data_np = train_data.numpy()\n",
    "\n",
    "pca = PCADigitReducer(100)\n",
    "train_data_reduced = pca.fit_transform(train_data_np)  \n",
    "\n",
    "train_data_reconstructed_np = pca.inverse_transform(train_data_reduced) \n",
    "train_data_reconstructed = torch.tensor(train_data_reconstructed_np, dtype=torch.float32)\n",
    "\n",
    "train_data_reconstructed = train_data_reconstructed.view(-1, 1, 28, 28)\n",
    "\n",
    "train_data_reconstructed = (train_data_reconstructed - 0.2860) / 0.3204\n",
    "\n",
    "batch_size_train = train_loader_pca.batch_size\n",
    "train_dataset_pca = CustomTensorDataset(train_data_reconstructed, train_labels)\n",
    "train_loader_reduced_pca = DataLoader(train_dataset_pca, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.6598843336105347\n",
      "Epoch [2/5], Loss: 0.6293150186538696\n",
      "Epoch [3/5], Loss: 0.6253346800804138\n",
      "Epoch [4/5], Loss: 0.5940108895301819\n",
      "Epoch [5/5], Loss: 0.6230719089508057\n"
     ]
    }
   ],
   "source": [
    "# Autoencoder\n",
    "latent_dim = 100  \n",
    "autoencoder = Autoencoder(latent_dim=latent_dim)\n",
    "auto_criterion = nn.MSELoss()\n",
    "auto_optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "auto_num_epochs = 5\n",
    "for epoch in range(auto_num_epochs): \n",
    "    for images, _ in train_loader_auto:\n",
    "        auto_optimizer.zero_grad()\n",
    "        reconstructed = autoencoder(images)\n",
    "        loss = auto_criterion(reconstructed, images)  \n",
    "        loss.backward()\n",
    "        auto_optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "latent_features, labels = reduce_dimensions(train_loader_auto, autoencoder.encoder, device)\n",
    "latent_features = latent_features.detach()\n",
    "\n",
    "reconstructed_images = autoencoder.decoder(latent_features.to(device))  \n",
    "reconstructed_images = reconstructed_images.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfPartitions = [4, 6, 8, 10]\n",
    "results = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}\n",
    "clusteredResults = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = fashion_mnist_train_loader.dataset\n",
    "trial_model_strong = MultilayerPerceptron()\n",
    "global_model_classic_strong = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with 4 partitions...\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.303705\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.182063\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.062437\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 1.895887\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 1.713035\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 1.572574\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.370751\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 1.172862\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.134668\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 1.062771\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.016013\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.904901\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.792116\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.858402\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.947624\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.775462\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.840745\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.771041\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.621593\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.666749\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.716144\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.688134\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.674057\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.648509\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.609773\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.748004\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.591048\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.709553\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.680222\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.617089\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.685544\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.561944\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.727711\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.615039\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.665516\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.649738\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.781814\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.562129\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.532246\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.720261\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.477854\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.561997\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.474352\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.625095\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.469587\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.676538\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.491493\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.641030\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.412912\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.492814\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.449849\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.664971\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.525452\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.570664\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.528495\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.583623\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.346746\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.475317\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.521485\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.597727\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.691827\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.489055\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.508470\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.399320\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.415282\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.402328\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.490349\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.419509\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.539744\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.542011\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.621712\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.374815\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.495823\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.499290\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.471917\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.357656\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.403661\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.488542\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.611991\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.543276\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.527718\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.557517\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.534283\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.451235\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.501352\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.553599\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.425712\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.445634\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.544621\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.441124\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.571727\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.491819\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.507939\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.446483\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.369740\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.370029\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.494576\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.343088\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.370970\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.347411\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.458114\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.379502\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.472606\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.338147\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.486765\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.251651\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.429594\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.417210\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.490923\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.396009\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.356360\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.578619\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.426373\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.551762\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.459985\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.409266\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.477857\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.592434\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.541635\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.349802\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.649759\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.350533\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.583610\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.333663\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.464933\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.376229\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.381225\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.411905\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.298908\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.515952\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.415110\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.410348\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.419832\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.348194\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.464615\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.337721\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.478142\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.359508\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.434999\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.357264\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.345714\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.283143\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.486609\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.260576\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.443829\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.456513\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.447563\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.610319\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.392047\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.417623\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.509681\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.347468\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.379333\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.413331\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.423114\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.419715\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.442055\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.615446\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.352667\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.441932\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.385582\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.656176\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.464849\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.447760\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.505578\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.457671\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.399611\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.421934\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.382113\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.378002\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.480108\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.547653\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.298148\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.370628\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.393025\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.353736\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.399259\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.289798\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.404421\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.413957\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.284653\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.401277\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.530154\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.332564\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.352574\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.370627\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.298296\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.357817\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.590058\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.498028\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.540124\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.360514\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.428447\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.389464\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.392066\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.539637\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.519423\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.495643\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.307370\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.308374\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.407253\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.319416\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.397299\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.487387\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.380134\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.391938\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.260631\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.366744\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.412738\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.384623\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.335185\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.506722\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.446714\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.214787\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.510913\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.502960\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.469925\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.392927\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.469014\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.376436\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.469215\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.386591\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.401491\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.348528\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.266761\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.311425\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.369213\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.420796\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.457987\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.395250\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.378690\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.334869\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.280799\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.199038\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.365548\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.548739\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.510190\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.291399\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.323481\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.438617\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.436545\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.430806\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.365608\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.335316\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.478019\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.414797\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.410225\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.325055\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.415116\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.251300\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.276625\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.201711\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.422689\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.354749\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.325721\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.336254\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.559326\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.367357\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.503865\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.281596\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.447713\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.375256\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.286070\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.306376\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.362466\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.339591\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.380273\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.391956\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.379975\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.355956\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.290386\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.394305\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.594884\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.395129\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.384550\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.355072\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.397944\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.361028\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.388653\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.291467\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.332749\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.411543\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.289179\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.248047\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.391703\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.300276\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.283094\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.404828\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.428327\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.332518\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.435698\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.401804\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.432279\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.396944\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.258418\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.304317\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.335999\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.504149\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.342144\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.584235\n",
      "\n",
      "Test set: Avg. loss: 0.3644, Accuracy: 52248/60000 (87%)\n",
      "\n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/23974 (0%)]\tLoss: 2.275770\n",
      "Train Epoch: 1 [1000/23974 (4%)]\tLoss: 2.086965\n",
      "Train Epoch: 1 [2000/23974 (8%)]\tLoss: 1.947061\n",
      "Train Epoch: 1 [3000/23974 (12%)]\tLoss: 1.755052\n",
      "Train Epoch: 1 [4000/23974 (17%)]\tLoss: 1.710376\n",
      "Train Epoch: 1 [5000/23974 (21%)]\tLoss: 1.361838\n",
      "Train Epoch: 1 [6000/23974 (25%)]\tLoss: 1.366126\n",
      "Train Epoch: 1 [7000/23974 (29%)]\tLoss: 1.101895\n",
      "Train Epoch: 1 [8000/23974 (33%)]\tLoss: 0.931267\n",
      "Train Epoch: 1 [9000/23974 (38%)]\tLoss: 0.993239\n",
      "Train Epoch: 1 [10000/23974 (42%)]\tLoss: 0.742059\n",
      "Train Epoch: 1 [11000/23974 (46%)]\tLoss: 0.823166\n",
      "Train Epoch: 1 [12000/23974 (50%)]\tLoss: 0.750222\n",
      "Train Epoch: 1 [13000/23974 (54%)]\tLoss: 0.682957\n",
      "Train Epoch: 1 [14000/23974 (58%)]\tLoss: 0.805271\n",
      "Train Epoch: 1 [15000/23974 (62%)]\tLoss: 0.687587\n",
      "Train Epoch: 1 [16000/23974 (67%)]\tLoss: 0.768435\n",
      "Train Epoch: 1 [17000/23974 (71%)]\tLoss: 0.479227\n",
      "Train Epoch: 1 [18000/23974 (75%)]\tLoss: 0.567264\n",
      "Train Epoch: 1 [19000/23974 (79%)]\tLoss: 0.536287\n",
      "Train Epoch: 1 [20000/23974 (83%)]\tLoss: 0.570942\n",
      "Train Epoch: 1 [21000/23974 (88%)]\tLoss: 0.730327\n",
      "Train Epoch: 1 [22000/23974 (92%)]\tLoss: 0.687516\n",
      "Train Epoch: 1 [23000/23974 (96%)]\tLoss: 0.525539\n",
      "Train Epoch: 2 [0/23974 (0%)]\tLoss: 0.670774\n",
      "Train Epoch: 2 [1000/23974 (4%)]\tLoss: 0.413641\n",
      "Train Epoch: 2 [2000/23974 (8%)]\tLoss: 0.497043\n",
      "Train Epoch: 2 [3000/23974 (12%)]\tLoss: 0.393336\n",
      "Train Epoch: 2 [4000/23974 (17%)]\tLoss: 0.618735\n",
      "Train Epoch: 2 [5000/23974 (21%)]\tLoss: 0.492650\n",
      "Train Epoch: 2 [6000/23974 (25%)]\tLoss: 0.557705\n",
      "Train Epoch: 2 [7000/23974 (29%)]\tLoss: 0.492910\n",
      "Train Epoch: 2 [8000/23974 (33%)]\tLoss: 0.329878\n",
      "Train Epoch: 2 [9000/23974 (38%)]\tLoss: 0.581963\n",
      "Train Epoch: 2 [10000/23974 (42%)]\tLoss: 0.513766\n",
      "Train Epoch: 2 [11000/23974 (46%)]\tLoss: 0.400352\n",
      "Train Epoch: 2 [12000/23974 (50%)]\tLoss: 0.495851\n",
      "Train Epoch: 2 [13000/23974 (54%)]\tLoss: 0.440163\n",
      "Train Epoch: 2 [14000/23974 (58%)]\tLoss: 0.442961\n",
      "Train Epoch: 2 [15000/23974 (62%)]\tLoss: 0.512734\n",
      "Train Epoch: 2 [16000/23974 (67%)]\tLoss: 0.556828\n",
      "Train Epoch: 2 [17000/23974 (71%)]\tLoss: 0.552449\n",
      "Train Epoch: 2 [18000/23974 (75%)]\tLoss: 0.416002\n",
      "Train Epoch: 2 [19000/23974 (79%)]\tLoss: 0.483240\n",
      "Train Epoch: 2 [20000/23974 (83%)]\tLoss: 0.405764\n",
      "Train Epoch: 2 [21000/23974 (88%)]\tLoss: 0.564256\n",
      "Train Epoch: 2 [22000/23974 (92%)]\tLoss: 0.477535\n",
      "Train Epoch: 2 [23000/23974 (96%)]\tLoss: 0.502071\n",
      "Train Epoch: 3 [0/23974 (0%)]\tLoss: 0.459569\n",
      "Train Epoch: 3 [1000/23974 (4%)]\tLoss: 0.755004\n",
      "Train Epoch: 3 [2000/23974 (8%)]\tLoss: 0.536614\n",
      "Train Epoch: 3 [3000/23974 (12%)]\tLoss: 0.300467\n",
      "Train Epoch: 3 [4000/23974 (17%)]\tLoss: 0.414128\n",
      "Train Epoch: 3 [5000/23974 (21%)]\tLoss: 0.232790\n",
      "Train Epoch: 3 [6000/23974 (25%)]\tLoss: 0.351295\n",
      "Train Epoch: 3 [7000/23974 (29%)]\tLoss: 0.361992\n",
      "Train Epoch: 3 [8000/23974 (33%)]\tLoss: 0.506505\n",
      "Train Epoch: 3 [9000/23974 (38%)]\tLoss: 0.544782\n",
      "Train Epoch: 3 [10000/23974 (42%)]\tLoss: 0.470979\n",
      "Train Epoch: 3 [11000/23974 (46%)]\tLoss: 0.326577\n",
      "Train Epoch: 3 [12000/23974 (50%)]\tLoss: 0.578125\n",
      "Train Epoch: 3 [13000/23974 (54%)]\tLoss: 0.626215\n",
      "Train Epoch: 3 [14000/23974 (58%)]\tLoss: 0.414244\n",
      "Train Epoch: 3 [15000/23974 (62%)]\tLoss: 0.482682\n",
      "Train Epoch: 3 [16000/23974 (67%)]\tLoss: 0.444432\n",
      "Train Epoch: 3 [17000/23974 (71%)]\tLoss: 0.416067\n",
      "Train Epoch: 3 [18000/23974 (75%)]\tLoss: 0.331535\n",
      "Train Epoch: 3 [19000/23974 (79%)]\tLoss: 0.391805\n",
      "Train Epoch: 3 [20000/23974 (83%)]\tLoss: 0.286447\n",
      "Train Epoch: 3 [21000/23974 (88%)]\tLoss: 0.442510\n",
      "Train Epoch: 3 [22000/23974 (92%)]\tLoss: 0.496459\n",
      "Train Epoch: 3 [23000/23974 (96%)]\tLoss: 0.456260\n",
      "Train Epoch: 4 [0/23974 (0%)]\tLoss: 0.357123\n",
      "Train Epoch: 4 [1000/23974 (4%)]\tLoss: 0.433092\n",
      "Train Epoch: 4 [2000/23974 (8%)]\tLoss: 0.527469\n",
      "Train Epoch: 4 [3000/23974 (12%)]\tLoss: 0.513336\n",
      "Train Epoch: 4 [4000/23974 (17%)]\tLoss: 0.351675\n",
      "Train Epoch: 4 [5000/23974 (21%)]\tLoss: 0.372090\n",
      "Train Epoch: 4 [6000/23974 (25%)]\tLoss: 0.608956\n",
      "Train Epoch: 4 [7000/23974 (29%)]\tLoss: 0.357893\n",
      "Train Epoch: 4 [8000/23974 (33%)]\tLoss: 0.373074\n",
      "Train Epoch: 4 [9000/23974 (38%)]\tLoss: 0.326100\n",
      "Train Epoch: 4 [10000/23974 (42%)]\tLoss: 0.361416\n",
      "Train Epoch: 4 [11000/23974 (46%)]\tLoss: 0.375023\n",
      "Train Epoch: 4 [12000/23974 (50%)]\tLoss: 0.448085\n",
      "Train Epoch: 4 [13000/23974 (54%)]\tLoss: 0.473203\n",
      "Train Epoch: 4 [14000/23974 (58%)]\tLoss: 0.509046\n",
      "Train Epoch: 4 [15000/23974 (62%)]\tLoss: 0.414411\n",
      "Train Epoch: 4 [16000/23974 (67%)]\tLoss: 0.386659\n",
      "Train Epoch: 4 [17000/23974 (71%)]\tLoss: 0.455545\n",
      "Train Epoch: 4 [18000/23974 (75%)]\tLoss: 0.401703\n",
      "Train Epoch: 4 [19000/23974 (79%)]\tLoss: 0.440371\n",
      "Train Epoch: 4 [20000/23974 (83%)]\tLoss: 0.304061\n",
      "Train Epoch: 4 [21000/23974 (88%)]\tLoss: 0.294553\n",
      "Train Epoch: 4 [22000/23974 (92%)]\tLoss: 0.506818\n",
      "Train Epoch: 4 [23000/23974 (96%)]\tLoss: 0.423645\n",
      "Train Epoch: 5 [0/23974 (0%)]\tLoss: 0.380828\n",
      "Train Epoch: 5 [1000/23974 (4%)]\tLoss: 0.561673\n",
      "Train Epoch: 5 [2000/23974 (8%)]\tLoss: 0.464709\n",
      "Train Epoch: 5 [3000/23974 (12%)]\tLoss: 0.335022\n",
      "Train Epoch: 5 [4000/23974 (17%)]\tLoss: 0.661699\n",
      "Train Epoch: 5 [5000/23974 (21%)]\tLoss: 0.508733\n",
      "Train Epoch: 5 [6000/23974 (25%)]\tLoss: 0.352500\n",
      "Train Epoch: 5 [7000/23974 (29%)]\tLoss: 0.448424\n",
      "Train Epoch: 5 [8000/23974 (33%)]\tLoss: 0.375689\n",
      "Train Epoch: 5 [9000/23974 (38%)]\tLoss: 0.404142\n",
      "Train Epoch: 5 [10000/23974 (42%)]\tLoss: 0.364411\n",
      "Train Epoch: 5 [11000/23974 (46%)]\tLoss: 0.447810\n",
      "Train Epoch: 5 [12000/23974 (50%)]\tLoss: 0.267530\n",
      "Train Epoch: 5 [13000/23974 (54%)]\tLoss: 0.223784\n",
      "Train Epoch: 5 [14000/23974 (58%)]\tLoss: 0.509374\n",
      "Train Epoch: 5 [15000/23974 (62%)]\tLoss: 0.319400\n",
      "Train Epoch: 5 [16000/23974 (67%)]\tLoss: 0.384864\n",
      "Train Epoch: 5 [17000/23974 (71%)]\tLoss: 0.554890\n",
      "Train Epoch: 5 [18000/23974 (75%)]\tLoss: 0.338214\n",
      "Train Epoch: 5 [19000/23974 (79%)]\tLoss: 0.413375\n",
      "Train Epoch: 5 [20000/23974 (83%)]\tLoss: 0.311713\n",
      "Train Epoch: 5 [21000/23974 (88%)]\tLoss: 0.281906\n",
      "Train Epoch: 5 [22000/23974 (92%)]\tLoss: 0.488258\n",
      "Train Epoch: 5 [23000/23974 (96%)]\tLoss: 0.352215\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/7961 (0%)]\tLoss: 2.306581\n",
      "Train Epoch: 1 [1000/7961 (12%)]\tLoss: 2.147155\n",
      "Train Epoch: 1 [2000/7961 (25%)]\tLoss: 1.966637\n",
      "Train Epoch: 1 [3000/7961 (38%)]\tLoss: 1.919959\n",
      "Train Epoch: 1 [4000/7961 (50%)]\tLoss: 1.549585\n",
      "Train Epoch: 1 [5000/7961 (62%)]\tLoss: 1.328614\n",
      "Train Epoch: 1 [6000/7961 (75%)]\tLoss: 1.340038\n",
      "Train Epoch: 1 [7000/7961 (88%)]\tLoss: 1.334207\n",
      "Train Epoch: 2 [0/7961 (0%)]\tLoss: 1.246884\n",
      "Train Epoch: 2 [1000/7961 (12%)]\tLoss: 0.983239\n",
      "Train Epoch: 2 [2000/7961 (25%)]\tLoss: 0.976374\n",
      "Train Epoch: 2 [3000/7961 (38%)]\tLoss: 1.092856\n",
      "Train Epoch: 2 [4000/7961 (50%)]\tLoss: 1.058060\n",
      "Train Epoch: 2 [5000/7961 (62%)]\tLoss: 0.888469\n",
      "Train Epoch: 2 [6000/7961 (75%)]\tLoss: 0.952498\n",
      "Train Epoch: 2 [7000/7961 (88%)]\tLoss: 0.801833\n",
      "Train Epoch: 3 [0/7961 (0%)]\tLoss: 0.991708\n",
      "Train Epoch: 3 [1000/7961 (12%)]\tLoss: 0.700526\n",
      "Train Epoch: 3 [2000/7961 (25%)]\tLoss: 0.830068\n",
      "Train Epoch: 3 [3000/7961 (38%)]\tLoss: 0.562585\n",
      "Train Epoch: 3 [4000/7961 (50%)]\tLoss: 0.801146\n",
      "Train Epoch: 3 [5000/7961 (62%)]\tLoss: 0.648091\n",
      "Train Epoch: 3 [6000/7961 (75%)]\tLoss: 0.760207\n",
      "Train Epoch: 3 [7000/7961 (88%)]\tLoss: 0.602595\n",
      "Train Epoch: 4 [0/7961 (0%)]\tLoss: 0.676352\n",
      "Train Epoch: 4 [1000/7961 (12%)]\tLoss: 0.640466\n",
      "Train Epoch: 4 [2000/7961 (25%)]\tLoss: 0.740429\n",
      "Train Epoch: 4 [3000/7961 (38%)]\tLoss: 0.745071\n",
      "Train Epoch: 4 [4000/7961 (50%)]\tLoss: 0.687295\n",
      "Train Epoch: 4 [5000/7961 (62%)]\tLoss: 0.726350\n",
      "Train Epoch: 4 [6000/7961 (75%)]\tLoss: 0.622509\n",
      "Train Epoch: 4 [7000/7961 (88%)]\tLoss: 0.660129\n",
      "Train Epoch: 5 [0/7961 (0%)]\tLoss: 0.606998\n",
      "Train Epoch: 5 [1000/7961 (12%)]\tLoss: 0.523376\n",
      "Train Epoch: 5 [2000/7961 (25%)]\tLoss: 0.573731\n",
      "Train Epoch: 5 [3000/7961 (38%)]\tLoss: 0.756104\n",
      "Train Epoch: 5 [4000/7961 (50%)]\tLoss: 0.548708\n",
      "Train Epoch: 5 [5000/7961 (62%)]\tLoss: 0.573915\n",
      "Train Epoch: 5 [6000/7961 (75%)]\tLoss: 0.616755\n",
      "Train Epoch: 5 [7000/7961 (88%)]\tLoss: 0.612786\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/15584 (0%)]\tLoss: 2.329801\n",
      "Train Epoch: 1 [1000/15584 (6%)]\tLoss: 2.155456\n",
      "Train Epoch: 1 [2000/15584 (13%)]\tLoss: 1.823551\n",
      "Train Epoch: 1 [3000/15584 (19%)]\tLoss: 1.449330\n",
      "Train Epoch: 1 [4000/15584 (26%)]\tLoss: 1.345145\n",
      "Train Epoch: 1 [5000/15584 (32%)]\tLoss: 1.149117\n",
      "Train Epoch: 1 [6000/15584 (38%)]\tLoss: 1.037930\n",
      "Train Epoch: 1 [7000/15584 (45%)]\tLoss: 1.083538\n",
      "Train Epoch: 1 [8000/15584 (51%)]\tLoss: 0.885421\n",
      "Train Epoch: 1 [9000/15584 (58%)]\tLoss: 0.823500\n",
      "Train Epoch: 1 [10000/15584 (64%)]\tLoss: 0.982828\n",
      "Train Epoch: 1 [11000/15584 (71%)]\tLoss: 0.704700\n",
      "Train Epoch: 1 [12000/15584 (77%)]\tLoss: 0.924008\n",
      "Train Epoch: 1 [13000/15584 (83%)]\tLoss: 0.778064\n",
      "Train Epoch: 1 [14000/15584 (90%)]\tLoss: 0.632917\n",
      "Train Epoch: 1 [15000/15584 (96%)]\tLoss: 0.647643\n",
      "Train Epoch: 2 [0/15584 (0%)]\tLoss: 0.542521\n",
      "Train Epoch: 2 [1000/15584 (6%)]\tLoss: 0.515074\n",
      "Train Epoch: 2 [2000/15584 (13%)]\tLoss: 0.595876\n",
      "Train Epoch: 2 [3000/15584 (19%)]\tLoss: 0.708972\n",
      "Train Epoch: 2 [4000/15584 (26%)]\tLoss: 0.691437\n",
      "Train Epoch: 2 [5000/15584 (32%)]\tLoss: 0.600299\n",
      "Train Epoch: 2 [6000/15584 (38%)]\tLoss: 0.549423\n",
      "Train Epoch: 2 [7000/15584 (45%)]\tLoss: 0.543346\n",
      "Train Epoch: 2 [8000/15584 (51%)]\tLoss: 0.453929\n",
      "Train Epoch: 2 [9000/15584 (58%)]\tLoss: 0.613914\n",
      "Train Epoch: 2 [10000/15584 (64%)]\tLoss: 0.551464\n",
      "Train Epoch: 2 [11000/15584 (71%)]\tLoss: 0.500994\n",
      "Train Epoch: 2 [12000/15584 (77%)]\tLoss: 0.508843\n",
      "Train Epoch: 2 [13000/15584 (83%)]\tLoss: 0.534365\n",
      "Train Epoch: 2 [14000/15584 (90%)]\tLoss: 0.510482\n",
      "Train Epoch: 2 [15000/15584 (96%)]\tLoss: 0.466599\n",
      "Train Epoch: 3 [0/15584 (0%)]\tLoss: 0.516334\n",
      "Train Epoch: 3 [1000/15584 (6%)]\tLoss: 0.505739\n",
      "Train Epoch: 3 [2000/15584 (13%)]\tLoss: 0.452453\n",
      "Train Epoch: 3 [3000/15584 (19%)]\tLoss: 0.376268\n",
      "Train Epoch: 3 [4000/15584 (26%)]\tLoss: 0.379805\n",
      "Train Epoch: 3 [5000/15584 (32%)]\tLoss: 0.435844\n",
      "Train Epoch: 3 [6000/15584 (38%)]\tLoss: 0.325501\n",
      "Train Epoch: 3 [7000/15584 (45%)]\tLoss: 0.447052\n",
      "Train Epoch: 3 [8000/15584 (51%)]\tLoss: 0.398389\n",
      "Train Epoch: 3 [9000/15584 (58%)]\tLoss: 0.322532\n",
      "Train Epoch: 3 [10000/15584 (64%)]\tLoss: 0.458153\n",
      "Train Epoch: 3 [11000/15584 (71%)]\tLoss: 0.422047\n",
      "Train Epoch: 3 [12000/15584 (77%)]\tLoss: 0.365428\n",
      "Train Epoch: 3 [13000/15584 (83%)]\tLoss: 0.533824\n",
      "Train Epoch: 3 [14000/15584 (90%)]\tLoss: 0.345000\n",
      "Train Epoch: 3 [15000/15584 (96%)]\tLoss: 0.420631\n",
      "Train Epoch: 4 [0/15584 (0%)]\tLoss: 0.386914\n",
      "Train Epoch: 4 [1000/15584 (6%)]\tLoss: 0.461414\n",
      "Train Epoch: 4 [2000/15584 (13%)]\tLoss: 0.456581\n",
      "Train Epoch: 4 [3000/15584 (19%)]\tLoss: 0.475106\n",
      "Train Epoch: 4 [4000/15584 (26%)]\tLoss: 0.330095\n",
      "Train Epoch: 4 [5000/15584 (32%)]\tLoss: 0.414865\n",
      "Train Epoch: 4 [6000/15584 (38%)]\tLoss: 0.318133\n",
      "Train Epoch: 4 [7000/15584 (45%)]\tLoss: 0.262765\n",
      "Train Epoch: 4 [8000/15584 (51%)]\tLoss: 0.365413\n",
      "Train Epoch: 4 [9000/15584 (58%)]\tLoss: 0.331972\n",
      "Train Epoch: 4 [10000/15584 (64%)]\tLoss: 0.353513\n",
      "Train Epoch: 4 [11000/15584 (71%)]\tLoss: 0.448988\n",
      "Train Epoch: 4 [12000/15584 (77%)]\tLoss: 0.338482\n",
      "Train Epoch: 4 [13000/15584 (83%)]\tLoss: 0.357295\n",
      "Train Epoch: 4 [14000/15584 (90%)]\tLoss: 0.289507\n",
      "Train Epoch: 4 [15000/15584 (96%)]\tLoss: 0.470303\n",
      "Train Epoch: 5 [0/15584 (0%)]\tLoss: 0.279664\n",
      "Train Epoch: 5 [1000/15584 (6%)]\tLoss: 0.341158\n",
      "Train Epoch: 5 [2000/15584 (13%)]\tLoss: 0.326035\n",
      "Train Epoch: 5 [3000/15584 (19%)]\tLoss: 0.231985\n",
      "Train Epoch: 5 [4000/15584 (26%)]\tLoss: 0.422157\n",
      "Train Epoch: 5 [5000/15584 (32%)]\tLoss: 0.501283\n",
      "Train Epoch: 5 [6000/15584 (38%)]\tLoss: 0.448747\n",
      "Train Epoch: 5 [7000/15584 (45%)]\tLoss: 0.436606\n",
      "Train Epoch: 5 [8000/15584 (51%)]\tLoss: 0.277247\n",
      "Train Epoch: 5 [9000/15584 (58%)]\tLoss: 0.446421\n",
      "Train Epoch: 5 [10000/15584 (64%)]\tLoss: 0.232131\n",
      "Train Epoch: 5 [11000/15584 (71%)]\tLoss: 0.504281\n",
      "Train Epoch: 5 [12000/15584 (77%)]\tLoss: 0.315749\n",
      "Train Epoch: 5 [13000/15584 (83%)]\tLoss: 0.359958\n",
      "Train Epoch: 5 [14000/15584 (90%)]\tLoss: 0.454898\n",
      "Train Epoch: 5 [15000/15584 (96%)]\tLoss: 0.359434\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/12481 (0%)]\tLoss: 2.356701\n",
      "Train Epoch: 1 [1000/12481 (8%)]\tLoss: 2.124139\n",
      "Train Epoch: 1 [2000/12481 (16%)]\tLoss: 1.632665\n",
      "Train Epoch: 1 [3000/12481 (24%)]\tLoss: 1.297014\n",
      "Train Epoch: 1 [4000/12481 (32%)]\tLoss: 0.944220\n",
      "Train Epoch: 1 [5000/12481 (40%)]\tLoss: 0.972010\n",
      "Train Epoch: 1 [6000/12481 (48%)]\tLoss: 0.815120\n",
      "Train Epoch: 1 [7000/12481 (56%)]\tLoss: 0.688581\n",
      "Train Epoch: 1 [8000/12481 (64%)]\tLoss: 0.597360\n",
      "Train Epoch: 1 [9000/12481 (72%)]\tLoss: 0.569266\n",
      "Train Epoch: 1 [10000/12481 (80%)]\tLoss: 0.619432\n",
      "Train Epoch: 1 [11000/12481 (88%)]\tLoss: 0.601214\n",
      "Train Epoch: 1 [12000/12481 (96%)]\tLoss: 0.627420\n",
      "Train Epoch: 2 [0/12481 (0%)]\tLoss: 0.511732\n",
      "Train Epoch: 2 [1000/12481 (8%)]\tLoss: 0.453669\n",
      "Train Epoch: 2 [2000/12481 (16%)]\tLoss: 0.569239\n",
      "Train Epoch: 2 [3000/12481 (24%)]\tLoss: 0.446176\n",
      "Train Epoch: 2 [4000/12481 (32%)]\tLoss: 0.619088\n",
      "Train Epoch: 2 [5000/12481 (40%)]\tLoss: 0.403194\n",
      "Train Epoch: 2 [6000/12481 (48%)]\tLoss: 0.325833\n",
      "Train Epoch: 2 [7000/12481 (56%)]\tLoss: 0.515071\n",
      "Train Epoch: 2 [8000/12481 (64%)]\tLoss: 0.376764\n",
      "Train Epoch: 2 [9000/12481 (72%)]\tLoss: 0.333294\n",
      "Train Epoch: 2 [10000/12481 (80%)]\tLoss: 0.377729\n",
      "Train Epoch: 2 [11000/12481 (88%)]\tLoss: 0.332272\n",
      "Train Epoch: 2 [12000/12481 (96%)]\tLoss: 0.450249\n",
      "Train Epoch: 3 [0/12481 (0%)]\tLoss: 0.388895\n",
      "Train Epoch: 3 [1000/12481 (8%)]\tLoss: 0.264041\n",
      "Train Epoch: 3 [2000/12481 (16%)]\tLoss: 0.430901\n",
      "Train Epoch: 3 [3000/12481 (24%)]\tLoss: 0.412490\n",
      "Train Epoch: 3 [4000/12481 (32%)]\tLoss: 0.438271\n",
      "Train Epoch: 3 [5000/12481 (40%)]\tLoss: 0.228681\n",
      "Train Epoch: 3 [6000/12481 (48%)]\tLoss: 0.446197\n",
      "Train Epoch: 3 [7000/12481 (56%)]\tLoss: 0.396905\n",
      "Train Epoch: 3 [8000/12481 (64%)]\tLoss: 0.401186\n",
      "Train Epoch: 3 [9000/12481 (72%)]\tLoss: 0.331668\n",
      "Train Epoch: 3 [10000/12481 (80%)]\tLoss: 0.347910\n",
      "Train Epoch: 3 [11000/12481 (88%)]\tLoss: 0.265569\n",
      "Train Epoch: 3 [12000/12481 (96%)]\tLoss: 0.344230\n",
      "Train Epoch: 4 [0/12481 (0%)]\tLoss: 0.399168\n",
      "Train Epoch: 4 [1000/12481 (8%)]\tLoss: 0.354363\n",
      "Train Epoch: 4 [2000/12481 (16%)]\tLoss: 0.252980\n",
      "Train Epoch: 4 [3000/12481 (24%)]\tLoss: 0.325313\n",
      "Train Epoch: 4 [4000/12481 (32%)]\tLoss: 0.320459\n",
      "Train Epoch: 4 [5000/12481 (40%)]\tLoss: 0.544993\n",
      "Train Epoch: 4 [6000/12481 (48%)]\tLoss: 0.280398\n",
      "Train Epoch: 4 [7000/12481 (56%)]\tLoss: 0.308862\n",
      "Train Epoch: 4 [8000/12481 (64%)]\tLoss: 0.558171\n",
      "Train Epoch: 4 [9000/12481 (72%)]\tLoss: 0.287242\n",
      "Train Epoch: 4 [10000/12481 (80%)]\tLoss: 0.413281\n",
      "Train Epoch: 4 [11000/12481 (88%)]\tLoss: 0.387595\n",
      "Train Epoch: 4 [12000/12481 (96%)]\tLoss: 0.525871\n",
      "Train Epoch: 5 [0/12481 (0%)]\tLoss: 0.265941\n",
      "Train Epoch: 5 [1000/12481 (8%)]\tLoss: 0.369809\n",
      "Train Epoch: 5 [2000/12481 (16%)]\tLoss: 0.402876\n",
      "Train Epoch: 5 [3000/12481 (24%)]\tLoss: 0.435482\n",
      "Train Epoch: 5 [4000/12481 (32%)]\tLoss: 0.336077\n",
      "Train Epoch: 5 [5000/12481 (40%)]\tLoss: 0.295578\n",
      "Train Epoch: 5 [6000/12481 (48%)]\tLoss: 0.257147\n",
      "Train Epoch: 5 [7000/12481 (56%)]\tLoss: 0.413074\n",
      "Train Epoch: 5 [8000/12481 (64%)]\tLoss: 0.341035\n",
      "Train Epoch: 5 [9000/12481 (72%)]\tLoss: 0.458401\n",
      "Train Epoch: 5 [10000/12481 (80%)]\tLoss: 0.228437\n",
      "Train Epoch: 5 [11000/12481 (88%)]\tLoss: 0.360954\n",
      "Train Epoch: 5 [12000/12481 (96%)]\tLoss: 0.276619\n",
      "local_models in the distribute function [MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")]\n",
      "4\n",
      "local_models in the distribute function [MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 0.7089, Accuracy: 7441/10000 (74%)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28000x28 and 784x120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 65\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     64\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m fashion_mnist_test_loader:\n\u001b[1;32m---> 65\u001b[0m         output \u001b[38;5;241m=\u001b[39m global_model_classic_strong(data)\n\u001b[0;32m     66\u001b[0m         pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     67\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Documents\\Federated-Dimensionality-Reduction\\model4.py:12\u001b[0m, in \u001b[0;36mMultilayerPerceptron.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m---> 12\u001b[0m     X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(X))\n\u001b[0;32m     13\u001b[0m     X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(X))\n\u001b[0;32m     14\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28000x28 and 784x120)"
     ]
    }
   ],
   "source": [
    "for partitions_number in numberOfPartitions:\n",
    "    print(f\"Running experiment with {partitions_number} partitions...\")\n",
    "\n",
    "    partitioned_data_classic = partition.balanced_dirichlet_partition(trainingset, partitions_number=partitions_number, alpha=0.5)\n",
    "\n",
    "    classic_client_loaders = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic.values()\n",
    "    ]\n",
    "\n",
    "    num_clients = partitions_number\n",
    "    local_models_classic_strong = [copy.deepcopy(global_model_classic_strong) for _ in range(num_clients)]\n",
    "\n",
    "  # Classic strong\n",
    "    optimizer = optim.SGD(trial_model_strong.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        train_fashion(epoch, trial_model_strong, fashion_mnist_train_loader, optimizer, log_interval, train_losses, train_counter)\n",
    "    \n",
    "\n",
    "\n",
    "    test_losses_classic_strong = []\n",
    "    test_fashion(trial_model_strong,fashion_mnist_train_loader,test_losses_classic_strong)\n",
    "\n",
    "    rounds_classic = 4\n",
    "    \n",
    "    for round_idx in range(rounds_classic):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "    \n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic_strong):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "    \n",
    "    \n",
    "        distribute_global_model(global_weights_classic,local_models_classic_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,global_model_classic_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_classic_strong,fashion_mnist_test_loader,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in fashion_mnist_test_loader:\n",
    "                output = global_model_classic_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(fashion_mnist_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for non-clustered classic\n",
    "        if partitions_number not in results[\"classic\"]:\n",
    "            results[\"classic\"][partitions_number] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        results[\"classic\"][partitions_number][\"losses\"].extend(test_losses)\n",
    "        results[\"classic\"][partitions_number][\"accuracy\"].extend(test_accuracies_classic)\n",
    "\n",
    "    ######################\n",
    "    import cluster\n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_classic, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_classic_clustered = clustered_data\n",
    "\n",
    "    classic_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic_clustered.values()\n",
    "    ]\n",
    "\n",
    "    for round_idx in range(rounds_classic):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic_strong[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "    \n",
    "    \n",
    "        distribute_global_model(global_weights_classic,local_models_classic_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,global_model_classic_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_classic_strong,fashion_mnist_test_loader,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in fashion_mnist_test_loader:\n",
    "                output = global_model_classic_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(fashion_mnist_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if partitions_number not in clusteredResults[\"classic\"]:\n",
    "            clusteredResults[\"classic\"][partitions_number] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        clusteredResults[\"classic\"][partitions_number][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"classic\"][partitions_number][\"accuracy\"].extend(test_accuracies_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset_pca = train_loader_reduced_pca.dataset\n",
    "trial_model_pca_strong = MultilayerPerceptron()\n",
    "global_model_pca_strong = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with 4 partitions...\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.264816\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 1.471032\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 1.095888\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 0.890535\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 0.739806\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 0.719797\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 0.689181\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 0.668072\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 0.551762\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 0.518368\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 0.624633\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.563820\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.534271\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.645974\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.550040\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.451547\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.438389\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.490549\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.579216\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.544260\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 0.631583\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.697456\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.567023\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.431236\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.448299\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.454619\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.484205\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.509527\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.462782\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.456897\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.476311\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.412149\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.681140\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.539430\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.369230\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.397645\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.548466\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.520494\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.522536\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.419933\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.365245\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.378010\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.581468\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.525342\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.553732\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.363052\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.367324\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.487341\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.481525\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.510218\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.527007\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.404302\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.371702\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.563308\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.365972\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.312869\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.562720\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.502893\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.319461\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.412200\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.408364\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.298956\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.449038\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.284887\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.492911\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.337218\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.457428\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.420771\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.414157\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.392354\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.601734\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.320979\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.368935\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.556587\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.429163\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.417752\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.500759\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.454908\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.441439\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.521686\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.443806\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.382561\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.427963\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.457937\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.292485\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.463195\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.422984\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.372575\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.355355\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.408104\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.504045\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.396593\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.453119\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.351729\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.604151\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.431497\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.516160\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.365816\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.461631\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.540951\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.536415\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.471252\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.533813\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.428945\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.482840\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.308363\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.292453\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.370686\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.423315\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.404507\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.478611\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.313346\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.436426\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.399533\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.310427\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.419433\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.543251\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.429203\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.348731\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.377379\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.454660\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.349508\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.326070\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.402679\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.301748\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.295139\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.296878\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.428465\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.573337\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.382277\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.523929\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.338687\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.516519\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.343140\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.338384\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.360297\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.363614\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.234379\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.303292\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.327895\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.320026\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.327678\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.486829\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.353673\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.550485\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.475124\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.323211\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.359478\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.276430\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.382383\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.207658\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.382574\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.475767\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.425431\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.263736\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.563488\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.262210\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.421791\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.361375\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.307635\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.400453\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.378722\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.335642\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.379813\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.375116\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.400448\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.304645\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.280722\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.477394\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.308402\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.411439\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.291371\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.301021\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.352321\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.251550\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.291505\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.453544\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.394428\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.282980\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.237786\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.243758\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.426548\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.361265\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.341954\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.356395\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.451146\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.314145\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.287209\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.341543\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.258773\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.310673\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.380313\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.325485\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.357966\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.357472\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.309656\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.285305\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.314960\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.235943\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.277197\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.231870\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.265811\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.294606\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.492564\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.319787\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.204872\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.456272\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.426342\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.377196\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.209446\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.285592\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.384480\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.424250\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.376516\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.386552\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.310285\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.376968\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.293894\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.200928\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.318424\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.439441\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.406592\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.328254\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.260286\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.363120\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.444455\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.474908\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.356238\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.406198\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.273963\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.303793\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.392562\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.379294\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.247390\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.357014\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.370683\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.330700\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.388139\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.396494\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.404343\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.379087\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.361337\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.260272\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.331050\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.346084\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.329140\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.328430\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.279917\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.202791\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.283969\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.187742\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.433977\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.269937\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.224662\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.352207\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.313294\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.421600\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.359626\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.242297\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.376817\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.369875\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.352835\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.302654\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.423445\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.451166\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.242994\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.345879\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.457683\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.410117\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.502784\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.316951\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.381958\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.302394\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.251967\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.303777\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.237163\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.427537\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.269621\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.374481\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.342744\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.289240\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.202266\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.354626\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.320643\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.396182\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.381560\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.367335\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.294890\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.264673\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.263770\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.334706\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.372705\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.420079\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.431655\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.303676\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.339122\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.303509\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.424343\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.340469\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.350194\n",
      "\n",
      "Test set: Avg. loss: 0.3257, Accuracy: 52885/60000 (88%)\n",
      "\n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/23974 (0%)]\tLoss: 2.328217\n",
      "Train Epoch: 1 [1000/23974 (4%)]\tLoss: 1.497930\n",
      "Train Epoch: 1 [2000/23974 (8%)]\tLoss: 0.918747\n",
      "Train Epoch: 1 [3000/23974 (12%)]\tLoss: 0.687191\n",
      "Train Epoch: 1 [4000/23974 (17%)]\tLoss: 0.644471\n",
      "Train Epoch: 1 [5000/23974 (21%)]\tLoss: 0.771925\n",
      "Train Epoch: 1 [6000/23974 (25%)]\tLoss: 0.488088\n",
      "Train Epoch: 1 [7000/23974 (29%)]\tLoss: 0.665989\n",
      "Train Epoch: 1 [8000/23974 (33%)]\tLoss: 0.610935\n",
      "Train Epoch: 1 [9000/23974 (38%)]\tLoss: 0.504568\n",
      "Train Epoch: 1 [10000/23974 (42%)]\tLoss: 0.424749\n",
      "Train Epoch: 1 [11000/23974 (46%)]\tLoss: 0.450574\n",
      "Train Epoch: 1 [12000/23974 (50%)]\tLoss: 0.341054\n",
      "Train Epoch: 1 [13000/23974 (54%)]\tLoss: 0.475537\n",
      "Train Epoch: 1 [14000/23974 (58%)]\tLoss: 0.464700\n",
      "Train Epoch: 1 [15000/23974 (62%)]\tLoss: 0.451495\n",
      "Train Epoch: 1 [16000/23974 (67%)]\tLoss: 0.405785\n",
      "Train Epoch: 1 [17000/23974 (71%)]\tLoss: 0.414014\n",
      "Train Epoch: 1 [18000/23974 (75%)]\tLoss: 0.595635\n",
      "Train Epoch: 1 [19000/23974 (79%)]\tLoss: 0.608291\n",
      "Train Epoch: 1 [20000/23974 (83%)]\tLoss: 0.318959\n",
      "Train Epoch: 1 [21000/23974 (88%)]\tLoss: 0.387883\n",
      "Train Epoch: 1 [22000/23974 (92%)]\tLoss: 0.308212\n",
      "Train Epoch: 1 [23000/23974 (96%)]\tLoss: 0.541255\n",
      "Train Epoch: 2 [0/23974 (0%)]\tLoss: 0.324832\n",
      "Train Epoch: 2 [1000/23974 (4%)]\tLoss: 0.407976\n",
      "Train Epoch: 2 [2000/23974 (8%)]\tLoss: 0.378497\n",
      "Train Epoch: 2 [3000/23974 (12%)]\tLoss: 0.433631\n",
      "Train Epoch: 2 [4000/23974 (17%)]\tLoss: 0.369366\n",
      "Train Epoch: 2 [5000/23974 (21%)]\tLoss: 0.324005\n",
      "Train Epoch: 2 [6000/23974 (25%)]\tLoss: 0.455387\n",
      "Train Epoch: 2 [7000/23974 (29%)]\tLoss: 0.325393\n",
      "Train Epoch: 2 [8000/23974 (33%)]\tLoss: 0.337814\n",
      "Train Epoch: 2 [9000/23974 (38%)]\tLoss: 0.241376\n",
      "Train Epoch: 2 [10000/23974 (42%)]\tLoss: 0.320981\n",
      "Train Epoch: 2 [11000/23974 (46%)]\tLoss: 0.519088\n",
      "Train Epoch: 2 [12000/23974 (50%)]\tLoss: 0.458795\n",
      "Train Epoch: 2 [13000/23974 (54%)]\tLoss: 0.299481\n",
      "Train Epoch: 2 [14000/23974 (58%)]\tLoss: 0.527422\n",
      "Train Epoch: 2 [15000/23974 (62%)]\tLoss: 0.397576\n",
      "Train Epoch: 2 [16000/23974 (67%)]\tLoss: 0.355876\n",
      "Train Epoch: 2 [17000/23974 (71%)]\tLoss: 0.364007\n",
      "Train Epoch: 2 [18000/23974 (75%)]\tLoss: 0.442909\n",
      "Train Epoch: 2 [19000/23974 (79%)]\tLoss: 0.483925\n",
      "Train Epoch: 2 [20000/23974 (83%)]\tLoss: 0.329928\n",
      "Train Epoch: 2 [21000/23974 (88%)]\tLoss: 0.338216\n",
      "Train Epoch: 2 [22000/23974 (92%)]\tLoss: 0.372546\n",
      "Train Epoch: 2 [23000/23974 (96%)]\tLoss: 0.496822\n",
      "Train Epoch: 3 [0/23974 (0%)]\tLoss: 0.254436\n",
      "Train Epoch: 3 [1000/23974 (4%)]\tLoss: 0.315159\n",
      "Train Epoch: 3 [2000/23974 (8%)]\tLoss: 0.338487\n",
      "Train Epoch: 3 [3000/23974 (12%)]\tLoss: 0.314266\n",
      "Train Epoch: 3 [4000/23974 (17%)]\tLoss: 0.308294\n",
      "Train Epoch: 3 [5000/23974 (21%)]\tLoss: 0.486134\n",
      "Train Epoch: 3 [6000/23974 (25%)]\tLoss: 0.354475\n",
      "Train Epoch: 3 [7000/23974 (29%)]\tLoss: 0.358725\n",
      "Train Epoch: 3 [8000/23974 (33%)]\tLoss: 0.287789\n",
      "Train Epoch: 3 [9000/23974 (38%)]\tLoss: 0.450023\n",
      "Train Epoch: 3 [10000/23974 (42%)]\tLoss: 0.363649\n",
      "Train Epoch: 3 [11000/23974 (46%)]\tLoss: 0.350538\n",
      "Train Epoch: 3 [12000/23974 (50%)]\tLoss: 0.457159\n",
      "Train Epoch: 3 [13000/23974 (54%)]\tLoss: 0.377759\n",
      "Train Epoch: 3 [14000/23974 (58%)]\tLoss: 0.251942\n",
      "Train Epoch: 3 [15000/23974 (62%)]\tLoss: 0.445409\n",
      "Train Epoch: 3 [16000/23974 (67%)]\tLoss: 0.325564\n",
      "Train Epoch: 3 [17000/23974 (71%)]\tLoss: 0.300899\n",
      "Train Epoch: 3 [18000/23974 (75%)]\tLoss: 0.333197\n",
      "Train Epoch: 3 [19000/23974 (79%)]\tLoss: 0.318281\n",
      "Train Epoch: 3 [20000/23974 (83%)]\tLoss: 0.272764\n",
      "Train Epoch: 3 [21000/23974 (88%)]\tLoss: 0.241958\n",
      "Train Epoch: 3 [22000/23974 (92%)]\tLoss: 0.264519\n",
      "Train Epoch: 3 [23000/23974 (96%)]\tLoss: 0.388635\n",
      "Train Epoch: 4 [0/23974 (0%)]\tLoss: 0.295907\n",
      "Train Epoch: 4 [1000/23974 (4%)]\tLoss: 0.302998\n",
      "Train Epoch: 4 [2000/23974 (8%)]\tLoss: 0.325820\n",
      "Train Epoch: 4 [3000/23974 (12%)]\tLoss: 0.287637\n",
      "Train Epoch: 4 [4000/23974 (17%)]\tLoss: 0.248397\n",
      "Train Epoch: 4 [5000/23974 (21%)]\tLoss: 0.289650\n",
      "Train Epoch: 4 [6000/23974 (25%)]\tLoss: 0.514261\n",
      "Train Epoch: 4 [7000/23974 (29%)]\tLoss: 0.511282\n",
      "Train Epoch: 4 [8000/23974 (33%)]\tLoss: 0.354167\n",
      "Train Epoch: 4 [9000/23974 (38%)]\tLoss: 0.276425\n",
      "Train Epoch: 4 [10000/23974 (42%)]\tLoss: 0.370145\n",
      "Train Epoch: 4 [11000/23974 (46%)]\tLoss: 0.207648\n",
      "Train Epoch: 4 [12000/23974 (50%)]\tLoss: 0.271214\n",
      "Train Epoch: 4 [13000/23974 (54%)]\tLoss: 0.313465\n",
      "Train Epoch: 4 [14000/23974 (58%)]\tLoss: 0.190159\n",
      "Train Epoch: 4 [15000/23974 (62%)]\tLoss: 0.318263\n",
      "Train Epoch: 4 [16000/23974 (67%)]\tLoss: 0.571270\n",
      "Train Epoch: 4 [17000/23974 (71%)]\tLoss: 0.337027\n",
      "Train Epoch: 4 [18000/23974 (75%)]\tLoss: 0.312512\n",
      "Train Epoch: 4 [19000/23974 (79%)]\tLoss: 0.403552\n",
      "Train Epoch: 4 [20000/23974 (83%)]\tLoss: 0.249843\n",
      "Train Epoch: 4 [21000/23974 (88%)]\tLoss: 0.296370\n",
      "Train Epoch: 4 [22000/23974 (92%)]\tLoss: 0.287988\n",
      "Train Epoch: 4 [23000/23974 (96%)]\tLoss: 0.347022\n",
      "Train Epoch: 5 [0/23974 (0%)]\tLoss: 0.413875\n",
      "Train Epoch: 5 [1000/23974 (4%)]\tLoss: 0.322676\n",
      "Train Epoch: 5 [2000/23974 (8%)]\tLoss: 0.296454\n",
      "Train Epoch: 5 [3000/23974 (12%)]\tLoss: 0.302560\n",
      "Train Epoch: 5 [4000/23974 (17%)]\tLoss: 0.401075\n",
      "Train Epoch: 5 [5000/23974 (21%)]\tLoss: 0.373347\n",
      "Train Epoch: 5 [6000/23974 (25%)]\tLoss: 0.365238\n",
      "Train Epoch: 5 [7000/23974 (29%)]\tLoss: 0.241162\n",
      "Train Epoch: 5 [8000/23974 (33%)]\tLoss: 0.272175\n",
      "Train Epoch: 5 [9000/23974 (38%)]\tLoss: 0.370389\n",
      "Train Epoch: 5 [10000/23974 (42%)]\tLoss: 0.178271\n",
      "Train Epoch: 5 [11000/23974 (46%)]\tLoss: 0.336991\n",
      "Train Epoch: 5 [12000/23974 (50%)]\tLoss: 0.174726\n",
      "Train Epoch: 5 [13000/23974 (54%)]\tLoss: 0.415999\n",
      "Train Epoch: 5 [14000/23974 (58%)]\tLoss: 0.362505\n",
      "Train Epoch: 5 [15000/23974 (62%)]\tLoss: 0.414687\n",
      "Train Epoch: 5 [16000/23974 (67%)]\tLoss: 0.274687\n",
      "Train Epoch: 5 [17000/23974 (71%)]\tLoss: 0.436189\n",
      "Train Epoch: 5 [18000/23974 (75%)]\tLoss: 0.267306\n",
      "Train Epoch: 5 [19000/23974 (79%)]\tLoss: 0.239819\n",
      "Train Epoch: 5 [20000/23974 (83%)]\tLoss: 0.307194\n",
      "Train Epoch: 5 [21000/23974 (88%)]\tLoss: 0.418248\n",
      "Train Epoch: 5 [22000/23974 (92%)]\tLoss: 0.214893\n",
      "Train Epoch: 5 [23000/23974 (96%)]\tLoss: 0.355394\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/7961 (0%)]\tLoss: 2.382658\n",
      "Train Epoch: 1 [1000/7961 (12%)]\tLoss: 1.351231\n",
      "Train Epoch: 1 [2000/7961 (25%)]\tLoss: 1.103528\n",
      "Train Epoch: 1 [3000/7961 (38%)]\tLoss: 0.939174\n",
      "Train Epoch: 1 [4000/7961 (50%)]\tLoss: 0.825928\n",
      "Train Epoch: 1 [5000/7961 (62%)]\tLoss: 0.747757\n",
      "Train Epoch: 1 [6000/7961 (75%)]\tLoss: 0.785574\n",
      "Train Epoch: 1 [7000/7961 (88%)]\tLoss: 0.740463\n",
      "Train Epoch: 2 [0/7961 (0%)]\tLoss: 0.579288\n",
      "Train Epoch: 2 [1000/7961 (12%)]\tLoss: 0.798478\n",
      "Train Epoch: 2 [2000/7961 (25%)]\tLoss: 0.644289\n",
      "Train Epoch: 2 [3000/7961 (38%)]\tLoss: 0.577697\n",
      "Train Epoch: 2 [4000/7961 (50%)]\tLoss: 0.506828\n",
      "Train Epoch: 2 [5000/7961 (62%)]\tLoss: 0.631895\n",
      "Train Epoch: 2 [6000/7961 (75%)]\tLoss: 0.586116\n",
      "Train Epoch: 2 [7000/7961 (88%)]\tLoss: 0.532552\n",
      "Train Epoch: 3 [0/7961 (0%)]\tLoss: 0.658175\n",
      "Train Epoch: 3 [1000/7961 (12%)]\tLoss: 0.590292\n",
      "Train Epoch: 3 [2000/7961 (25%)]\tLoss: 0.691535\n",
      "Train Epoch: 3 [3000/7961 (38%)]\tLoss: 0.615715\n",
      "Train Epoch: 3 [4000/7961 (50%)]\tLoss: 0.563270\n",
      "Train Epoch: 3 [5000/7961 (62%)]\tLoss: 0.619878\n",
      "Train Epoch: 3 [6000/7961 (75%)]\tLoss: 0.477441\n",
      "Train Epoch: 3 [7000/7961 (88%)]\tLoss: 0.520340\n",
      "Train Epoch: 4 [0/7961 (0%)]\tLoss: 0.481037\n",
      "Train Epoch: 4 [1000/7961 (12%)]\tLoss: 0.432281\n",
      "Train Epoch: 4 [2000/7961 (25%)]\tLoss: 0.502987\n",
      "Train Epoch: 4 [3000/7961 (38%)]\tLoss: 0.581271\n",
      "Train Epoch: 4 [4000/7961 (50%)]\tLoss: 0.546092\n",
      "Train Epoch: 4 [5000/7961 (62%)]\tLoss: 0.666658\n",
      "Train Epoch: 4 [6000/7961 (75%)]\tLoss: 0.597991\n",
      "Train Epoch: 4 [7000/7961 (88%)]\tLoss: 0.481633\n",
      "Train Epoch: 5 [0/7961 (0%)]\tLoss: 0.559281\n",
      "Train Epoch: 5 [1000/7961 (12%)]\tLoss: 0.511968\n",
      "Train Epoch: 5 [2000/7961 (25%)]\tLoss: 0.518630\n",
      "Train Epoch: 5 [3000/7961 (38%)]\tLoss: 0.577702\n",
      "Train Epoch: 5 [4000/7961 (50%)]\tLoss: 0.529196\n",
      "Train Epoch: 5 [5000/7961 (62%)]\tLoss: 0.488555\n",
      "Train Epoch: 5 [6000/7961 (75%)]\tLoss: 0.414532\n",
      "Train Epoch: 5 [7000/7961 (88%)]\tLoss: 0.622597\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/15584 (0%)]\tLoss: 2.384287\n",
      "Train Epoch: 1 [1000/15584 (6%)]\tLoss: 1.299348\n",
      "Train Epoch: 1 [2000/15584 (13%)]\tLoss: 0.960221\n",
      "Train Epoch: 1 [3000/15584 (19%)]\tLoss: 0.826621\n",
      "Train Epoch: 1 [4000/15584 (26%)]\tLoss: 0.749709\n",
      "Train Epoch: 1 [5000/15584 (32%)]\tLoss: 0.642581\n",
      "Train Epoch: 1 [6000/15584 (38%)]\tLoss: 0.523457\n",
      "Train Epoch: 1 [7000/15584 (45%)]\tLoss: 0.570188\n",
      "Train Epoch: 1 [8000/15584 (51%)]\tLoss: 0.563370\n",
      "Train Epoch: 1 [9000/15584 (58%)]\tLoss: 0.398728\n",
      "Train Epoch: 1 [10000/15584 (64%)]\tLoss: 0.464429\n",
      "Train Epoch: 1 [11000/15584 (71%)]\tLoss: 0.508982\n",
      "Train Epoch: 1 [12000/15584 (77%)]\tLoss: 0.496323\n",
      "Train Epoch: 1 [13000/15584 (83%)]\tLoss: 0.468416\n",
      "Train Epoch: 1 [14000/15584 (90%)]\tLoss: 0.511166\n",
      "Train Epoch: 1 [15000/15584 (96%)]\tLoss: 0.413721\n",
      "Train Epoch: 2 [0/15584 (0%)]\tLoss: 0.543313\n",
      "Train Epoch: 2 [1000/15584 (6%)]\tLoss: 0.350799\n",
      "Train Epoch: 2 [2000/15584 (13%)]\tLoss: 0.447935\n",
      "Train Epoch: 2 [3000/15584 (19%)]\tLoss: 0.307943\n",
      "Train Epoch: 2 [4000/15584 (26%)]\tLoss: 0.484596\n",
      "Train Epoch: 2 [5000/15584 (32%)]\tLoss: 0.363450\n",
      "Train Epoch: 2 [6000/15584 (38%)]\tLoss: 0.261610\n",
      "Train Epoch: 2 [7000/15584 (45%)]\tLoss: 0.509996\n",
      "Train Epoch: 2 [8000/15584 (51%)]\tLoss: 0.329725\n",
      "Train Epoch: 2 [9000/15584 (58%)]\tLoss: 0.431169\n",
      "Train Epoch: 2 [10000/15584 (64%)]\tLoss: 0.403381\n",
      "Train Epoch: 2 [11000/15584 (71%)]\tLoss: 0.447727\n",
      "Train Epoch: 2 [12000/15584 (77%)]\tLoss: 0.471396\n",
      "Train Epoch: 2 [13000/15584 (83%)]\tLoss: 0.433134\n",
      "Train Epoch: 2 [14000/15584 (90%)]\tLoss: 0.306814\n",
      "Train Epoch: 2 [15000/15584 (96%)]\tLoss: 0.363269\n",
      "Train Epoch: 3 [0/15584 (0%)]\tLoss: 0.276965\n",
      "Train Epoch: 3 [1000/15584 (6%)]\tLoss: 0.402694\n",
      "Train Epoch: 3 [2000/15584 (13%)]\tLoss: 0.360924\n",
      "Train Epoch: 3 [3000/15584 (19%)]\tLoss: 0.378703\n",
      "Train Epoch: 3 [4000/15584 (26%)]\tLoss: 0.254421\n",
      "Train Epoch: 3 [5000/15584 (32%)]\tLoss: 0.282462\n",
      "Train Epoch: 3 [6000/15584 (38%)]\tLoss: 0.391321\n",
      "Train Epoch: 3 [7000/15584 (45%)]\tLoss: 0.348874\n",
      "Train Epoch: 3 [8000/15584 (51%)]\tLoss: 0.306285\n",
      "Train Epoch: 3 [9000/15584 (58%)]\tLoss: 0.232607\n",
      "Train Epoch: 3 [10000/15584 (64%)]\tLoss: 0.227141\n",
      "Train Epoch: 3 [11000/15584 (71%)]\tLoss: 0.513696\n",
      "Train Epoch: 3 [12000/15584 (77%)]\tLoss: 0.330318\n",
      "Train Epoch: 3 [13000/15584 (83%)]\tLoss: 0.432200\n",
      "Train Epoch: 3 [14000/15584 (90%)]\tLoss: 0.318112\n",
      "Train Epoch: 3 [15000/15584 (96%)]\tLoss: 0.314791\n",
      "Train Epoch: 4 [0/15584 (0%)]\tLoss: 0.326263\n",
      "Train Epoch: 4 [1000/15584 (6%)]\tLoss: 0.418712\n",
      "Train Epoch: 4 [2000/15584 (13%)]\tLoss: 0.356217\n",
      "Train Epoch: 4 [3000/15584 (19%)]\tLoss: 0.335114\n",
      "Train Epoch: 4 [4000/15584 (26%)]\tLoss: 0.250313\n",
      "Train Epoch: 4 [5000/15584 (32%)]\tLoss: 0.347875\n",
      "Train Epoch: 4 [6000/15584 (38%)]\tLoss: 0.396068\n",
      "Train Epoch: 4 [7000/15584 (45%)]\tLoss: 0.402963\n",
      "Train Epoch: 4 [8000/15584 (51%)]\tLoss: 0.281866\n",
      "Train Epoch: 4 [9000/15584 (58%)]\tLoss: 0.359831\n",
      "Train Epoch: 4 [10000/15584 (64%)]\tLoss: 0.270460\n",
      "Train Epoch: 4 [11000/15584 (71%)]\tLoss: 0.295277\n",
      "Train Epoch: 4 [12000/15584 (77%)]\tLoss: 0.254057\n",
      "Train Epoch: 4 [13000/15584 (83%)]\tLoss: 0.311725\n",
      "Train Epoch: 4 [14000/15584 (90%)]\tLoss: 0.235137\n",
      "Train Epoch: 4 [15000/15584 (96%)]\tLoss: 0.313877\n",
      "Train Epoch: 5 [0/15584 (0%)]\tLoss: 0.352591\n",
      "Train Epoch: 5 [1000/15584 (6%)]\tLoss: 0.248266\n",
      "Train Epoch: 5 [2000/15584 (13%)]\tLoss: 0.363589\n",
      "Train Epoch: 5 [3000/15584 (19%)]\tLoss: 0.222577\n",
      "Train Epoch: 5 [4000/15584 (26%)]\tLoss: 0.352357\n",
      "Train Epoch: 5 [5000/15584 (32%)]\tLoss: 0.365869\n",
      "Train Epoch: 5 [6000/15584 (38%)]\tLoss: 0.263348\n",
      "Train Epoch: 5 [7000/15584 (45%)]\tLoss: 0.299792\n",
      "Train Epoch: 5 [8000/15584 (51%)]\tLoss: 0.199001\n",
      "Train Epoch: 5 [9000/15584 (58%)]\tLoss: 0.363333\n",
      "Train Epoch: 5 [10000/15584 (64%)]\tLoss: 0.243086\n",
      "Train Epoch: 5 [11000/15584 (71%)]\tLoss: 0.272419\n",
      "Train Epoch: 5 [12000/15584 (77%)]\tLoss: 0.370512\n",
      "Train Epoch: 5 [13000/15584 (83%)]\tLoss: 0.406695\n",
      "Train Epoch: 5 [14000/15584 (90%)]\tLoss: 0.239869\n",
      "Train Epoch: 5 [15000/15584 (96%)]\tLoss: 0.291402\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/12481 (0%)]\tLoss: 2.438308\n",
      "Train Epoch: 1 [1000/12481 (8%)]\tLoss: 1.015471\n",
      "Train Epoch: 1 [2000/12481 (16%)]\tLoss: 0.701044\n",
      "Train Epoch: 1 [3000/12481 (24%)]\tLoss: 0.582209\n",
      "Train Epoch: 1 [4000/12481 (32%)]\tLoss: 0.434688\n",
      "Train Epoch: 1 [5000/12481 (40%)]\tLoss: 0.533699\n",
      "Train Epoch: 1 [6000/12481 (48%)]\tLoss: 0.428266\n",
      "Train Epoch: 1 [7000/12481 (56%)]\tLoss: 0.593372\n",
      "Train Epoch: 1 [8000/12481 (64%)]\tLoss: 0.454487\n",
      "Train Epoch: 1 [9000/12481 (72%)]\tLoss: 0.437754\n",
      "Train Epoch: 1 [10000/12481 (80%)]\tLoss: 0.277602\n",
      "Train Epoch: 1 [11000/12481 (88%)]\tLoss: 0.397415\n",
      "Train Epoch: 1 [12000/12481 (96%)]\tLoss: 0.356267\n",
      "Train Epoch: 2 [0/12481 (0%)]\tLoss: 0.227211\n",
      "Train Epoch: 2 [1000/12481 (8%)]\tLoss: 0.386874\n",
      "Train Epoch: 2 [2000/12481 (16%)]\tLoss: 0.201576\n",
      "Train Epoch: 2 [3000/12481 (24%)]\tLoss: 0.214106\n",
      "Train Epoch: 2 [4000/12481 (32%)]\tLoss: 0.276785\n",
      "Train Epoch: 2 [5000/12481 (40%)]\tLoss: 0.344945\n",
      "Train Epoch: 2 [6000/12481 (48%)]\tLoss: 0.357337\n",
      "Train Epoch: 2 [7000/12481 (56%)]\tLoss: 0.273076\n",
      "Train Epoch: 2 [8000/12481 (64%)]\tLoss: 0.235824\n",
      "Train Epoch: 2 [9000/12481 (72%)]\tLoss: 0.317422\n",
      "Train Epoch: 2 [10000/12481 (80%)]\tLoss: 0.286345\n",
      "Train Epoch: 2 [11000/12481 (88%)]\tLoss: 0.339452\n",
      "Train Epoch: 2 [12000/12481 (96%)]\tLoss: 0.411737\n",
      "Train Epoch: 3 [0/12481 (0%)]\tLoss: 0.439537\n",
      "Train Epoch: 3 [1000/12481 (8%)]\tLoss: 0.233191\n",
      "Train Epoch: 3 [2000/12481 (16%)]\tLoss: 0.280021\n",
      "Train Epoch: 3 [3000/12481 (24%)]\tLoss: 0.314770\n",
      "Train Epoch: 3 [4000/12481 (32%)]\tLoss: 0.166100\n",
      "Train Epoch: 3 [5000/12481 (40%)]\tLoss: 0.300090\n",
      "Train Epoch: 3 [6000/12481 (48%)]\tLoss: 0.292889\n",
      "Train Epoch: 3 [7000/12481 (56%)]\tLoss: 0.304843\n",
      "Train Epoch: 3 [8000/12481 (64%)]\tLoss: 0.241385\n",
      "Train Epoch: 3 [9000/12481 (72%)]\tLoss: 0.178649\n",
      "Train Epoch: 3 [10000/12481 (80%)]\tLoss: 0.232399\n",
      "Train Epoch: 3 [11000/12481 (88%)]\tLoss: 0.183768\n",
      "Train Epoch: 3 [12000/12481 (96%)]\tLoss: 0.280699\n",
      "Train Epoch: 4 [0/12481 (0%)]\tLoss: 0.220261\n",
      "Train Epoch: 4 [1000/12481 (8%)]\tLoss: 0.312004\n",
      "Train Epoch: 4 [2000/12481 (16%)]\tLoss: 0.152470\n",
      "Train Epoch: 4 [3000/12481 (24%)]\tLoss: 0.163323\n",
      "Train Epoch: 4 [4000/12481 (32%)]\tLoss: 0.195930\n",
      "Train Epoch: 4 [5000/12481 (40%)]\tLoss: 0.251630\n",
      "Train Epoch: 4 [6000/12481 (48%)]\tLoss: 0.241387\n",
      "Train Epoch: 4 [7000/12481 (56%)]\tLoss: 0.327401\n",
      "Train Epoch: 4 [8000/12481 (64%)]\tLoss: 0.289438\n",
      "Train Epoch: 4 [9000/12481 (72%)]\tLoss: 0.227435\n",
      "Train Epoch: 4 [10000/12481 (80%)]\tLoss: 0.340215\n",
      "Train Epoch: 4 [11000/12481 (88%)]\tLoss: 0.211822\n",
      "Train Epoch: 4 [12000/12481 (96%)]\tLoss: 0.458894\n",
      "Train Epoch: 5 [0/12481 (0%)]\tLoss: 0.286986\n",
      "Train Epoch: 5 [1000/12481 (8%)]\tLoss: 0.319855\n",
      "Train Epoch: 5 [2000/12481 (16%)]\tLoss: 0.285808\n",
      "Train Epoch: 5 [3000/12481 (24%)]\tLoss: 0.250219\n",
      "Train Epoch: 5 [4000/12481 (32%)]\tLoss: 0.123291\n",
      "Train Epoch: 5 [5000/12481 (40%)]\tLoss: 0.288616\n",
      "Train Epoch: 5 [6000/12481 (48%)]\tLoss: 0.197832\n",
      "Train Epoch: 5 [7000/12481 (56%)]\tLoss: 0.167996\n",
      "Train Epoch: 5 [8000/12481 (64%)]\tLoss: 0.206451\n",
      "Train Epoch: 5 [9000/12481 (72%)]\tLoss: 0.239848\n",
      "Train Epoch: 5 [10000/12481 (80%)]\tLoss: 0.267913\n",
      "Train Epoch: 5 [11000/12481 (88%)]\tLoss: 0.297248\n",
      "Train Epoch: 5 [12000/12481 (96%)]\tLoss: 0.240595\n",
      "local_models in the distribute function [MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      "), MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")]\n",
      "4\n",
      "local_models in the distribute function [MultilayerPerceptron(\n",
      "  (fc1): Linear(in_features=784, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1052, Accuracy: 7858/10000 (79%)\n",
      "\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (28000x28 and 784x120)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m data, target \u001b[38;5;129;01min\u001b[39;00m test_loader_pca:\n\u001b[1;32m---> 63\u001b[0m         output \u001b[38;5;241m=\u001b[39m global_model_pca_strong(data)\n\u001b[0;32m     64\u001b[0m         pred \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmax(\u001b[38;5;241m1\u001b[39m, keepdim\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     65\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39meq(target\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mview_as(pred))\u001b[38;5;241m.\u001b[39msum()\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\Documents\\Federated-Dimensionality-Reduction\\model4.py:12\u001b[0m, in \u001b[0;36mMultilayerPerceptron.forward\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,X):\n\u001b[1;32m---> 12\u001b[0m     X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(X))\n\u001b[0;32m     13\u001b[0m     X \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc2(X))\n\u001b[0;32m     14\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc3(X)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (28000x28 and 784x120)"
     ]
    }
   ],
   "source": [
    "for partitions_number in numberOfPartitions:\n",
    "    print(f\"Running experiment with {partitions_number} partitions...\")\n",
    "\n",
    "    partitioned_data_pca = partition.balanced_dirichlet_partition(trainingset_pca, partitions_number=partitions_number, alpha=0.5)\n",
    "    pca_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_pca, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_pca.values()\n",
    "    ]\n",
    "\n",
    "    num_clients = partitions_number\n",
    "    local_models_pca_strong = [copy.deepcopy(global_model_pca_strong) for _ in range(num_clients)]\n",
    "\n",
    "  # Pca strong\n",
    "    optimizer = optim.SGD(trial_model_pca_strong.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        train_fashion(epoch, trial_model_pca_strong, train_loader_reduced_pca, optimizer, log_interval, train_losses, train_counter)\n",
    "    \n",
    "\n",
    "    \n",
    "    test_losses_pca_strong = []\n",
    "    test_fashion(trial_model_pca_strong,train_loader_reduced_pca,test_losses_pca_strong)\n",
    "    \n",
    "    rounds_pca = 4\n",
    "    for round_idx in range(rounds_pca):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "    \n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca_strong):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,local_models_pca_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,global_model_pca_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_pca_strong,test_loader_pca,test_losses)\n",
    "    \n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                output = global_model_pca_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "    \n",
    "        # Save results for non-clustered classic\n",
    "        if partitions_number not in results[\"pca\"]:\n",
    "            results[\"pca\"][partitions_number] = {\"losses\": [], \"accuracy\": []}\n",
    "    \n",
    "        results[\"pca\"][partitions_number][\"losses\"].extend(test_losses)\n",
    "        results[\"pca\"][partitions_number][\"accuracy\"].extend(test_accuracies_classic)\n",
    "\n",
    "    ######################\n",
    "    import cluster\n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset_pca.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_pca, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_pca_clustered = clustered_data\n",
    "\n",
    "    pca_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset_pca, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_pca_clustered.values()\n",
    "    ]\n",
    "\n",
    "    for round_idx in range(rounds_classic):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca_strong[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,local_models_pca_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,global_model_pca_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_pca_strong,test_loader_pca,test_losses)\n",
    "    \n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                output = global_model_pca_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if partitions_number not in clusteredResults[\"pca\"]:\n",
    "            clusteredResults[\"pca\"][partitions_number] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        clusteredResults[\"pca\"][partitions_number][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"pca\"][partitions_number][\"accuracy\"].extend(test_accuracies_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "trainingset_auto = reduced_train_loader_auto.dataset\n",
    "trial_model_auto_strong = MultilayerPerceptron()\n",
    "global_model_auto_strong = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for partitions_number in numberOfPartitions:\n",
    "    print(f\"Running experiment with {partitions_number} partitions...\")\n",
    "    \n",
    "    partitioned_data_auto = partition.balanced_dirichlet_partition(trainingset_auto, partitions_number=partitions_number, alpha=0.5)\n",
    "    auto_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto.values()\n",
    "    ]\n",
    "    \n",
    "    num_clients = partitions_number\n",
    "    local_model_autoencoder_strong = [copy.deepcopy(global_model_auto_strong) for _ in range(num_clients)]\n",
    "    \n",
    "    optimizer = optim.SGD(trial_model_auto_strong.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "    train_losses = []\n",
    "    train_counter = []\n",
    "    \n",
    "    for epoch in range(1, n_epochs + 1):  \n",
    "        train_fashion(epoch, trial_model_auto_strong, reduced_train_loader_auto, optimizer, log_interval, train_losses, train_counter)\n",
    "    \n",
    "    test_losses_auto_strong = []\n",
    "    test_fashion(trial_model_auto_strong, reduced_train_loader_auto, test_losses_auto_strong)\n",
    "\n",
    "    rounds_auto = 4\n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "        \n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder_strong):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "        \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, auto_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "        \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "        \n",
    "        distribute_global_model(global_weights_auto, local_model_autoencoder_strong, single=False)\n",
    "        distribute_global_model(global_weights_auto, global_model_auto_strong, single=True)\n",
    "        \n",
    "        test_losses = []\n",
    "        test_fashion(global_model_auto_strong, test_loader_auto, test_losses)\n",
    "        \n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_auto:\n",
    "                output = global_model_auto_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_auto.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "        \n",
    "        # Save results for non-clustered classic\n",
    "        if partitions_number not in results[\"autoencoder\"]:\n",
    "            results[\"autoencoder\"][partitions_number] = {\"losses\": [], \"accuracy\": []}\n",
    "        \n",
    "        results[\"autoencoder\"][partitions_number][\"losses\"].extend(test_losses)\n",
    "        results[\"autoencoder\"][partitions_number][\"accuracy\"].extend(test_accuracies_classic)\n",
    "\n",
    "    ######################\n",
    "    # Clustering process\n",
    "    import cluster\n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset_auto.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_auto, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_auto_clustered = clustered_data\n",
    "    \n",
    "    auto_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto_clustered.values()\n",
    "    ]\n",
    "    \n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "        \n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder_strong[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "        \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, auto_client_loaders_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "        \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "        \n",
    "        distribute_global_model(global_weights_auto, local_model_autoencoder_strong, single=False)\n",
    "        distribute_global_model(global_weights_auto, global_model_auto_strong, single=True)\n",
    "        \n",
    "        test_losses = []\n",
    "        test_fashion(global_model_auto_strong, test_loader_auto, test_losses)\n",
    "        \n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_auto:\n",
    "                output = global_model_auto_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_auto.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "        \n",
    "        # Save results for clustered classic\n",
    "        if partitions_number not in clusteredResults[\"autoencoder\"]:\n",
    "            clusteredResults[\"autoencoder\"][partitions_number] = {\"losses\": [], \"accuracy\": []}\n",
    "        \n",
    "        clusteredResults[\"autoencoder\"][partitions_number][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"autoencoder\"][partitions_number][\"accuracy\"].extend(test_accuracies_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_client_averages(data):\n",
    "    client_averages = {}\n",
    "    for client, client_data in data.items():  \n",
    "        client_averages[client] = {}\n",
    "        for partition_number, metrics in client_data.items():  # Iterate over partitions\n",
    "            avg_loss = sum(metrics['losses']) / len(metrics['losses']) if metrics['losses'] else 0\n",
    "            avg_accuracy = sum(metrics['accuracy']) / len(metrics['accuracy']) if metrics['accuracy'] else 0\n",
    "            client_averages[client][partition_number] = {\"average_loss\": avg_loss, \"average_accuracy\": avg_accuracy}\n",
    "    return client_averages\n",
    "\n",
    "\n",
    "non_clustered_averages = calculate_client_averages(results)\n",
    "clustered_averages = calculate_client_averages(clusteredResults)\n",
    "\n",
    "\n",
    "def plot_averages(non_clustered, clustered):\n",
    "    for client_type in non_clustered.keys(): \n",
    "        partitions_non_clustered = list(non_clustered[client_type].keys())\n",
    "        losses_non_clustered = [\n",
    "            non_clustered[client_type][partition][\"average_loss\"] for partition in partitions_non_clustered\n",
    "        ]\n",
    "        accuracies_non_clustered = [\n",
    "            non_clustered[client_type][partition][\"average_accuracy\"] for partition in partitions_non_clustered\n",
    "        ]\n",
    "\n",
    "        partitions_clustered = list(clustered[client_type].keys())\n",
    "        losses_clustered = [\n",
    "            clustered[client_type][partition][\"average_loss\"] for partition in partitions_clustered\n",
    "        ]\n",
    "        accuracies_clustered = [\n",
    "            clustered[client_type][partition][\"average_accuracy\"] for partition in partitions_clustered\n",
    "        ]\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "\n",
    " \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(partitions_non_clustered, losses_non_clustered, label=\"Non-Clustered\", marker=\"o\")\n",
    "        plt.plot(partitions_clustered, losses_clustered, label=\"Clustered\", marker=\"o\")\n",
    "        plt.xlabel(\"Number of Partitions\")\n",
    "        plt.ylabel(\"Average Loss\")\n",
    "        plt.title(f\"Average Loss per Partition ({client_type.capitalize()})\")\n",
    "        plt.legend()\n",
    "\n",
    " \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(partitions_non_clustered, accuracies_non_clustered, label=\"Non-Clustered\", marker=\"o\")\n",
    "        plt.plot(partitions_clustered, accuracies_clustered, label=\"Clustered\", marker=\"o\")\n",
    "        plt.xlabel(\"Number of Partitions\")\n",
    "        plt.ylabel(\"Average Accuracy (%)\")\n",
    "        plt.title(f\"Average Accuracy per Partition ({client_type.capitalize()})\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot averages for all client types\n",
    "plot_averages(non_clustered_averages, clustered_averages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
