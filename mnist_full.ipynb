{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "from autoencoder import Autoencoder\n",
    "import torchvision\n",
    "from model2 import classification_model\n",
    "import copy\n",
    "import partition\n",
    "from pca import PCADigitReducer\n",
    "from autoencoder import reduce_dimensions\n",
    "from training import train,test, train_fashion,test_fashion\n",
    "from federated_learning import distribute_global_model, federated_averaging\n",
    "from model4 import MultilayerPerceptron\n",
    "import cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x17ba4d8d190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predefined stuff\n",
    "\n",
    "n_epochs = 5\n",
    "batch_size_train = 100\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "num_clusters = 2\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing data as dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.1307,), (0.3081,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_pca = copy.copy(train_loader)\n",
    "test_loader_pca = copy.copy(test_loader)\n",
    "\n",
    "train_loader_auto = copy.copy(train_loader)\n",
    "test_loader_auto = copy.copy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, *tensors):\n",
    "        super().__init__(*tensors)\n",
    "        self.data = tensors[0]\n",
    "        self.targets = tensors[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for data, labels in train_loader_pca:\n",
    "    train_data.append(data.view(data.size(0), -1))  \n",
    "    train_labels.append(labels)\n",
    "train_data = torch.cat(train_data, dim=0)  \n",
    "train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "train_data_np = train_data.numpy()\n",
    "\n",
    "pca = PCADigitReducer(100)\n",
    "train_data_reduced = pca.fit_transform(train_data_np)  \n",
    "\n",
    "train_data_reconstructed_np = pca.inverse_transform(train_data_reduced) \n",
    "train_data_reconstructed = torch.tensor(train_data_reconstructed_np, dtype=torch.float32)\n",
    "\n",
    "train_data_reconstructed = train_data_reconstructed.view(-1, 1, 28, 28)\n",
    "\n",
    "train_data_reconstructed = (train_data_reconstructed - 0.1307) / 0.3081\n",
    "\n",
    "batch_size_train = train_loader_pca.batch_size\n",
    "train_dataset_pca = CustomTensorDataset(train_data_reconstructed, train_labels)\n",
    "train_loader_reduced_pca = DataLoader(train_dataset_pca, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 0.5663461685180664\n",
      "Epoch [2/5], Loss: 0.5424520969390869\n",
      "Epoch [3/5], Loss: 0.5068625211715698\n",
      "Epoch [4/5], Loss: 0.5391815900802612\n",
      "Epoch [5/5], Loss: 0.5141973495483398\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100  \n",
    "autoencoder = Autoencoder(latent_dim=latent_dim)\n",
    "auto_criterion = nn.MSELoss()\n",
    "auto_optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "auto_num_epochs = 5\n",
    "for epoch in range(auto_num_epochs): \n",
    "    for images, _ in train_loader_auto:\n",
    "        auto_optimizer.zero_grad()\n",
    "        reconstructed = autoencoder(images)\n",
    "        loss = auto_criterion(reconstructed, images)  \n",
    "        loss.backward()\n",
    "        auto_optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "latent_features, labels = reduce_dimensions(train_loader_auto, autoencoder.encoder, device)\n",
    "latent_features = latent_features.detach()\n",
    "\n",
    "reconstructed_images = autoencoder.decoder(latent_features.to(device))  \n",
    "reconstructed_images = reconstructed_images.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Partition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classic\n",
    "trainingset = train_loader.dataset\n",
    "partitioned_data_classic = partition.balanced_dirichlet_partition(trainingset, partitions_number=4, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster\n",
    "cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "\n",
    "targets = trainingset.targets\n",
    "num_classes = len(set(targets)) \n",
    "clustered_data = cluster.apply_clustering(partitioned_data_classic, targets, num_classes)\n",
    "\n",
    "partitioned_data_classic_clustered = clustered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_client_loaders = [\n",
    "    DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_classic.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classic_client_loaders_clustered = [\n",
    "    DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_classic_clustered.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "trainingset_pca = train_loader_reduced_pca.dataset\n",
    "partitioned_data_pca = partition.balanced_dirichlet_partition(trainingset_pca, partitions_number=4, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster\n",
    "cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "\n",
    "targets = trainingset_pca.targets\n",
    "num_classes = len(set(targets)) \n",
    "clustered_data = cluster.apply_clustering(partitioned_data_pca, targets, num_classes)\n",
    "\n",
    "partitioned_data_pca_clustered = clustered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_client_loaders = [\n",
    "    DataLoader(Subset(trainingset_pca, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_pca.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_client_loaders_clustered = [\n",
    "    DataLoader(Subset(trainingset_pca, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_pca_clustered.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "trainingset_auto = reduced_train_loader_auto.dataset\n",
    "\n",
    "partitioned_data_auto = partition.balanced_dirichlet_partition(trainingset_auto, partitions_number=4, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cluster\n",
    "cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "\n",
    "targets = trainingset_auto.targets\n",
    "num_classes = len(set(targets)) \n",
    "clustered_data = cluster.apply_clustering(partitioned_data_auto, targets, num_classes)\n",
    "\n",
    "partitioned_data_auto_clustered = clustered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_client_loaders = [\n",
    "    DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_auto.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "auto_client_loaders_clustered = [\n",
    "    DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_auto_clustered.values()\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "trial_model = classification_model()\n",
    "trial_model_pca = classification_model()\n",
    "trial_model_auto = classification_model()\n",
    "\n",
    "global_model_pca = classification_model()\n",
    "global_model_auto = classification_model()\n",
    "global_model_classic = classification_model()\n",
    "\n",
    "num_clients = 4\n",
    "# classic models\n",
    "local_models_classic = [copy.deepcopy(global_model_pca) for _ in range(num_clients)]\n",
    "# pca models \n",
    "local_models_pca = [copy.deepcopy(global_model_pca) for _ in range(num_clients)]\n",
    "# autoencodere models\n",
    "local_model_autoencoder = [copy.deepcopy(global_model_pca) for _ in range(num_clients)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\Downloads\\Federated-Dimensionality-Reduction\\model2.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.322914\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.309317\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.271955\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.271565\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.221788\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 2.217618\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.188999\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 2.126586\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.123816\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 1.976046\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.870121\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 1.892674\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 1.821305\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 1.651540\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 1.468705\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 1.569610\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 1.482701\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 1.272433\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 1.279954\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 1.166096\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.243810\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 1.033403\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 1.171702\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 1.006444\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.208318\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.807234\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.814866\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.755424\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.037294\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.888854\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.875736\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.844082\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.832282\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.710744\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.742627\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.718517\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.902490\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.893556\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.903459\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.714954\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.701722\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.715472\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.681746\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.747317\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.784821\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.606605\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.754395\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.869584\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.792403\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.630901\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.561478\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.514700\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.704081\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.747089\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.543011\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.605696\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.719736\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.642167\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.586633\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.487750\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.829527\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.693861\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.650048\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.733228\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.670961\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.524894\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.757042\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.482638\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.573418\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.586982\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.648055\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.734777\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.729606\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.541445\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.531251\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.445575\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.425756\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.466670\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.739340\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.350945\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.649186\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.525024\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.517701\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.394672\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.452375\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.393094\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.460948\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.372724\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.632508\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.484070\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.507366\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.702514\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.522015\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.279529\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.339372\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.611406\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.435705\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.387095\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.582698\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.415275\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.504808\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.395082\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.623268\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.707445\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.829355\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.468881\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.630954\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.515911\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.519567\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.552089\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.485770\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.300019\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.370553\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.390355\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.422851\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.358761\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.304486\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.460082\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.279571\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.416967\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.364711\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.444239\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.442245\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.568399\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.291875\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.584715\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.333050\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.475780\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.399302\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.393652\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.448622\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.346242\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.337791\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.434817\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.431315\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.407333\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.339641\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.330951\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.544779\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.373401\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.237490\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.526448\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.303368\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.500824\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.484518\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.462632\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.458604\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.259957\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.379593\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.378251\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.377427\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.404289\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.293971\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.375359\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.389383\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.417724\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.338777\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.388725\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.476236\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.302695\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.260833\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.246771\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.240980\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.339409\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.467628\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.485984\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.420688\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.547865\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.470573\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.331677\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.563312\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.184320\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.340045\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.352584\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.311530\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.541900\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.413164\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.299346\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.524928\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.347293\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.302131\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.524314\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.393421\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.240374\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.410327\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.446638\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.325614\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.350847\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.393776\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.495280\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.505968\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.373656\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.322535\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.379631\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.262266\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.537313\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.582209\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.232584\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.323886\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.247367\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.412302\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.371711\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.391360\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.262980\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.264965\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.199090\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.331265\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.379023\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.438985\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.252396\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.234711\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.303386\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.208198\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.371801\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.374907\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.442499\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.384598\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.440262\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.242286\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.454188\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.317296\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.520983\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.316868\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.437774\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.338211\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.317771\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.308507\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.259359\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.346655\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.201243\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.404651\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.448294\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.244992\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.457387\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.333635\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.348435\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.220880\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.289679\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.386361\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.534254\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.324433\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.498624\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.220061\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.238390\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.388638\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.376528\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.329103\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.307381\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.171562\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.244906\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.465676\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.257942\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.375014\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.218794\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.297746\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.350208\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.178087\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.269708\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.328232\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.278884\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.205953\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.212532\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.302321\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.358897\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.287212\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.431064\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.276160\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.394300\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.421682\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.184070\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.252692\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.278465\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.293144\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.172926\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.253533\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.325338\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.295007\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.247759\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.500295\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.235056\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.171386\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.275860\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.387982\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.323275\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.117292\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.346323\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.311619\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.200838\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.305841\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.282821\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.226705\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.265998\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.226771\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.265863\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.264705\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.190084\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.332534\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.276799\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.272777\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.682540\n"
     ]
    }
   ],
   "source": [
    "# test for errors\n",
    "\n",
    "# Classic \n",
    "\n",
    "optimizer = optim.SGD(trial_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):  \n",
    "    train(epoch, trial_model, train_loader, optimizer, log_interval, train_losses, train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\micha\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:42: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0980, Accuracy: 9699/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses_classic_weak = []\n",
    "test(trial_model,test_loader,test_losses_classic_weak)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.468736\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.251172\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.249715\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.138798\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 1.883769\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 1.760172\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 1.609760\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 1.320532\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 1.254012\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 1.103052\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 1.252422\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 0.974234\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 0.966534\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 0.976349\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 0.881376\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 0.848165\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 0.910968\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 0.822875\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 0.760597\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 0.809391\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 1.110649\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 0.641663\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 0.717562\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 0.672143\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 0.598646\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 0.615223\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 0.691567\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 0.751947\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 0.676390\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 0.619828\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 0.692740\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 0.560805\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.629294\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 0.534244\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 0.617837\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 0.495146\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 0.528249\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 0.560627\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 0.483718\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 0.685629\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 0.488432\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 0.478091\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 0.629351\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.458152\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 0.414035\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 0.392157\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.480447\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.470579\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.716935\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.445690\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.594730\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.495171\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.518726\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.356322\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 0.282274\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.455754\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 0.437402\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.280015\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.454911\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.493201\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.611192\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.397330\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.339171\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.551822\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.453033\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.483063\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.467685\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.409501\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.400480\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.279204\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.332954\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.220677\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.453562\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.525376\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.297496\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.259800\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.421004\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.385798\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.461577\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.527991\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.588222\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.293159\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.489414\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.386255\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.364464\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.316085\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.367754\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.312350\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.324541\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.365980\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.397649\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.374074\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.349402\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.507811\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.317349\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.221020\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.321872\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.377308\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.202812\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.303148\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.309393\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.296337\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.416080\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.291153\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.348172\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.371121\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.268800\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.359514\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.489818\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.290309\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.329204\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.301239\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.221424\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.254863\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.371681\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.242709\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.429055\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.365533\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.298774\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.283576\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.257344\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.239238\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.283105\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.228431\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.480383\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.339876\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.401110\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.296041\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.266949\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.399230\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.228832\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.252268\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.250364\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.357316\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.263168\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.298957\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.091989\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.147428\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.274920\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.288264\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.234629\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.267582\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.181539\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.227862\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.371502\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.256509\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.361481\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.208979\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.473200\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.184726\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.289970\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.260602\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.268880\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.251356\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.308993\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.410108\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.354251\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.361632\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.252679\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.249123\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.252286\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.355758\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.158834\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.217820\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.244514\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.310322\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.135756\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.201254\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.343737\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.183465\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.182290\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.152513\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.240444\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.269255\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.731438\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.247861\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.310293\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.208791\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.306568\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.300549\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.299455\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.430088\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.243701\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.333378\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.308564\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.271961\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.206830\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.247360\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.240014\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.147789\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.223612\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.260903\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.097154\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.234966\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.146420\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.358608\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.221374\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.303304\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.218996\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.336258\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.278392\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.270815\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.306389\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.262664\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.204681\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.294185\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.241383\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.239662\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.333250\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.248811\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.229354\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.217281\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.411720\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.371329\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.302916\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.232856\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.295432\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.190731\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.374270\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.222647\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.140538\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.151368\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.134330\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.173846\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.176456\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.150571\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.364707\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.109309\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.263913\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.211116\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.232606\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.164131\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.369852\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.141112\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.286727\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.434656\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.237461\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.128249\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.184592\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.288526\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.156141\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.219105\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.159739\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.248719\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.196555\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.239079\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.169987\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.323662\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.150134\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.331824\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.293063\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.424466\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.082456\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.302776\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.154418\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.198870\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.089878\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.149514\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.213235\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.305446\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.236177\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.145308\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.159112\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.240201\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.187673\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.298685\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.184089\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.227480\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.188809\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.561190\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.176056\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.167360\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.229353\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.281957\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.157159\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.259876\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.117442\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.093584\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.160645\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.278365\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.144312\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.172371\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.113067\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.134385\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.350047\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.214238\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.110821\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.124511\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.139891\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.138132\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.110154\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.117308\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.251217\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.189207\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.222955\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.219962\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.240122\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.225765\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.290577\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.207205\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(trial_model_pca.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):  \n",
    "    train(epoch, trial_model_pca, train_loader_reduced_pca, optimizer, log_interval, train_losses, train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.0736, Accuracy: 58625/60000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses_classic_pca = []\n",
    "test(trial_model_pca,train_loader_reduced_pca,test_losses_classic_pca)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318559\n",
      "Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.319729\n",
      "Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.307912\n",
      "Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.300609\n",
      "Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.288991\n",
      "Train Epoch: 1 [5000/60000 (8%)]\tLoss: 2.313698\n",
      "Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.293541\n",
      "Train Epoch: 1 [7000/60000 (12%)]\tLoss: 2.267741\n",
      "Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.287977\n",
      "Train Epoch: 1 [9000/60000 (15%)]\tLoss: 2.287069\n",
      "Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.262686\n",
      "Train Epoch: 1 [11000/60000 (18%)]\tLoss: 2.271168\n",
      "Train Epoch: 1 [12000/60000 (20%)]\tLoss: 2.278646\n",
      "Train Epoch: 1 [13000/60000 (22%)]\tLoss: 2.262750\n",
      "Train Epoch: 1 [14000/60000 (23%)]\tLoss: 2.270869\n",
      "Train Epoch: 1 [15000/60000 (25%)]\tLoss: 2.243819\n",
      "Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.229177\n",
      "Train Epoch: 1 [17000/60000 (28%)]\tLoss: 2.219525\n",
      "Train Epoch: 1 [18000/60000 (30%)]\tLoss: 2.233061\n",
      "Train Epoch: 1 [19000/60000 (32%)]\tLoss: 2.179585\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.122956\n",
      "Train Epoch: 1 [21000/60000 (35%)]\tLoss: 2.135455\n",
      "Train Epoch: 1 [22000/60000 (37%)]\tLoss: 2.068083\n",
      "Train Epoch: 1 [23000/60000 (38%)]\tLoss: 2.067057\n",
      "Train Epoch: 1 [24000/60000 (40%)]\tLoss: 1.980799\n",
      "Train Epoch: 1 [25000/60000 (42%)]\tLoss: 1.873094\n",
      "Train Epoch: 1 [26000/60000 (43%)]\tLoss: 1.879265\n",
      "Train Epoch: 1 [27000/60000 (45%)]\tLoss: 1.836605\n",
      "Train Epoch: 1 [28000/60000 (47%)]\tLoss: 1.652219\n",
      "Train Epoch: 1 [29000/60000 (48%)]\tLoss: 1.645828\n",
      "Train Epoch: 1 [30000/60000 (50%)]\tLoss: 1.575707\n",
      "Train Epoch: 1 [31000/60000 (52%)]\tLoss: 1.554852\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 1.419184\n",
      "Train Epoch: 1 [33000/60000 (55%)]\tLoss: 1.455514\n",
      "Train Epoch: 1 [34000/60000 (57%)]\tLoss: 1.316905\n",
      "Train Epoch: 1 [35000/60000 (58%)]\tLoss: 1.232777\n",
      "Train Epoch: 1 [36000/60000 (60%)]\tLoss: 1.161259\n",
      "Train Epoch: 1 [37000/60000 (62%)]\tLoss: 1.282348\n",
      "Train Epoch: 1 [38000/60000 (63%)]\tLoss: 1.152904\n",
      "Train Epoch: 1 [39000/60000 (65%)]\tLoss: 1.225149\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 1.051209\n",
      "Train Epoch: 1 [41000/60000 (68%)]\tLoss: 1.135648\n",
      "Train Epoch: 1 [42000/60000 (70%)]\tLoss: 1.271507\n",
      "Train Epoch: 1 [43000/60000 (72%)]\tLoss: 0.829838\n",
      "Train Epoch: 1 [44000/60000 (73%)]\tLoss: 1.246643\n",
      "Train Epoch: 1 [45000/60000 (75%)]\tLoss: 1.152386\n",
      "Train Epoch: 1 [46000/60000 (77%)]\tLoss: 0.889297\n",
      "Train Epoch: 1 [47000/60000 (78%)]\tLoss: 0.902437\n",
      "Train Epoch: 1 [48000/60000 (80%)]\tLoss: 0.939210\n",
      "Train Epoch: 1 [49000/60000 (82%)]\tLoss: 0.984679\n",
      "Train Epoch: 1 [50000/60000 (83%)]\tLoss: 0.942603\n",
      "Train Epoch: 1 [51000/60000 (85%)]\tLoss: 0.838396\n",
      "Train Epoch: 1 [52000/60000 (87%)]\tLoss: 0.892612\n",
      "Train Epoch: 1 [53000/60000 (88%)]\tLoss: 0.735098\n",
      "Train Epoch: 1 [54000/60000 (90%)]\tLoss: 1.070657\n",
      "Train Epoch: 1 [55000/60000 (92%)]\tLoss: 0.842400\n",
      "Train Epoch: 1 [56000/60000 (93%)]\tLoss: 1.023608\n",
      "Train Epoch: 1 [57000/60000 (95%)]\tLoss: 0.824974\n",
      "Train Epoch: 1 [58000/60000 (97%)]\tLoss: 0.846937\n",
      "Train Epoch: 1 [59000/60000 (98%)]\tLoss: 0.786454\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 1.111830\n",
      "Train Epoch: 2 [1000/60000 (2%)]\tLoss: 0.873623\n",
      "Train Epoch: 2 [2000/60000 (3%)]\tLoss: 0.814491\n",
      "Train Epoch: 2 [3000/60000 (5%)]\tLoss: 0.619509\n",
      "Train Epoch: 2 [4000/60000 (7%)]\tLoss: 0.703161\n",
      "Train Epoch: 2 [5000/60000 (8%)]\tLoss: 0.779868\n",
      "Train Epoch: 2 [6000/60000 (10%)]\tLoss: 0.660173\n",
      "Train Epoch: 2 [7000/60000 (12%)]\tLoss: 0.809930\n",
      "Train Epoch: 2 [8000/60000 (13%)]\tLoss: 0.766605\n",
      "Train Epoch: 2 [9000/60000 (15%)]\tLoss: 0.804236\n",
      "Train Epoch: 2 [10000/60000 (17%)]\tLoss: 0.890261\n",
      "Train Epoch: 2 [11000/60000 (18%)]\tLoss: 0.612030\n",
      "Train Epoch: 2 [12000/60000 (20%)]\tLoss: 0.731748\n",
      "Train Epoch: 2 [13000/60000 (22%)]\tLoss: 0.879641\n",
      "Train Epoch: 2 [14000/60000 (23%)]\tLoss: 0.700152\n",
      "Train Epoch: 2 [15000/60000 (25%)]\tLoss: 0.743225\n",
      "Train Epoch: 2 [16000/60000 (27%)]\tLoss: 0.792539\n",
      "Train Epoch: 2 [17000/60000 (28%)]\tLoss: 0.854558\n",
      "Train Epoch: 2 [18000/60000 (30%)]\tLoss: 0.661857\n",
      "Train Epoch: 2 [19000/60000 (32%)]\tLoss: 0.524158\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 0.709541\n",
      "Train Epoch: 2 [21000/60000 (35%)]\tLoss: 0.561160\n",
      "Train Epoch: 2 [22000/60000 (37%)]\tLoss: 0.820561\n",
      "Train Epoch: 2 [23000/60000 (38%)]\tLoss: 0.794550\n",
      "Train Epoch: 2 [24000/60000 (40%)]\tLoss: 0.629284\n",
      "Train Epoch: 2 [25000/60000 (42%)]\tLoss: 0.454878\n",
      "Train Epoch: 2 [26000/60000 (43%)]\tLoss: 0.666911\n",
      "Train Epoch: 2 [27000/60000 (45%)]\tLoss: 0.601282\n",
      "Train Epoch: 2 [28000/60000 (47%)]\tLoss: 0.799473\n",
      "Train Epoch: 2 [29000/60000 (48%)]\tLoss: 0.672966\n",
      "Train Epoch: 2 [30000/60000 (50%)]\tLoss: 0.593340\n",
      "Train Epoch: 2 [31000/60000 (52%)]\tLoss: 0.830114\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.575526\n",
      "Train Epoch: 2 [33000/60000 (55%)]\tLoss: 0.701770\n",
      "Train Epoch: 2 [34000/60000 (57%)]\tLoss: 0.566973\n",
      "Train Epoch: 2 [35000/60000 (58%)]\tLoss: 0.587753\n",
      "Train Epoch: 2 [36000/60000 (60%)]\tLoss: 0.494600\n",
      "Train Epoch: 2 [37000/60000 (62%)]\tLoss: 0.581013\n",
      "Train Epoch: 2 [38000/60000 (63%)]\tLoss: 0.637613\n",
      "Train Epoch: 2 [39000/60000 (65%)]\tLoss: 0.608506\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 0.485005\n",
      "Train Epoch: 2 [41000/60000 (68%)]\tLoss: 0.673204\n",
      "Train Epoch: 2 [42000/60000 (70%)]\tLoss: 0.676860\n",
      "Train Epoch: 2 [43000/60000 (72%)]\tLoss: 0.475747\n",
      "Train Epoch: 2 [44000/60000 (73%)]\tLoss: 0.516510\n",
      "Train Epoch: 2 [45000/60000 (75%)]\tLoss: 0.590749\n",
      "Train Epoch: 2 [46000/60000 (77%)]\tLoss: 0.470369\n",
      "Train Epoch: 2 [47000/60000 (78%)]\tLoss: 0.604539\n",
      "Train Epoch: 2 [48000/60000 (80%)]\tLoss: 0.754770\n",
      "Train Epoch: 2 [49000/60000 (82%)]\tLoss: 0.622105\n",
      "Train Epoch: 2 [50000/60000 (83%)]\tLoss: 0.593435\n",
      "Train Epoch: 2 [51000/60000 (85%)]\tLoss: 0.598113\n",
      "Train Epoch: 2 [52000/60000 (87%)]\tLoss: 0.512370\n",
      "Train Epoch: 2 [53000/60000 (88%)]\tLoss: 0.456395\n",
      "Train Epoch: 2 [54000/60000 (90%)]\tLoss: 0.499164\n",
      "Train Epoch: 2 [55000/60000 (92%)]\tLoss: 0.513122\n",
      "Train Epoch: 2 [56000/60000 (93%)]\tLoss: 0.467779\n",
      "Train Epoch: 2 [57000/60000 (95%)]\tLoss: 0.572370\n",
      "Train Epoch: 2 [58000/60000 (97%)]\tLoss: 0.646091\n",
      "Train Epoch: 2 [59000/60000 (98%)]\tLoss: 0.484984\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.529337\n",
      "Train Epoch: 3 [1000/60000 (2%)]\tLoss: 0.598596\n",
      "Train Epoch: 3 [2000/60000 (3%)]\tLoss: 0.570425\n",
      "Train Epoch: 3 [3000/60000 (5%)]\tLoss: 0.481074\n",
      "Train Epoch: 3 [4000/60000 (7%)]\tLoss: 0.496125\n",
      "Train Epoch: 3 [5000/60000 (8%)]\tLoss: 0.573716\n",
      "Train Epoch: 3 [6000/60000 (10%)]\tLoss: 0.503151\n",
      "Train Epoch: 3 [7000/60000 (12%)]\tLoss: 0.663568\n",
      "Train Epoch: 3 [8000/60000 (13%)]\tLoss: 0.704160\n",
      "Train Epoch: 3 [9000/60000 (15%)]\tLoss: 0.342909\n",
      "Train Epoch: 3 [10000/60000 (17%)]\tLoss: 0.476567\n",
      "Train Epoch: 3 [11000/60000 (18%)]\tLoss: 0.565706\n",
      "Train Epoch: 3 [12000/60000 (20%)]\tLoss: 0.678032\n",
      "Train Epoch: 3 [13000/60000 (22%)]\tLoss: 0.627519\n",
      "Train Epoch: 3 [14000/60000 (23%)]\tLoss: 0.459353\n",
      "Train Epoch: 3 [15000/60000 (25%)]\tLoss: 0.521728\n",
      "Train Epoch: 3 [16000/60000 (27%)]\tLoss: 0.473967\n",
      "Train Epoch: 3 [17000/60000 (28%)]\tLoss: 0.734727\n",
      "Train Epoch: 3 [18000/60000 (30%)]\tLoss: 0.521858\n",
      "Train Epoch: 3 [19000/60000 (32%)]\tLoss: 0.571133\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 0.472508\n",
      "Train Epoch: 3 [21000/60000 (35%)]\tLoss: 0.405249\n",
      "Train Epoch: 3 [22000/60000 (37%)]\tLoss: 0.473165\n",
      "Train Epoch: 3 [23000/60000 (38%)]\tLoss: 0.453840\n",
      "Train Epoch: 3 [24000/60000 (40%)]\tLoss: 0.445580\n",
      "Train Epoch: 3 [25000/60000 (42%)]\tLoss: 0.426147\n",
      "Train Epoch: 3 [26000/60000 (43%)]\tLoss: 0.587655\n",
      "Train Epoch: 3 [27000/60000 (45%)]\tLoss: 0.463325\n",
      "Train Epoch: 3 [28000/60000 (47%)]\tLoss: 0.589333\n",
      "Train Epoch: 3 [29000/60000 (48%)]\tLoss: 0.787562\n",
      "Train Epoch: 3 [30000/60000 (50%)]\tLoss: 0.482980\n",
      "Train Epoch: 3 [31000/60000 (52%)]\tLoss: 0.423562\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.691096\n",
      "Train Epoch: 3 [33000/60000 (55%)]\tLoss: 0.744777\n",
      "Train Epoch: 3 [34000/60000 (57%)]\tLoss: 0.564356\n",
      "Train Epoch: 3 [35000/60000 (58%)]\tLoss: 0.708754\n",
      "Train Epoch: 3 [36000/60000 (60%)]\tLoss: 0.646992\n",
      "Train Epoch: 3 [37000/60000 (62%)]\tLoss: 0.576931\n",
      "Train Epoch: 3 [38000/60000 (63%)]\tLoss: 0.517402\n",
      "Train Epoch: 3 [39000/60000 (65%)]\tLoss: 0.523175\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 0.649961\n",
      "Train Epoch: 3 [41000/60000 (68%)]\tLoss: 0.372629\n",
      "Train Epoch: 3 [42000/60000 (70%)]\tLoss: 0.559919\n",
      "Train Epoch: 3 [43000/60000 (72%)]\tLoss: 0.496142\n",
      "Train Epoch: 3 [44000/60000 (73%)]\tLoss: 0.528966\n",
      "Train Epoch: 3 [45000/60000 (75%)]\tLoss: 0.433869\n",
      "Train Epoch: 3 [46000/60000 (77%)]\tLoss: 0.502751\n",
      "Train Epoch: 3 [47000/60000 (78%)]\tLoss: 0.348649\n",
      "Train Epoch: 3 [48000/60000 (80%)]\tLoss: 0.584726\n",
      "Train Epoch: 3 [49000/60000 (82%)]\tLoss: 0.710158\n",
      "Train Epoch: 3 [50000/60000 (83%)]\tLoss: 0.307852\n",
      "Train Epoch: 3 [51000/60000 (85%)]\tLoss: 0.520704\n",
      "Train Epoch: 3 [52000/60000 (87%)]\tLoss: 0.584084\n",
      "Train Epoch: 3 [53000/60000 (88%)]\tLoss: 0.557468\n",
      "Train Epoch: 3 [54000/60000 (90%)]\tLoss: 0.541984\n",
      "Train Epoch: 3 [55000/60000 (92%)]\tLoss: 0.533890\n",
      "Train Epoch: 3 [56000/60000 (93%)]\tLoss: 0.432401\n",
      "Train Epoch: 3 [57000/60000 (95%)]\tLoss: 0.451881\n",
      "Train Epoch: 3 [58000/60000 (97%)]\tLoss: 0.391548\n",
      "Train Epoch: 3 [59000/60000 (98%)]\tLoss: 0.633464\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.544080\n",
      "Train Epoch: 4 [1000/60000 (2%)]\tLoss: 0.360408\n",
      "Train Epoch: 4 [2000/60000 (3%)]\tLoss: 0.521532\n",
      "Train Epoch: 4 [3000/60000 (5%)]\tLoss: 0.470875\n",
      "Train Epoch: 4 [4000/60000 (7%)]\tLoss: 0.476941\n",
      "Train Epoch: 4 [5000/60000 (8%)]\tLoss: 0.590501\n",
      "Train Epoch: 4 [6000/60000 (10%)]\tLoss: 0.501010\n",
      "Train Epoch: 4 [7000/60000 (12%)]\tLoss: 0.321348\n",
      "Train Epoch: 4 [8000/60000 (13%)]\tLoss: 0.500672\n",
      "Train Epoch: 4 [9000/60000 (15%)]\tLoss: 0.726241\n",
      "Train Epoch: 4 [10000/60000 (17%)]\tLoss: 0.481153\n",
      "Train Epoch: 4 [11000/60000 (18%)]\tLoss: 0.832044\n",
      "Train Epoch: 4 [12000/60000 (20%)]\tLoss: 0.503197\n",
      "Train Epoch: 4 [13000/60000 (22%)]\tLoss: 0.439410\n",
      "Train Epoch: 4 [14000/60000 (23%)]\tLoss: 0.427840\n",
      "Train Epoch: 4 [15000/60000 (25%)]\tLoss: 0.732913\n",
      "Train Epoch: 4 [16000/60000 (27%)]\tLoss: 0.496766\n",
      "Train Epoch: 4 [17000/60000 (28%)]\tLoss: 0.503615\n",
      "Train Epoch: 4 [18000/60000 (30%)]\tLoss: 0.548025\n",
      "Train Epoch: 4 [19000/60000 (32%)]\tLoss: 0.620703\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 0.358811\n",
      "Train Epoch: 4 [21000/60000 (35%)]\tLoss: 0.448923\n",
      "Train Epoch: 4 [22000/60000 (37%)]\tLoss: 0.443817\n",
      "Train Epoch: 4 [23000/60000 (38%)]\tLoss: 0.480195\n",
      "Train Epoch: 4 [24000/60000 (40%)]\tLoss: 0.434938\n",
      "Train Epoch: 4 [25000/60000 (42%)]\tLoss: 0.537708\n",
      "Train Epoch: 4 [26000/60000 (43%)]\tLoss: 0.421677\n",
      "Train Epoch: 4 [27000/60000 (45%)]\tLoss: 0.492286\n",
      "Train Epoch: 4 [28000/60000 (47%)]\tLoss: 0.531539\n",
      "Train Epoch: 4 [29000/60000 (48%)]\tLoss: 0.475432\n",
      "Train Epoch: 4 [30000/60000 (50%)]\tLoss: 0.306535\n",
      "Train Epoch: 4 [31000/60000 (52%)]\tLoss: 0.366964\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.568476\n",
      "Train Epoch: 4 [33000/60000 (55%)]\tLoss: 0.607155\n",
      "Train Epoch: 4 [34000/60000 (57%)]\tLoss: 0.599665\n",
      "Train Epoch: 4 [35000/60000 (58%)]\tLoss: 0.345056\n",
      "Train Epoch: 4 [36000/60000 (60%)]\tLoss: 0.395657\n",
      "Train Epoch: 4 [37000/60000 (62%)]\tLoss: 0.351027\n",
      "Train Epoch: 4 [38000/60000 (63%)]\tLoss: 0.495960\n",
      "Train Epoch: 4 [39000/60000 (65%)]\tLoss: 0.549023\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 0.328385\n",
      "Train Epoch: 4 [41000/60000 (68%)]\tLoss: 0.335414\n",
      "Train Epoch: 4 [42000/60000 (70%)]\tLoss: 0.353118\n",
      "Train Epoch: 4 [43000/60000 (72%)]\tLoss: 0.518820\n",
      "Train Epoch: 4 [44000/60000 (73%)]\tLoss: 0.456661\n",
      "Train Epoch: 4 [45000/60000 (75%)]\tLoss: 0.466490\n",
      "Train Epoch: 4 [46000/60000 (77%)]\tLoss: 0.401883\n",
      "Train Epoch: 4 [47000/60000 (78%)]\tLoss: 0.297526\n",
      "Train Epoch: 4 [48000/60000 (80%)]\tLoss: 0.453443\n",
      "Train Epoch: 4 [49000/60000 (82%)]\tLoss: 0.506368\n",
      "Train Epoch: 4 [50000/60000 (83%)]\tLoss: 0.581069\n",
      "Train Epoch: 4 [51000/60000 (85%)]\tLoss: 0.578404\n",
      "Train Epoch: 4 [52000/60000 (87%)]\tLoss: 0.476935\n",
      "Train Epoch: 4 [53000/60000 (88%)]\tLoss: 0.422329\n",
      "Train Epoch: 4 [54000/60000 (90%)]\tLoss: 0.544586\n",
      "Train Epoch: 4 [55000/60000 (92%)]\tLoss: 0.406098\n",
      "Train Epoch: 4 [56000/60000 (93%)]\tLoss: 0.359243\n",
      "Train Epoch: 4 [57000/60000 (95%)]\tLoss: 0.512150\n",
      "Train Epoch: 4 [58000/60000 (97%)]\tLoss: 0.389003\n",
      "Train Epoch: 4 [59000/60000 (98%)]\tLoss: 0.410859\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.585365\n",
      "Train Epoch: 5 [1000/60000 (2%)]\tLoss: 0.539342\n",
      "Train Epoch: 5 [2000/60000 (3%)]\tLoss: 0.321199\n",
      "Train Epoch: 5 [3000/60000 (5%)]\tLoss: 0.379122\n",
      "Train Epoch: 5 [4000/60000 (7%)]\tLoss: 0.437391\n",
      "Train Epoch: 5 [5000/60000 (8%)]\tLoss: 0.489966\n",
      "Train Epoch: 5 [6000/60000 (10%)]\tLoss: 0.522871\n",
      "Train Epoch: 5 [7000/60000 (12%)]\tLoss: 0.247597\n",
      "Train Epoch: 5 [8000/60000 (13%)]\tLoss: 0.198932\n",
      "Train Epoch: 5 [9000/60000 (15%)]\tLoss: 0.415496\n",
      "Train Epoch: 5 [10000/60000 (17%)]\tLoss: 0.503162\n",
      "Train Epoch: 5 [11000/60000 (18%)]\tLoss: 0.500684\n",
      "Train Epoch: 5 [12000/60000 (20%)]\tLoss: 0.402193\n",
      "Train Epoch: 5 [13000/60000 (22%)]\tLoss: 0.453722\n",
      "Train Epoch: 5 [14000/60000 (23%)]\tLoss: 0.262614\n",
      "Train Epoch: 5 [15000/60000 (25%)]\tLoss: 0.453057\n",
      "Train Epoch: 5 [16000/60000 (27%)]\tLoss: 0.236972\n",
      "Train Epoch: 5 [17000/60000 (28%)]\tLoss: 0.478062\n",
      "Train Epoch: 5 [18000/60000 (30%)]\tLoss: 0.461444\n",
      "Train Epoch: 5 [19000/60000 (32%)]\tLoss: 0.579387\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 0.320354\n",
      "Train Epoch: 5 [21000/60000 (35%)]\tLoss: 0.501084\n",
      "Train Epoch: 5 [22000/60000 (37%)]\tLoss: 0.433032\n",
      "Train Epoch: 5 [23000/60000 (38%)]\tLoss: 0.302839\n",
      "Train Epoch: 5 [24000/60000 (40%)]\tLoss: 0.413619\n",
      "Train Epoch: 5 [25000/60000 (42%)]\tLoss: 0.195587\n",
      "Train Epoch: 5 [26000/60000 (43%)]\tLoss: 0.284156\n",
      "Train Epoch: 5 [27000/60000 (45%)]\tLoss: 0.363553\n",
      "Train Epoch: 5 [28000/60000 (47%)]\tLoss: 0.229060\n",
      "Train Epoch: 5 [29000/60000 (48%)]\tLoss: 0.426811\n",
      "Train Epoch: 5 [30000/60000 (50%)]\tLoss: 0.356350\n",
      "Train Epoch: 5 [31000/60000 (52%)]\tLoss: 0.425965\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.657530\n",
      "Train Epoch: 5 [33000/60000 (55%)]\tLoss: 0.434575\n",
      "Train Epoch: 5 [34000/60000 (57%)]\tLoss: 0.498605\n",
      "Train Epoch: 5 [35000/60000 (58%)]\tLoss: 0.444186\n",
      "Train Epoch: 5 [36000/60000 (60%)]\tLoss: 0.391784\n",
      "Train Epoch: 5 [37000/60000 (62%)]\tLoss: 0.334483\n",
      "Train Epoch: 5 [38000/60000 (63%)]\tLoss: 0.268863\n",
      "Train Epoch: 5 [39000/60000 (65%)]\tLoss: 0.407901\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 0.517503\n",
      "Train Epoch: 5 [41000/60000 (68%)]\tLoss: 0.279761\n",
      "Train Epoch: 5 [42000/60000 (70%)]\tLoss: 0.468051\n",
      "Train Epoch: 5 [43000/60000 (72%)]\tLoss: 0.246527\n",
      "Train Epoch: 5 [44000/60000 (73%)]\tLoss: 0.446737\n",
      "Train Epoch: 5 [45000/60000 (75%)]\tLoss: 0.398130\n",
      "Train Epoch: 5 [46000/60000 (77%)]\tLoss: 0.337957\n",
      "Train Epoch: 5 [47000/60000 (78%)]\tLoss: 0.437057\n",
      "Train Epoch: 5 [48000/60000 (80%)]\tLoss: 0.389235\n",
      "Train Epoch: 5 [49000/60000 (82%)]\tLoss: 0.391562\n",
      "Train Epoch: 5 [50000/60000 (83%)]\tLoss: 0.349382\n",
      "Train Epoch: 5 [51000/60000 (85%)]\tLoss: 0.289269\n",
      "Train Epoch: 5 [52000/60000 (87%)]\tLoss: 0.374803\n",
      "Train Epoch: 5 [53000/60000 (88%)]\tLoss: 0.295759\n",
      "Train Epoch: 5 [54000/60000 (90%)]\tLoss: 0.191048\n",
      "Train Epoch: 5 [55000/60000 (92%)]\tLoss: 0.391344\n",
      "Train Epoch: 5 [56000/60000 (93%)]\tLoss: 0.401622\n",
      "Train Epoch: 5 [57000/60000 (95%)]\tLoss: 0.237795\n",
      "Train Epoch: 5 [58000/60000 (97%)]\tLoss: 0.504224\n",
      "Train Epoch: 5 [59000/60000 (98%)]\tLoss: 0.318365\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD(trial_model_auto.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "train_losses = []\n",
    "train_counter = []\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):  \n",
    "    train(epoch, trial_model_auto, reduced_train_loader_auto, optimizer, log_interval, train_losses, train_counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 0.1869, Accuracy: 56548/60000 (94%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_losses_classic_auto = []\n",
    "test(trial_model_auto,reduced_train_loader_auto,test_losses_classic_auto)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# federated learning loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_classic = 4\n",
    "\n",
    "for round_idx in range(rounds_classic):\n",
    "    \n",
    "    print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "    local_weights_classic = []\n",
    "    for client_idx, client_model in enumerate(local_models_classic):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_classic.append(client_weights)\n",
    "        \n",
    "    print(f\"after training{local_models_classic}\")\n",
    "    global_weights_classic = federated_averaging(local_weights_classic)\n",
    "    print(f\"after fedaveraging{local_models_classic}\")\n",
    "\n",
    "    distribute_global_model(global_weights_classic,local_models_classic,single=False)\n",
    "\n",
    "    distribute_global_model(global_weights_classic,global_model_classic,single=True)\n",
    "    test_losses = []\n",
    "    test(global_model_classic,test_loader,test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rounds_pca = 4\n",
    "\n",
    "for round_idx in range(rounds_pca):\n",
    "    \n",
    "    print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "\n",
    "    local_weights_pca = []\n",
    "    for client_idx, client_model in enumerate(local_models_pca):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_pca.append(client_weights)\n",
    "        \n",
    "    print(f\"after training{local_models_pca}\")\n",
    "    global_weights_pca = federated_averaging(local_weights_pca)\n",
    "    print(f\"after fedaveraging{local_models_pca}\")\n",
    "\n",
    "    distribute_global_model(global_weights_pca,local_models_pca,single=False)\n",
    "\n",
    "    distribute_global_model(global_weights_pca,global_model_pca,single=True)\n",
    "    test_losses = []\n",
    "    test(global_model_pca,test_loader_pca,test_losses)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_auto = 4\n",
    "\n",
    "for round_idx in range(rounds_auto):\n",
    "    print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "\n",
    "    local_weights_auto = []\n",
    "    for client_idx, client_model in enumerate(local_model_autoencoder):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, auto_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_auto.append(client_weights)\n",
    "        \n",
    "    global_weights_auto = federated_averaging(local_weights_auto)\n",
    "\n",
    "    distribute_global_model(global_weights_auto,local_model_autoencoder,single=False)\n",
    "\n",
    "    distribute_global_model(global_weights_auto, global_model_auto,single=True)\n",
    "    test_losses = []\n",
    "    test(global_model_auto,test_loader_auto,test_losses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_classic = 4\n",
    "\n",
    "for round_idx in range(rounds_classic):\n",
    "    \n",
    "    print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "    local_weights_classic = []\n",
    "    for client_idx, client_model in enumerate(local_models_classic):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, classic_client_loaders_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_classic.append(client_weights)\n",
    "        \n",
    "    print(f\"after training{local_models_classic}\")\n",
    "    global_weights_classic = federated_averaging(local_weights_classic)\n",
    "    print(f\"after fedaveraging{local_models_classic}\")\n",
    "\n",
    "    distribute_global_model(global_weights_classic,local_models_classic,single=False)\n",
    "\n",
    "    distribute_global_model(global_weights_classic,global_model_classic,single=True)\n",
    "    test_losses = []\n",
    "    test(global_model_classic,test_loader,test_losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "rounds_pca = 4\n",
    "\n",
    "for round_idx in range(rounds_pca):\n",
    "    \n",
    "    print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "\n",
    "    local_weights_pca = []\n",
    "    for client_idx, client_model in enumerate(local_models_pca):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, pca_client_loaders_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_pca.append(client_weights)\n",
    "        \n",
    "    print(f\"after training{local_models_pca}\")\n",
    "    global_weights_pca = federated_averaging(local_weights_pca)\n",
    "    print(f\"after fedaveraging{local_models_pca}\")\n",
    "\n",
    "    distribute_global_model(global_weights_pca,local_models_pca,single=False)\n",
    "\n",
    "    distribute_global_model(global_weights_pca,global_model_pca,single=True)\n",
    "    test_losses = []\n",
    "    test(global_model_pca,test_loader_pca,test_losses)\n",
    "\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rounds_auto = 4\n",
    "\n",
    "for round_idx in range(rounds_auto):\n",
    "    print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "\n",
    "    local_weights_auto = []\n",
    "    for client_idx, client_model in enumerate(local_model_autoencoder):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                      momentum=momentum)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, auto_client_loaders_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_auto.append(client_weights)\n",
    "        \n",
    "    global_weights_auto = federated_averaging(local_weights_auto)\n",
    "\n",
    "    distribute_global_model(global_weights_auto,local_model_autoencoder,single=False)\n",
    "\n",
    "    distribute_global_model(global_weights_auto, global_model_auto,single=True)\n",
    "    test_losses = []\n",
    "    test(global_model_auto,test_loader_auto,test_losses)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
