{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "from autoencoder2 import Autoencoder2, reduce_dimensions2\n",
    "import torchvision\n",
    "from model2 import classification_model\n",
    "import copy\n",
    "import partition\n",
    "from pca import PCADigitReducer\n",
    "from training import train,test, train_fashion,test_fashion\n",
    "from federated_learning import distribute_global_model, federated_averaging\n",
    "from model4 import MultilayerPerceptron\n",
    "import cluster2\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01 # ev. 0.001\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training and testing data as dataloaders\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=True, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.0,), (1,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  torchvision.datasets.MNIST('/files/', train=False, download=True,\n",
    "                             transform=torchvision.transforms.Compose([\n",
    "                               torchvision.transforms.ToTensor(),\n",
    "                               torchvision.transforms.Normalize(\n",
    "                                 (0.0,), (1,))\n",
    "                             ])),\n",
    "  batch_size=batch_size_test, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_pca = copy.copy(train_loader)\n",
    "test_loader_pca = copy.copy(test_loader)\n",
    "\n",
    "train_loader_auto = copy.copy(train_loader)\n",
    "test_loader_auto = copy.copy(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, *tensors):\n",
    "        super().__init__(*tensors)\n",
    "        self.data = tensors[0]\n",
    "        self.targets = tensors[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder2 = Autoencoder2()\n",
    "auto2_criterion = nn.BCELoss()  \n",
    "auto2_optimizer = optim.Adam(autoencoder2.parameters(), lr=0.001)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5 # optimal nach vielen testen\n",
    "for epoch in range(num_epochs):\n",
    "    autoencoder2.train()\n",
    "    train_loss = 0\n",
    "\n",
    "    for images, _ in train_loader_auto:\n",
    "        images = images.to(device)\n",
    "\n",
    "        auto2_optimizer.zero_grad()\n",
    "        outputs = autoencoder2(images)\n",
    "        loss = auto2_criterion(outputs, images) \n",
    "        \n",
    "        loss.backward()\n",
    "        auto2_optimizer.step()\n",
    "\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    avg_train_loss = train_loss / len(train_loader_auto)\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {avg_train_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 10\n",
    "num_clients = 10\n",
    "num_clusters = [2, 4, 6, 8, 10]\n",
    "results = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}\n",
    "clusteredResults = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# classic\n",
    "trainingset = train_loader.dataset\n",
    "trial_model = classification_model()\n",
    "global_model_classic = classification_model()\n",
    "rounds_classic = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classic\n",
    "partitioned_data_classic = partition.balanced_dirichlet_partition(\n",
    "    trainingset, partitions_number=num_clients, alpha=alpha\n",
    ")\n",
    "\n",
    "classic_client_loaders = [\n",
    "    DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_classic.values()\n",
    "]\n",
    "\n",
    "\n",
    "local_models_classic = [copy.deepcopy(global_model_classic) for _ in range(num_clients)]\n",
    "\n",
    "\n",
    "\n",
    "for round_idx in range(rounds_classic):\n",
    "    print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "    local_weights_classic = []\n",
    "    for client_idx, client_model in enumerate(local_models_classic):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "\n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):\n",
    "            train(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "\n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_classic.append(client_weights)\n",
    "\n",
    "    global_weights_classic = federated_averaging(local_weights_classic)\n",
    "\n",
    "    distribute_global_model(global_weights_classic, local_models_classic, single=False)\n",
    "    distribute_global_model(global_weights_classic, global_model_classic, single=True)\n",
    "\n",
    "    test_losses = []\n",
    "    test(global_model_classic, test_loader, test_losses)\n",
    "\n",
    "    \n",
    "    test_accuracies_classic = []\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            output = global_model_classic(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    test_accuracies_classic.append(accuracy)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    \n",
    "    results[\"classic\"][\"NoCluster\"] = {\"losses\": [], \"accuracy\": [],\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "    results[\"classic\"][\"NoCluster\"][\"losses\"].extend(test_losses)\n",
    "    results[\"classic\"][\"NoCluster\"][\"accuracy\"].extend(test_accuracies_classic)\n",
    "    results[\"classic\"][\"NoCluster\"][\"precision\"].append(precision)\n",
    "    results[\"classic\"][\"NoCluster\"][\"recall\"].append(recall)\n",
    "    results[\"classic\"][\"NoCluster\"][\"f1\"].append(f1)\n",
    "\n",
    "    ######################\n",
    "\n",
    "for num_cluster in num_clusters:\n",
    "    import cluster2\n",
    "\n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clients = [cluster2.FederatedClient(cid, indices, targets, num_classes) for cid, indices in partitioned_data_classic.items()]\n",
    "    client_distributions = [client.compute_label_distribution() for client in clients]\n",
    "    server = cluster2.FederatedClusterServer(num_cluster)\n",
    "    aggregated_data = server.aggregate_client_data(client_distributions)\n",
    "    clustered_data = server.perform_greedy_clustering(aggregated_data, partitioned_data_classic)\n",
    "    \n",
    "    partitioned_data_classic_clustered = clustered_data\n",
    "\n",
    "\n",
    "    classic_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic_clustered.values()\n",
    "    ]\n",
    "\n",
    "    for round_idx in range(rounds_classic):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic[0: num_cluster]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "\n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "\n",
    "            for epoch in range(1, n_epochs + 1):\n",
    "                train(epoch, client_model, classic_client_loaders_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "\n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "\n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "\n",
    "        distribute_global_model(global_weights_classic, local_models_classic, single=False)\n",
    "        distribute_global_model(global_weights_classic, global_model_classic, single=True)\n",
    "\n",
    "        test_losses = []\n",
    "        test(global_model_classic, test_loader, test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader:\n",
    "                output = global_model_classic(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / len(test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if num_cluster not in clusteredResults[\"classic\"]:\n",
    "            clusteredResults[\"classic\"][num_cluster] = {\"losses\": [], \"accuracy\": [],\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "        clusteredResults[\"classic\"][num_cluster][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"classic\"][num_cluster][\"accuracy\"].extend(test_accuracies_classic)\n",
    "        clusteredResults[\"classic\"][num_cluster][\"precision\"].append(precision)\n",
    "        clusteredResults[\"classic\"][num_cluster][\"recall\"].append(recall)\n",
    "        clusteredResults[\"classic\"][num_cluster][\"f1\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA\n",
    "#trainingset_pca = train_loader_reduced_pca.dataset\n",
    "trial_model_pca = classification_model()\n",
    "global_model_pca = classification_model() \n",
    "trainingset_pca = train_loader_pca.dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_data_pca = partition.balanced_dirichlet_partition(trainingset_pca, partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "pca_client_loaders = []\n",
    "local_models_pca = [copy.deepcopy(global_model_pca) for _ in range(num_clients)]\n",
    "\n",
    "for client_idx, client_indices in partitioned_data_pca.items():\n",
    "    client_data = [trainingset_pca[i][0] for i in client_indices]  \n",
    "    client_labels = [trainingset_pca[i][1] for i in client_indices]  \n",
    "    \n",
    "    client_data = torch.stack(client_data, dim=0)\n",
    "    client_data = client_data.view(client_data.size(0), -1)  \n",
    "    \n",
    "    pca = PCADigitReducer(100)\n",
    "    client_data_reduced = pca.fit_transform(client_data.numpy())  \n",
    "    \n",
    "    client_data_reconstructed_np = pca.inverse_transform(client_data_reduced) \n",
    "    client_data_reconstructed = torch.tensor(client_data_reconstructed_np, dtype=torch.float32)\n",
    "    \n",
    "    client_data_reconstructed = client_data_reconstructed.view(-1, 1, 28, 28)\n",
    "    client_data_reconstructed = (client_data_reconstructed - 0.1307) / 0.3081  \n",
    "    \n",
    "    client_dataset_pca = CustomTensorDataset(client_data_reconstructed, torch.tensor(client_labels))\n",
    "    pca_client_loaders.append(DataLoader(client_dataset_pca, batch_size=batch_size_train, shuffle=True))\n",
    "\n",
    "\n",
    "\n",
    "rounds_pca = 4\n",
    "\n",
    "for round_idx in range(rounds_pca):\n",
    "    print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "\n",
    "    local_weights_pca = []\n",
    "    for client_idx, client_model in enumerate(local_models_pca):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "\n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_pca.append(client_weights)\n",
    "        \n",
    "    global_weights_pca = federated_averaging(local_weights_pca)\n",
    "\n",
    "    distribute_global_model(global_weights_pca, local_models_pca, single=False)\n",
    "    distribute_global_model(global_weights_pca, global_model_pca, single=True)\n",
    "\n",
    "    test_losses = []\n",
    "    test(global_model_pca, test_loader_pca, test_losses)\n",
    "    \n",
    "    test_accuracies_pca = []\n",
    "    correct = 0\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader_pca:\n",
    "            output = global_model_pca(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "            correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "    \n",
    "    accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "    test_accuracies_pca.append(accuracy)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    results[\"pca\"][\"NoCluster\"] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "    results[\"pca\"][\"NoCluster\"][\"losses\"].extend(test_losses)\n",
    "    results[\"pca\"][\"NoCluster\"][\"accuracy\"].extend(test_accuracies_pca)\n",
    "    results[\"pca\"][\"NoCluster\"][\"precision\"].append(precision)\n",
    "    results[\"pca\"][\"NoCluster\"][\"recall\"].append(recall)\n",
    "    results[\"pca\"][\"NoCluster\"][\"f1\"].append(f1)\n",
    "\n",
    "    ######################\n",
    "\n",
    "for num_cluster in num_clusters:\n",
    "    import cluster2\n",
    "\n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clients = [cluster2.FederatedClient(cid, indices, targets, num_classes) for cid, indices in partitioned_data_pca.items()]\n",
    "    client_distributions = [client.compute_label_distribution() for client in clients]\n",
    "    server = cluster2.FederatedClusterServer(num_cluster)\n",
    "    aggregated_data = server.aggregate_client_data(client_distributions)\n",
    "    clustered_data = server.perform_greedy_clustering(aggregated_data, partitioned_data_pca)\n",
    "    \n",
    "    partitioned_data_pca_clustered = clustered_data\n",
    "\n",
    "    \n",
    "    pca_client_loaders_clustered = []\n",
    "\n",
    "    # Apply PCA after clustering\n",
    "    for client_idx, client_indices in partitioned_data_pca_clustered.items():\n",
    "        client_data = [trainingset_pca[i][0] for i in client_indices]  \n",
    "        client_labels = [trainingset_pca[i][1] for i in client_indices]  \n",
    "\n",
    "        client_data = torch.stack(client_data, dim=0)\n",
    "        client_data = client_data.view(client_data.size(0), -1)  \n",
    "        \n",
    "        pca = PCADigitReducer(100)\n",
    "        client_data_reduced = pca.fit_transform(client_data.numpy())  \n",
    "\n",
    "        client_data_reconstructed_np = pca.inverse_transform(client_data_reduced)  \n",
    "        client_data_reconstructed = torch.tensor(client_data_reconstructed_np, dtype=torch.float32)\n",
    "        \n",
    "        client_data_reconstructed = client_data_reconstructed.view(-1, 1, 28, 28)\n",
    "        client_data_reconstructed = (client_data_reconstructed - 0.1307) / 0.3081  \n",
    "\n",
    "        client_dataset_pca_clustered = CustomTensorDataset(client_data_reconstructed, torch.tensor(client_labels))\n",
    "        pca_client_loaders_clustered.append(DataLoader(client_dataset_pca_clustered, batch_size=batch_size_train, shuffle=True))\n",
    "\n",
    "    \n",
    "    for round_idx in range(rounds_pca):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "    \n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca[0:num_cluster]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, pca_client_loaders_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "\n",
    "        distribute_global_model(global_weights_pca, local_models_pca, single=False)\n",
    "        distribute_global_model(global_weights_pca, global_model_pca, single=True)\n",
    "\n",
    "        test_losses = []\n",
    "        test(global_model_pca, test_loader_pca, test_losses)\n",
    "        \n",
    "        test_accuracies_pca = []\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                output = global_model_pca(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        \n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_pca.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "        # Save clustered results for each partitions_number\n",
    "        if num_cluster not in clusteredResults[\"pca\"]:\n",
    "            clusteredResults[\"pca\"][num_cluster] = {\"losses\": [], \"accuracy\": [],\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "        clusteredResults[\"pca\"][num_cluster][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"pca\"][num_cluster][\"accuracy\"].extend(test_accuracies_pca)\n",
    "        clusteredResults[\"pca\"][num_cluster][\"precision\"].append(precision)\n",
    "        clusteredResults[\"pca\"][num_cluster][\"recall\"].append(recall)\n",
    "        clusteredResults[\"pca\"][num_cluster][\"f1\"].append(f1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset_auto = train_loader_pca.dataset\n",
    "trial_model_auto = classification_model()\n",
    "global_model_auto = classification_model()\n",
    "autoencoder2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partitioned_data_auto = partition.balanced_dirichlet_partition(trainingset_auto, partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "auto_client_loaders = [\n",
    "    DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "    for indices in partitioned_data_auto.values()\n",
    "]\n",
    "\n",
    "auto_client_loader_reduced = []\n",
    "\n",
    "for i,client in enumerate(auto_client_loaders):\n",
    "    \n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    latent_features, labels = reduce_dimensions2(auto_client_loaders[i], autoencoder2.encoder, device)\n",
    "    latent_features = latent_features.detach()\n",
    "\n",
    "    reconstructed_images = autoencoder2.decoder(latent_features.to(device))  \n",
    "    reconstructed_images = reconstructed_images.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "    reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "    reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "    auto_client_loader_reduced.append(reduced_train_loader_auto)\n",
    "\n",
    "\n",
    "local_model_autoencoder = [copy.deepcopy(global_model_auto) for _ in range(num_clients)]\n",
    "\n",
    "\n",
    "\n",
    "rounds_auto = 4\n",
    "for round_idx in range(rounds_auto):\n",
    "    print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "\n",
    "    local_weights_auto = []\n",
    "    for client_idx, client_model in enumerate(local_model_autoencoder):\n",
    "        print(f\"Training client {client_idx + 1}\")\n",
    "        \n",
    "        optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "        \n",
    "        train_losses = []\n",
    "        train_counter = []\n",
    "\n",
    "        for epoch in range(1, n_epochs + 1):  \n",
    "            train(epoch, client_model, auto_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "        \n",
    "        client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "        local_weights_auto.append(client_weights)\n",
    "        \n",
    "    global_weights_auto = federated_averaging(local_weights_auto)\n",
    "\n",
    "    distribute_global_model(global_weights_auto, local_model_autoencoder, single=False)\n",
    "    distribute_global_model(global_weights_auto, global_model_auto, single=True)\n",
    "\n",
    "    test_losses = []\n",
    "    test(global_model_auto, test_loader_auto, test_losses)\n",
    "\n",
    "    test_accuracies_auto = []\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader_auto:\n",
    "            output = global_model_auto(data)\n",
    "            pred = output.data.max(1, keepdim=True)[1]\n",
    "            all_preds.extend(pred.cpu().numpy().flatten())\n",
    "            all_targets.extend(target.cpu().numpy().flatten())\n",
    "    \n",
    "    correct = sum(p == t for p, t in zip(all_preds, all_targets))\n",
    "    accuracy = 100. * correct / len(all_targets)\n",
    "    test_accuracies_auto.append(accuracy)\n",
    "    precision = precision_score(all_targets, all_preds, average='macro')\n",
    "    recall = recall_score(all_targets, all_preds, average='macro')\n",
    "    f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "    # Save results for each partitions_number\n",
    "    \n",
    "    results[\"autoencoder\"][\"NoCluster\"] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "    results[\"autoencoder\"][\"NoCluster\"][\"losses\"].extend(test_losses)\n",
    "    results[\"autoencoder\"][\"NoCluster\"][\"accuracy\"].extend(test_accuracies_auto)\n",
    "    results[\"autoencoder\"][\"NoCluster\"][\"precision\"].append(precision)\n",
    "    results[\"autoencoder\"][\"NoCluster\"][\"recall\"].append(recall)\n",
    "    results[\"autoencoder\"][\"NoCluster\"][\"f1\"].append(f1)\n",
    "\n",
    "    ######################\n",
    "\n",
    "for num_cluster in num_clusters:\n",
    "    import cluster2\n",
    "\n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clients = [cluster2.FederatedClient(cid, indices, targets, num_classes) for cid, indices in partitioned_data_auto.items()]\n",
    "    client_distributions = [client.compute_label_distribution() for client in clients]\n",
    "    server = cluster2.FederatedClusterServer(num_cluster)\n",
    "    aggregated_data = server.aggregate_client_data(client_distributions)\n",
    "    clustered_data = server.perform_greedy_clustering(aggregated_data, partitioned_data_auto)\n",
    "    \n",
    "    partitioned_data_auto_clustered = clustered_data\n",
    "\n",
    "\n",
    "    auto_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto_clustered.values()\n",
    "    ]\n",
    "    \n",
    "    auto_client_loader_reduced = []\n",
    "\n",
    "    for i,client in enumerate(auto_client_loaders_clustered):\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        latent_features, labels = reduce_dimensions2(auto_client_loaders_clustered[i], autoencoder2.encoder, device)\n",
    "        latent_features = latent_features.detach()\n",
    "\n",
    "        reconstructed_images = autoencoder2.decoder(latent_features.to(device))  \n",
    "        reconstructed_images = reconstructed_images.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "        reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "        reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "        auto_client_loader_reduced.append(reduced_train_loader_auto)\n",
    "\n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "\n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder[0:num_cluster]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "\n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, auto_client_loader_reduced[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "            \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "\n",
    "        distribute_global_model(global_weights_auto, local_model_autoencoder, single=False)\n",
    "        distribute_global_model(global_weights_auto, global_model_auto, single=True)\n",
    "\n",
    "        test_losses = []\n",
    "        test(global_model_auto, test_loader_auto, test_losses)\n",
    "        \n",
    "        test_accuracies_auto = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_auto:\n",
    "                output = global_model_auto(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "        \n",
    "        correct = sum(p == t for p, t in zip(all_preds, all_targets))\n",
    "        accuracy = 100. * correct / len(all_targets)\n",
    "        test_accuracies_auto.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "        # Save clustered results for each partitions_number\n",
    "        if num_cluster not in clusteredResults[\"autoencoder\"]:\n",
    "            clusteredResults[\"autoencoder\"][num_cluster] = {\"losses\": [], \"accuracy\": [],\"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "        clusteredResults[\"autoencoder\"][num_cluster][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"autoencoder\"][num_cluster][\"accuracy\"].extend(test_accuracies_auto)\n",
    "        clusteredResults[\"autoencoder\"][num_cluster][\"precision\"].append(precision)\n",
    "        clusteredResults[\"autoencoder\"][num_cluster][\"recall\"].append(recall)\n",
    "        clusteredResults[\"autoencoder\"][num_cluster][\"f1\"].append(f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Function to compute final metrics\n",
    "def compute_final_metrics(metrics):\n",
    "    final_loss = {alpha: data['losses'][-1] for alpha, data in metrics.items()}\n",
    "    final_accuracy = {alpha: data['accuracy'][-1] for alpha, data in metrics.items()}\n",
    "    final_precision = {alpha: data['precision'][-1] for alpha, data in metrics.items()}\n",
    "    final_f1 = {alpha: data['f1'][-1] for alpha, data in metrics.items()}\n",
    "    final_recall = {alpha: data['recall'][-1] for alpha, data in metrics.items()}\n",
    "\n",
    "\n",
    "\n",
    "    return final_loss, final_accuracy, final_precision, final_f1, final_recall\n",
    "\n",
    "# Function to print results\n",
    "def print_results(results, title):\n",
    "    print(f\"\\n{title}:\")\n",
    "    for method in results.keys():\n",
    "        final_loss, final_accuracy, final_precision, final_f1, final_recall = compute_final_metrics(results[method])\n",
    "        print(f\"{method}:\")\n",
    "        print(\"  Final Loss:\", final_loss)\n",
    "        print(\"  Final Accuracy:\", final_accuracy)\n",
    "        print(\"  Final Precision:\", final_precision)\n",
    "        print(\"  Final F1 Score:\", final_f1)\n",
    "        print(\"  Final Recall:\", final_recall)\n",
    "\n",
    "# Function to plot the results\n",
    "def plot_results(results, title, filename):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    # List of metrics to plot\n",
    "    metrics_names = [\"Loss\", \"Accuracy\", \"Precision\", \"F1 Score\", \"Recall\"]\n",
    "    for i, metric in enumerate(metrics_names):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "\n",
    "        # Plotting each method's metrics\n",
    "        for method in results.keys():\n",
    "            final_loss, final_accuracy, final_precision, final_f1, final_recall = compute_final_metrics(results[method])\n",
    "\n",
    "            # Select the right metric based on the iteration\n",
    "            metric_values = {\n",
    "                \"Loss\": final_loss,\n",
    "                \"Accuracy\": final_accuracy,\n",
    "                \"Precision\": final_precision,\n",
    "                \"F1 Score\": final_f1,\n",
    "                \"Recall\": final_recall\n",
    "            }[metric]\n",
    "\n",
    "            # Plot the metric values for each method\n",
    "            plt.plot(list(metric_values.keys()) + ['NoCluster'], list(metric_values.values()) + [metric_values.get('NoCluster', np.nan)], marker='o', label=method)\n",
    "\n",
    "        plt.xlabel('Clusters')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{title} - Clusters vs {metric}')\n",
    "        plt.legend()\n",
    "\n",
    "    # Adjust layout and save the plot\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "# Assuming 'results' and 'clusteredResults' are the data for the methods\n",
    "# Print and plot results for non-clustered and clustered data\n",
    "\n",
    "print_results(results, \"Non-Clustered Results\")\n",
    "plot_results(results, \"Non-Clustered Results\", \"non_clustered_results.png\")\n",
    "\n",
    "print_results(clusteredResults, \"Clustered Results\")\n",
    "plot_results(clusteredResults, \"Clustered Results\", \"clustered_results.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
