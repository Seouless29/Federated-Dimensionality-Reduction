{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "from abc import ABC, abstractmethod\n",
    "import time\n",
    "from sklearn.manifold import TSNE\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets, transforms\n",
    "import keras \n",
    "from keras.datasets import mnist\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.layers import Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from keras import backend as k\n",
    "from torch.utils.data import Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Stuff\n",
    "# Set download to False if already downloaded\n",
    "\"\"\"\"\"\n",
    "transform = transforms.Compose([transforms.ToTensor()])\n",
    "\n",
    "# Load the MNIST dataset \n",
    "mnist_trainset = datasets.MNIST(root='./data', train=True, download=False, transform=transform)\n",
    "mnist_testset = datasets.MNIST(root='./data', train=False, download=False, transform=transform)\n",
    "\n",
    "print(f\"Data: {mnist_trainset.data.shape}, Targets: {mnist_trainset.targets.shape}\")\n",
    "\"\"\"\"\"\n",
    "# test data\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_rows, img_cols=28, 28\n",
    "\n",
    "if k.image_data_format() == 'channels_first':\n",
    "   x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n",
    "   x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n",
    "   inpx = (1, img_rows, img_cols)\n",
    "\n",
    "else:\n",
    "   x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "   x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "   inpx = (img_rows, img_cols, 1)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28, 1)\n",
      "(60000,)\n",
      "<class 'numpy.ndarray'>\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "(28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(type(y_train))\n",
    "K = np.unique(y_train)\n",
    "print(K)\n",
    "print(inpx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom dataset class that wraps x_train and y_train\n",
    "class MNISTDataset(Dataset):\n",
    "    def __init__(self, data, targets):\n",
    "        # data: the images (flattened or original, depending on your requirement)\n",
    "        # targets: the labels (e.g., the digit for each image)\n",
    "        self.data = data\n",
    "        self.targets = targets\n",
    "\n",
    "    def __len__(self):\n",
    "        # Return the number of samples in the dataset\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Returns a single sample and its corresponding label\n",
    "        return self.data[idx], self.targets[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = MNISTDataset(x_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Help Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def balanced_dirichlet_partition(dataset, partitions_number=10, alpha=0.5, seed=42):\n",
    "    \"\"\"\n",
    "    Partition the dataset into multiple subsets using a Dirichlet distribution,\n",
    "    ensuring that each partition contains samples from every class.\n",
    "\n",
    "    Args:\n",
    "        dataset (torch.utils.data.Dataset): The dataset to partition. It should have 'data' and 'targets' attributes.\n",
    "        partitions_number (int): Number of partitions to create.\n",
    "        alpha (float): The concentration parameter of the Dirichlet distribution. A lower alpha value leads to more imbalanced partitions.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "\n",
    "    Returns:\n",
    "        dict: A dictionary where keys are partition indices (0 to partitions_number-1)\n",
    "              and values are lists of indices corresponding to the samples in each partition.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Extract targets (labels) from the dataset\n",
    "    y_train = dataset.targets\n",
    "    \n",
    "    # Number of classes in the dataset\n",
    "    num_classes = len(np.unique(y_train))\n",
    "    \n",
    "    # Initialize the map that will store the indices for each partition\n",
    "    net_dataidx_map = {}\n",
    "    \n",
    "    # Initialize lists to store the indices for each class\n",
    "    class_indices = {k: np.where(y_train == k)[0] for k in range(num_classes)}\n",
    "    \n",
    "    # Shuffle the indices within each class to ensure random distribution\n",
    "    for k in class_indices.keys():\n",
    "        np.random.shuffle(class_indices[k])\n",
    "    \n",
    "    # Ensure that each partition gets at least one sample from each class\n",
    "    min_size = 10  # Ensuring each class has at least 10 samples in each partition\n",
    "    idx_batch = [[] for _ in range(partitions_number)]\n",
    "\n",
    "    # Assign at least `min_size` samples from each class to every partition\n",
    "    for k in range(num_classes):\n",
    "        # Split the class indices equally across the partitions\n",
    "        split = np.array_split(class_indices[k], partitions_number)\n",
    "        for i in range(partitions_number):\n",
    "            idx_batch[i].extend(split[i][:min_size])\n",
    "\n",
    "        # Remove the samples that were assigned equally\n",
    "        class_indices[k] = class_indices[k][min_size*partitions_number:]\n",
    "\n",
    "    # Now distribute the remaining samples using the Dirichlet distribution\n",
    "    for k in range(num_classes):\n",
    "        remaining_indices = class_indices[k]\n",
    "        proportions = np.random.dirichlet(np.repeat(alpha, partitions_number))\n",
    "        proportions = (np.cumsum(proportions) * len(remaining_indices)).astype(int)[:-1]\n",
    "        split_remaining = np.split(remaining_indices, proportions)\n",
    "        \n",
    "        for i in range(partitions_number):\n",
    "            idx_batch[i].extend(split_remaining[i])\n",
    "\n",
    "    # Shuffle the indices within each partition\n",
    "    for i in range(partitions_number):\n",
    "        np.random.shuffle(idx_batch[i])\n",
    "        net_dataidx_map[i] = idx_batch[i]\n",
    "\n",
    "    return net_dataidx_map\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Partition \n",
    "\n",
    "partitioned_data = balanced_dirichlet_partition(train_dataset, partitions_number=10, alpha=0.5)\n",
    "partition_0_dataset = Subset(train_dataset, partitioned_data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8115\n",
      "6299\n",
      "7690\n",
      "3996\n",
      "5373\n",
      "5975\n",
      "6862\n",
      "3842\n",
      "6307\n",
      "5541\n"
     ]
    }
   ],
   "source": [
    "# Check sizes\n",
    "\n",
    "for i in partitioned_data:\n",
    "    print(len(partitioned_data[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train1 shape: (8115, 28, 28, 1)\n",
      "y_train1 shape: (8115,)\n"
     ]
    }
   ],
   "source": [
    "# Reformatting Subsets\n",
    "\n",
    "x_train1 = []\n",
    "y_train1 = []\n",
    "\n",
    "for img, label in partition_0_dataset:\n",
    "    x_train1.append(img)\n",
    "    y_train1.append(label)\n",
    "\n",
    "x_train1 = np.array(x_train1)\n",
    "y_train1 = np.array(y_train1)\n",
    "\n",
    "# Check shapes to confirm they match the original structure\n",
    "print(\"x_train1 shape:\", x_train1.shape)  # Should match the shape (num_samples, 784) if flattened\n",
    "print(\"y_train1 shape:\", y_train1.shape)  # Should match the shape (num_samples,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 2 3 4 5 6 7 8 9]\n"
     ]
    }
   ],
   "source": [
    "L = np.unique(y_train1)\n",
    "print(L)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the labels for model\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train)\n",
    "\n",
    "y_train1 = keras.utils.to_categorical(y_train1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8115, 28, 28, 1)\n",
      "(8115, 10)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(x_train1.shape)\n",
    "print(y_train1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inpx = Input(shape=inpx)\n",
    "layer1 = Conv2D(32, kernel_size=(3, 3), activation='relu')(inpx)\n",
    "layer2 = Conv2D(64, (3, 3), activation='relu')(layer1)\n",
    "layer3 = MaxPooling2D(pool_size=(3, 3))(layer2)\n",
    "layer4 = Dropout(0.5)(layer3)\n",
    "layer5 = Flatten()(layer4)\n",
    "layer6 = Dense(250, activation='sigmoid')(layer5)\n",
    "layer7 = Dense(10, activation='softmax')(layer6)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 25ms/step - accuracy: 0.1295 - loss: 2.7049\n",
      "Epoch 2/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.1311 - loss: 2.6580\n",
      "Epoch 3/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1305 - loss: 2.6198\n",
      "Epoch 4/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1344 - loss: 2.5679\n",
      "Epoch 5/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1358 - loss: 2.5173\n",
      "Epoch 6/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1319 - loss: 2.4605\n",
      "Epoch 7/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1390 - loss: 2.4072\n",
      "Epoch 8/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1442 - loss: 2.3512\n",
      "Epoch 9/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 26ms/step - accuracy: 0.1456 - loss: 2.3003\n",
      "Epoch 10/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1455 - loss: 2.2552\n",
      "Epoch 11/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 24ms/step - accuracy: 0.1642 - loss: 2.2028\n",
      "Epoch 12/12\n",
      "\u001b[1m127/127\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 25ms/step - accuracy: 0.1879 - loss: 2.1622\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x2b2813b2fd0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = Model([inpx], layer7)\n",
    "model.compile(optimizer=keras.optimizers.Adadelta(),\n",
    "              loss=keras.losses.categorical_crossentropy,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "model.fit(x_train1, y_train1, epochs=12, batch_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss= 2.291794538497925\n",
      "accuracy= 0.09769999980926514\n"
     ]
    }
   ],
   "source": [
    "y_test = keras.utils.to_categorical(y_test)\n",
    "\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('loss=', score[0])\n",
    "print('accuracy=', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
