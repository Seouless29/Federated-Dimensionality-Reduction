{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "from autoencoder import Autoencoder\n",
    "from cifar_autoencoder import Cifar_Autoencoder\n",
    "import torchvision\n",
    "from model2 import classification_model\n",
    "from model import cifar_classification_model\n",
    "import copy\n",
    "import partition\n",
    "from pca import PCADigitReducer\n",
    "from autoencoder import reduce_dimensions\n",
    "from training import train,test\n",
    "from federated_learning import distribute_global_model, federated_averaging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x21794e43df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predefined stuff\n",
    "\n",
    "n_epochs = 17\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean: tensor([0.4914, 0.4822, 0.4465])\n",
    "Std: tensor([0.2023, 0.1994, 0.2010])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar10_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))  # Precomputed mean and std for CIFAR-10\n",
    "])\n",
    "\n",
    "cifar10_train_loader = DataLoader(\n",
    "    datasets.CIFAR10('/files/', train=True, download=True, transform=cifar10_transform),\n",
    "    batch_size=batch_size_train, shuffle=True\n",
    ")\n",
    "\n",
    "cifar10_test_loader = DataLoader(\n",
    "    datasets.CIFAR10('/files/', train=False, download=True, transform=cifar10_transform),\n",
    "    batch_size=batch_size_test, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch size: 64, Target batch size: 64\n"
     ]
    }
   ],
   "source": [
    "for data, target in cifar10_train_loader:\n",
    "    print(f\"Data batch size: {data.size(0)}, Target batch size: {target.size(0)}\")\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, *tensors):\n",
    "        super().__init__(*tensors)\n",
    "        self.data = tensors[0]\n",
    "        self.targets = tensors[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_pca = copy.copy(cifar10_train_loader)\n",
    "test_loader_pca = copy.copy(cifar10_test_loader)\n",
    "\n",
    "train_loader_auto = copy.copy(cifar10_train_loader)\n",
    "test_loader_auto = copy.copy(cifar10_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset CIFAR10\n",
      "    Number of datapoints: 50000\n",
      "    Root location: /files/\n",
      "    Split: Train\n",
      "    StandardTransform\n",
      "Transform: Compose(\n",
      "               ToTensor()\n",
      "               Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n",
      "           )\n"
     ]
    }
   ],
   "source": [
    "print((cifar10_train_loader.dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = []\n",
    "train_labels = []\n",
    "for data, labels in train_loader_pca:  # Use your CIFAR-10 DataLoader here\n",
    "    train_data.append(data.view(data.size(0), -1))  # Flatten images\n",
    "    train_labels.append(labels)\n",
    "train_data = torch.cat(train_data, dim=0)  # Combine all batches\n",
    "train_labels = torch.cat(train_labels, dim=0)\n",
    "\n",
    "# Convert to numpy for PCA\n",
    "train_data_np = train_data.numpy()\n",
    "\n",
    "# Perform PCA\n",
    "n_components = 100  # Set the desired number of components\n",
    "pca = PCADigitReducer(n_components)\n",
    "train_data_reduced = pca.fit_transform(train_data_np)  # Reduce dimensions\n",
    "\n",
    "# Reconstruct the dataset from the reduced dimensions\n",
    "train_data_reconstructed_np = pca.inverse_transform(train_data_reduced) \n",
    "train_data_reconstructed = torch.tensor(train_data_reconstructed_np, dtype=torch.float32)\n",
    "\n",
    "# Reshape the reconstructed data back into the original image dimensions\n",
    "train_data_reconstructed = train_data_reconstructed.view(-1, 3, 32, 32)\n",
    "\n",
    "# Normalize the reconstructed dataset (use CIFAR-10 mean and std)\n",
    "train_data_reconstructed = (train_data_reconstructed - torch.tensor([0.4914, 0.4822, 0.4465]).view(1, 3, 1, 1)) / \\\n",
    "                           torch.tensor([0.2470, 0.2435, 0.2616]).view(1, 3, 1, 1)\n",
    "\n",
    "# Create a new DataLoader for the reconstructed data\n",
    "batch_size_train = cifar10_train_loader.batch_size\n",
    "train_dataset_pca = CustomTensorDataset(train_data_reconstructed, train_labels)\n",
    "train_loader_reduced_pca = DataLoader(train_dataset_pca, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 1, 1,  ..., 0, 5, 9])\n"
     ]
    }
   ],
   "source": [
    "trainingset_pca = train_loader_reduced_pca.dataset\n",
    "\n",
    "print(trainingset_pca.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 1.0315274000167847\n",
      "Epoch [2/5], Loss: 0.8774186968803406\n",
      "Epoch [3/5], Loss: 0.8388128280639648\n",
      "Epoch [4/5], Loss: 0.9051497578620911\n",
      "Epoch [5/5], Loss: 0.8178097605705261\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 100  # Adjust latent dimension as needed\n",
    "autoencoder = Cifar_Autoencoder(latent_dim=latent_dim)\n",
    "auto_criterion = nn.MSELoss()\n",
    "auto_optimizer = torch.optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "auto_num_epochs = 5\n",
    "\n",
    "for epoch in range(auto_num_epochs):\n",
    "    for images, _ in cifar10_train_loader:  # Use your CIFAR-10 DataLoader here\n",
    "        auto_optimizer.zero_grad()\n",
    "        \n",
    "        # Forward pass\n",
    "        reconstructed = autoencoder(images)\n",
    "        \n",
    "        # Compute reconstruction loss\n",
    "        loss = auto_criterion(reconstructed, images)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        auto_optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/{auto_num_epochs}], Loss: {loss.item()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder.eval()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "latent_features, labels = reduce_dimensions(train_loader_auto, autoencoder.encoder, device)\n",
    "latent_features = latent_features.detach()\n",
    "\n",
    "reconstructed_images = autoencoder.decoder(latent_features.to(device))  \n",
    "reconstructed_images = reconstructed_images.view(-1, 3, 32, 32) # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_values = [0.1, 0.5, 1, 5, 10, 20]\n",
    "num_clients = 10\n",
    "num_clusters = 6\n",
    "results = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}\n",
    "clusteredResults = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = cifar10_train_loader.dataset\n",
    "gloabl_model_classic =cifar_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running experiment with alpha: 0.1 \n",
      "Round 1/4\n",
      "Training client 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazek\\Documents\\Federated-Dimensionality-Reduction\\model.py:21: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return F.log_softmax(x)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 2.230106\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 1.543742\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.838537\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 1.044346\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 1.008984\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.613676\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.758816\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.798671\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.892449\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.910164\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.595293\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.763686\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.643091\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 1.165991\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.938777\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.993421\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.953252\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.685108\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.538482\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.743206\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.673842\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.880362\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.498229\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.511870\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.617779\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.603544\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.532701\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.516020\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.938902\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.772875\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.594583\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.583362\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.818011\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.415090\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.567032\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.619067\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.541278\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.516757\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.722418\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.791563\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.830397\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.605049\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.523645\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.362987\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.445969\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.311120\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.505964\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.556548\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.351543\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.525920\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.382866\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.507339\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.340560\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.578978\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.496758\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.384795\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.569060\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.367722\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.363153\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.382726\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.689680\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.542401\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.383820\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.549451\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.402443\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.510023\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.207632\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.350570\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.608405\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.467227\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.560635\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.625850\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.244102\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.331422\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.375330\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.499052\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.563451\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.536968\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.368961\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.258111\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.255729\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.572663\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.198112\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.538161\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.429829\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.201234\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.413351\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.292735\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.770384\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.440217\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.355762\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.824150\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.221587\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.342117\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.419323\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.450285\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.511767\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.399162\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.262743\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.630407\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.486246\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.315156\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 2.304283\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 2.202559\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 1.933719\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 1.476808\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 1.377095\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 1.658300\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 1.837323\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 1.641216\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 1.415589\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 1.463194\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 1.587871\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 1.317302\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 1.685544\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 1.547569\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 1.435953\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 1.312771\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 1.321750\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 2.256108\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 2.113374\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.893476\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 1.462912\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 1.534531\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 1.421123\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 1.398513\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 1.299030\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 1.313013\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 1.242661\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 1.336488\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 1.364133\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 1.239067\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 1.068769\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 1.397474\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 1.444715\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 1.316661\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 1.328141\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 1.461417\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 1.364897\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 1.239920\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 0.755431\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 1.127289\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 1.242363\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 1.292116\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 1.085803\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 1.038076\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 0.971004\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 0.909591\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.915492\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 0.841540\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.985107\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 1.179657\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 1.045948\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 1.167751\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 0.995178\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 1.203984\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 1.050727\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 1.080503\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 1.028272\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.837444\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 1.201326\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 1.177104\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 1.049375\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 0.945841\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 1.156619\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.994157\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 1.326467\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 0.963105\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 1.309651\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 1.210836\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 1.086020\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 1.142064\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 1.160986\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 1.183985\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 1.072588\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 1.115838\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 1.226649\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.976822\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.916796\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.931408\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 1.014928\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 1.095822\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 1.001238\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 1.084677\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 0.855273\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.885168\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 1.179620\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.995033\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.986462\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 1.055601\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 1.071407\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.997725\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 0.891383\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.857496\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 1.154981\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 1.104670\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.934555\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.997822\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 1.241091\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 1.076932\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.922174\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 1.543188\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.955964\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.925376\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 1.015380\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.870560\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 1.301851\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 1.153279\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 1.038978\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 1.021779\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.855709\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.864271\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.901406\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.990121\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.896087\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 0.779189\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 1.102856\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 1.002635\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.944333\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.982985\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 1.181077\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 1.064630\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.879722\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.955422\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 1.188486\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 1.142296\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.838457\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.814145\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 1.044410\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 1.084909\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.877320\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.908960\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.863492\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.889508\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.800339\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.929826\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.824015\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 0.892556\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 1.238884\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 1.058889\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.765511\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 0.852777\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.744582\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.857917\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 1.168928\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 1.033265\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.808523\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.787878\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.787720\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.907732\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 1.012710\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.859420\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.967009\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 1.157717\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.759443\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 2.285434\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.867875\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.389167\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.759895\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.594459\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.396899\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.613483\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.525567\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.719821\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.739955\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.753672\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.274427\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.599891\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.475590\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.762448\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.283120\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.250091\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.300646\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.437027\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.441813\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.457725\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.279636\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.547953\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.234173\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.190667\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.327702\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.413362\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.358066\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.707558\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.539562\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.415591\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.761148\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.569105\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.609970\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.432457\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.247178\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.281616\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.788393\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.180212\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.393613\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.374009\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.387435\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.620669\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.703553\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.460378\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.548073\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.903238\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.261625\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.355646\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.277142\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.287714\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.354113\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.469093\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.358936\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.345481\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.646353\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.491808\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.434124\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.291824\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.496117\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.481330\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.377115\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.650321\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.393146\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.348685\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.395692\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.346275\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.520966\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.347292\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.577931\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.446956\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.302832\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.247633\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.275014\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.355143\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.327807\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.529791\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.146663\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.224272\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.384000\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.326133\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.324716\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.401872\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.272030\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.505470\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.579515\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.233819\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.218357\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.172124\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.368320\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.165075\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.346412\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.670293\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.182132\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.307632\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.325341\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.248144\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.371735\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.504167\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.457073\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.635659\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.360987\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 2.312128\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 1.983702\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 1.461446\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 1.643930\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 1.839444\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 1.457419\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 1.336873\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 1.567667\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 1.304442\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 1.440931\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 1.190924\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 1.281359\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 1.427617\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 1.402422\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 1.505921\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 1.279584\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 1.181138\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 1.190547\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 1.164235\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 1.093558\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 1.015719\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 1.205567\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 1.649710\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 1.004556\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 1.127079\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.993650\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 1.453573\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 1.429160\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 1.152249\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 0.967436\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 1.383309\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 1.219416\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 1.249506\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 1.129622\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 1.166703\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 1.108905\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 1.419957\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 1.187466\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 1.191710\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 1.298589\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 1.002867\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.936031\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 0.947120\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 1.179480\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 0.688598\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.956452\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 1.274400\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 1.250421\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 1.265298\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 1.074075\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 0.942801\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 1.218269\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 1.092357\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 1.173020\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 1.261662\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.990888\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 1.307805\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 1.025215\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.947410\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.983526\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.858012\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.926521\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.951847\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 0.976521\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 1.276541\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.871468\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 1.051818\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 1.161375\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.869448\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.998401\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.772740\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 1.203383\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 1.049514\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 0.876129\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 0.782966\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.800267\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.969102\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 1.056268\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.979142\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.854524\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 1.199841\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 1.348030\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.902405\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.864486\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 1.360725\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 1.249410\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 0.947422\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 0.774407\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 1.191203\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 1.044163\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 0.923705\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.996336\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.881016\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.836986\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.876207\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 0.976557\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.743970\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 1.302834\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 1.010598\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.960416\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 1.015497\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 0.975786\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.939036\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 1.399557\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.995570\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 1.037864\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 1.088953\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 1.091561\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.956711\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.947526\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.833013\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.896155\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.841429\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.995495\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.658829\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.975354\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 0.867887\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.852576\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 1.005652\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.739568\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 1.120510\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 1.032077\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.817299\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 1.021266\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.814965\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.741209\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.905337\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 1.263941\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.933056\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.672704\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 0.916354\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.720474\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.677044\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.962034\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.981942\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.858440\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 1.307562\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.816413\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.974996\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.881633\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 1.187873\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.981804\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.899671\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.915053\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.678881\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 1.228060\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 1.020476\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.797398\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 1.062995\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 1.018092\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.942315\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 1.061849\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 1.140159\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.993165\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.753947\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.904324\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.644461\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.811542\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.778950\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.701890\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.796794\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.713527\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.862615\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 1.111309\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.899412\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.862234\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.782362\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.705216\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.821159\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 1.006817\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.896427\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 1.199798\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.795480\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.839009\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 1.108941\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.872508\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.989124\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.792890\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.859378\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.913287\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.797568\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.807117\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.729421\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.825763\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 1.104242\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.839615\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 1.047279\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.615256\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.733053\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 1.127064\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.733278\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.983149\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.858783\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.712817\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.951218\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.993192\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.652040\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 1.105251\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.959470\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 1.002045\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.935901\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.839415\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.729081\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 1.135062\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 1.031547\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.832974\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.994456\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.815352\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.860071\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 1.047064\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.823933\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.853586\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.715125\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 1.162477\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.866894\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.881763\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.935411\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.787562\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.944951\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.870058\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.872029\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.897752\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.790245\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.833351\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 1.063935\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.726033\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.895106\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.759257\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.791267\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.803945\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.723545\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.747528\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.749235\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.763191\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.737658\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.985796\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.843061\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.760298\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.838942\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.829538\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 1.086618\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.870514\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.754083\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.775545\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.852322\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.678609\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.908170\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.836825\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.729022\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.960392\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.750442\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.739138\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.970447\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.843388\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.868485\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 2.233368\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 1.989739\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 1.158787\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 1.292973\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 1.163250\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 1.264306\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 1.397932\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 1.379473\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 1.069068\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 1.086868\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 1.053252\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 1.379794\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 1.290831\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 1.240486\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 1.052903\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 1.039193\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 1.065656\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 1.125974\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 1.104322\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.899707\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.996838\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.936574\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 1.215358\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 0.978394\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 0.907374\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 1.316389\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 0.919442\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.980263\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 1.084458\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 1.066874\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.901054\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.923145\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 1.131118\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 1.066136\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 1.124780\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 1.289441\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 1.329775\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.888479\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 1.154873\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.969108\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.884129\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 1.037459\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.867312\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.938556\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 1.066972\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.917375\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.799972\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.837461\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 1.160956\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 0.944837\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 1.042623\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 1.125123\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.735182\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.888698\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.946011\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 1.003016\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.940885\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.798508\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.872163\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 1.116635\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.731146\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.957530\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 1.021803\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.700977\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.920169\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.898944\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 1.094956\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 1.165097\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.983194\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.777128\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 1.119333\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.788072\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 1.049166\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.777827\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.869464\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.905560\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 1.111631\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.777823\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.901144\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 1.153565\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.900861\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.832088\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.810264\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.786323\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.971647\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.892101\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.896580\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.917169\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.846361\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.887572\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.906473\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.942169\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.728879\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.931297\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.944868\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.794564\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.880347\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 1.055500\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.862106\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 1.007282\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.767536\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.649955\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/8838 (0%)]\tLoss: 2.347484\n",
      "Train Epoch: 1 [640/8838 (7%)]\tLoss: 1.980862\n",
      "Train Epoch: 1 [1280/8838 (14%)]\tLoss: 1.270864\n",
      "Train Epoch: 1 [1920/8838 (22%)]\tLoss: 0.972535\n",
      "Train Epoch: 1 [2560/8838 (29%)]\tLoss: 1.066783\n",
      "Train Epoch: 1 [3200/8838 (36%)]\tLoss: 1.238816\n",
      "Train Epoch: 1 [3840/8838 (43%)]\tLoss: 0.943828\n",
      "Train Epoch: 1 [4480/8838 (50%)]\tLoss: 0.790536\n",
      "Train Epoch: 1 [5120/8838 (58%)]\tLoss: 0.857257\n",
      "Train Epoch: 1 [5760/8838 (65%)]\tLoss: 1.157680\n",
      "Train Epoch: 1 [6400/8838 (72%)]\tLoss: 0.940219\n",
      "Train Epoch: 1 [7040/8838 (79%)]\tLoss: 1.175782\n",
      "Train Epoch: 1 [7680/8838 (86%)]\tLoss: 0.885404\n",
      "Train Epoch: 1 [8320/8838 (94%)]\tLoss: 0.987107\n",
      "Train Epoch: 2 [0/8838 (0%)]\tLoss: 0.721934\n",
      "Train Epoch: 2 [640/8838 (7%)]\tLoss: 0.972221\n",
      "Train Epoch: 2 [1280/8838 (14%)]\tLoss: 1.015578\n",
      "Train Epoch: 2 [1920/8838 (22%)]\tLoss: 0.759409\n",
      "Train Epoch: 2 [2560/8838 (29%)]\tLoss: 0.852439\n",
      "Train Epoch: 2 [3200/8838 (36%)]\tLoss: 0.981105\n",
      "Train Epoch: 2 [3840/8838 (43%)]\tLoss: 1.171260\n",
      "Train Epoch: 2 [4480/8838 (50%)]\tLoss: 0.767530\n",
      "Train Epoch: 2 [5120/8838 (58%)]\tLoss: 0.958343\n",
      "Train Epoch: 2 [5760/8838 (65%)]\tLoss: 0.811937\n",
      "Train Epoch: 2 [6400/8838 (72%)]\tLoss: 0.836167\n",
      "Train Epoch: 2 [7040/8838 (79%)]\tLoss: 0.801015\n",
      "Train Epoch: 2 [7680/8838 (86%)]\tLoss: 0.751898\n",
      "Train Epoch: 2 [8320/8838 (94%)]\tLoss: 0.997024\n",
      "Train Epoch: 3 [0/8838 (0%)]\tLoss: 1.005916\n",
      "Train Epoch: 3 [640/8838 (7%)]\tLoss: 0.713138\n",
      "Train Epoch: 3 [1280/8838 (14%)]\tLoss: 0.959244\n",
      "Train Epoch: 3 [1920/8838 (22%)]\tLoss: 0.963914\n",
      "Train Epoch: 3 [2560/8838 (29%)]\tLoss: 1.042011\n",
      "Train Epoch: 3 [3200/8838 (36%)]\tLoss: 0.681030\n",
      "Train Epoch: 3 [3840/8838 (43%)]\tLoss: 1.030521\n",
      "Train Epoch: 3 [4480/8838 (50%)]\tLoss: 0.898367\n",
      "Train Epoch: 3 [5120/8838 (58%)]\tLoss: 0.745382\n",
      "Train Epoch: 3 [5760/8838 (65%)]\tLoss: 0.788628\n",
      "Train Epoch: 3 [6400/8838 (72%)]\tLoss: 0.676482\n",
      "Train Epoch: 3 [7040/8838 (79%)]\tLoss: 1.164197\n",
      "Train Epoch: 3 [7680/8838 (86%)]\tLoss: 0.809788\n",
      "Train Epoch: 3 [8320/8838 (94%)]\tLoss: 1.174788\n",
      "Train Epoch: 4 [0/8838 (0%)]\tLoss: 0.884987\n",
      "Train Epoch: 4 [640/8838 (7%)]\tLoss: 0.743257\n",
      "Train Epoch: 4 [1280/8838 (14%)]\tLoss: 0.817928\n",
      "Train Epoch: 4 [1920/8838 (22%)]\tLoss: 0.700305\n",
      "Train Epoch: 4 [2560/8838 (29%)]\tLoss: 0.837234\n",
      "Train Epoch: 4 [3200/8838 (36%)]\tLoss: 0.876584\n",
      "Train Epoch: 4 [3840/8838 (43%)]\tLoss: 0.762081\n",
      "Train Epoch: 4 [4480/8838 (50%)]\tLoss: 0.957136\n",
      "Train Epoch: 4 [5120/8838 (58%)]\tLoss: 0.791718\n",
      "Train Epoch: 4 [5760/8838 (65%)]\tLoss: 0.826892\n",
      "Train Epoch: 4 [6400/8838 (72%)]\tLoss: 0.694468\n",
      "Train Epoch: 4 [7040/8838 (79%)]\tLoss: 1.001158\n",
      "Train Epoch: 4 [7680/8838 (86%)]\tLoss: 0.843039\n",
      "Train Epoch: 4 [8320/8838 (94%)]\tLoss: 0.711551\n",
      "Train Epoch: 5 [0/8838 (0%)]\tLoss: 0.845510\n",
      "Train Epoch: 5 [640/8838 (7%)]\tLoss: 0.915044\n",
      "Train Epoch: 5 [1280/8838 (14%)]\tLoss: 0.755794\n",
      "Train Epoch: 5 [1920/8838 (22%)]\tLoss: 0.941566\n",
      "Train Epoch: 5 [2560/8838 (29%)]\tLoss: 0.941925\n",
      "Train Epoch: 5 [3200/8838 (36%)]\tLoss: 0.724417\n",
      "Train Epoch: 5 [3840/8838 (43%)]\tLoss: 0.833699\n",
      "Train Epoch: 5 [4480/8838 (50%)]\tLoss: 0.701261\n",
      "Train Epoch: 5 [5120/8838 (58%)]\tLoss: 0.676950\n",
      "Train Epoch: 5 [5760/8838 (65%)]\tLoss: 0.839109\n",
      "Train Epoch: 5 [6400/8838 (72%)]\tLoss: 0.730215\n",
      "Train Epoch: 5 [7040/8838 (79%)]\tLoss: 0.760421\n",
      "Train Epoch: 5 [7680/8838 (86%)]\tLoss: 0.997856\n",
      "Train Epoch: 5 [8320/8838 (94%)]\tLoss: 0.846753\n",
      "Train Epoch: 6 [0/8838 (0%)]\tLoss: 0.996172\n",
      "Train Epoch: 6 [640/8838 (7%)]\tLoss: 0.838833\n",
      "Train Epoch: 6 [1280/8838 (14%)]\tLoss: 1.025742\n",
      "Train Epoch: 6 [1920/8838 (22%)]\tLoss: 0.653048\n",
      "Train Epoch: 6 [2560/8838 (29%)]\tLoss: 0.708104\n",
      "Train Epoch: 6 [3200/8838 (36%)]\tLoss: 0.569388\n",
      "Train Epoch: 6 [3840/8838 (43%)]\tLoss: 0.844554\n",
      "Train Epoch: 6 [4480/8838 (50%)]\tLoss: 0.744765\n",
      "Train Epoch: 6 [5120/8838 (58%)]\tLoss: 0.773777\n",
      "Train Epoch: 6 [5760/8838 (65%)]\tLoss: 0.717672\n",
      "Train Epoch: 6 [6400/8838 (72%)]\tLoss: 0.742243\n",
      "Train Epoch: 6 [7040/8838 (79%)]\tLoss: 0.791817\n",
      "Train Epoch: 6 [7680/8838 (86%)]\tLoss: 0.821792\n",
      "Train Epoch: 6 [8320/8838 (94%)]\tLoss: 0.850924\n",
      "Train Epoch: 7 [0/8838 (0%)]\tLoss: 0.887067\n",
      "Train Epoch: 7 [640/8838 (7%)]\tLoss: 0.663537\n",
      "Train Epoch: 7 [1280/8838 (14%)]\tLoss: 0.586118\n",
      "Train Epoch: 7 [1920/8838 (22%)]\tLoss: 0.577931\n",
      "Train Epoch: 7 [2560/8838 (29%)]\tLoss: 0.771204\n",
      "Train Epoch: 7 [3200/8838 (36%)]\tLoss: 0.598230\n",
      "Train Epoch: 7 [3840/8838 (43%)]\tLoss: 0.818958\n",
      "Train Epoch: 7 [4480/8838 (50%)]\tLoss: 0.599860\n",
      "Train Epoch: 7 [5120/8838 (58%)]\tLoss: 0.596078\n",
      "Train Epoch: 7 [5760/8838 (65%)]\tLoss: 0.712621\n",
      "Train Epoch: 7 [6400/8838 (72%)]\tLoss: 0.895925\n",
      "Train Epoch: 7 [7040/8838 (79%)]\tLoss: 0.750897\n",
      "Train Epoch: 7 [7680/8838 (86%)]\tLoss: 0.909116\n",
      "Train Epoch: 7 [8320/8838 (94%)]\tLoss: 0.828377\n",
      "Train Epoch: 8 [0/8838 (0%)]\tLoss: 0.875618\n",
      "Train Epoch: 8 [640/8838 (7%)]\tLoss: 0.667585\n",
      "Train Epoch: 8 [1280/8838 (14%)]\tLoss: 0.753975\n",
      "Train Epoch: 8 [1920/8838 (22%)]\tLoss: 0.711304\n",
      "Train Epoch: 8 [2560/8838 (29%)]\tLoss: 1.049053\n",
      "Train Epoch: 8 [3200/8838 (36%)]\tLoss: 0.874158\n",
      "Train Epoch: 8 [3840/8838 (43%)]\tLoss: 0.645144\n",
      "Train Epoch: 8 [4480/8838 (50%)]\tLoss: 0.798440\n",
      "Train Epoch: 8 [5120/8838 (58%)]\tLoss: 0.798424\n",
      "Train Epoch: 8 [5760/8838 (65%)]\tLoss: 0.769174\n",
      "Train Epoch: 8 [6400/8838 (72%)]\tLoss: 0.783851\n",
      "Train Epoch: 8 [7040/8838 (79%)]\tLoss: 0.807984\n",
      "Train Epoch: 8 [7680/8838 (86%)]\tLoss: 0.851942\n",
      "Train Epoch: 8 [8320/8838 (94%)]\tLoss: 0.739486\n",
      "Train Epoch: 9 [0/8838 (0%)]\tLoss: 0.921619\n",
      "Train Epoch: 9 [640/8838 (7%)]\tLoss: 0.791386\n",
      "Train Epoch: 9 [1280/8838 (14%)]\tLoss: 0.890888\n",
      "Train Epoch: 9 [1920/8838 (22%)]\tLoss: 0.584141\n",
      "Train Epoch: 9 [2560/8838 (29%)]\tLoss: 0.649035\n",
      "Train Epoch: 9 [3200/8838 (36%)]\tLoss: 0.764155\n",
      "Train Epoch: 9 [3840/8838 (43%)]\tLoss: 0.778779\n",
      "Train Epoch: 9 [4480/8838 (50%)]\tLoss: 1.006501\n",
      "Train Epoch: 9 [5120/8838 (58%)]\tLoss: 0.667240\n",
      "Train Epoch: 9 [5760/8838 (65%)]\tLoss: 0.848745\n",
      "Train Epoch: 9 [6400/8838 (72%)]\tLoss: 0.657482\n",
      "Train Epoch: 9 [7040/8838 (79%)]\tLoss: 0.803792\n",
      "Train Epoch: 9 [7680/8838 (86%)]\tLoss: 0.662285\n",
      "Train Epoch: 9 [8320/8838 (94%)]\tLoss: 0.667937\n",
      "Train Epoch: 10 [0/8838 (0%)]\tLoss: 0.815024\n",
      "Train Epoch: 10 [640/8838 (7%)]\tLoss: 0.693846\n",
      "Train Epoch: 10 [1280/8838 (14%)]\tLoss: 0.675511\n",
      "Train Epoch: 10 [1920/8838 (22%)]\tLoss: 0.478417\n",
      "Train Epoch: 10 [2560/8838 (29%)]\tLoss: 0.503737\n",
      "Train Epoch: 10 [3200/8838 (36%)]\tLoss: 0.755888\n",
      "Train Epoch: 10 [3840/8838 (43%)]\tLoss: 0.637409\n",
      "Train Epoch: 10 [4480/8838 (50%)]\tLoss: 0.819935\n",
      "Train Epoch: 10 [5120/8838 (58%)]\tLoss: 0.830666\n",
      "Train Epoch: 10 [5760/8838 (65%)]\tLoss: 0.744500\n",
      "Train Epoch: 10 [6400/8838 (72%)]\tLoss: 0.731445\n",
      "Train Epoch: 10 [7040/8838 (79%)]\tLoss: 0.879942\n",
      "Train Epoch: 10 [7680/8838 (86%)]\tLoss: 0.831510\n",
      "Train Epoch: 10 [8320/8838 (94%)]\tLoss: 0.810671\n",
      "Train Epoch: 11 [0/8838 (0%)]\tLoss: 0.878996\n",
      "Train Epoch: 11 [640/8838 (7%)]\tLoss: 0.762024\n",
      "Train Epoch: 11 [1280/8838 (14%)]\tLoss: 0.748320\n",
      "Train Epoch: 11 [1920/8838 (22%)]\tLoss: 0.589951\n",
      "Train Epoch: 11 [2560/8838 (29%)]\tLoss: 0.730796\n",
      "Train Epoch: 11 [3200/8838 (36%)]\tLoss: 0.797053\n",
      "Train Epoch: 11 [3840/8838 (43%)]\tLoss: 0.748397\n",
      "Train Epoch: 11 [4480/8838 (50%)]\tLoss: 0.920921\n",
      "Train Epoch: 11 [5120/8838 (58%)]\tLoss: 0.668498\n",
      "Train Epoch: 11 [5760/8838 (65%)]\tLoss: 0.620333\n",
      "Train Epoch: 11 [6400/8838 (72%)]\tLoss: 0.580531\n",
      "Train Epoch: 11 [7040/8838 (79%)]\tLoss: 0.726036\n",
      "Train Epoch: 11 [7680/8838 (86%)]\tLoss: 0.842923\n",
      "Train Epoch: 11 [8320/8838 (94%)]\tLoss: 0.668449\n",
      "Train Epoch: 12 [0/8838 (0%)]\tLoss: 0.777981\n",
      "Train Epoch: 12 [640/8838 (7%)]\tLoss: 0.668789\n",
      "Train Epoch: 12 [1280/8838 (14%)]\tLoss: 0.962701\n",
      "Train Epoch: 12 [1920/8838 (22%)]\tLoss: 0.899947\n",
      "Train Epoch: 12 [2560/8838 (29%)]\tLoss: 0.644833\n",
      "Train Epoch: 12 [3200/8838 (36%)]\tLoss: 0.858915\n",
      "Train Epoch: 12 [3840/8838 (43%)]\tLoss: 0.557769\n",
      "Train Epoch: 12 [4480/8838 (50%)]\tLoss: 0.672761\n",
      "Train Epoch: 12 [5120/8838 (58%)]\tLoss: 0.818905\n",
      "Train Epoch: 12 [5760/8838 (65%)]\tLoss: 0.645703\n",
      "Train Epoch: 12 [6400/8838 (72%)]\tLoss: 0.501244\n",
      "Train Epoch: 12 [7040/8838 (79%)]\tLoss: 0.745576\n",
      "Train Epoch: 12 [7680/8838 (86%)]\tLoss: 0.576449\n",
      "Train Epoch: 12 [8320/8838 (94%)]\tLoss: 0.677539\n",
      "Train Epoch: 13 [0/8838 (0%)]\tLoss: 0.507325\n",
      "Train Epoch: 13 [640/8838 (7%)]\tLoss: 0.656214\n",
      "Train Epoch: 13 [1280/8838 (14%)]\tLoss: 0.779726\n",
      "Train Epoch: 13 [1920/8838 (22%)]\tLoss: 0.591796\n",
      "Train Epoch: 13 [2560/8838 (29%)]\tLoss: 0.761054\n",
      "Train Epoch: 13 [3200/8838 (36%)]\tLoss: 0.502978\n",
      "Train Epoch: 13 [3840/8838 (43%)]\tLoss: 0.792416\n",
      "Train Epoch: 13 [4480/8838 (50%)]\tLoss: 0.695094\n",
      "Train Epoch: 13 [5120/8838 (58%)]\tLoss: 0.660879\n",
      "Train Epoch: 13 [5760/8838 (65%)]\tLoss: 0.708264\n",
      "Train Epoch: 13 [6400/8838 (72%)]\tLoss: 0.687610\n",
      "Train Epoch: 13 [7040/8838 (79%)]\tLoss: 0.616425\n",
      "Train Epoch: 13 [7680/8838 (86%)]\tLoss: 1.025349\n",
      "Train Epoch: 13 [8320/8838 (94%)]\tLoss: 0.612668\n",
      "Train Epoch: 14 [0/8838 (0%)]\tLoss: 0.697573\n",
      "Train Epoch: 14 [640/8838 (7%)]\tLoss: 0.554419\n",
      "Train Epoch: 14 [1280/8838 (14%)]\tLoss: 0.647999\n",
      "Train Epoch: 14 [1920/8838 (22%)]\tLoss: 0.641262\n",
      "Train Epoch: 14 [2560/8838 (29%)]\tLoss: 0.823178\n",
      "Train Epoch: 14 [3200/8838 (36%)]\tLoss: 0.720108\n",
      "Train Epoch: 14 [3840/8838 (43%)]\tLoss: 0.712323\n",
      "Train Epoch: 14 [4480/8838 (50%)]\tLoss: 0.719476\n",
      "Train Epoch: 14 [5120/8838 (58%)]\tLoss: 0.866540\n",
      "Train Epoch: 14 [5760/8838 (65%)]\tLoss: 0.873113\n",
      "Train Epoch: 14 [6400/8838 (72%)]\tLoss: 0.883480\n",
      "Train Epoch: 14 [7040/8838 (79%)]\tLoss: 0.612239\n",
      "Train Epoch: 14 [7680/8838 (86%)]\tLoss: 0.727076\n",
      "Train Epoch: 14 [8320/8838 (94%)]\tLoss: 0.689005\n",
      "Train Epoch: 15 [0/8838 (0%)]\tLoss: 0.587360\n",
      "Train Epoch: 15 [640/8838 (7%)]\tLoss: 0.665565\n",
      "Train Epoch: 15 [1280/8838 (14%)]\tLoss: 0.781245\n",
      "Train Epoch: 15 [1920/8838 (22%)]\tLoss: 0.681824\n",
      "Train Epoch: 15 [2560/8838 (29%)]\tLoss: 0.730644\n",
      "Train Epoch: 15 [3200/8838 (36%)]\tLoss: 0.560316\n",
      "Train Epoch: 15 [3840/8838 (43%)]\tLoss: 0.756524\n",
      "Train Epoch: 15 [4480/8838 (50%)]\tLoss: 0.947900\n",
      "Train Epoch: 15 [5120/8838 (58%)]\tLoss: 0.733557\n",
      "Train Epoch: 15 [5760/8838 (65%)]\tLoss: 0.679466\n",
      "Train Epoch: 15 [6400/8838 (72%)]\tLoss: 0.701122\n",
      "Train Epoch: 15 [7040/8838 (79%)]\tLoss: 0.733223\n",
      "Train Epoch: 15 [7680/8838 (86%)]\tLoss: 0.731885\n",
      "Train Epoch: 15 [8320/8838 (94%)]\tLoss: 0.887924\n",
      "Train Epoch: 16 [0/8838 (0%)]\tLoss: 0.506363\n",
      "Train Epoch: 16 [640/8838 (7%)]\tLoss: 0.536655\n",
      "Train Epoch: 16 [1280/8838 (14%)]\tLoss: 0.664954\n",
      "Train Epoch: 16 [1920/8838 (22%)]\tLoss: 0.522169\n",
      "Train Epoch: 16 [2560/8838 (29%)]\tLoss: 0.672318\n",
      "Train Epoch: 16 [3200/8838 (36%)]\tLoss: 0.698858\n",
      "Train Epoch: 16 [3840/8838 (43%)]\tLoss: 0.662838\n",
      "Train Epoch: 16 [4480/8838 (50%)]\tLoss: 0.503603\n",
      "Train Epoch: 16 [5120/8838 (58%)]\tLoss: 1.074808\n",
      "Train Epoch: 16 [5760/8838 (65%)]\tLoss: 0.651384\n",
      "Train Epoch: 16 [6400/8838 (72%)]\tLoss: 0.600844\n",
      "Train Epoch: 16 [7040/8838 (79%)]\tLoss: 0.527492\n",
      "Train Epoch: 16 [7680/8838 (86%)]\tLoss: 0.515390\n",
      "Train Epoch: 16 [8320/8838 (94%)]\tLoss: 0.720001\n",
      "Train Epoch: 17 [0/8838 (0%)]\tLoss: 0.621210\n",
      "Train Epoch: 17 [640/8838 (7%)]\tLoss: 0.592060\n",
      "Train Epoch: 17 [1280/8838 (14%)]\tLoss: 0.764129\n",
      "Train Epoch: 17 [1920/8838 (22%)]\tLoss: 0.746776\n",
      "Train Epoch: 17 [2560/8838 (29%)]\tLoss: 0.624597\n",
      "Train Epoch: 17 [3200/8838 (36%)]\tLoss: 0.693387\n",
      "Train Epoch: 17 [3840/8838 (43%)]\tLoss: 0.692359\n",
      "Train Epoch: 17 [4480/8838 (50%)]\tLoss: 0.634512\n",
      "Train Epoch: 17 [5120/8838 (58%)]\tLoss: 0.750169\n",
      "Train Epoch: 17 [5760/8838 (65%)]\tLoss: 0.660701\n",
      "Train Epoch: 17 [6400/8838 (72%)]\tLoss: 1.061771\n",
      "Train Epoch: 17 [7040/8838 (79%)]\tLoss: 0.792393\n",
      "Train Epoch: 17 [7680/8838 (86%)]\tLoss: 0.805533\n",
      "Train Epoch: 17 [8320/8838 (94%)]\tLoss: 0.642363\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/6366 (0%)]\tLoss: 2.405869\n",
      "Train Epoch: 1 [640/6366 (10%)]\tLoss: 1.674134\n",
      "Train Epoch: 1 [1280/6366 (20%)]\tLoss: 1.013277\n",
      "Train Epoch: 1 [1920/6366 (30%)]\tLoss: 1.233402\n",
      "Train Epoch: 1 [2560/6366 (40%)]\tLoss: 0.818442\n",
      "Train Epoch: 1 [3200/6366 (50%)]\tLoss: 0.817834\n",
      "Train Epoch: 1 [3840/6366 (60%)]\tLoss: 0.910908\n",
      "Train Epoch: 1 [4480/6366 (70%)]\tLoss: 0.883725\n",
      "Train Epoch: 1 [5120/6366 (80%)]\tLoss: 0.568482\n",
      "Train Epoch: 1 [5760/6366 (90%)]\tLoss: 0.771253\n",
      "Train Epoch: 2 [0/6366 (0%)]\tLoss: 0.680788\n",
      "Train Epoch: 2 [640/6366 (10%)]\tLoss: 0.979789\n",
      "Train Epoch: 2 [1280/6366 (20%)]\tLoss: 0.925838\n",
      "Train Epoch: 2 [1920/6366 (30%)]\tLoss: 0.825496\n",
      "Train Epoch: 2 [2560/6366 (40%)]\tLoss: 0.785463\n",
      "Train Epoch: 2 [3200/6366 (50%)]\tLoss: 0.860978\n",
      "Train Epoch: 2 [3840/6366 (60%)]\tLoss: 0.603675\n",
      "Train Epoch: 2 [4480/6366 (70%)]\tLoss: 0.844041\n",
      "Train Epoch: 2 [5120/6366 (80%)]\tLoss: 0.576214\n",
      "Train Epoch: 2 [5760/6366 (90%)]\tLoss: 0.971982\n",
      "Train Epoch: 3 [0/6366 (0%)]\tLoss: 0.961659\n",
      "Train Epoch: 3 [640/6366 (10%)]\tLoss: 0.630393\n",
      "Train Epoch: 3 [1280/6366 (20%)]\tLoss: 0.783709\n",
      "Train Epoch: 3 [1920/6366 (30%)]\tLoss: 0.800487\n",
      "Train Epoch: 3 [2560/6366 (40%)]\tLoss: 0.668720\n",
      "Train Epoch: 3 [3200/6366 (50%)]\tLoss: 0.760264\n",
      "Train Epoch: 3 [3840/6366 (60%)]\tLoss: 0.815886\n",
      "Train Epoch: 3 [4480/6366 (70%)]\tLoss: 0.640228\n",
      "Train Epoch: 3 [5120/6366 (80%)]\tLoss: 0.600403\n",
      "Train Epoch: 3 [5760/6366 (90%)]\tLoss: 0.850719\n",
      "Train Epoch: 4 [0/6366 (0%)]\tLoss: 0.620315\n",
      "Train Epoch: 4 [640/6366 (10%)]\tLoss: 0.912062\n",
      "Train Epoch: 4 [1280/6366 (20%)]\tLoss: 0.664619\n",
      "Train Epoch: 4 [1920/6366 (30%)]\tLoss: 0.534886\n",
      "Train Epoch: 4 [2560/6366 (40%)]\tLoss: 0.518283\n",
      "Train Epoch: 4 [3200/6366 (50%)]\tLoss: 0.781608\n",
      "Train Epoch: 4 [3840/6366 (60%)]\tLoss: 0.859515\n",
      "Train Epoch: 4 [4480/6366 (70%)]\tLoss: 0.693730\n",
      "Train Epoch: 4 [5120/6366 (80%)]\tLoss: 0.612584\n",
      "Train Epoch: 4 [5760/6366 (90%)]\tLoss: 0.734347\n",
      "Train Epoch: 5 [0/6366 (0%)]\tLoss: 0.477085\n",
      "Train Epoch: 5 [640/6366 (10%)]\tLoss: 0.447541\n",
      "Train Epoch: 5 [1280/6366 (20%)]\tLoss: 0.504416\n",
      "Train Epoch: 5 [1920/6366 (30%)]\tLoss: 0.569537\n",
      "Train Epoch: 5 [2560/6366 (40%)]\tLoss: 0.621204\n",
      "Train Epoch: 5 [3200/6366 (50%)]\tLoss: 0.546441\n",
      "Train Epoch: 5 [3840/6366 (60%)]\tLoss: 0.729109\n",
      "Train Epoch: 5 [4480/6366 (70%)]\tLoss: 0.449219\n",
      "Train Epoch: 5 [5120/6366 (80%)]\tLoss: 0.519523\n",
      "Train Epoch: 5 [5760/6366 (90%)]\tLoss: 0.455193\n",
      "Train Epoch: 6 [0/6366 (0%)]\tLoss: 0.325282\n",
      "Train Epoch: 6 [640/6366 (10%)]\tLoss: 0.377660\n",
      "Train Epoch: 6 [1280/6366 (20%)]\tLoss: 0.756855\n",
      "Train Epoch: 6 [1920/6366 (30%)]\tLoss: 0.477167\n",
      "Train Epoch: 6 [2560/6366 (40%)]\tLoss: 0.508272\n",
      "Train Epoch: 6 [3200/6366 (50%)]\tLoss: 0.725700\n",
      "Train Epoch: 6 [3840/6366 (60%)]\tLoss: 0.607939\n",
      "Train Epoch: 6 [4480/6366 (70%)]\tLoss: 0.511553\n",
      "Train Epoch: 6 [5120/6366 (80%)]\tLoss: 0.787998\n",
      "Train Epoch: 6 [5760/6366 (90%)]\tLoss: 0.423015\n",
      "Train Epoch: 7 [0/6366 (0%)]\tLoss: 0.808986\n",
      "Train Epoch: 7 [640/6366 (10%)]\tLoss: 0.527260\n",
      "Train Epoch: 7 [1280/6366 (20%)]\tLoss: 0.434806\n",
      "Train Epoch: 7 [1920/6366 (30%)]\tLoss: 0.574584\n",
      "Train Epoch: 7 [2560/6366 (40%)]\tLoss: 0.496372\n",
      "Train Epoch: 7 [3200/6366 (50%)]\tLoss: 0.457951\n",
      "Train Epoch: 7 [3840/6366 (60%)]\tLoss: 0.540685\n",
      "Train Epoch: 7 [4480/6366 (70%)]\tLoss: 0.754725\n",
      "Train Epoch: 7 [5120/6366 (80%)]\tLoss: 0.473000\n",
      "Train Epoch: 7 [5760/6366 (90%)]\tLoss: 0.657316\n",
      "Train Epoch: 8 [0/6366 (0%)]\tLoss: 0.494999\n",
      "Train Epoch: 8 [640/6366 (10%)]\tLoss: 0.318709\n",
      "Train Epoch: 8 [1280/6366 (20%)]\tLoss: 0.496872\n",
      "Train Epoch: 8 [1920/6366 (30%)]\tLoss: 0.448708\n",
      "Train Epoch: 8 [2560/6366 (40%)]\tLoss: 0.306955\n",
      "Train Epoch: 8 [3200/6366 (50%)]\tLoss: 0.740727\n",
      "Train Epoch: 8 [3840/6366 (60%)]\tLoss: 0.510218\n",
      "Train Epoch: 8 [4480/6366 (70%)]\tLoss: 0.343177\n",
      "Train Epoch: 8 [5120/6366 (80%)]\tLoss: 0.527916\n",
      "Train Epoch: 8 [5760/6366 (90%)]\tLoss: 0.619784\n",
      "Train Epoch: 9 [0/6366 (0%)]\tLoss: 0.500059\n",
      "Train Epoch: 9 [640/6366 (10%)]\tLoss: 0.556242\n",
      "Train Epoch: 9 [1280/6366 (20%)]\tLoss: 0.510481\n",
      "Train Epoch: 9 [1920/6366 (30%)]\tLoss: 0.344053\n",
      "Train Epoch: 9 [2560/6366 (40%)]\tLoss: 0.472695\n",
      "Train Epoch: 9 [3200/6366 (50%)]\tLoss: 0.520008\n",
      "Train Epoch: 9 [3840/6366 (60%)]\tLoss: 0.513154\n",
      "Train Epoch: 9 [4480/6366 (70%)]\tLoss: 0.689625\n",
      "Train Epoch: 9 [5120/6366 (80%)]\tLoss: 0.849425\n",
      "Train Epoch: 9 [5760/6366 (90%)]\tLoss: 0.607223\n",
      "Train Epoch: 10 [0/6366 (0%)]\tLoss: 0.461796\n",
      "Train Epoch: 10 [640/6366 (10%)]\tLoss: 0.411669\n",
      "Train Epoch: 10 [1280/6366 (20%)]\tLoss: 0.500359\n",
      "Train Epoch: 10 [1920/6366 (30%)]\tLoss: 0.497216\n",
      "Train Epoch: 10 [2560/6366 (40%)]\tLoss: 0.376474\n",
      "Train Epoch: 10 [3200/6366 (50%)]\tLoss: 0.473521\n",
      "Train Epoch: 10 [3840/6366 (60%)]\tLoss: 0.369987\n",
      "Train Epoch: 10 [4480/6366 (70%)]\tLoss: 0.196445\n",
      "Train Epoch: 10 [5120/6366 (80%)]\tLoss: 0.535053\n",
      "Train Epoch: 10 [5760/6366 (90%)]\tLoss: 0.514516\n",
      "Train Epoch: 11 [0/6366 (0%)]\tLoss: 0.598583\n",
      "Train Epoch: 11 [640/6366 (10%)]\tLoss: 0.461886\n",
      "Train Epoch: 11 [1280/6366 (20%)]\tLoss: 0.430602\n",
      "Train Epoch: 11 [1920/6366 (30%)]\tLoss: 0.593639\n",
      "Train Epoch: 11 [2560/6366 (40%)]\tLoss: 0.523963\n",
      "Train Epoch: 11 [3200/6366 (50%)]\tLoss: 0.584017\n",
      "Train Epoch: 11 [3840/6366 (60%)]\tLoss: 0.571414\n",
      "Train Epoch: 11 [4480/6366 (70%)]\tLoss: 0.486036\n",
      "Train Epoch: 11 [5120/6366 (80%)]\tLoss: 0.439184\n",
      "Train Epoch: 11 [5760/6366 (90%)]\tLoss: 0.712155\n",
      "Train Epoch: 12 [0/6366 (0%)]\tLoss: 0.389383\n",
      "Train Epoch: 12 [640/6366 (10%)]\tLoss: 0.508295\n",
      "Train Epoch: 12 [1280/6366 (20%)]\tLoss: 0.519269\n",
      "Train Epoch: 12 [1920/6366 (30%)]\tLoss: 0.508867\n",
      "Train Epoch: 12 [2560/6366 (40%)]\tLoss: 0.525343\n",
      "Train Epoch: 12 [3200/6366 (50%)]\tLoss: 0.383566\n",
      "Train Epoch: 12 [3840/6366 (60%)]\tLoss: 0.394137\n",
      "Train Epoch: 12 [4480/6366 (70%)]\tLoss: 0.473707\n",
      "Train Epoch: 12 [5120/6366 (80%)]\tLoss: 0.463643\n",
      "Train Epoch: 12 [5760/6366 (90%)]\tLoss: 0.261869\n",
      "Train Epoch: 13 [0/6366 (0%)]\tLoss: 0.435118\n",
      "Train Epoch: 13 [640/6366 (10%)]\tLoss: 0.374405\n",
      "Train Epoch: 13 [1280/6366 (20%)]\tLoss: 0.425050\n",
      "Train Epoch: 13 [1920/6366 (30%)]\tLoss: 0.556814\n",
      "Train Epoch: 13 [2560/6366 (40%)]\tLoss: 0.390727\n",
      "Train Epoch: 13 [3200/6366 (50%)]\tLoss: 0.529245\n",
      "Train Epoch: 13 [3840/6366 (60%)]\tLoss: 0.318270\n",
      "Train Epoch: 13 [4480/6366 (70%)]\tLoss: 0.499433\n",
      "Train Epoch: 13 [5120/6366 (80%)]\tLoss: 0.429005\n",
      "Train Epoch: 13 [5760/6366 (90%)]\tLoss: 0.278498\n",
      "Train Epoch: 14 [0/6366 (0%)]\tLoss: 0.494039\n",
      "Train Epoch: 14 [640/6366 (10%)]\tLoss: 0.268882\n",
      "Train Epoch: 14 [1280/6366 (20%)]\tLoss: 0.646931\n",
      "Train Epoch: 14 [1920/6366 (30%)]\tLoss: 0.289656\n",
      "Train Epoch: 14 [2560/6366 (40%)]\tLoss: 0.382324\n",
      "Train Epoch: 14 [3200/6366 (50%)]\tLoss: 0.402992\n",
      "Train Epoch: 14 [3840/6366 (60%)]\tLoss: 0.456954\n",
      "Train Epoch: 14 [4480/6366 (70%)]\tLoss: 0.313899\n",
      "Train Epoch: 14 [5120/6366 (80%)]\tLoss: 0.347668\n",
      "Train Epoch: 14 [5760/6366 (90%)]\tLoss: 0.470572\n",
      "Train Epoch: 15 [0/6366 (0%)]\tLoss: 0.327763\n",
      "Train Epoch: 15 [640/6366 (10%)]\tLoss: 0.462608\n",
      "Train Epoch: 15 [1280/6366 (20%)]\tLoss: 0.295999\n",
      "Train Epoch: 15 [1920/6366 (30%)]\tLoss: 0.343672\n",
      "Train Epoch: 15 [2560/6366 (40%)]\tLoss: 0.524748\n",
      "Train Epoch: 15 [3200/6366 (50%)]\tLoss: 0.336465\n",
      "Train Epoch: 15 [3840/6366 (60%)]\tLoss: 0.305881\n",
      "Train Epoch: 15 [4480/6366 (70%)]\tLoss: 0.342325\n",
      "Train Epoch: 15 [5120/6366 (80%)]\tLoss: 0.268884\n",
      "Train Epoch: 15 [5760/6366 (90%)]\tLoss: 0.420775\n",
      "Train Epoch: 16 [0/6366 (0%)]\tLoss: 0.428355\n",
      "Train Epoch: 16 [640/6366 (10%)]\tLoss: 0.430452\n",
      "Train Epoch: 16 [1280/6366 (20%)]\tLoss: 0.433598\n",
      "Train Epoch: 16 [1920/6366 (30%)]\tLoss: 0.416622\n",
      "Train Epoch: 16 [2560/6366 (40%)]\tLoss: 0.376276\n",
      "Train Epoch: 16 [3200/6366 (50%)]\tLoss: 0.284552\n",
      "Train Epoch: 16 [3840/6366 (60%)]\tLoss: 0.519452\n",
      "Train Epoch: 16 [4480/6366 (70%)]\tLoss: 0.302976\n",
      "Train Epoch: 16 [5120/6366 (80%)]\tLoss: 0.323619\n",
      "Train Epoch: 16 [5760/6366 (90%)]\tLoss: 0.547701\n",
      "Train Epoch: 17 [0/6366 (0%)]\tLoss: 0.395951\n",
      "Train Epoch: 17 [640/6366 (10%)]\tLoss: 0.482476\n",
      "Train Epoch: 17 [1280/6366 (20%)]\tLoss: 0.360995\n",
      "Train Epoch: 17 [1920/6366 (30%)]\tLoss: 0.555717\n",
      "Train Epoch: 17 [2560/6366 (40%)]\tLoss: 0.666501\n",
      "Train Epoch: 17 [3200/6366 (50%)]\tLoss: 0.307712\n",
      "Train Epoch: 17 [3840/6366 (60%)]\tLoss: 0.423301\n",
      "Train Epoch: 17 [4480/6366 (70%)]\tLoss: 0.353753\n",
      "Train Epoch: 17 [5120/6366 (80%)]\tLoss: 0.544387\n",
      "Train Epoch: 17 [5760/6366 (90%)]\tLoss: 0.404937\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/9451 (0%)]\tLoss: 2.286197\n",
      "Train Epoch: 1 [640/9451 (7%)]\tLoss: 1.963096\n",
      "Train Epoch: 1 [1280/9451 (14%)]\tLoss: 1.677279\n",
      "Train Epoch: 1 [1920/9451 (20%)]\tLoss: 1.403367\n",
      "Train Epoch: 1 [2560/9451 (27%)]\tLoss: 1.360460\n",
      "Train Epoch: 1 [3200/9451 (34%)]\tLoss: 1.280733\n",
      "Train Epoch: 1 [3840/9451 (41%)]\tLoss: 1.396094\n",
      "Train Epoch: 1 [4480/9451 (47%)]\tLoss: 1.135054\n",
      "Train Epoch: 1 [5120/9451 (54%)]\tLoss: 1.194049\n",
      "Train Epoch: 1 [5760/9451 (61%)]\tLoss: 1.250461\n",
      "Train Epoch: 1 [6400/9451 (68%)]\tLoss: 1.227680\n",
      "Train Epoch: 1 [7040/9451 (74%)]\tLoss: 1.261045\n",
      "Train Epoch: 1 [7680/9451 (81%)]\tLoss: 1.169487\n",
      "Train Epoch: 1 [8320/9451 (88%)]\tLoss: 1.293835\n",
      "Train Epoch: 1 [8960/9451 (95%)]\tLoss: 1.258962\n",
      "Train Epoch: 2 [0/9451 (0%)]\tLoss: 1.133966\n",
      "Train Epoch: 2 [640/9451 (7%)]\tLoss: 1.232244\n",
      "Train Epoch: 2 [1280/9451 (14%)]\tLoss: 1.108415\n",
      "Train Epoch: 2 [1920/9451 (20%)]\tLoss: 1.241572\n",
      "Train Epoch: 2 [2560/9451 (27%)]\tLoss: 1.267178\n",
      "Train Epoch: 2 [3200/9451 (34%)]\tLoss: 1.191246\n",
      "Train Epoch: 2 [3840/9451 (41%)]\tLoss: 1.019817\n",
      "Train Epoch: 2 [4480/9451 (47%)]\tLoss: 1.252791\n",
      "Train Epoch: 2 [5120/9451 (54%)]\tLoss: 1.315763\n",
      "Train Epoch: 2 [5760/9451 (61%)]\tLoss: 1.026694\n",
      "Train Epoch: 2 [6400/9451 (68%)]\tLoss: 0.927463\n",
      "Train Epoch: 2 [7040/9451 (74%)]\tLoss: 1.013547\n",
      "Train Epoch: 2 [7680/9451 (81%)]\tLoss: 1.268985\n",
      "Train Epoch: 2 [8320/9451 (88%)]\tLoss: 0.972528\n",
      "Train Epoch: 2 [8960/9451 (95%)]\tLoss: 1.120285\n",
      "Train Epoch: 3 [0/9451 (0%)]\tLoss: 1.044725\n",
      "Train Epoch: 3 [640/9451 (7%)]\tLoss: 1.411574\n",
      "Train Epoch: 3 [1280/9451 (14%)]\tLoss: 0.760913\n",
      "Train Epoch: 3 [1920/9451 (20%)]\tLoss: 1.112657\n",
      "Train Epoch: 3 [2560/9451 (27%)]\tLoss: 1.223155\n",
      "Train Epoch: 3 [3200/9451 (34%)]\tLoss: 1.161931\n",
      "Train Epoch: 3 [3840/9451 (41%)]\tLoss: 1.233665\n",
      "Train Epoch: 3 [4480/9451 (47%)]\tLoss: 1.242718\n",
      "Train Epoch: 3 [5120/9451 (54%)]\tLoss: 1.273566\n",
      "Train Epoch: 3 [5760/9451 (61%)]\tLoss: 1.066161\n",
      "Train Epoch: 3 [6400/9451 (68%)]\tLoss: 1.058236\n",
      "Train Epoch: 3 [7040/9451 (74%)]\tLoss: 1.028670\n",
      "Train Epoch: 3 [7680/9451 (81%)]\tLoss: 1.359302\n",
      "Train Epoch: 3 [8320/9451 (88%)]\tLoss: 0.969496\n",
      "Train Epoch: 3 [8960/9451 (95%)]\tLoss: 1.037360\n",
      "Train Epoch: 4 [0/9451 (0%)]\tLoss: 1.086065\n",
      "Train Epoch: 4 [640/9451 (7%)]\tLoss: 1.306603\n",
      "Train Epoch: 4 [1280/9451 (14%)]\tLoss: 1.021214\n",
      "Train Epoch: 4 [1920/9451 (20%)]\tLoss: 0.954068\n",
      "Train Epoch: 4 [2560/9451 (27%)]\tLoss: 1.328954\n",
      "Train Epoch: 4 [3200/9451 (34%)]\tLoss: 1.034560\n",
      "Train Epoch: 4 [3840/9451 (41%)]\tLoss: 1.254567\n",
      "Train Epoch: 4 [4480/9451 (47%)]\tLoss: 0.958008\n",
      "Train Epoch: 4 [5120/9451 (54%)]\tLoss: 1.111619\n",
      "Train Epoch: 4 [5760/9451 (61%)]\tLoss: 1.242904\n",
      "Train Epoch: 4 [6400/9451 (68%)]\tLoss: 1.090928\n",
      "Train Epoch: 4 [7040/9451 (74%)]\tLoss: 0.992599\n",
      "Train Epoch: 4 [7680/9451 (81%)]\tLoss: 1.035828\n",
      "Train Epoch: 4 [8320/9451 (88%)]\tLoss: 1.114627\n",
      "Train Epoch: 4 [8960/9451 (95%)]\tLoss: 0.984574\n",
      "Train Epoch: 5 [0/9451 (0%)]\tLoss: 0.946972\n",
      "Train Epoch: 5 [640/9451 (7%)]\tLoss: 1.022538\n",
      "Train Epoch: 5 [1280/9451 (14%)]\tLoss: 1.164616\n",
      "Train Epoch: 5 [1920/9451 (20%)]\tLoss: 1.265365\n",
      "Train Epoch: 5 [2560/9451 (27%)]\tLoss: 0.998326\n",
      "Train Epoch: 5 [3200/9451 (34%)]\tLoss: 0.992557\n",
      "Train Epoch: 5 [3840/9451 (41%)]\tLoss: 1.251272\n",
      "Train Epoch: 5 [4480/9451 (47%)]\tLoss: 1.095035\n",
      "Train Epoch: 5 [5120/9451 (54%)]\tLoss: 1.122912\n",
      "Train Epoch: 5 [5760/9451 (61%)]\tLoss: 0.946318\n",
      "Train Epoch: 5 [6400/9451 (68%)]\tLoss: 0.803980\n",
      "Train Epoch: 5 [7040/9451 (74%)]\tLoss: 1.131122\n",
      "Train Epoch: 5 [7680/9451 (81%)]\tLoss: 0.967041\n",
      "Train Epoch: 5 [8320/9451 (88%)]\tLoss: 0.924598\n",
      "Train Epoch: 5 [8960/9451 (95%)]\tLoss: 0.824839\n",
      "Train Epoch: 6 [0/9451 (0%)]\tLoss: 0.923446\n",
      "Train Epoch: 6 [640/9451 (7%)]\tLoss: 1.105786\n",
      "Train Epoch: 6 [1280/9451 (14%)]\tLoss: 1.331733\n",
      "Train Epoch: 6 [1920/9451 (20%)]\tLoss: 0.962264\n",
      "Train Epoch: 6 [2560/9451 (27%)]\tLoss: 1.107315\n",
      "Train Epoch: 6 [3200/9451 (34%)]\tLoss: 1.023070\n",
      "Train Epoch: 6 [3840/9451 (41%)]\tLoss: 0.973354\n",
      "Train Epoch: 6 [4480/9451 (47%)]\tLoss: 0.835355\n",
      "Train Epoch: 6 [5120/9451 (54%)]\tLoss: 0.876382\n",
      "Train Epoch: 6 [5760/9451 (61%)]\tLoss: 1.040587\n",
      "Train Epoch: 6 [6400/9451 (68%)]\tLoss: 0.890258\n",
      "Train Epoch: 6 [7040/9451 (74%)]\tLoss: 0.937344\n",
      "Train Epoch: 6 [7680/9451 (81%)]\tLoss: 1.003035\n",
      "Train Epoch: 6 [8320/9451 (88%)]\tLoss: 0.891834\n",
      "Train Epoch: 6 [8960/9451 (95%)]\tLoss: 0.988336\n",
      "Train Epoch: 7 [0/9451 (0%)]\tLoss: 1.015117\n",
      "Train Epoch: 7 [640/9451 (7%)]\tLoss: 1.134128\n",
      "Train Epoch: 7 [1280/9451 (14%)]\tLoss: 0.791663\n",
      "Train Epoch: 7 [1920/9451 (20%)]\tLoss: 0.943712\n",
      "Train Epoch: 7 [2560/9451 (27%)]\tLoss: 0.990680\n",
      "Train Epoch: 7 [3200/9451 (34%)]\tLoss: 0.983582\n",
      "Train Epoch: 7 [3840/9451 (41%)]\tLoss: 0.925362\n",
      "Train Epoch: 7 [4480/9451 (47%)]\tLoss: 0.960972\n",
      "Train Epoch: 7 [5120/9451 (54%)]\tLoss: 1.194117\n",
      "Train Epoch: 7 [5760/9451 (61%)]\tLoss: 0.969988\n",
      "Train Epoch: 7 [6400/9451 (68%)]\tLoss: 0.916286\n",
      "Train Epoch: 7 [7040/9451 (74%)]\tLoss: 1.190915\n",
      "Train Epoch: 7 [7680/9451 (81%)]\tLoss: 0.862183\n",
      "Train Epoch: 7 [8320/9451 (88%)]\tLoss: 1.131594\n",
      "Train Epoch: 7 [8960/9451 (95%)]\tLoss: 0.958243\n",
      "Train Epoch: 8 [0/9451 (0%)]\tLoss: 0.987522\n",
      "Train Epoch: 8 [640/9451 (7%)]\tLoss: 0.770990\n",
      "Train Epoch: 8 [1280/9451 (14%)]\tLoss: 1.189770\n",
      "Train Epoch: 8 [1920/9451 (20%)]\tLoss: 0.959679\n",
      "Train Epoch: 8 [2560/9451 (27%)]\tLoss: 1.112716\n",
      "Train Epoch: 8 [3200/9451 (34%)]\tLoss: 1.264997\n",
      "Train Epoch: 8 [3840/9451 (41%)]\tLoss: 1.040227\n",
      "Train Epoch: 8 [4480/9451 (47%)]\tLoss: 0.921522\n",
      "Train Epoch: 8 [5120/9451 (54%)]\tLoss: 1.019000\n",
      "Train Epoch: 8 [5760/9451 (61%)]\tLoss: 1.258538\n",
      "Train Epoch: 8 [6400/9451 (68%)]\tLoss: 0.953902\n",
      "Train Epoch: 8 [7040/9451 (74%)]\tLoss: 0.939341\n",
      "Train Epoch: 8 [7680/9451 (81%)]\tLoss: 1.063374\n",
      "Train Epoch: 8 [8320/9451 (88%)]\tLoss: 0.897921\n",
      "Train Epoch: 8 [8960/9451 (95%)]\tLoss: 0.963116\n",
      "Train Epoch: 9 [0/9451 (0%)]\tLoss: 0.955716\n",
      "Train Epoch: 9 [640/9451 (7%)]\tLoss: 0.965446\n",
      "Train Epoch: 9 [1280/9451 (14%)]\tLoss: 1.040192\n",
      "Train Epoch: 9 [1920/9451 (20%)]\tLoss: 0.961000\n",
      "Train Epoch: 9 [2560/9451 (27%)]\tLoss: 0.800055\n",
      "Train Epoch: 9 [3200/9451 (34%)]\tLoss: 0.974400\n",
      "Train Epoch: 9 [3840/9451 (41%)]\tLoss: 1.000455\n",
      "Train Epoch: 9 [4480/9451 (47%)]\tLoss: 1.022991\n",
      "Train Epoch: 9 [5120/9451 (54%)]\tLoss: 1.036724\n",
      "Train Epoch: 9 [5760/9451 (61%)]\tLoss: 0.841888\n",
      "Train Epoch: 9 [6400/9451 (68%)]\tLoss: 0.928258\n",
      "Train Epoch: 9 [7040/9451 (74%)]\tLoss: 0.734619\n",
      "Train Epoch: 9 [7680/9451 (81%)]\tLoss: 1.068893\n",
      "Train Epoch: 9 [8320/9451 (88%)]\tLoss: 0.797730\n",
      "Train Epoch: 9 [8960/9451 (95%)]\tLoss: 1.045325\n",
      "Train Epoch: 10 [0/9451 (0%)]\tLoss: 1.025862\n",
      "Train Epoch: 10 [640/9451 (7%)]\tLoss: 0.940696\n",
      "Train Epoch: 10 [1280/9451 (14%)]\tLoss: 0.826404\n",
      "Train Epoch: 10 [1920/9451 (20%)]\tLoss: 1.037531\n",
      "Train Epoch: 10 [2560/9451 (27%)]\tLoss: 0.830790\n",
      "Train Epoch: 10 [3200/9451 (34%)]\tLoss: 1.232915\n",
      "Train Epoch: 10 [3840/9451 (41%)]\tLoss: 1.125368\n",
      "Train Epoch: 10 [4480/9451 (47%)]\tLoss: 0.804568\n",
      "Train Epoch: 10 [5120/9451 (54%)]\tLoss: 0.846753\n",
      "Train Epoch: 10 [5760/9451 (61%)]\tLoss: 0.852199\n",
      "Train Epoch: 10 [6400/9451 (68%)]\tLoss: 0.763100\n",
      "Train Epoch: 10 [7040/9451 (74%)]\tLoss: 1.008356\n",
      "Train Epoch: 10 [7680/9451 (81%)]\tLoss: 0.772918\n",
      "Train Epoch: 10 [8320/9451 (88%)]\tLoss: 0.868084\n",
      "Train Epoch: 10 [8960/9451 (95%)]\tLoss: 1.016658\n",
      "Train Epoch: 11 [0/9451 (0%)]\tLoss: 0.757511\n",
      "Train Epoch: 11 [640/9451 (7%)]\tLoss: 0.971590\n",
      "Train Epoch: 11 [1280/9451 (14%)]\tLoss: 0.963657\n",
      "Train Epoch: 11 [1920/9451 (20%)]\tLoss: 1.148843\n",
      "Train Epoch: 11 [2560/9451 (27%)]\tLoss: 0.915374\n",
      "Train Epoch: 11 [3200/9451 (34%)]\tLoss: 0.917432\n",
      "Train Epoch: 11 [3840/9451 (41%)]\tLoss: 1.016772\n",
      "Train Epoch: 11 [4480/9451 (47%)]\tLoss: 0.927481\n",
      "Train Epoch: 11 [5120/9451 (54%)]\tLoss: 0.908597\n",
      "Train Epoch: 11 [5760/9451 (61%)]\tLoss: 0.916517\n",
      "Train Epoch: 11 [6400/9451 (68%)]\tLoss: 0.814309\n",
      "Train Epoch: 11 [7040/9451 (74%)]\tLoss: 1.056169\n",
      "Train Epoch: 11 [7680/9451 (81%)]\tLoss: 0.806938\n",
      "Train Epoch: 11 [8320/9451 (88%)]\tLoss: 0.943718\n",
      "Train Epoch: 11 [8960/9451 (95%)]\tLoss: 0.985060\n",
      "Train Epoch: 12 [0/9451 (0%)]\tLoss: 1.073569\n",
      "Train Epoch: 12 [640/9451 (7%)]\tLoss: 1.070614\n",
      "Train Epoch: 12 [1280/9451 (14%)]\tLoss: 0.848134\n",
      "Train Epoch: 12 [1920/9451 (20%)]\tLoss: 0.969973\n",
      "Train Epoch: 12 [2560/9451 (27%)]\tLoss: 0.786829\n",
      "Train Epoch: 12 [3200/9451 (34%)]\tLoss: 1.231384\n",
      "Train Epoch: 12 [3840/9451 (41%)]\tLoss: 0.788573\n",
      "Train Epoch: 12 [4480/9451 (47%)]\tLoss: 1.114486\n",
      "Train Epoch: 12 [5120/9451 (54%)]\tLoss: 0.891316\n",
      "Train Epoch: 12 [5760/9451 (61%)]\tLoss: 0.811216\n",
      "Train Epoch: 12 [6400/9451 (68%)]\tLoss: 0.865441\n",
      "Train Epoch: 12 [7040/9451 (74%)]\tLoss: 0.773816\n",
      "Train Epoch: 12 [7680/9451 (81%)]\tLoss: 0.881152\n",
      "Train Epoch: 12 [8320/9451 (88%)]\tLoss: 1.023613\n",
      "Train Epoch: 12 [8960/9451 (95%)]\tLoss: 0.724340\n",
      "Train Epoch: 13 [0/9451 (0%)]\tLoss: 0.889186\n",
      "Train Epoch: 13 [640/9451 (7%)]\tLoss: 0.933073\n",
      "Train Epoch: 13 [1280/9451 (14%)]\tLoss: 0.815661\n",
      "Train Epoch: 13 [1920/9451 (20%)]\tLoss: 0.757048\n",
      "Train Epoch: 13 [2560/9451 (27%)]\tLoss: 1.017140\n",
      "Train Epoch: 13 [3200/9451 (34%)]\tLoss: 0.966976\n",
      "Train Epoch: 13 [3840/9451 (41%)]\tLoss: 1.077315\n",
      "Train Epoch: 13 [4480/9451 (47%)]\tLoss: 1.104295\n",
      "Train Epoch: 13 [5120/9451 (54%)]\tLoss: 0.869155\n",
      "Train Epoch: 13 [5760/9451 (61%)]\tLoss: 0.961168\n",
      "Train Epoch: 13 [6400/9451 (68%)]\tLoss: 1.066105\n",
      "Train Epoch: 13 [7040/9451 (74%)]\tLoss: 0.895568\n",
      "Train Epoch: 13 [7680/9451 (81%)]\tLoss: 1.006326\n",
      "Train Epoch: 13 [8320/9451 (88%)]\tLoss: 0.842649\n",
      "Train Epoch: 13 [8960/9451 (95%)]\tLoss: 0.794340\n",
      "Train Epoch: 14 [0/9451 (0%)]\tLoss: 1.083348\n",
      "Train Epoch: 14 [640/9451 (7%)]\tLoss: 1.312082\n",
      "Train Epoch: 14 [1280/9451 (14%)]\tLoss: 0.926532\n",
      "Train Epoch: 14 [1920/9451 (20%)]\tLoss: 0.886183\n",
      "Train Epoch: 14 [2560/9451 (27%)]\tLoss: 0.884663\n",
      "Train Epoch: 14 [3200/9451 (34%)]\tLoss: 0.994732\n",
      "Train Epoch: 14 [3840/9451 (41%)]\tLoss: 1.073498\n",
      "Train Epoch: 14 [4480/9451 (47%)]\tLoss: 0.950152\n",
      "Train Epoch: 14 [5120/9451 (54%)]\tLoss: 0.882376\n",
      "Train Epoch: 14 [5760/9451 (61%)]\tLoss: 1.061188\n",
      "Train Epoch: 14 [6400/9451 (68%)]\tLoss: 1.049748\n",
      "Train Epoch: 14 [7040/9451 (74%)]\tLoss: 0.909801\n",
      "Train Epoch: 14 [7680/9451 (81%)]\tLoss: 0.913444\n",
      "Train Epoch: 14 [8320/9451 (88%)]\tLoss: 0.906539\n",
      "Train Epoch: 14 [8960/9451 (95%)]\tLoss: 0.703880\n",
      "Train Epoch: 15 [0/9451 (0%)]\tLoss: 1.172143\n",
      "Train Epoch: 15 [640/9451 (7%)]\tLoss: 0.876586\n",
      "Train Epoch: 15 [1280/9451 (14%)]\tLoss: 0.890205\n",
      "Train Epoch: 15 [1920/9451 (20%)]\tLoss: 0.778376\n",
      "Train Epoch: 15 [2560/9451 (27%)]\tLoss: 1.023384\n",
      "Train Epoch: 15 [3200/9451 (34%)]\tLoss: 0.983348\n",
      "Train Epoch: 15 [3840/9451 (41%)]\tLoss: 0.665806\n",
      "Train Epoch: 15 [4480/9451 (47%)]\tLoss: 0.787625\n",
      "Train Epoch: 15 [5120/9451 (54%)]\tLoss: 1.113649\n",
      "Train Epoch: 15 [5760/9451 (61%)]\tLoss: 1.082294\n",
      "Train Epoch: 15 [6400/9451 (68%)]\tLoss: 0.898238\n",
      "Train Epoch: 15 [7040/9451 (74%)]\tLoss: 0.922477\n",
      "Train Epoch: 15 [7680/9451 (81%)]\tLoss: 0.851930\n",
      "Train Epoch: 15 [8320/9451 (88%)]\tLoss: 0.969697\n",
      "Train Epoch: 15 [8960/9451 (95%)]\tLoss: 0.912540\n",
      "Train Epoch: 16 [0/9451 (0%)]\tLoss: 0.800672\n",
      "Train Epoch: 16 [640/9451 (7%)]\tLoss: 0.819812\n",
      "Train Epoch: 16 [1280/9451 (14%)]\tLoss: 1.164634\n",
      "Train Epoch: 16 [1920/9451 (20%)]\tLoss: 0.636574\n",
      "Train Epoch: 16 [2560/9451 (27%)]\tLoss: 1.010501\n",
      "Train Epoch: 16 [3200/9451 (34%)]\tLoss: 0.906717\n",
      "Train Epoch: 16 [3840/9451 (41%)]\tLoss: 1.044728\n",
      "Train Epoch: 16 [4480/9451 (47%)]\tLoss: 0.821242\n",
      "Train Epoch: 16 [5120/9451 (54%)]\tLoss: 1.278837\n",
      "Train Epoch: 16 [5760/9451 (61%)]\tLoss: 0.901108\n",
      "Train Epoch: 16 [6400/9451 (68%)]\tLoss: 0.838256\n",
      "Train Epoch: 16 [7040/9451 (74%)]\tLoss: 1.068651\n",
      "Train Epoch: 16 [7680/9451 (81%)]\tLoss: 1.029050\n",
      "Train Epoch: 16 [8320/9451 (88%)]\tLoss: 1.006748\n",
      "Train Epoch: 16 [8960/9451 (95%)]\tLoss: 0.825357\n",
      "Train Epoch: 17 [0/9451 (0%)]\tLoss: 1.031118\n",
      "Train Epoch: 17 [640/9451 (7%)]\tLoss: 0.819091\n",
      "Train Epoch: 17 [1280/9451 (14%)]\tLoss: 0.954724\n",
      "Train Epoch: 17 [1920/9451 (20%)]\tLoss: 1.086428\n",
      "Train Epoch: 17 [2560/9451 (27%)]\tLoss: 1.098281\n",
      "Train Epoch: 17 [3200/9451 (34%)]\tLoss: 0.863551\n",
      "Train Epoch: 17 [3840/9451 (41%)]\tLoss: 0.963410\n",
      "Train Epoch: 17 [4480/9451 (47%)]\tLoss: 0.917895\n",
      "Train Epoch: 17 [5120/9451 (54%)]\tLoss: 0.961296\n",
      "Train Epoch: 17 [5760/9451 (61%)]\tLoss: 0.705957\n",
      "Train Epoch: 17 [6400/9451 (68%)]\tLoss: 0.822999\n",
      "Train Epoch: 17 [7040/9451 (74%)]\tLoss: 0.798630\n",
      "Train Epoch: 17 [7680/9451 (81%)]\tLoss: 0.932208\n",
      "Train Epoch: 17 [8320/9451 (88%)]\tLoss: 1.082273\n",
      "Train Epoch: 17 [8960/9451 (95%)]\tLoss: 0.985469\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/1026 (0%)]\tLoss: 2.223633\n",
      "Train Epoch: 1 [640/1026 (59%)]\tLoss: 1.688477\n",
      "Train Epoch: 2 [0/1026 (0%)]\tLoss: 1.232720\n",
      "Train Epoch: 2 [640/1026 (59%)]\tLoss: 1.155763\n",
      "Train Epoch: 3 [0/1026 (0%)]\tLoss: 1.386965\n",
      "Train Epoch: 3 [640/1026 (59%)]\tLoss: 1.455615\n",
      "Train Epoch: 4 [0/1026 (0%)]\tLoss: 1.259657\n",
      "Train Epoch: 4 [640/1026 (59%)]\tLoss: 1.492463\n",
      "Train Epoch: 5 [0/1026 (0%)]\tLoss: 1.563019\n",
      "Train Epoch: 5 [640/1026 (59%)]\tLoss: 1.474828\n",
      "Train Epoch: 6 [0/1026 (0%)]\tLoss: 1.293402\n",
      "Train Epoch: 6 [640/1026 (59%)]\tLoss: 1.086285\n",
      "Train Epoch: 7 [0/1026 (0%)]\tLoss: 1.125325\n",
      "Train Epoch: 7 [640/1026 (59%)]\tLoss: 1.153013\n",
      "Train Epoch: 8 [0/1026 (0%)]\tLoss: 1.211334\n",
      "Train Epoch: 8 [640/1026 (59%)]\tLoss: 1.235469\n",
      "Train Epoch: 9 [0/1026 (0%)]\tLoss: 1.264695\n",
      "Train Epoch: 9 [640/1026 (59%)]\tLoss: 1.316631\n",
      "Train Epoch: 10 [0/1026 (0%)]\tLoss: 1.002371\n",
      "Train Epoch: 10 [640/1026 (59%)]\tLoss: 1.029896\n",
      "Train Epoch: 11 [0/1026 (0%)]\tLoss: 1.359416\n",
      "Train Epoch: 11 [640/1026 (59%)]\tLoss: 1.199662\n",
      "Train Epoch: 12 [0/1026 (0%)]\tLoss: 0.949687\n",
      "Train Epoch: 12 [640/1026 (59%)]\tLoss: 1.120110\n",
      "Train Epoch: 13 [0/1026 (0%)]\tLoss: 1.066828\n",
      "Train Epoch: 13 [640/1026 (59%)]\tLoss: 1.119816\n",
      "Train Epoch: 14 [0/1026 (0%)]\tLoss: 1.244102\n",
      "Train Epoch: 14 [640/1026 (59%)]\tLoss: 1.190337\n",
      "Train Epoch: 15 [0/1026 (0%)]\tLoss: 1.140622\n",
      "Train Epoch: 15 [640/1026 (59%)]\tLoss: 1.334198\n",
      "Train Epoch: 16 [0/1026 (0%)]\tLoss: 1.209623\n",
      "Train Epoch: 16 [640/1026 (59%)]\tLoss: 1.378209\n",
      "Train Epoch: 17 [0/1026 (0%)]\tLoss: 1.281636\n",
      "Train Epoch: 17 [640/1026 (59%)]\tLoss: 1.273269\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nazek\\anaconda3\\Lib\\site-packages\\torch\\nn\\_reduction.py:51: UserWarning: size_average and reduce args will be deprecated, please use reduction='sum' instead.\n",
      "  warnings.warn(warning.format(ret))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Avg. loss: 2.1831, Accuracy: 2523/10000 (25%)\n",
      "\n",
      "Round 2/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 2.015502\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.790448\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.634966\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.624187\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.744822\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.810564\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.738604\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.389133\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.691034\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.366926\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.499156\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.453201\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.834585\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.500525\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.485653\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.539580\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.478990\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.350013\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.540406\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.475448\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.521608\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.504687\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.744936\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.400952\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.601740\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.343362\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.498350\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.415645\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.381882\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.515401\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.419872\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.473127\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.433719\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.627698\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.558551\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.536907\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.412930\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.221607\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.370989\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.493172\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.410938\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.529438\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.395589\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.373069\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.334427\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.356782\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.425807\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.303067\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.322910\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.298522\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.642604\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.310942\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.365036\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.370259\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.459885\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.392440\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.295779\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.293245\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.431490\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.224692\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.343749\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.275312\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.362203\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.261465\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.329377\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.442484\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.251325\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.352394\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.273463\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.451240\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.334194\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.565997\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.132632\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.341808\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.559139\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.228598\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.292671\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.544051\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.391751\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.224537\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.327381\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.400021\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.346943\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.201466\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.518329\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.478693\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.318679\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.615733\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.511660\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.639943\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.340172\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.389618\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.153083\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.219045\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.137551\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.314289\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.354764\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.199855\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.406206\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.416747\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.272491\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.181228\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 2.108765\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 1.666029\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 1.328953\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 1.579924\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 1.464042\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 1.445573\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 1.365558\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 1.121291\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 1.384511\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 0.976736\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 1.532379\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 1.286011\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 1.285729\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 1.202725\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 1.300555\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 1.310968\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 1.295432\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 2.020911\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 1.636979\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.239009\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 1.680640\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 0.969500\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 1.642845\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 1.084117\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 0.921374\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 0.865408\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 1.203751\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 1.620099\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 1.103856\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 1.464336\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 0.790311\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 1.004630\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 1.297622\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 1.013232\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.920786\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 0.957044\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 1.220279\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 1.373067\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 1.241039\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 1.008252\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 1.263093\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 1.021354\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 1.263590\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 1.045045\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 1.256169\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 0.948438\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.788199\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 1.117500\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 1.005948\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 1.107594\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 1.243446\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 1.113683\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 1.017879\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 0.718873\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 1.038638\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 1.178508\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 1.227488\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.975049\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 0.965761\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.685145\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.970592\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 1.010982\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 1.097257\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.848086\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.883941\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 0.948499\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 1.064008\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 0.931033\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 0.945280\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.983622\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 1.017169\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 1.149822\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 0.896029\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 1.096054\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 0.774154\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 1.194047\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.861469\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.978149\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 1.138480\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 0.940188\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 1.021887\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 0.928855\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 0.992548\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 1.085670\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 0.733898\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.953267\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.805974\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.844332\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 0.985140\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.899825\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 0.690120\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 1.115208\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 0.718866\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.956508\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.891263\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.931119\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 1.142084\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 0.772730\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 1.173674\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 1.008414\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.752221\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 1.217699\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 1.014054\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.735607\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.963606\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.971137\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 0.715798\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.910119\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.876114\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 1.004092\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.783585\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.999355\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.927959\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 1.174115\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 1.132630\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 1.097982\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.865868\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.765575\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.678550\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.987839\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 1.021368\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.759063\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.924423\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.876632\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.876565\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.934805\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 1.038952\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.967338\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.694963\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.732148\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.905223\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.857937\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.802451\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.843848\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.928843\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 1.296520\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 0.915369\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.768891\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.873245\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 1.005868\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.686365\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.869418\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 1.052593\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 0.780736\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.676841\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.891322\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.924042\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.940295\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 0.865306\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.925421\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.969075\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.935579\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.533838\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 2.107919\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.827082\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.328848\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.277497\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.470102\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.362771\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.383410\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.343994\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.582285\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.270878\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.304094\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.403240\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.571347\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.368287\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.622089\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.731139\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.425938\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.291427\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.573783\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.405412\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.258168\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.537174\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.332339\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.232993\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.377230\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.457046\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.579546\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.202406\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.500582\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.522783\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.553305\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.212508\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.408414\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.313365\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.303020\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.172078\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.400922\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.850454\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.349773\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.267245\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.297374\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.282996\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.236913\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.302845\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.517462\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.471669\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.315074\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.603607\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.529274\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.295033\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.400723\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.172699\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.202941\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.584318\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.332031\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.244740\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.349600\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.161448\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.246224\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.287915\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.208929\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.404996\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.409677\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.307891\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.249224\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.794813\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.479791\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.350044\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.261561\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.120899\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.319544\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.266035\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.272637\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.301712\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.272149\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.325028\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.159145\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.726462\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.257416\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.201808\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.369814\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.436993\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.327486\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.458024\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.197394\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.349243\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.311168\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.263113\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.222532\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.205932\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.251326\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.176024\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.133980\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.320660\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.239520\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.431429\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.336246\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.273567\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.208569\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.264809\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.443389\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.160738\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 2.185446\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 1.416080\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 1.434147\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 1.321195\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 1.444420\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 1.242980\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 1.331320\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 1.348865\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 1.196390\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 1.121665\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 1.086922\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 1.221145\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 1.356728\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 1.097061\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 1.319326\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 1.131381\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 1.165312\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 1.068126\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 0.747777\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 1.258038\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 0.957100\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 0.931027\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 1.125586\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 1.076229\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 1.097504\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.886471\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.966918\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 0.811399\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 1.147363\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 1.084506\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 0.993914\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 0.781654\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 1.023958\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.880454\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 0.981624\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.690347\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 0.959744\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 1.188946\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.944144\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 1.169176\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 1.414381\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 1.066157\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 0.876328\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 0.942302\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 0.958042\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.906326\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 0.925572\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.919585\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.782426\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 0.904711\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 1.008155\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 1.120887\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 1.174181\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.971571\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.992930\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.965532\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.853793\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 1.076591\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.917088\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.792392\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.834862\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.696036\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 1.144431\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 0.757169\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 0.902093\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.762525\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.883819\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 1.168045\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 1.094963\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.817696\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.935265\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 0.877486\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.955149\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 1.012183\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 1.209380\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 1.017766\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.998031\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 0.903619\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.755458\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.699646\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 1.001848\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.773968\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.827393\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.827412\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 0.799755\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 0.908589\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 1.121798\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 1.038625\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 1.004049\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 1.066690\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 0.946207\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 1.041874\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.977521\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.879446\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.864846\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 0.766654\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.952993\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 0.744350\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.898507\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.950512\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.749166\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 1.070822\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.773212\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.875229\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 1.019154\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 0.727815\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.971164\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 1.105119\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.942939\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.837275\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.660590\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.994013\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.849182\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.817886\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.957135\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.832654\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 1.219716\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.777729\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 0.882724\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.723812\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 0.771070\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 0.895664\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.947543\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.807527\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 1.199360\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.798790\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.759553\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.995287\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.735254\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.816442\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 0.886366\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 1.013534\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.874550\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.689943\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.805689\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.977120\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.853688\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 1.009961\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.789714\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.673843\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.982979\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.835091\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.869025\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.969016\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.861758\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.925201\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.991816\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 1.103600\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 0.978542\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.751578\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 1.074210\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.660609\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.968801\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.893151\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.863045\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.997997\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.935495\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.931638\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.907361\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 1.164652\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.880166\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.842439\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 1.014727\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.735971\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.917600\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.820649\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.769132\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.726193\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.767059\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 1.109835\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.592354\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.690207\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.879866\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.787467\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.774512\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 1.164028\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.834872\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 1.017137\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.640446\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 1.007421\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.644254\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.898063\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.722036\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.612430\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 0.945827\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.841638\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 0.662003\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.974787\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.626527\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.714361\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.769480\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.776958\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.922406\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.875610\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.750931\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.865039\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.977723\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.764576\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.825659\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.905173\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.872071\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.606188\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.872838\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.723303\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.949646\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.981780\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.495377\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.447784\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.877196\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 0.985519\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.464417\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.704211\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.947358\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.907840\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.719733\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.807239\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.815681\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.665713\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.709033\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.978909\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.811886\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.741409\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.834222\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.653344\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.692822\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.757737\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.629515\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.673445\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.930757\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.802297\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.710178\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.803070\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.737949\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.782946\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.968130\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.874243\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.659077\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.977471\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.856431\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.618786\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.807954\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.696653\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.700160\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.613605\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.605056\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.760804\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.671964\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.647038\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.861692\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.858464\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.799804\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.849213\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.666895\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.743811\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.806453\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.990258\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 1.306559\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 1.395833\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 1.030849\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 1.086792\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 0.929621\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 1.031273\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 0.915054\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 1.156837\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 1.097393\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.901565\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 0.950816\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 0.863398\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 1.053045\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 0.840430\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.990375\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 1.217561\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 1.253405\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.910560\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.914728\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.935021\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 1.016381\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.865479\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 1.064480\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 1.025587\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 1.199968\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 0.955969\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.916395\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 1.064155\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 0.966940\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.850346\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.718774\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 1.046742\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 0.604196\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 1.117496\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 1.033853\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 1.121666\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.818265\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.848089\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 1.171352\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.785472\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.857082\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 1.014454\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.752099\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 0.975557\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.825604\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.860233\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.866905\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.963586\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 1.249070\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.692484\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 1.078884\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 1.003851\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.811732\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.781808\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 0.804981\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 1.085630\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.958225\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.800332\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 1.070158\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.720717\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.943866\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.975218\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.960665\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.735707\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.812937\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.774371\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.648074\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.735062\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.828352\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.945724\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.753759\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.647855\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.724456\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.840422\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.757726\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 1.066652\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.867573\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.840576\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.690108\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.576359\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.819466\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.857145\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.853424\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.733270\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.825972\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.769663\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.929952\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.648043\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.695572\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.946439\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.928971\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.437126\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.870426\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.903379\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.818485\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.903205\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.389147\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.934729\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.899426\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.943463\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.997319\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/8838 (0%)]\tLoss: 2.431897\n",
      "Train Epoch: 1 [640/8838 (7%)]\tLoss: 1.458904\n",
      "Train Epoch: 1 [1280/8838 (14%)]\tLoss: 1.147503\n",
      "Train Epoch: 1 [1920/8838 (22%)]\tLoss: 1.140808\n",
      "Train Epoch: 1 [2560/8838 (29%)]\tLoss: 0.999940\n",
      "Train Epoch: 1 [3200/8838 (36%)]\tLoss: 0.969203\n",
      "Train Epoch: 1 [3840/8838 (43%)]\tLoss: 1.080791\n",
      "Train Epoch: 1 [4480/8838 (50%)]\tLoss: 0.827875\n",
      "Train Epoch: 1 [5120/8838 (58%)]\tLoss: 1.016420\n",
      "Train Epoch: 1 [5760/8838 (65%)]\tLoss: 0.920704\n",
      "Train Epoch: 1 [6400/8838 (72%)]\tLoss: 0.814411\n",
      "Train Epoch: 1 [7040/8838 (79%)]\tLoss: 0.924696\n",
      "Train Epoch: 1 [7680/8838 (86%)]\tLoss: 0.899022\n",
      "Train Epoch: 1 [8320/8838 (94%)]\tLoss: 0.780589\n",
      "Train Epoch: 2 [0/8838 (0%)]\tLoss: 0.789914\n",
      "Train Epoch: 2 [640/8838 (7%)]\tLoss: 0.825375\n",
      "Train Epoch: 2 [1280/8838 (14%)]\tLoss: 0.761570\n",
      "Train Epoch: 2 [1920/8838 (22%)]\tLoss: 0.842392\n",
      "Train Epoch: 2 [2560/8838 (29%)]\tLoss: 0.975698\n",
      "Train Epoch: 2 [3200/8838 (36%)]\tLoss: 0.815269\n",
      "Train Epoch: 2 [3840/8838 (43%)]\tLoss: 0.771108\n",
      "Train Epoch: 2 [4480/8838 (50%)]\tLoss: 1.135961\n",
      "Train Epoch: 2 [5120/8838 (58%)]\tLoss: 0.681174\n",
      "Train Epoch: 2 [5760/8838 (65%)]\tLoss: 0.844120\n",
      "Train Epoch: 2 [6400/8838 (72%)]\tLoss: 0.941990\n",
      "Train Epoch: 2 [7040/8838 (79%)]\tLoss: 0.743051\n",
      "Train Epoch: 2 [7680/8838 (86%)]\tLoss: 0.740605\n",
      "Train Epoch: 2 [8320/8838 (94%)]\tLoss: 0.820436\n",
      "Train Epoch: 3 [0/8838 (0%)]\tLoss: 1.187423\n",
      "Train Epoch: 3 [640/8838 (7%)]\tLoss: 1.035923\n",
      "Train Epoch: 3 [1280/8838 (14%)]\tLoss: 0.704132\n",
      "Train Epoch: 3 [1920/8838 (22%)]\tLoss: 0.847452\n",
      "Train Epoch: 3 [2560/8838 (29%)]\tLoss: 0.668301\n",
      "Train Epoch: 3 [3200/8838 (36%)]\tLoss: 0.894108\n",
      "Train Epoch: 3 [3840/8838 (43%)]\tLoss: 0.956909\n",
      "Train Epoch: 3 [4480/8838 (50%)]\tLoss: 0.787438\n",
      "Train Epoch: 3 [5120/8838 (58%)]\tLoss: 0.806636\n",
      "Train Epoch: 3 [5760/8838 (65%)]\tLoss: 0.621058\n",
      "Train Epoch: 3 [6400/8838 (72%)]\tLoss: 0.748275\n",
      "Train Epoch: 3 [7040/8838 (79%)]\tLoss: 1.006329\n",
      "Train Epoch: 3 [7680/8838 (86%)]\tLoss: 0.975263\n",
      "Train Epoch: 3 [8320/8838 (94%)]\tLoss: 0.813641\n",
      "Train Epoch: 4 [0/8838 (0%)]\tLoss: 0.606944\n",
      "Train Epoch: 4 [640/8838 (7%)]\tLoss: 0.839424\n",
      "Train Epoch: 4 [1280/8838 (14%)]\tLoss: 0.839491\n",
      "Train Epoch: 4 [1920/8838 (22%)]\tLoss: 0.688181\n",
      "Train Epoch: 4 [2560/8838 (29%)]\tLoss: 0.889014\n",
      "Train Epoch: 4 [3200/8838 (36%)]\tLoss: 0.780670\n",
      "Train Epoch: 4 [3840/8838 (43%)]\tLoss: 0.562748\n",
      "Train Epoch: 4 [4480/8838 (50%)]\tLoss: 1.128890\n",
      "Train Epoch: 4 [5120/8838 (58%)]\tLoss: 0.723790\n",
      "Train Epoch: 4 [5760/8838 (65%)]\tLoss: 0.770060\n",
      "Train Epoch: 4 [6400/8838 (72%)]\tLoss: 0.647092\n",
      "Train Epoch: 4 [7040/8838 (79%)]\tLoss: 0.668627\n",
      "Train Epoch: 4 [7680/8838 (86%)]\tLoss: 0.780227\n",
      "Train Epoch: 4 [8320/8838 (94%)]\tLoss: 0.668117\n",
      "Train Epoch: 5 [0/8838 (0%)]\tLoss: 0.847635\n",
      "Train Epoch: 5 [640/8838 (7%)]\tLoss: 0.853366\n",
      "Train Epoch: 5 [1280/8838 (14%)]\tLoss: 0.661617\n",
      "Train Epoch: 5 [1920/8838 (22%)]\tLoss: 0.821731\n",
      "Train Epoch: 5 [2560/8838 (29%)]\tLoss: 0.506737\n",
      "Train Epoch: 5 [3200/8838 (36%)]\tLoss: 0.704041\n",
      "Train Epoch: 5 [3840/8838 (43%)]\tLoss: 0.608166\n",
      "Train Epoch: 5 [4480/8838 (50%)]\tLoss: 0.588008\n",
      "Train Epoch: 5 [5120/8838 (58%)]\tLoss: 0.674228\n",
      "Train Epoch: 5 [5760/8838 (65%)]\tLoss: 0.776449\n",
      "Train Epoch: 5 [6400/8838 (72%)]\tLoss: 0.605581\n",
      "Train Epoch: 5 [7040/8838 (79%)]\tLoss: 0.685307\n",
      "Train Epoch: 5 [7680/8838 (86%)]\tLoss: 0.881931\n",
      "Train Epoch: 5 [8320/8838 (94%)]\tLoss: 0.761675\n",
      "Train Epoch: 6 [0/8838 (0%)]\tLoss: 0.571986\n",
      "Train Epoch: 6 [640/8838 (7%)]\tLoss: 0.714185\n",
      "Train Epoch: 6 [1280/8838 (14%)]\tLoss: 0.546774\n",
      "Train Epoch: 6 [1920/8838 (22%)]\tLoss: 0.786514\n",
      "Train Epoch: 6 [2560/8838 (29%)]\tLoss: 0.736917\n",
      "Train Epoch: 6 [3200/8838 (36%)]\tLoss: 0.742855\n",
      "Train Epoch: 6 [3840/8838 (43%)]\tLoss: 0.701407\n",
      "Train Epoch: 6 [4480/8838 (50%)]\tLoss: 0.888119\n",
      "Train Epoch: 6 [5120/8838 (58%)]\tLoss: 0.882154\n",
      "Train Epoch: 6 [5760/8838 (65%)]\tLoss: 0.481197\n",
      "Train Epoch: 6 [6400/8838 (72%)]\tLoss: 0.637282\n",
      "Train Epoch: 6 [7040/8838 (79%)]\tLoss: 0.725730\n",
      "Train Epoch: 6 [7680/8838 (86%)]\tLoss: 0.867007\n",
      "Train Epoch: 6 [8320/8838 (94%)]\tLoss: 0.786779\n",
      "Train Epoch: 7 [0/8838 (0%)]\tLoss: 0.777691\n",
      "Train Epoch: 7 [640/8838 (7%)]\tLoss: 0.567211\n",
      "Train Epoch: 7 [1280/8838 (14%)]\tLoss: 0.739414\n",
      "Train Epoch: 7 [1920/8838 (22%)]\tLoss: 0.536602\n",
      "Train Epoch: 7 [2560/8838 (29%)]\tLoss: 0.800253\n",
      "Train Epoch: 7 [3200/8838 (36%)]\tLoss: 0.700295\n",
      "Train Epoch: 7 [3840/8838 (43%)]\tLoss: 0.661641\n",
      "Train Epoch: 7 [4480/8838 (50%)]\tLoss: 0.681009\n",
      "Train Epoch: 7 [5120/8838 (58%)]\tLoss: 0.695367\n",
      "Train Epoch: 7 [5760/8838 (65%)]\tLoss: 0.707571\n",
      "Train Epoch: 7 [6400/8838 (72%)]\tLoss: 0.792022\n",
      "Train Epoch: 7 [7040/8838 (79%)]\tLoss: 0.766331\n",
      "Train Epoch: 7 [7680/8838 (86%)]\tLoss: 0.655215\n",
      "Train Epoch: 7 [8320/8838 (94%)]\tLoss: 0.747648\n",
      "Train Epoch: 8 [0/8838 (0%)]\tLoss: 0.637483\n",
      "Train Epoch: 8 [640/8838 (7%)]\tLoss: 0.749838\n",
      "Train Epoch: 8 [1280/8838 (14%)]\tLoss: 0.877021\n",
      "Train Epoch: 8 [1920/8838 (22%)]\tLoss: 0.676960\n",
      "Train Epoch: 8 [2560/8838 (29%)]\tLoss: 0.507546\n",
      "Train Epoch: 8 [3200/8838 (36%)]\tLoss: 0.597748\n",
      "Train Epoch: 8 [3840/8838 (43%)]\tLoss: 0.961321\n",
      "Train Epoch: 8 [4480/8838 (50%)]\tLoss: 0.875581\n",
      "Train Epoch: 8 [5120/8838 (58%)]\tLoss: 0.771315\n",
      "Train Epoch: 8 [5760/8838 (65%)]\tLoss: 0.677160\n",
      "Train Epoch: 8 [6400/8838 (72%)]\tLoss: 0.704929\n",
      "Train Epoch: 8 [7040/8838 (79%)]\tLoss: 0.559595\n",
      "Train Epoch: 8 [7680/8838 (86%)]\tLoss: 0.550183\n",
      "Train Epoch: 8 [8320/8838 (94%)]\tLoss: 0.641451\n",
      "Train Epoch: 9 [0/8838 (0%)]\tLoss: 0.904259\n",
      "Train Epoch: 9 [640/8838 (7%)]\tLoss: 0.590214\n",
      "Train Epoch: 9 [1280/8838 (14%)]\tLoss: 0.654036\n",
      "Train Epoch: 9 [1920/8838 (22%)]\tLoss: 0.769792\n",
      "Train Epoch: 9 [2560/8838 (29%)]\tLoss: 0.606209\n",
      "Train Epoch: 9 [3200/8838 (36%)]\tLoss: 0.695154\n",
      "Train Epoch: 9 [3840/8838 (43%)]\tLoss: 0.711219\n",
      "Train Epoch: 9 [4480/8838 (50%)]\tLoss: 1.243464\n",
      "Train Epoch: 9 [5120/8838 (58%)]\tLoss: 0.716650\n",
      "Train Epoch: 9 [5760/8838 (65%)]\tLoss: 0.577317\n",
      "Train Epoch: 9 [6400/8838 (72%)]\tLoss: 0.735532\n",
      "Train Epoch: 9 [7040/8838 (79%)]\tLoss: 0.762785\n",
      "Train Epoch: 9 [7680/8838 (86%)]\tLoss: 0.790951\n",
      "Train Epoch: 9 [8320/8838 (94%)]\tLoss: 0.650120\n",
      "Train Epoch: 10 [0/8838 (0%)]\tLoss: 0.594428\n",
      "Train Epoch: 10 [640/8838 (7%)]\tLoss: 0.992592\n",
      "Train Epoch: 10 [1280/8838 (14%)]\tLoss: 0.552100\n",
      "Train Epoch: 10 [1920/8838 (22%)]\tLoss: 0.839598\n",
      "Train Epoch: 10 [2560/8838 (29%)]\tLoss: 0.660336\n",
      "Train Epoch: 10 [3200/8838 (36%)]\tLoss: 0.661194\n",
      "Train Epoch: 10 [3840/8838 (43%)]\tLoss: 0.450200\n",
      "Train Epoch: 10 [4480/8838 (50%)]\tLoss: 0.511999\n",
      "Train Epoch: 10 [5120/8838 (58%)]\tLoss: 0.767157\n",
      "Train Epoch: 10 [5760/8838 (65%)]\tLoss: 0.564327\n",
      "Train Epoch: 10 [6400/8838 (72%)]\tLoss: 0.733513\n",
      "Train Epoch: 10 [7040/8838 (79%)]\tLoss: 0.780618\n",
      "Train Epoch: 10 [7680/8838 (86%)]\tLoss: 0.737129\n",
      "Train Epoch: 10 [8320/8838 (94%)]\tLoss: 0.675778\n",
      "Train Epoch: 11 [0/8838 (0%)]\tLoss: 0.927662\n",
      "Train Epoch: 11 [640/8838 (7%)]\tLoss: 0.712343\n",
      "Train Epoch: 11 [1280/8838 (14%)]\tLoss: 0.845729\n",
      "Train Epoch: 11 [1920/8838 (22%)]\tLoss: 0.821294\n",
      "Train Epoch: 11 [2560/8838 (29%)]\tLoss: 0.533065\n",
      "Train Epoch: 11 [3200/8838 (36%)]\tLoss: 0.743651\n",
      "Train Epoch: 11 [3840/8838 (43%)]\tLoss: 0.630654\n",
      "Train Epoch: 11 [4480/8838 (50%)]\tLoss: 0.555545\n",
      "Train Epoch: 11 [5120/8838 (58%)]\tLoss: 0.711669\n",
      "Train Epoch: 11 [5760/8838 (65%)]\tLoss: 0.754165\n",
      "Train Epoch: 11 [6400/8838 (72%)]\tLoss: 0.580634\n",
      "Train Epoch: 11 [7040/8838 (79%)]\tLoss: 0.611215\n",
      "Train Epoch: 11 [7680/8838 (86%)]\tLoss: 0.587000\n",
      "Train Epoch: 11 [8320/8838 (94%)]\tLoss: 0.801602\n",
      "Train Epoch: 12 [0/8838 (0%)]\tLoss: 0.498962\n",
      "Train Epoch: 12 [640/8838 (7%)]\tLoss: 0.630928\n",
      "Train Epoch: 12 [1280/8838 (14%)]\tLoss: 0.636933\n",
      "Train Epoch: 12 [1920/8838 (22%)]\tLoss: 0.781685\n",
      "Train Epoch: 12 [2560/8838 (29%)]\tLoss: 0.772880\n",
      "Train Epoch: 12 [3200/8838 (36%)]\tLoss: 0.694327\n",
      "Train Epoch: 12 [3840/8838 (43%)]\tLoss: 0.603335\n",
      "Train Epoch: 12 [4480/8838 (50%)]\tLoss: 0.824948\n",
      "Train Epoch: 12 [5120/8838 (58%)]\tLoss: 0.726628\n",
      "Train Epoch: 12 [5760/8838 (65%)]\tLoss: 0.787507\n",
      "Train Epoch: 12 [6400/8838 (72%)]\tLoss: 0.501002\n",
      "Train Epoch: 12 [7040/8838 (79%)]\tLoss: 0.670988\n",
      "Train Epoch: 12 [7680/8838 (86%)]\tLoss: 0.676048\n",
      "Train Epoch: 12 [8320/8838 (94%)]\tLoss: 0.893867\n",
      "Train Epoch: 13 [0/8838 (0%)]\tLoss: 0.805877\n",
      "Train Epoch: 13 [640/8838 (7%)]\tLoss: 0.727229\n",
      "Train Epoch: 13 [1280/8838 (14%)]\tLoss: 0.707853\n",
      "Train Epoch: 13 [1920/8838 (22%)]\tLoss: 0.908268\n",
      "Train Epoch: 13 [2560/8838 (29%)]\tLoss: 0.651786\n",
      "Train Epoch: 13 [3200/8838 (36%)]\tLoss: 0.532943\n",
      "Train Epoch: 13 [3840/8838 (43%)]\tLoss: 0.663958\n",
      "Train Epoch: 13 [4480/8838 (50%)]\tLoss: 0.647641\n",
      "Train Epoch: 13 [5120/8838 (58%)]\tLoss: 0.917110\n",
      "Train Epoch: 13 [5760/8838 (65%)]\tLoss: 0.596022\n",
      "Train Epoch: 13 [6400/8838 (72%)]\tLoss: 0.613186\n",
      "Train Epoch: 13 [7040/8838 (79%)]\tLoss: 0.936205\n",
      "Train Epoch: 13 [7680/8838 (86%)]\tLoss: 0.494239\n",
      "Train Epoch: 13 [8320/8838 (94%)]\tLoss: 0.724881\n",
      "Train Epoch: 14 [0/8838 (0%)]\tLoss: 0.677142\n",
      "Train Epoch: 14 [640/8838 (7%)]\tLoss: 0.598398\n",
      "Train Epoch: 14 [1280/8838 (14%)]\tLoss: 0.539426\n",
      "Train Epoch: 14 [1920/8838 (22%)]\tLoss: 0.810858\n",
      "Train Epoch: 14 [2560/8838 (29%)]\tLoss: 0.666862\n",
      "Train Epoch: 14 [3200/8838 (36%)]\tLoss: 0.762113\n",
      "Train Epoch: 14 [3840/8838 (43%)]\tLoss: 0.627940\n",
      "Train Epoch: 14 [4480/8838 (50%)]\tLoss: 0.637539\n",
      "Train Epoch: 14 [5120/8838 (58%)]\tLoss: 0.721671\n",
      "Train Epoch: 14 [5760/8838 (65%)]\tLoss: 0.734794\n",
      "Train Epoch: 14 [6400/8838 (72%)]\tLoss: 0.593808\n",
      "Train Epoch: 14 [7040/8838 (79%)]\tLoss: 0.669593\n",
      "Train Epoch: 14 [7680/8838 (86%)]\tLoss: 0.814570\n",
      "Train Epoch: 14 [8320/8838 (94%)]\tLoss: 0.535579\n",
      "Train Epoch: 15 [0/8838 (0%)]\tLoss: 0.542067\n",
      "Train Epoch: 15 [640/8838 (7%)]\tLoss: 0.629825\n",
      "Train Epoch: 15 [1280/8838 (14%)]\tLoss: 0.798170\n",
      "Train Epoch: 15 [1920/8838 (22%)]\tLoss: 0.518149\n",
      "Train Epoch: 15 [2560/8838 (29%)]\tLoss: 0.450905\n",
      "Train Epoch: 15 [3200/8838 (36%)]\tLoss: 0.546424\n",
      "Train Epoch: 15 [3840/8838 (43%)]\tLoss: 0.538960\n",
      "Train Epoch: 15 [4480/8838 (50%)]\tLoss: 0.569663\n",
      "Train Epoch: 15 [5120/8838 (58%)]\tLoss: 0.974523\n",
      "Train Epoch: 15 [5760/8838 (65%)]\tLoss: 0.765406\n",
      "Train Epoch: 15 [6400/8838 (72%)]\tLoss: 0.597738\n",
      "Train Epoch: 15 [7040/8838 (79%)]\tLoss: 0.507544\n",
      "Train Epoch: 15 [7680/8838 (86%)]\tLoss: 0.940077\n",
      "Train Epoch: 15 [8320/8838 (94%)]\tLoss: 0.583624\n",
      "Train Epoch: 16 [0/8838 (0%)]\tLoss: 0.584867\n",
      "Train Epoch: 16 [640/8838 (7%)]\tLoss: 0.780932\n",
      "Train Epoch: 16 [1280/8838 (14%)]\tLoss: 0.760050\n",
      "Train Epoch: 16 [1920/8838 (22%)]\tLoss: 0.578649\n",
      "Train Epoch: 16 [2560/8838 (29%)]\tLoss: 0.915807\n",
      "Train Epoch: 16 [3200/8838 (36%)]\tLoss: 0.608720\n",
      "Train Epoch: 16 [3840/8838 (43%)]\tLoss: 0.721740\n",
      "Train Epoch: 16 [4480/8838 (50%)]\tLoss: 0.765096\n",
      "Train Epoch: 16 [5120/8838 (58%)]\tLoss: 0.648442\n",
      "Train Epoch: 16 [5760/8838 (65%)]\tLoss: 0.721065\n",
      "Train Epoch: 16 [6400/8838 (72%)]\tLoss: 0.565950\n",
      "Train Epoch: 16 [7040/8838 (79%)]\tLoss: 0.602556\n",
      "Train Epoch: 16 [7680/8838 (86%)]\tLoss: 0.618895\n",
      "Train Epoch: 16 [8320/8838 (94%)]\tLoss: 0.756476\n",
      "Train Epoch: 17 [0/8838 (0%)]\tLoss: 0.623805\n",
      "Train Epoch: 17 [640/8838 (7%)]\tLoss: 0.564682\n",
      "Train Epoch: 17 [1280/8838 (14%)]\tLoss: 0.620872\n",
      "Train Epoch: 17 [1920/8838 (22%)]\tLoss: 0.600961\n",
      "Train Epoch: 17 [2560/8838 (29%)]\tLoss: 0.725615\n",
      "Train Epoch: 17 [3200/8838 (36%)]\tLoss: 0.835393\n",
      "Train Epoch: 17 [3840/8838 (43%)]\tLoss: 0.663990\n",
      "Train Epoch: 17 [4480/8838 (50%)]\tLoss: 0.693643\n",
      "Train Epoch: 17 [5120/8838 (58%)]\tLoss: 0.625500\n",
      "Train Epoch: 17 [5760/8838 (65%)]\tLoss: 0.739313\n",
      "Train Epoch: 17 [6400/8838 (72%)]\tLoss: 0.597109\n",
      "Train Epoch: 17 [7040/8838 (79%)]\tLoss: 0.564932\n",
      "Train Epoch: 17 [7680/8838 (86%)]\tLoss: 0.454313\n",
      "Train Epoch: 17 [8320/8838 (94%)]\tLoss: 0.786359\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/6366 (0%)]\tLoss: 2.307431\n",
      "Train Epoch: 1 [640/6366 (10%)]\tLoss: 0.882168\n",
      "Train Epoch: 1 [1280/6366 (20%)]\tLoss: 0.855651\n",
      "Train Epoch: 1 [1920/6366 (30%)]\tLoss: 0.795582\n",
      "Train Epoch: 1 [2560/6366 (40%)]\tLoss: 1.163024\n",
      "Train Epoch: 1 [3200/6366 (50%)]\tLoss: 1.036677\n",
      "Train Epoch: 1 [3840/6366 (60%)]\tLoss: 0.768228\n",
      "Train Epoch: 1 [4480/6366 (70%)]\tLoss: 0.499641\n",
      "Train Epoch: 1 [5120/6366 (80%)]\tLoss: 0.526520\n",
      "Train Epoch: 1 [5760/6366 (90%)]\tLoss: 0.571843\n",
      "Train Epoch: 2 [0/6366 (0%)]\tLoss: 0.476003\n",
      "Train Epoch: 2 [640/6366 (10%)]\tLoss: 0.571607\n",
      "Train Epoch: 2 [1280/6366 (20%)]\tLoss: 0.735987\n",
      "Train Epoch: 2 [1920/6366 (30%)]\tLoss: 0.841577\n",
      "Train Epoch: 2 [2560/6366 (40%)]\tLoss: 0.574647\n",
      "Train Epoch: 2 [3200/6366 (50%)]\tLoss: 0.521599\n",
      "Train Epoch: 2 [3840/6366 (60%)]\tLoss: 0.532292\n",
      "Train Epoch: 2 [4480/6366 (70%)]\tLoss: 0.581378\n",
      "Train Epoch: 2 [5120/6366 (80%)]\tLoss: 0.562581\n",
      "Train Epoch: 2 [5760/6366 (90%)]\tLoss: 0.663100\n",
      "Train Epoch: 3 [0/6366 (0%)]\tLoss: 0.546575\n",
      "Train Epoch: 3 [640/6366 (10%)]\tLoss: 0.705231\n",
      "Train Epoch: 3 [1280/6366 (20%)]\tLoss: 0.792291\n",
      "Train Epoch: 3 [1920/6366 (30%)]\tLoss: 0.367442\n",
      "Train Epoch: 3 [2560/6366 (40%)]\tLoss: 0.527274\n",
      "Train Epoch: 3 [3200/6366 (50%)]\tLoss: 0.683283\n",
      "Train Epoch: 3 [3840/6366 (60%)]\tLoss: 0.605091\n",
      "Train Epoch: 3 [4480/6366 (70%)]\tLoss: 0.456620\n",
      "Train Epoch: 3 [5120/6366 (80%)]\tLoss: 0.539501\n",
      "Train Epoch: 3 [5760/6366 (90%)]\tLoss: 0.442372\n",
      "Train Epoch: 4 [0/6366 (0%)]\tLoss: 0.701912\n",
      "Train Epoch: 4 [640/6366 (10%)]\tLoss: 0.566278\n",
      "Train Epoch: 4 [1280/6366 (20%)]\tLoss: 0.874960\n",
      "Train Epoch: 4 [1920/6366 (30%)]\tLoss: 0.667171\n",
      "Train Epoch: 4 [2560/6366 (40%)]\tLoss: 0.471041\n",
      "Train Epoch: 4 [3200/6366 (50%)]\tLoss: 0.736534\n",
      "Train Epoch: 4 [3840/6366 (60%)]\tLoss: 0.491181\n",
      "Train Epoch: 4 [4480/6366 (70%)]\tLoss: 0.617134\n",
      "Train Epoch: 4 [5120/6366 (80%)]\tLoss: 0.403546\n",
      "Train Epoch: 4 [5760/6366 (90%)]\tLoss: 0.635330\n",
      "Train Epoch: 5 [0/6366 (0%)]\tLoss: 0.261390\n",
      "Train Epoch: 5 [640/6366 (10%)]\tLoss: 0.428544\n",
      "Train Epoch: 5 [1280/6366 (20%)]\tLoss: 0.519563\n",
      "Train Epoch: 5 [1920/6366 (30%)]\tLoss: 0.723957\n",
      "Train Epoch: 5 [2560/6366 (40%)]\tLoss: 0.371078\n",
      "Train Epoch: 5 [3200/6366 (50%)]\tLoss: 0.426312\n",
      "Train Epoch: 5 [3840/6366 (60%)]\tLoss: 0.462294\n",
      "Train Epoch: 5 [4480/6366 (70%)]\tLoss: 0.695876\n",
      "Train Epoch: 5 [5120/6366 (80%)]\tLoss: 0.747422\n",
      "Train Epoch: 5 [5760/6366 (90%)]\tLoss: 0.579753\n",
      "Train Epoch: 6 [0/6366 (0%)]\tLoss: 0.284874\n",
      "Train Epoch: 6 [640/6366 (10%)]\tLoss: 0.470599\n",
      "Train Epoch: 6 [1280/6366 (20%)]\tLoss: 0.481321\n",
      "Train Epoch: 6 [1920/6366 (30%)]\tLoss: 0.533249\n",
      "Train Epoch: 6 [2560/6366 (40%)]\tLoss: 0.370493\n",
      "Train Epoch: 6 [3200/6366 (50%)]\tLoss: 0.571158\n",
      "Train Epoch: 6 [3840/6366 (60%)]\tLoss: 0.461831\n",
      "Train Epoch: 6 [4480/6366 (70%)]\tLoss: 0.497775\n",
      "Train Epoch: 6 [5120/6366 (80%)]\tLoss: 0.358209\n",
      "Train Epoch: 6 [5760/6366 (90%)]\tLoss: 0.712060\n",
      "Train Epoch: 7 [0/6366 (0%)]\tLoss: 0.404249\n",
      "Train Epoch: 7 [640/6366 (10%)]\tLoss: 0.436024\n",
      "Train Epoch: 7 [1280/6366 (20%)]\tLoss: 0.661036\n",
      "Train Epoch: 7 [1920/6366 (30%)]\tLoss: 0.344735\n",
      "Train Epoch: 7 [2560/6366 (40%)]\tLoss: 0.376247\n",
      "Train Epoch: 7 [3200/6366 (50%)]\tLoss: 0.293090\n",
      "Train Epoch: 7 [3840/6366 (60%)]\tLoss: 0.491021\n",
      "Train Epoch: 7 [4480/6366 (70%)]\tLoss: 0.494446\n",
      "Train Epoch: 7 [5120/6366 (80%)]\tLoss: 0.382728\n",
      "Train Epoch: 7 [5760/6366 (90%)]\tLoss: 0.452849\n",
      "Train Epoch: 8 [0/6366 (0%)]\tLoss: 0.313917\n",
      "Train Epoch: 8 [640/6366 (10%)]\tLoss: 0.759274\n",
      "Train Epoch: 8 [1280/6366 (20%)]\tLoss: 0.489380\n",
      "Train Epoch: 8 [1920/6366 (30%)]\tLoss: 0.397305\n",
      "Train Epoch: 8 [2560/6366 (40%)]\tLoss: 0.371477\n",
      "Train Epoch: 8 [3200/6366 (50%)]\tLoss: 0.260923\n",
      "Train Epoch: 8 [3840/6366 (60%)]\tLoss: 0.506112\n",
      "Train Epoch: 8 [4480/6366 (70%)]\tLoss: 0.463939\n",
      "Train Epoch: 8 [5120/6366 (80%)]\tLoss: 0.584408\n",
      "Train Epoch: 8 [5760/6366 (90%)]\tLoss: 0.396448\n",
      "Train Epoch: 9 [0/6366 (0%)]\tLoss: 0.340698\n",
      "Train Epoch: 9 [640/6366 (10%)]\tLoss: 0.447455\n",
      "Train Epoch: 9 [1280/6366 (20%)]\tLoss: 0.273084\n",
      "Train Epoch: 9 [1920/6366 (30%)]\tLoss: 0.528376\n",
      "Train Epoch: 9 [2560/6366 (40%)]\tLoss: 0.376546\n",
      "Train Epoch: 9 [3200/6366 (50%)]\tLoss: 0.563493\n",
      "Train Epoch: 9 [3840/6366 (60%)]\tLoss: 0.415220\n",
      "Train Epoch: 9 [4480/6366 (70%)]\tLoss: 0.646904\n",
      "Train Epoch: 9 [5120/6366 (80%)]\tLoss: 0.416829\n",
      "Train Epoch: 9 [5760/6366 (90%)]\tLoss: 0.540998\n",
      "Train Epoch: 10 [0/6366 (0%)]\tLoss: 0.457206\n",
      "Train Epoch: 10 [640/6366 (10%)]\tLoss: 0.582931\n",
      "Train Epoch: 10 [1280/6366 (20%)]\tLoss: 0.319471\n",
      "Train Epoch: 10 [1920/6366 (30%)]\tLoss: 0.316847\n",
      "Train Epoch: 10 [2560/6366 (40%)]\tLoss: 0.457120\n",
      "Train Epoch: 10 [3200/6366 (50%)]\tLoss: 0.645681\n",
      "Train Epoch: 10 [3840/6366 (60%)]\tLoss: 0.231613\n",
      "Train Epoch: 10 [4480/6366 (70%)]\tLoss: 0.444086\n",
      "Train Epoch: 10 [5120/6366 (80%)]\tLoss: 0.363716\n",
      "Train Epoch: 10 [5760/6366 (90%)]\tLoss: 0.462850\n",
      "Train Epoch: 11 [0/6366 (0%)]\tLoss: 0.307662\n",
      "Train Epoch: 11 [640/6366 (10%)]\tLoss: 0.446878\n",
      "Train Epoch: 11 [1280/6366 (20%)]\tLoss: 0.450939\n",
      "Train Epoch: 11 [1920/6366 (30%)]\tLoss: 0.636003\n",
      "Train Epoch: 11 [2560/6366 (40%)]\tLoss: 0.411792\n",
      "Train Epoch: 11 [3200/6366 (50%)]\tLoss: 0.415228\n",
      "Train Epoch: 11 [3840/6366 (60%)]\tLoss: 0.283731\n",
      "Train Epoch: 11 [4480/6366 (70%)]\tLoss: 0.251629\n",
      "Train Epoch: 11 [5120/6366 (80%)]\tLoss: 0.281939\n",
      "Train Epoch: 11 [5760/6366 (90%)]\tLoss: 0.392070\n",
      "Train Epoch: 12 [0/6366 (0%)]\tLoss: 0.306892\n",
      "Train Epoch: 12 [640/6366 (10%)]\tLoss: 0.300651\n",
      "Train Epoch: 12 [1280/6366 (20%)]\tLoss: 0.521474\n",
      "Train Epoch: 12 [1920/6366 (30%)]\tLoss: 0.361937\n",
      "Train Epoch: 12 [2560/6366 (40%)]\tLoss: 0.613195\n",
      "Train Epoch: 12 [3200/6366 (50%)]\tLoss: 0.528231\n",
      "Train Epoch: 12 [3840/6366 (60%)]\tLoss: 0.198618\n",
      "Train Epoch: 12 [4480/6366 (70%)]\tLoss: 0.289792\n",
      "Train Epoch: 12 [5120/6366 (80%)]\tLoss: 0.381959\n",
      "Train Epoch: 12 [5760/6366 (90%)]\tLoss: 0.580266\n",
      "Train Epoch: 13 [0/6366 (0%)]\tLoss: 0.275201\n",
      "Train Epoch: 13 [640/6366 (10%)]\tLoss: 0.265614\n",
      "Train Epoch: 13 [1280/6366 (20%)]\tLoss: 0.361094\n",
      "Train Epoch: 13 [1920/6366 (30%)]\tLoss: 0.290397\n",
      "Train Epoch: 13 [2560/6366 (40%)]\tLoss: 0.811058\n",
      "Train Epoch: 13 [3200/6366 (50%)]\tLoss: 0.354270\n",
      "Train Epoch: 13 [3840/6366 (60%)]\tLoss: 0.504575\n",
      "Train Epoch: 13 [4480/6366 (70%)]\tLoss: 0.306994\n",
      "Train Epoch: 13 [5120/6366 (80%)]\tLoss: 0.344352\n",
      "Train Epoch: 13 [5760/6366 (90%)]\tLoss: 0.348674\n",
      "Train Epoch: 14 [0/6366 (0%)]\tLoss: 0.203719\n",
      "Train Epoch: 14 [640/6366 (10%)]\tLoss: 0.475919\n",
      "Train Epoch: 14 [1280/6366 (20%)]\tLoss: 0.340652\n",
      "Train Epoch: 14 [1920/6366 (30%)]\tLoss: 0.455521\n",
      "Train Epoch: 14 [2560/6366 (40%)]\tLoss: 0.310016\n",
      "Train Epoch: 14 [3200/6366 (50%)]\tLoss: 0.174366\n",
      "Train Epoch: 14 [3840/6366 (60%)]\tLoss: 0.464660\n",
      "Train Epoch: 14 [4480/6366 (70%)]\tLoss: 0.280472\n",
      "Train Epoch: 14 [5120/6366 (80%)]\tLoss: 0.497980\n",
      "Train Epoch: 14 [5760/6366 (90%)]\tLoss: 0.564322\n",
      "Train Epoch: 15 [0/6366 (0%)]\tLoss: 0.347047\n",
      "Train Epoch: 15 [640/6366 (10%)]\tLoss: 0.596458\n",
      "Train Epoch: 15 [1280/6366 (20%)]\tLoss: 0.498895\n",
      "Train Epoch: 15 [1920/6366 (30%)]\tLoss: 0.289840\n",
      "Train Epoch: 15 [2560/6366 (40%)]\tLoss: 0.368375\n",
      "Train Epoch: 15 [3200/6366 (50%)]\tLoss: 0.446723\n",
      "Train Epoch: 15 [3840/6366 (60%)]\tLoss: 0.470253\n",
      "Train Epoch: 15 [4480/6366 (70%)]\tLoss: 0.318049\n",
      "Train Epoch: 15 [5120/6366 (80%)]\tLoss: 0.460699\n",
      "Train Epoch: 15 [5760/6366 (90%)]\tLoss: 0.561137\n",
      "Train Epoch: 16 [0/6366 (0%)]\tLoss: 0.498205\n",
      "Train Epoch: 16 [640/6366 (10%)]\tLoss: 0.345564\n",
      "Train Epoch: 16 [1280/6366 (20%)]\tLoss: 0.419714\n",
      "Train Epoch: 16 [1920/6366 (30%)]\tLoss: 0.439034\n",
      "Train Epoch: 16 [2560/6366 (40%)]\tLoss: 0.406364\n",
      "Train Epoch: 16 [3200/6366 (50%)]\tLoss: 0.743372\n",
      "Train Epoch: 16 [3840/6366 (60%)]\tLoss: 0.813716\n",
      "Train Epoch: 16 [4480/6366 (70%)]\tLoss: 0.357341\n",
      "Train Epoch: 16 [5120/6366 (80%)]\tLoss: 0.514895\n",
      "Train Epoch: 16 [5760/6366 (90%)]\tLoss: 0.353929\n",
      "Train Epoch: 17 [0/6366 (0%)]\tLoss: 0.350000\n",
      "Train Epoch: 17 [640/6366 (10%)]\tLoss: 0.337359\n",
      "Train Epoch: 17 [1280/6366 (20%)]\tLoss: 0.420395\n",
      "Train Epoch: 17 [1920/6366 (30%)]\tLoss: 0.626328\n",
      "Train Epoch: 17 [2560/6366 (40%)]\tLoss: 0.373056\n",
      "Train Epoch: 17 [3200/6366 (50%)]\tLoss: 0.342548\n",
      "Train Epoch: 17 [3840/6366 (60%)]\tLoss: 0.346947\n",
      "Train Epoch: 17 [4480/6366 (70%)]\tLoss: 0.332144\n",
      "Train Epoch: 17 [5120/6366 (80%)]\tLoss: 0.295500\n",
      "Train Epoch: 17 [5760/6366 (90%)]\tLoss: 0.498449\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/9451 (0%)]\tLoss: 2.255222\n",
      "Train Epoch: 1 [640/9451 (7%)]\tLoss: 1.594702\n",
      "Train Epoch: 1 [1280/9451 (14%)]\tLoss: 1.365938\n",
      "Train Epoch: 1 [1920/9451 (20%)]\tLoss: 1.463311\n",
      "Train Epoch: 1 [2560/9451 (27%)]\tLoss: 1.299078\n",
      "Train Epoch: 1 [3200/9451 (34%)]\tLoss: 1.313926\n",
      "Train Epoch: 1 [3840/9451 (41%)]\tLoss: 1.327224\n",
      "Train Epoch: 1 [4480/9451 (47%)]\tLoss: 1.031069\n",
      "Train Epoch: 1 [5120/9451 (54%)]\tLoss: 1.103615\n",
      "Train Epoch: 1 [5760/9451 (61%)]\tLoss: 1.164430\n",
      "Train Epoch: 1 [6400/9451 (68%)]\tLoss: 0.928045\n",
      "Train Epoch: 1 [7040/9451 (74%)]\tLoss: 1.191252\n",
      "Train Epoch: 1 [7680/9451 (81%)]\tLoss: 1.074754\n",
      "Train Epoch: 1 [8320/9451 (88%)]\tLoss: 1.299555\n",
      "Train Epoch: 1 [8960/9451 (95%)]\tLoss: 1.002027\n",
      "Train Epoch: 2 [0/9451 (0%)]\tLoss: 1.191301\n",
      "Train Epoch: 2 [640/9451 (7%)]\tLoss: 1.189116\n",
      "Train Epoch: 2 [1280/9451 (14%)]\tLoss: 0.981612\n",
      "Train Epoch: 2 [1920/9451 (20%)]\tLoss: 1.171411\n",
      "Train Epoch: 2 [2560/9451 (27%)]\tLoss: 1.039937\n",
      "Train Epoch: 2 [3200/9451 (34%)]\tLoss: 1.116581\n",
      "Train Epoch: 2 [3840/9451 (41%)]\tLoss: 0.951159\n",
      "Train Epoch: 2 [4480/9451 (47%)]\tLoss: 1.006045\n",
      "Train Epoch: 2 [5120/9451 (54%)]\tLoss: 1.327019\n",
      "Train Epoch: 2 [5760/9451 (61%)]\tLoss: 1.190154\n",
      "Train Epoch: 2 [6400/9451 (68%)]\tLoss: 0.984507\n",
      "Train Epoch: 2 [7040/9451 (74%)]\tLoss: 1.138971\n",
      "Train Epoch: 2 [7680/9451 (81%)]\tLoss: 1.259799\n",
      "Train Epoch: 2 [8320/9451 (88%)]\tLoss: 1.081006\n",
      "Train Epoch: 2 [8960/9451 (95%)]\tLoss: 1.123712\n",
      "Train Epoch: 3 [0/9451 (0%)]\tLoss: 1.107955\n",
      "Train Epoch: 3 [640/9451 (7%)]\tLoss: 1.015669\n",
      "Train Epoch: 3 [1280/9451 (14%)]\tLoss: 1.159433\n",
      "Train Epoch: 3 [1920/9451 (20%)]\tLoss: 1.208364\n",
      "Train Epoch: 3 [2560/9451 (27%)]\tLoss: 1.182084\n",
      "Train Epoch: 3 [3200/9451 (34%)]\tLoss: 1.108310\n",
      "Train Epoch: 3 [3840/9451 (41%)]\tLoss: 0.933625\n",
      "Train Epoch: 3 [4480/9451 (47%)]\tLoss: 1.260227\n",
      "Train Epoch: 3 [5120/9451 (54%)]\tLoss: 0.912498\n",
      "Train Epoch: 3 [5760/9451 (61%)]\tLoss: 1.187751\n",
      "Train Epoch: 3 [6400/9451 (68%)]\tLoss: 1.081295\n",
      "Train Epoch: 3 [7040/9451 (74%)]\tLoss: 1.016597\n",
      "Train Epoch: 3 [7680/9451 (81%)]\tLoss: 1.054871\n",
      "Train Epoch: 3 [8320/9451 (88%)]\tLoss: 1.108965\n",
      "Train Epoch: 3 [8960/9451 (95%)]\tLoss: 1.219414\n",
      "Train Epoch: 4 [0/9451 (0%)]\tLoss: 0.902017\n",
      "Train Epoch: 4 [640/9451 (7%)]\tLoss: 1.036150\n",
      "Train Epoch: 4 [1280/9451 (14%)]\tLoss: 0.950978\n",
      "Train Epoch: 4 [1920/9451 (20%)]\tLoss: 1.296227\n",
      "Train Epoch: 4 [2560/9451 (27%)]\tLoss: 1.031842\n",
      "Train Epoch: 4 [3200/9451 (34%)]\tLoss: 1.023599\n",
      "Train Epoch: 4 [3840/9451 (41%)]\tLoss: 0.955234\n",
      "Train Epoch: 4 [4480/9451 (47%)]\tLoss: 1.068548\n",
      "Train Epoch: 4 [5120/9451 (54%)]\tLoss: 1.022326\n",
      "Train Epoch: 4 [5760/9451 (61%)]\tLoss: 1.148102\n",
      "Train Epoch: 4 [6400/9451 (68%)]\tLoss: 0.834280\n",
      "Train Epoch: 4 [7040/9451 (74%)]\tLoss: 0.982185\n",
      "Train Epoch: 4 [7680/9451 (81%)]\tLoss: 1.128064\n",
      "Train Epoch: 4 [8320/9451 (88%)]\tLoss: 1.183605\n",
      "Train Epoch: 4 [8960/9451 (95%)]\tLoss: 0.865162\n",
      "Train Epoch: 5 [0/9451 (0%)]\tLoss: 1.069220\n",
      "Train Epoch: 5 [640/9451 (7%)]\tLoss: 1.041889\n",
      "Train Epoch: 5 [1280/9451 (14%)]\tLoss: 1.076101\n",
      "Train Epoch: 5 [1920/9451 (20%)]\tLoss: 0.938293\n",
      "Train Epoch: 5 [2560/9451 (27%)]\tLoss: 0.987810\n",
      "Train Epoch: 5 [3200/9451 (34%)]\tLoss: 1.152763\n",
      "Train Epoch: 5 [3840/9451 (41%)]\tLoss: 0.924662\n",
      "Train Epoch: 5 [4480/9451 (47%)]\tLoss: 1.101488\n",
      "Train Epoch: 5 [5120/9451 (54%)]\tLoss: 0.781723\n",
      "Train Epoch: 5 [5760/9451 (61%)]\tLoss: 1.123707\n",
      "Train Epoch: 5 [6400/9451 (68%)]\tLoss: 0.953640\n",
      "Train Epoch: 5 [7040/9451 (74%)]\tLoss: 1.159067\n",
      "Train Epoch: 5 [7680/9451 (81%)]\tLoss: 0.937897\n",
      "Train Epoch: 5 [8320/9451 (88%)]\tLoss: 0.850506\n",
      "Train Epoch: 5 [8960/9451 (95%)]\tLoss: 0.957370\n",
      "Train Epoch: 6 [0/9451 (0%)]\tLoss: 0.832404\n",
      "Train Epoch: 6 [640/9451 (7%)]\tLoss: 1.044649\n",
      "Train Epoch: 6 [1280/9451 (14%)]\tLoss: 1.045065\n",
      "Train Epoch: 6 [1920/9451 (20%)]\tLoss: 0.857829\n",
      "Train Epoch: 6 [2560/9451 (27%)]\tLoss: 0.746857\n",
      "Train Epoch: 6 [3200/9451 (34%)]\tLoss: 0.934050\n",
      "Train Epoch: 6 [3840/9451 (41%)]\tLoss: 0.890416\n",
      "Train Epoch: 6 [4480/9451 (47%)]\tLoss: 0.811256\n",
      "Train Epoch: 6 [5120/9451 (54%)]\tLoss: 0.965188\n",
      "Train Epoch: 6 [5760/9451 (61%)]\tLoss: 1.085858\n",
      "Train Epoch: 6 [6400/9451 (68%)]\tLoss: 0.763221\n",
      "Train Epoch: 6 [7040/9451 (74%)]\tLoss: 0.879265\n",
      "Train Epoch: 6 [7680/9451 (81%)]\tLoss: 0.850292\n",
      "Train Epoch: 6 [8320/9451 (88%)]\tLoss: 1.136607\n",
      "Train Epoch: 6 [8960/9451 (95%)]\tLoss: 1.000596\n",
      "Train Epoch: 7 [0/9451 (0%)]\tLoss: 0.862707\n",
      "Train Epoch: 7 [640/9451 (7%)]\tLoss: 1.004567\n",
      "Train Epoch: 7 [1280/9451 (14%)]\tLoss: 0.919078\n",
      "Train Epoch: 7 [1920/9451 (20%)]\tLoss: 1.064462\n",
      "Train Epoch: 7 [2560/9451 (27%)]\tLoss: 1.110862\n",
      "Train Epoch: 7 [3200/9451 (34%)]\tLoss: 1.158910\n",
      "Train Epoch: 7 [3840/9451 (41%)]\tLoss: 0.894408\n",
      "Train Epoch: 7 [4480/9451 (47%)]\tLoss: 1.223771\n",
      "Train Epoch: 7 [5120/9451 (54%)]\tLoss: 0.769376\n",
      "Train Epoch: 7 [5760/9451 (61%)]\tLoss: 1.129231\n",
      "Train Epoch: 7 [6400/9451 (68%)]\tLoss: 1.215995\n",
      "Train Epoch: 7 [7040/9451 (74%)]\tLoss: 0.902188\n",
      "Train Epoch: 7 [7680/9451 (81%)]\tLoss: 1.101295\n",
      "Train Epoch: 7 [8320/9451 (88%)]\tLoss: 1.012232\n",
      "Train Epoch: 7 [8960/9451 (95%)]\tLoss: 1.153355\n",
      "Train Epoch: 8 [0/9451 (0%)]\tLoss: 1.067930\n",
      "Train Epoch: 8 [640/9451 (7%)]\tLoss: 0.806881\n",
      "Train Epoch: 8 [1280/9451 (14%)]\tLoss: 1.263634\n",
      "Train Epoch: 8 [1920/9451 (20%)]\tLoss: 0.906875\n",
      "Train Epoch: 8 [2560/9451 (27%)]\tLoss: 0.889416\n",
      "Train Epoch: 8 [3200/9451 (34%)]\tLoss: 0.852434\n",
      "Train Epoch: 8 [3840/9451 (41%)]\tLoss: 0.846944\n",
      "Train Epoch: 8 [4480/9451 (47%)]\tLoss: 1.074395\n",
      "Train Epoch: 8 [5120/9451 (54%)]\tLoss: 0.913499\n",
      "Train Epoch: 8 [5760/9451 (61%)]\tLoss: 0.998345\n",
      "Train Epoch: 8 [6400/9451 (68%)]\tLoss: 0.732685\n",
      "Train Epoch: 8 [7040/9451 (74%)]\tLoss: 0.970702\n",
      "Train Epoch: 8 [7680/9451 (81%)]\tLoss: 1.052647\n",
      "Train Epoch: 8 [8320/9451 (88%)]\tLoss: 0.836438\n",
      "Train Epoch: 8 [8960/9451 (95%)]\tLoss: 0.990561\n",
      "Train Epoch: 9 [0/9451 (0%)]\tLoss: 1.044197\n",
      "Train Epoch: 9 [640/9451 (7%)]\tLoss: 0.769068\n",
      "Train Epoch: 9 [1280/9451 (14%)]\tLoss: 0.995001\n",
      "Train Epoch: 9 [1920/9451 (20%)]\tLoss: 0.826078\n",
      "Train Epoch: 9 [2560/9451 (27%)]\tLoss: 0.776954\n",
      "Train Epoch: 9 [3200/9451 (34%)]\tLoss: 1.094080\n",
      "Train Epoch: 9 [3840/9451 (41%)]\tLoss: 0.835518\n",
      "Train Epoch: 9 [4480/9451 (47%)]\tLoss: 0.753186\n",
      "Train Epoch: 9 [5120/9451 (54%)]\tLoss: 1.083736\n",
      "Train Epoch: 9 [5760/9451 (61%)]\tLoss: 1.127864\n",
      "Train Epoch: 9 [6400/9451 (68%)]\tLoss: 0.773312\n",
      "Train Epoch: 9 [7040/9451 (74%)]\tLoss: 0.953980\n",
      "Train Epoch: 9 [7680/9451 (81%)]\tLoss: 0.841168\n",
      "Train Epoch: 9 [8320/9451 (88%)]\tLoss: 1.010489\n",
      "Train Epoch: 9 [8960/9451 (95%)]\tLoss: 1.263691\n",
      "Train Epoch: 10 [0/9451 (0%)]\tLoss: 1.112994\n",
      "Train Epoch: 10 [640/9451 (7%)]\tLoss: 0.997489\n",
      "Train Epoch: 10 [1280/9451 (14%)]\tLoss: 1.005685\n",
      "Train Epoch: 10 [1920/9451 (20%)]\tLoss: 1.232563\n",
      "Train Epoch: 10 [2560/9451 (27%)]\tLoss: 1.135165\n",
      "Train Epoch: 10 [3200/9451 (34%)]\tLoss: 1.047933\n",
      "Train Epoch: 10 [3840/9451 (41%)]\tLoss: 0.719718\n",
      "Train Epoch: 10 [4480/9451 (47%)]\tLoss: 1.056781\n",
      "Train Epoch: 10 [5120/9451 (54%)]\tLoss: 0.826919\n",
      "Train Epoch: 10 [5760/9451 (61%)]\tLoss: 0.808527\n",
      "Train Epoch: 10 [6400/9451 (68%)]\tLoss: 0.758099\n",
      "Train Epoch: 10 [7040/9451 (74%)]\tLoss: 0.882945\n",
      "Train Epoch: 10 [7680/9451 (81%)]\tLoss: 1.018585\n",
      "Train Epoch: 10 [8320/9451 (88%)]\tLoss: 0.782613\n",
      "Train Epoch: 10 [8960/9451 (95%)]\tLoss: 0.981764\n",
      "Train Epoch: 11 [0/9451 (0%)]\tLoss: 0.870167\n",
      "Train Epoch: 11 [640/9451 (7%)]\tLoss: 0.924555\n",
      "Train Epoch: 11 [1280/9451 (14%)]\tLoss: 0.791375\n",
      "Train Epoch: 11 [1920/9451 (20%)]\tLoss: 0.837479\n",
      "Train Epoch: 11 [2560/9451 (27%)]\tLoss: 1.020685\n",
      "Train Epoch: 11 [3200/9451 (34%)]\tLoss: 1.075462\n",
      "Train Epoch: 11 [3840/9451 (41%)]\tLoss: 0.818516\n",
      "Train Epoch: 11 [4480/9451 (47%)]\tLoss: 1.309395\n",
      "Train Epoch: 11 [5120/9451 (54%)]\tLoss: 0.734671\n",
      "Train Epoch: 11 [5760/9451 (61%)]\tLoss: 1.116703\n",
      "Train Epoch: 11 [6400/9451 (68%)]\tLoss: 0.853039\n",
      "Train Epoch: 11 [7040/9451 (74%)]\tLoss: 0.884499\n",
      "Train Epoch: 11 [7680/9451 (81%)]\tLoss: 0.803026\n",
      "Train Epoch: 11 [8320/9451 (88%)]\tLoss: 0.729229\n",
      "Train Epoch: 11 [8960/9451 (95%)]\tLoss: 1.224456\n",
      "Train Epoch: 12 [0/9451 (0%)]\tLoss: 0.832713\n",
      "Train Epoch: 12 [640/9451 (7%)]\tLoss: 0.797966\n",
      "Train Epoch: 12 [1280/9451 (14%)]\tLoss: 0.949782\n",
      "Train Epoch: 12 [1920/9451 (20%)]\tLoss: 0.830839\n",
      "Train Epoch: 12 [2560/9451 (27%)]\tLoss: 0.896307\n",
      "Train Epoch: 12 [3200/9451 (34%)]\tLoss: 0.902911\n",
      "Train Epoch: 12 [3840/9451 (41%)]\tLoss: 0.854307\n",
      "Train Epoch: 12 [4480/9451 (47%)]\tLoss: 0.788607\n",
      "Train Epoch: 12 [5120/9451 (54%)]\tLoss: 0.929437\n",
      "Train Epoch: 12 [5760/9451 (61%)]\tLoss: 0.868543\n",
      "Train Epoch: 12 [6400/9451 (68%)]\tLoss: 0.933340\n",
      "Train Epoch: 12 [7040/9451 (74%)]\tLoss: 0.750960\n",
      "Train Epoch: 12 [7680/9451 (81%)]\tLoss: 0.783698\n",
      "Train Epoch: 12 [8320/9451 (88%)]\tLoss: 1.090526\n",
      "Train Epoch: 12 [8960/9451 (95%)]\tLoss: 0.781857\n",
      "Train Epoch: 13 [0/9451 (0%)]\tLoss: 0.918624\n",
      "Train Epoch: 13 [640/9451 (7%)]\tLoss: 0.892602\n",
      "Train Epoch: 13 [1280/9451 (14%)]\tLoss: 1.064977\n",
      "Train Epoch: 13 [1920/9451 (20%)]\tLoss: 1.085525\n",
      "Train Epoch: 13 [2560/9451 (27%)]\tLoss: 0.718121\n",
      "Train Epoch: 13 [3200/9451 (34%)]\tLoss: 0.994333\n",
      "Train Epoch: 13 [3840/9451 (41%)]\tLoss: 0.726701\n",
      "Train Epoch: 13 [4480/9451 (47%)]\tLoss: 0.805979\n",
      "Train Epoch: 13 [5120/9451 (54%)]\tLoss: 0.787109\n",
      "Train Epoch: 13 [5760/9451 (61%)]\tLoss: 0.942821\n",
      "Train Epoch: 13 [6400/9451 (68%)]\tLoss: 1.032004\n",
      "Train Epoch: 13 [7040/9451 (74%)]\tLoss: 0.912045\n",
      "Train Epoch: 13 [7680/9451 (81%)]\tLoss: 0.803760\n",
      "Train Epoch: 13 [8320/9451 (88%)]\tLoss: 1.013928\n",
      "Train Epoch: 13 [8960/9451 (95%)]\tLoss: 0.810879\n",
      "Train Epoch: 14 [0/9451 (0%)]\tLoss: 1.098154\n",
      "Train Epoch: 14 [640/9451 (7%)]\tLoss: 0.917415\n",
      "Train Epoch: 14 [1280/9451 (14%)]\tLoss: 0.766790\n",
      "Train Epoch: 14 [1920/9451 (20%)]\tLoss: 0.856025\n",
      "Train Epoch: 14 [2560/9451 (27%)]\tLoss: 1.086758\n",
      "Train Epoch: 14 [3200/9451 (34%)]\tLoss: 1.223274\n",
      "Train Epoch: 14 [3840/9451 (41%)]\tLoss: 0.897178\n",
      "Train Epoch: 14 [4480/9451 (47%)]\tLoss: 0.996247\n",
      "Train Epoch: 14 [5120/9451 (54%)]\tLoss: 0.647325\n",
      "Train Epoch: 14 [5760/9451 (61%)]\tLoss: 1.020969\n",
      "Train Epoch: 14 [6400/9451 (68%)]\tLoss: 0.771388\n",
      "Train Epoch: 14 [7040/9451 (74%)]\tLoss: 0.748750\n",
      "Train Epoch: 14 [7680/9451 (81%)]\tLoss: 0.782629\n",
      "Train Epoch: 14 [8320/9451 (88%)]\tLoss: 0.895464\n",
      "Train Epoch: 14 [8960/9451 (95%)]\tLoss: 0.769886\n",
      "Train Epoch: 15 [0/9451 (0%)]\tLoss: 1.028270\n",
      "Train Epoch: 15 [640/9451 (7%)]\tLoss: 0.688633\n",
      "Train Epoch: 15 [1280/9451 (14%)]\tLoss: 0.879124\n",
      "Train Epoch: 15 [1920/9451 (20%)]\tLoss: 0.905081\n",
      "Train Epoch: 15 [2560/9451 (27%)]\tLoss: 1.069529\n",
      "Train Epoch: 15 [3200/9451 (34%)]\tLoss: 0.948282\n",
      "Train Epoch: 15 [3840/9451 (41%)]\tLoss: 0.828340\n",
      "Train Epoch: 15 [4480/9451 (47%)]\tLoss: 1.278006\n",
      "Train Epoch: 15 [5120/9451 (54%)]\tLoss: 1.143406\n",
      "Train Epoch: 15 [5760/9451 (61%)]\tLoss: 0.757761\n",
      "Train Epoch: 15 [6400/9451 (68%)]\tLoss: 0.888811\n",
      "Train Epoch: 15 [7040/9451 (74%)]\tLoss: 0.780018\n",
      "Train Epoch: 15 [7680/9451 (81%)]\tLoss: 0.779126\n",
      "Train Epoch: 15 [8320/9451 (88%)]\tLoss: 0.849309\n",
      "Train Epoch: 15 [8960/9451 (95%)]\tLoss: 1.050858\n",
      "Train Epoch: 16 [0/9451 (0%)]\tLoss: 0.832690\n",
      "Train Epoch: 16 [640/9451 (7%)]\tLoss: 0.804353\n",
      "Train Epoch: 16 [1280/9451 (14%)]\tLoss: 0.715308\n",
      "Train Epoch: 16 [1920/9451 (20%)]\tLoss: 0.965801\n",
      "Train Epoch: 16 [2560/9451 (27%)]\tLoss: 1.040077\n",
      "Train Epoch: 16 [3200/9451 (34%)]\tLoss: 0.809133\n",
      "Train Epoch: 16 [3840/9451 (41%)]\tLoss: 1.053045\n",
      "Train Epoch: 16 [4480/9451 (47%)]\tLoss: 0.905696\n",
      "Train Epoch: 16 [5120/9451 (54%)]\tLoss: 0.794910\n",
      "Train Epoch: 16 [5760/9451 (61%)]\tLoss: 0.902843\n",
      "Train Epoch: 16 [6400/9451 (68%)]\tLoss: 0.849849\n",
      "Train Epoch: 16 [7040/9451 (74%)]\tLoss: 0.880752\n",
      "Train Epoch: 16 [7680/9451 (81%)]\tLoss: 0.945578\n",
      "Train Epoch: 16 [8320/9451 (88%)]\tLoss: 0.861588\n",
      "Train Epoch: 16 [8960/9451 (95%)]\tLoss: 0.863178\n",
      "Train Epoch: 17 [0/9451 (0%)]\tLoss: 0.831559\n",
      "Train Epoch: 17 [640/9451 (7%)]\tLoss: 0.859681\n",
      "Train Epoch: 17 [1280/9451 (14%)]\tLoss: 0.725406\n",
      "Train Epoch: 17 [1920/9451 (20%)]\tLoss: 0.884707\n",
      "Train Epoch: 17 [2560/9451 (27%)]\tLoss: 0.895550\n",
      "Train Epoch: 17 [3200/9451 (34%)]\tLoss: 0.900490\n",
      "Train Epoch: 17 [3840/9451 (41%)]\tLoss: 0.770940\n",
      "Train Epoch: 17 [4480/9451 (47%)]\tLoss: 0.791850\n",
      "Train Epoch: 17 [5120/9451 (54%)]\tLoss: 0.625737\n",
      "Train Epoch: 17 [5760/9451 (61%)]\tLoss: 0.800438\n",
      "Train Epoch: 17 [6400/9451 (68%)]\tLoss: 0.936148\n",
      "Train Epoch: 17 [7040/9451 (74%)]\tLoss: 0.609304\n",
      "Train Epoch: 17 [7680/9451 (81%)]\tLoss: 0.851446\n",
      "Train Epoch: 17 [8320/9451 (88%)]\tLoss: 0.964215\n",
      "Train Epoch: 17 [8960/9451 (95%)]\tLoss: 1.076173\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/1026 (0%)]\tLoss: 2.083052\n",
      "Train Epoch: 1 [640/1026 (59%)]\tLoss: 1.436459\n",
      "Train Epoch: 2 [0/1026 (0%)]\tLoss: 1.242796\n",
      "Train Epoch: 2 [640/1026 (59%)]\tLoss: 1.008133\n",
      "Train Epoch: 3 [0/1026 (0%)]\tLoss: 1.129061\n",
      "Train Epoch: 3 [640/1026 (59%)]\tLoss: 1.055361\n",
      "Train Epoch: 4 [0/1026 (0%)]\tLoss: 1.325315\n",
      "Train Epoch: 4 [640/1026 (59%)]\tLoss: 1.301527\n",
      "Train Epoch: 5 [0/1026 (0%)]\tLoss: 1.165624\n",
      "Train Epoch: 5 [640/1026 (59%)]\tLoss: 1.142397\n",
      "Train Epoch: 6 [0/1026 (0%)]\tLoss: 1.369471\n",
      "Train Epoch: 6 [640/1026 (59%)]\tLoss: 1.475165\n",
      "Train Epoch: 7 [0/1026 (0%)]\tLoss: 1.156911\n",
      "Train Epoch: 7 [640/1026 (59%)]\tLoss: 1.119479\n",
      "Train Epoch: 8 [0/1026 (0%)]\tLoss: 1.239826\n",
      "Train Epoch: 8 [640/1026 (59%)]\tLoss: 1.106859\n",
      "Train Epoch: 9 [0/1026 (0%)]\tLoss: 1.513673\n",
      "Train Epoch: 9 [640/1026 (59%)]\tLoss: 1.001044\n",
      "Train Epoch: 10 [0/1026 (0%)]\tLoss: 1.239228\n",
      "Train Epoch: 10 [640/1026 (59%)]\tLoss: 0.990275\n",
      "Train Epoch: 11 [0/1026 (0%)]\tLoss: 1.094943\n",
      "Train Epoch: 11 [640/1026 (59%)]\tLoss: 1.173649\n",
      "Train Epoch: 12 [0/1026 (0%)]\tLoss: 1.195166\n",
      "Train Epoch: 12 [640/1026 (59%)]\tLoss: 1.112907\n",
      "Train Epoch: 13 [0/1026 (0%)]\tLoss: 1.229103\n",
      "Train Epoch: 13 [640/1026 (59%)]\tLoss: 0.885481\n",
      "Train Epoch: 14 [0/1026 (0%)]\tLoss: 1.132812\n",
      "Train Epoch: 14 [640/1026 (59%)]\tLoss: 1.143851\n",
      "Train Epoch: 15 [0/1026 (0%)]\tLoss: 1.037629\n",
      "Train Epoch: 15 [640/1026 (59%)]\tLoss: 1.181868\n",
      "Train Epoch: 16 [0/1026 (0%)]\tLoss: 1.294493\n",
      "Train Epoch: 16 [640/1026 (59%)]\tLoss: 1.188091\n",
      "Train Epoch: 17 [0/1026 (0%)]\tLoss: 1.200162\n",
      "Train Epoch: 17 [640/1026 (59%)]\tLoss: 1.188468\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.9879, Accuracy: 2804/10000 (28%)\n",
      "\n",
      "Round 3/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 1.758417\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.814717\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.535013\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.509834\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.626216\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.444808\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.702038\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.458938\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.523497\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.667696\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.609517\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.555024\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.204360\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.511107\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.335850\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.289612\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.356407\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.316627\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.231633\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.576015\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.506990\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.405199\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.581897\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.529340\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.390246\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.200971\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.294930\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.268977\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.816311\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.307415\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.446350\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.315150\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.313972\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.437760\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.437953\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.300822\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.383053\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.350743\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.463325\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.404643\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.638703\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.287711\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.511558\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.285094\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.342560\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.260762\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.308741\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.538712\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.661277\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.416703\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.387379\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.495879\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.305474\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.278649\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.298075\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.417482\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.285730\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.208433\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.490867\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.212184\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.480167\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.248905\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.454646\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.373715\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.322122\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.265764\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.491258\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.295419\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.486994\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.331601\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.400850\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.367574\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.582474\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.401480\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.522728\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.221657\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.275141\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.487278\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.295413\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.249430\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.433532\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.263918\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.287755\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.148752\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.398537\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.367839\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.618490\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.324434\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.516368\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.560559\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.226937\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.417297\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.132990\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.221940\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.309368\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.205866\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.256980\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.140656\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.510586\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.420212\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.219533\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.288667\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 1.621403\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 0.989827\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 1.256250\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 1.136076\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 1.081920\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 1.436713\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 1.100424\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 1.318766\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 1.231375\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 1.240914\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 1.024515\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 1.305776\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 1.351291\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 1.277035\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 0.971607\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 1.028261\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 0.999430\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 1.866449\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 1.214517\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.025453\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 1.177989\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 1.411154\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 1.065990\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 1.054513\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 1.194631\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 1.157291\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 0.973920\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 1.405554\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 1.127272\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 1.301749\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 1.137878\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 0.987454\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 1.090938\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 1.118406\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.959762\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 1.177973\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 1.047768\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 1.135286\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 0.977189\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 0.816209\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 0.818778\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 1.027349\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 1.250328\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 1.022715\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 0.848416\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 1.216386\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.947509\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 0.848207\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.883785\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 0.875349\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 0.985861\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 0.762432\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 1.018004\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 0.929962\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 1.080400\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 0.757867\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 0.906391\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.947960\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 0.876283\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.978930\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.853483\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 1.029988\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 0.883793\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.994085\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.980543\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 1.058169\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 0.819054\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 1.081580\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 1.114438\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.956133\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 0.893848\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 0.821936\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 0.990820\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 0.643201\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 0.976915\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.778466\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.712962\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.782353\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 1.040049\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 1.010821\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 1.064287\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 1.023646\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 0.775564\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.946701\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 0.797569\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.728122\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.831953\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.801951\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 0.892327\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.897206\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 0.737624\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.768268\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 0.883517\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.771200\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.881142\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.647833\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 0.728949\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 0.797758\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.783198\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 0.917633\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.953474\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.792325\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 0.739252\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.765253\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.771056\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.884215\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 0.785184\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.795353\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 1.014598\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.907755\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.789418\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.835665\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 1.010665\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 0.876863\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 0.863182\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 0.837351\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.766373\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.694578\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.828579\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.991415\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.892459\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.778396\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.956210\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.868303\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.742920\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.877938\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 0.582370\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.799783\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.821842\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.877448\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 1.137073\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.863375\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.720580\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.762240\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.826742\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 1.183489\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 1.066252\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.908433\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.936684\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 0.880248\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.986777\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.722579\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 0.896504\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 0.948506\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.846541\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.692419\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.686272\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.729878\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 1.135485\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.930269\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.856871\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.695597\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.820613\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 1.916136\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.462175\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.789734\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.694389\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.477785\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.265857\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.829832\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.247123\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.417662\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.341273\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.210596\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.551994\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.221430\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.141851\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.243167\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.372331\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.652498\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.191297\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.258383\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.284053\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.176997\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.381962\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.479015\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.473979\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.456351\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.500149\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.438560\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.465942\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.192306\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.164480\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.144115\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.278378\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.558750\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.222009\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.513340\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.199619\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.258591\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.154739\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.411094\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.170523\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.483327\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.385876\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.487404\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.311913\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.209225\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.503644\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.234757\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.056158\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.298410\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.465328\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.357174\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.267828\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.298884\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.427912\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.135735\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.321703\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.263807\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.422349\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.173239\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.293011\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.413719\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.138281\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.489102\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.362665\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.376876\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.538695\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.067149\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.127029\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.478433\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.375117\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.180701\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.261055\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.216777\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.235413\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.296857\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.260935\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.268871\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.468306\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.357326\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.099869\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.286960\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.210360\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.322568\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.395142\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.397093\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.069479\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.275136\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.289734\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.222493\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.297703\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.151335\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.353252\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.732273\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.353079\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.190090\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.443399\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.293523\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.334755\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.297967\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.258487\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.206973\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.345423\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 1.871917\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 1.540874\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 1.195113\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 1.142234\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 1.131405\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 1.071508\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 0.920854\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 1.170817\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 0.832238\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 1.092791\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 0.762507\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 1.218343\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 1.111811\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 1.167789\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 0.946527\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 1.074472\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 0.824888\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 1.016132\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 0.955754\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 0.947549\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 0.690156\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 0.807812\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 1.304062\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 1.033120\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 1.087966\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 1.098140\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.953936\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 0.913098\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 0.882179\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 0.735586\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 0.780171\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 0.869884\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 1.073259\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.791281\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 0.920374\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.896897\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 1.217657\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 0.893835\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.807597\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 1.035566\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 0.885716\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.816175\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 1.072922\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 0.877210\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 1.072245\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.974348\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 0.824728\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.829435\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.750580\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 0.671369\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 0.715054\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 0.823098\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 0.673463\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.811332\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.828812\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.913070\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.877377\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 0.939121\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.848902\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.848021\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.970389\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.646583\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.939133\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 0.842907\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 1.094091\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.925137\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.691782\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 0.636434\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.929509\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 1.013652\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 1.043715\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 0.975760\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.848002\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 0.861166\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 1.005521\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.999946\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.794797\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 0.914828\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.759412\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.585761\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 1.018905\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.816035\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 1.001105\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.780536\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 0.996351\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 0.513277\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 1.084952\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 0.880760\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 0.623455\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 1.005093\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 1.023674\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.799139\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.690650\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.993523\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.808456\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 1.028053\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.715725\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 1.067386\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.756503\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.746367\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.904774\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 0.702288\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.760023\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.940042\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.918602\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 0.706441\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.919057\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 0.705753\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.794736\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.939084\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.662744\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.740023\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.657288\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.858822\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.732321\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.721234\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 1.009320\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.907526\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 0.968189\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.996001\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 0.801534\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 0.680499\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 1.044940\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.590972\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.817709\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.944915\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.735345\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.675295\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.988729\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.847643\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 0.544666\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.912381\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.750969\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.814254\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.925310\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.723355\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.723647\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.771928\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.873073\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.838614\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.958011\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.581928\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.784756\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.865528\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.934646\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.778132\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.693526\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.786743\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 1.285054\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.860205\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.724286\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.658814\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.758390\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.817803\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.869364\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.603987\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.640328\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.957009\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.824120\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.595104\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 1.029877\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.711009\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.728357\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.629784\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.698400\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.889025\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.643318\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.989110\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.696467\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 0.693402\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.804753\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.802308\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.799562\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.682731\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.819151\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.717465\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.556693\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.720870\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.935457\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.744362\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.582835\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.990211\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.557131\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.759431\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 1.031408\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.947307\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 0.802690\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.874972\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.569861\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.714236\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.592926\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.800415\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.703293\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 1.137206\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.627524\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.964062\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.732149\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.751465\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.839693\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.772201\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.784492\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.823173\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.925581\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.732106\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.795509\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 1.014001\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.910913\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.828631\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.756443\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 1.060600\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.811541\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.696106\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.841818\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.821180\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.601050\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.770075\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.753041\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.809601\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.646362\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.651817\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.874717\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.718620\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.798098\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.543485\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.583769\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.817361\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.865330\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.737321\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.683323\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.636407\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.724854\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.613966\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.652319\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.776019\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.983096\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.523070\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 1.000818\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.764251\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.831621\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.743565\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.759828\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.799341\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.649504\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.820071\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.458878\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.834006\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.822164\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.717770\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.934206\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.721121\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.732424\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.897177\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.698847\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.672553\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.742770\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.892054\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 0.994323\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 1.048131\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 1.243487\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 0.868629\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 1.094989\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 1.146159\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 0.963536\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 0.956134\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 1.098754\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.695113\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 0.852394\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 0.751209\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 0.973994\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 1.087466\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.803923\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 1.221132\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 0.882119\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.809838\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.762838\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.929025\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.860746\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.851104\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 1.017196\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 0.790770\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 0.982376\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 1.004739\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.947089\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 0.834472\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 0.678922\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.858518\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.940785\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 0.810784\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 0.821568\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 0.582641\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 0.673699\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 0.755822\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.694253\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.917929\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.917752\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 1.051213\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.750358\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.907186\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.725887\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 1.087891\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.709604\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.706106\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.679329\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.666443\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 0.789429\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.947769\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 0.970920\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.813383\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.755538\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.869865\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 0.664513\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.680111\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.775195\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.895998\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 0.778284\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.686235\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.980408\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.567588\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.745286\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.806977\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.704176\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.834224\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.998184\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.946347\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.969131\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.864946\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.753917\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.747787\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.811727\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.729469\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.729304\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 0.765960\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.796441\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.595114\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.629022\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.748814\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.705471\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 1.062192\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.802915\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.531241\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.988037\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.613534\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.858128\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.788146\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.520625\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.507624\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.716500\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.697388\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.606952\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.697048\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.636323\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.753755\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.692858\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.848799\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.610744\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.619958\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.485393\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/8838 (0%)]\tLoss: 2.489650\n",
      "Train Epoch: 1 [640/8838 (7%)]\tLoss: 1.375731\n",
      "Train Epoch: 1 [1280/8838 (14%)]\tLoss: 0.874655\n",
      "Train Epoch: 1 [1920/8838 (22%)]\tLoss: 0.981915\n",
      "Train Epoch: 1 [2560/8838 (29%)]\tLoss: 0.762901\n",
      "Train Epoch: 1 [3200/8838 (36%)]\tLoss: 0.873910\n",
      "Train Epoch: 1 [3840/8838 (43%)]\tLoss: 0.833446\n",
      "Train Epoch: 1 [4480/8838 (50%)]\tLoss: 0.851199\n",
      "Train Epoch: 1 [5120/8838 (58%)]\tLoss: 0.853283\n",
      "Train Epoch: 1 [5760/8838 (65%)]\tLoss: 0.799258\n",
      "Train Epoch: 1 [6400/8838 (72%)]\tLoss: 0.842776\n",
      "Train Epoch: 1 [7040/8838 (79%)]\tLoss: 0.978856\n",
      "Train Epoch: 1 [7680/8838 (86%)]\tLoss: 0.864878\n",
      "Train Epoch: 1 [8320/8838 (94%)]\tLoss: 0.900742\n",
      "Train Epoch: 2 [0/8838 (0%)]\tLoss: 0.756985\n",
      "Train Epoch: 2 [640/8838 (7%)]\tLoss: 0.987027\n",
      "Train Epoch: 2 [1280/8838 (14%)]\tLoss: 0.697170\n",
      "Train Epoch: 2 [1920/8838 (22%)]\tLoss: 0.646104\n",
      "Train Epoch: 2 [2560/8838 (29%)]\tLoss: 0.825535\n",
      "Train Epoch: 2 [3200/8838 (36%)]\tLoss: 0.972051\n",
      "Train Epoch: 2 [3840/8838 (43%)]\tLoss: 0.875655\n",
      "Train Epoch: 2 [4480/8838 (50%)]\tLoss: 0.664763\n",
      "Train Epoch: 2 [5120/8838 (58%)]\tLoss: 0.744372\n",
      "Train Epoch: 2 [5760/8838 (65%)]\tLoss: 0.869890\n",
      "Train Epoch: 2 [6400/8838 (72%)]\tLoss: 0.678614\n",
      "Train Epoch: 2 [7040/8838 (79%)]\tLoss: 0.834071\n",
      "Train Epoch: 2 [7680/8838 (86%)]\tLoss: 0.793623\n",
      "Train Epoch: 2 [8320/8838 (94%)]\tLoss: 0.718723\n",
      "Train Epoch: 3 [0/8838 (0%)]\tLoss: 0.826292\n",
      "Train Epoch: 3 [640/8838 (7%)]\tLoss: 0.853934\n",
      "Train Epoch: 3 [1280/8838 (14%)]\tLoss: 0.825073\n",
      "Train Epoch: 3 [1920/8838 (22%)]\tLoss: 0.735502\n",
      "Train Epoch: 3 [2560/8838 (29%)]\tLoss: 0.618942\n",
      "Train Epoch: 3 [3200/8838 (36%)]\tLoss: 0.786699\n",
      "Train Epoch: 3 [3840/8838 (43%)]\tLoss: 0.729747\n",
      "Train Epoch: 3 [4480/8838 (50%)]\tLoss: 0.866358\n",
      "Train Epoch: 3 [5120/8838 (58%)]\tLoss: 0.806833\n",
      "Train Epoch: 3 [5760/8838 (65%)]\tLoss: 0.901245\n",
      "Train Epoch: 3 [6400/8838 (72%)]\tLoss: 0.637598\n",
      "Train Epoch: 3 [7040/8838 (79%)]\tLoss: 0.579786\n",
      "Train Epoch: 3 [7680/8838 (86%)]\tLoss: 0.736563\n",
      "Train Epoch: 3 [8320/8838 (94%)]\tLoss: 0.604687\n",
      "Train Epoch: 4 [0/8838 (0%)]\tLoss: 0.706828\n",
      "Train Epoch: 4 [640/8838 (7%)]\tLoss: 0.725443\n",
      "Train Epoch: 4 [1280/8838 (14%)]\tLoss: 0.667194\n",
      "Train Epoch: 4 [1920/8838 (22%)]\tLoss: 0.693430\n",
      "Train Epoch: 4 [2560/8838 (29%)]\tLoss: 1.087976\n",
      "Train Epoch: 4 [3200/8838 (36%)]\tLoss: 0.892472\n",
      "Train Epoch: 4 [3840/8838 (43%)]\tLoss: 0.596280\n",
      "Train Epoch: 4 [4480/8838 (50%)]\tLoss: 0.875892\n",
      "Train Epoch: 4 [5120/8838 (58%)]\tLoss: 0.776999\n",
      "Train Epoch: 4 [5760/8838 (65%)]\tLoss: 0.748110\n",
      "Train Epoch: 4 [6400/8838 (72%)]\tLoss: 0.489538\n",
      "Train Epoch: 4 [7040/8838 (79%)]\tLoss: 1.156031\n",
      "Train Epoch: 4 [7680/8838 (86%)]\tLoss: 0.715779\n",
      "Train Epoch: 4 [8320/8838 (94%)]\tLoss: 0.745079\n",
      "Train Epoch: 5 [0/8838 (0%)]\tLoss: 0.660958\n",
      "Train Epoch: 5 [640/8838 (7%)]\tLoss: 0.723768\n",
      "Train Epoch: 5 [1280/8838 (14%)]\tLoss: 0.900299\n",
      "Train Epoch: 5 [1920/8838 (22%)]\tLoss: 0.858022\n",
      "Train Epoch: 5 [2560/8838 (29%)]\tLoss: 0.777661\n",
      "Train Epoch: 5 [3200/8838 (36%)]\tLoss: 0.715823\n",
      "Train Epoch: 5 [3840/8838 (43%)]\tLoss: 0.519135\n",
      "Train Epoch: 5 [4480/8838 (50%)]\tLoss: 0.696583\n",
      "Train Epoch: 5 [5120/8838 (58%)]\tLoss: 0.641228\n",
      "Train Epoch: 5 [5760/8838 (65%)]\tLoss: 0.513993\n",
      "Train Epoch: 5 [6400/8838 (72%)]\tLoss: 0.954222\n",
      "Train Epoch: 5 [7040/8838 (79%)]\tLoss: 0.733305\n",
      "Train Epoch: 5 [7680/8838 (86%)]\tLoss: 0.508689\n",
      "Train Epoch: 5 [8320/8838 (94%)]\tLoss: 0.678356\n",
      "Train Epoch: 6 [0/8838 (0%)]\tLoss: 0.593956\n",
      "Train Epoch: 6 [640/8838 (7%)]\tLoss: 1.008458\n",
      "Train Epoch: 6 [1280/8838 (14%)]\tLoss: 0.578270\n",
      "Train Epoch: 6 [1920/8838 (22%)]\tLoss: 0.715652\n",
      "Train Epoch: 6 [2560/8838 (29%)]\tLoss: 0.857780\n",
      "Train Epoch: 6 [3200/8838 (36%)]\tLoss: 0.675584\n",
      "Train Epoch: 6 [3840/8838 (43%)]\tLoss: 0.595239\n",
      "Train Epoch: 6 [4480/8838 (50%)]\tLoss: 0.660883\n",
      "Train Epoch: 6 [5120/8838 (58%)]\tLoss: 0.764242\n",
      "Train Epoch: 6 [5760/8838 (65%)]\tLoss: 0.692696\n",
      "Train Epoch: 6 [6400/8838 (72%)]\tLoss: 0.813096\n",
      "Train Epoch: 6 [7040/8838 (79%)]\tLoss: 0.861388\n",
      "Train Epoch: 6 [7680/8838 (86%)]\tLoss: 0.609438\n",
      "Train Epoch: 6 [8320/8838 (94%)]\tLoss: 0.838946\n",
      "Train Epoch: 7 [0/8838 (0%)]\tLoss: 0.664052\n",
      "Train Epoch: 7 [640/8838 (7%)]\tLoss: 0.653127\n",
      "Train Epoch: 7 [1280/8838 (14%)]\tLoss: 0.653865\n",
      "Train Epoch: 7 [1920/8838 (22%)]\tLoss: 0.634523\n",
      "Train Epoch: 7 [2560/8838 (29%)]\tLoss: 0.536515\n",
      "Train Epoch: 7 [3200/8838 (36%)]\tLoss: 0.884931\n",
      "Train Epoch: 7 [3840/8838 (43%)]\tLoss: 0.793262\n",
      "Train Epoch: 7 [4480/8838 (50%)]\tLoss: 0.831459\n",
      "Train Epoch: 7 [5120/8838 (58%)]\tLoss: 0.672300\n",
      "Train Epoch: 7 [5760/8838 (65%)]\tLoss: 0.676209\n",
      "Train Epoch: 7 [6400/8838 (72%)]\tLoss: 0.650148\n",
      "Train Epoch: 7 [7040/8838 (79%)]\tLoss: 0.501681\n",
      "Train Epoch: 7 [7680/8838 (86%)]\tLoss: 0.522873\n",
      "Train Epoch: 7 [8320/8838 (94%)]\tLoss: 0.542187\n",
      "Train Epoch: 8 [0/8838 (0%)]\tLoss: 0.649375\n",
      "Train Epoch: 8 [640/8838 (7%)]\tLoss: 0.550418\n",
      "Train Epoch: 8 [1280/8838 (14%)]\tLoss: 0.640588\n",
      "Train Epoch: 8 [1920/8838 (22%)]\tLoss: 0.616244\n",
      "Train Epoch: 8 [2560/8838 (29%)]\tLoss: 0.669537\n",
      "Train Epoch: 8 [3200/8838 (36%)]\tLoss: 0.844373\n",
      "Train Epoch: 8 [3840/8838 (43%)]\tLoss: 0.634419\n",
      "Train Epoch: 8 [4480/8838 (50%)]\tLoss: 0.651185\n",
      "Train Epoch: 8 [5120/8838 (58%)]\tLoss: 0.542967\n",
      "Train Epoch: 8 [5760/8838 (65%)]\tLoss: 0.661686\n",
      "Train Epoch: 8 [6400/8838 (72%)]\tLoss: 0.793345\n",
      "Train Epoch: 8 [7040/8838 (79%)]\tLoss: 0.565075\n",
      "Train Epoch: 8 [7680/8838 (86%)]\tLoss: 0.598855\n",
      "Train Epoch: 8 [8320/8838 (94%)]\tLoss: 0.687673\n",
      "Train Epoch: 9 [0/8838 (0%)]\tLoss: 0.833188\n",
      "Train Epoch: 9 [640/8838 (7%)]\tLoss: 0.672440\n",
      "Train Epoch: 9 [1280/8838 (14%)]\tLoss: 0.639814\n",
      "Train Epoch: 9 [1920/8838 (22%)]\tLoss: 0.813119\n",
      "Train Epoch: 9 [2560/8838 (29%)]\tLoss: 0.615561\n",
      "Train Epoch: 9 [3200/8838 (36%)]\tLoss: 0.534201\n",
      "Train Epoch: 9 [3840/8838 (43%)]\tLoss: 0.508977\n",
      "Train Epoch: 9 [4480/8838 (50%)]\tLoss: 0.572051\n",
      "Train Epoch: 9 [5120/8838 (58%)]\tLoss: 0.646489\n",
      "Train Epoch: 9 [5760/8838 (65%)]\tLoss: 0.536733\n",
      "Train Epoch: 9 [6400/8838 (72%)]\tLoss: 0.748378\n",
      "Train Epoch: 9 [7040/8838 (79%)]\tLoss: 0.733673\n",
      "Train Epoch: 9 [7680/8838 (86%)]\tLoss: 0.520474\n",
      "Train Epoch: 9 [8320/8838 (94%)]\tLoss: 0.769881\n",
      "Train Epoch: 10 [0/8838 (0%)]\tLoss: 0.750791\n",
      "Train Epoch: 10 [640/8838 (7%)]\tLoss: 0.792282\n",
      "Train Epoch: 10 [1280/8838 (14%)]\tLoss: 0.868776\n",
      "Train Epoch: 10 [1920/8838 (22%)]\tLoss: 0.641989\n",
      "Train Epoch: 10 [2560/8838 (29%)]\tLoss: 0.599900\n",
      "Train Epoch: 10 [3200/8838 (36%)]\tLoss: 0.709045\n",
      "Train Epoch: 10 [3840/8838 (43%)]\tLoss: 0.639424\n",
      "Train Epoch: 10 [4480/8838 (50%)]\tLoss: 0.572185\n",
      "Train Epoch: 10 [5120/8838 (58%)]\tLoss: 0.811165\n",
      "Train Epoch: 10 [5760/8838 (65%)]\tLoss: 0.797849\n",
      "Train Epoch: 10 [6400/8838 (72%)]\tLoss: 0.714896\n",
      "Train Epoch: 10 [7040/8838 (79%)]\tLoss: 0.682281\n",
      "Train Epoch: 10 [7680/8838 (86%)]\tLoss: 0.634277\n",
      "Train Epoch: 10 [8320/8838 (94%)]\tLoss: 0.556932\n",
      "Train Epoch: 11 [0/8838 (0%)]\tLoss: 0.565542\n",
      "Train Epoch: 11 [640/8838 (7%)]\tLoss: 0.626210\n",
      "Train Epoch: 11 [1280/8838 (14%)]\tLoss: 0.485189\n",
      "Train Epoch: 11 [1920/8838 (22%)]\tLoss: 0.640153\n",
      "Train Epoch: 11 [2560/8838 (29%)]\tLoss: 0.688106\n",
      "Train Epoch: 11 [3200/8838 (36%)]\tLoss: 0.574945\n",
      "Train Epoch: 11 [3840/8838 (43%)]\tLoss: 0.494464\n",
      "Train Epoch: 11 [4480/8838 (50%)]\tLoss: 0.766398\n",
      "Train Epoch: 11 [5120/8838 (58%)]\tLoss: 0.510109\n",
      "Train Epoch: 11 [5760/8838 (65%)]\tLoss: 0.596395\n",
      "Train Epoch: 11 [6400/8838 (72%)]\tLoss: 0.534246\n",
      "Train Epoch: 11 [7040/8838 (79%)]\tLoss: 0.918122\n",
      "Train Epoch: 11 [7680/8838 (86%)]\tLoss: 0.656696\n",
      "Train Epoch: 11 [8320/8838 (94%)]\tLoss: 0.733101\n",
      "Train Epoch: 12 [0/8838 (0%)]\tLoss: 0.539651\n",
      "Train Epoch: 12 [640/8838 (7%)]\tLoss: 0.674034\n",
      "Train Epoch: 12 [1280/8838 (14%)]\tLoss: 0.571719\n",
      "Train Epoch: 12 [1920/8838 (22%)]\tLoss: 0.528516\n",
      "Train Epoch: 12 [2560/8838 (29%)]\tLoss: 0.664516\n",
      "Train Epoch: 12 [3200/8838 (36%)]\tLoss: 0.448837\n",
      "Train Epoch: 12 [3840/8838 (43%)]\tLoss: 0.614905\n",
      "Train Epoch: 12 [4480/8838 (50%)]\tLoss: 0.666078\n",
      "Train Epoch: 12 [5120/8838 (58%)]\tLoss: 0.513204\n",
      "Train Epoch: 12 [5760/8838 (65%)]\tLoss: 0.743388\n",
      "Train Epoch: 12 [6400/8838 (72%)]\tLoss: 0.851082\n",
      "Train Epoch: 12 [7040/8838 (79%)]\tLoss: 0.687118\n",
      "Train Epoch: 12 [7680/8838 (86%)]\tLoss: 0.654076\n",
      "Train Epoch: 12 [8320/8838 (94%)]\tLoss: 0.654851\n",
      "Train Epoch: 13 [0/8838 (0%)]\tLoss: 0.567372\n",
      "Train Epoch: 13 [640/8838 (7%)]\tLoss: 0.511156\n",
      "Train Epoch: 13 [1280/8838 (14%)]\tLoss: 0.735506\n",
      "Train Epoch: 13 [1920/8838 (22%)]\tLoss: 0.547692\n",
      "Train Epoch: 13 [2560/8838 (29%)]\tLoss: 0.794960\n",
      "Train Epoch: 13 [3200/8838 (36%)]\tLoss: 0.482888\n",
      "Train Epoch: 13 [3840/8838 (43%)]\tLoss: 0.843253\n",
      "Train Epoch: 13 [4480/8838 (50%)]\tLoss: 0.695994\n",
      "Train Epoch: 13 [5120/8838 (58%)]\tLoss: 0.550754\n",
      "Train Epoch: 13 [5760/8838 (65%)]\tLoss: 0.754145\n",
      "Train Epoch: 13 [6400/8838 (72%)]\tLoss: 0.759837\n",
      "Train Epoch: 13 [7040/8838 (79%)]\tLoss: 0.668211\n",
      "Train Epoch: 13 [7680/8838 (86%)]\tLoss: 0.738928\n",
      "Train Epoch: 13 [8320/8838 (94%)]\tLoss: 0.899158\n",
      "Train Epoch: 14 [0/8838 (0%)]\tLoss: 0.582796\n",
      "Train Epoch: 14 [640/8838 (7%)]\tLoss: 0.601056\n",
      "Train Epoch: 14 [1280/8838 (14%)]\tLoss: 0.726213\n",
      "Train Epoch: 14 [1920/8838 (22%)]\tLoss: 0.759675\n",
      "Train Epoch: 14 [2560/8838 (29%)]\tLoss: 0.623426\n",
      "Train Epoch: 14 [3200/8838 (36%)]\tLoss: 0.737590\n",
      "Train Epoch: 14 [3840/8838 (43%)]\tLoss: 0.563267\n",
      "Train Epoch: 14 [4480/8838 (50%)]\tLoss: 0.850828\n",
      "Train Epoch: 14 [5120/8838 (58%)]\tLoss: 0.666473\n",
      "Train Epoch: 14 [5760/8838 (65%)]\tLoss: 0.596649\n",
      "Train Epoch: 14 [6400/8838 (72%)]\tLoss: 0.685717\n",
      "Train Epoch: 14 [7040/8838 (79%)]\tLoss: 0.689208\n",
      "Train Epoch: 14 [7680/8838 (86%)]\tLoss: 0.566376\n",
      "Train Epoch: 14 [8320/8838 (94%)]\tLoss: 0.803839\n",
      "Train Epoch: 15 [0/8838 (0%)]\tLoss: 0.675043\n",
      "Train Epoch: 15 [640/8838 (7%)]\tLoss: 0.655374\n",
      "Train Epoch: 15 [1280/8838 (14%)]\tLoss: 0.717314\n",
      "Train Epoch: 15 [1920/8838 (22%)]\tLoss: 0.723779\n",
      "Train Epoch: 15 [2560/8838 (29%)]\tLoss: 0.928625\n",
      "Train Epoch: 15 [3200/8838 (36%)]\tLoss: 0.657573\n",
      "Train Epoch: 15 [3840/8838 (43%)]\tLoss: 0.401424\n",
      "Train Epoch: 15 [4480/8838 (50%)]\tLoss: 0.594845\n",
      "Train Epoch: 15 [5120/8838 (58%)]\tLoss: 0.517757\n",
      "Train Epoch: 15 [5760/8838 (65%)]\tLoss: 0.661013\n",
      "Train Epoch: 15 [6400/8838 (72%)]\tLoss: 0.728300\n",
      "Train Epoch: 15 [7040/8838 (79%)]\tLoss: 0.618080\n",
      "Train Epoch: 15 [7680/8838 (86%)]\tLoss: 0.627901\n",
      "Train Epoch: 15 [8320/8838 (94%)]\tLoss: 0.761918\n",
      "Train Epoch: 16 [0/8838 (0%)]\tLoss: 0.671139\n",
      "Train Epoch: 16 [640/8838 (7%)]\tLoss: 0.585340\n",
      "Train Epoch: 16 [1280/8838 (14%)]\tLoss: 0.583572\n",
      "Train Epoch: 16 [1920/8838 (22%)]\tLoss: 0.586756\n",
      "Train Epoch: 16 [2560/8838 (29%)]\tLoss: 0.621559\n",
      "Train Epoch: 16 [3200/8838 (36%)]\tLoss: 0.567129\n",
      "Train Epoch: 16 [3840/8838 (43%)]\tLoss: 0.556455\n",
      "Train Epoch: 16 [4480/8838 (50%)]\tLoss: 0.567718\n",
      "Train Epoch: 16 [5120/8838 (58%)]\tLoss: 0.639889\n",
      "Train Epoch: 16 [5760/8838 (65%)]\tLoss: 0.638353\n",
      "Train Epoch: 16 [6400/8838 (72%)]\tLoss: 0.454324\n",
      "Train Epoch: 16 [7040/8838 (79%)]\tLoss: 0.608246\n",
      "Train Epoch: 16 [7680/8838 (86%)]\tLoss: 0.533535\n",
      "Train Epoch: 16 [8320/8838 (94%)]\tLoss: 0.517372\n",
      "Train Epoch: 17 [0/8838 (0%)]\tLoss: 0.786295\n",
      "Train Epoch: 17 [640/8838 (7%)]\tLoss: 0.456218\n",
      "Train Epoch: 17 [1280/8838 (14%)]\tLoss: 0.677987\n",
      "Train Epoch: 17 [1920/8838 (22%)]\tLoss: 0.871274\n",
      "Train Epoch: 17 [2560/8838 (29%)]\tLoss: 0.781878\n",
      "Train Epoch: 17 [3200/8838 (36%)]\tLoss: 0.585156\n",
      "Train Epoch: 17 [3840/8838 (43%)]\tLoss: 0.643333\n",
      "Train Epoch: 17 [4480/8838 (50%)]\tLoss: 0.628586\n",
      "Train Epoch: 17 [5120/8838 (58%)]\tLoss: 0.844515\n",
      "Train Epoch: 17 [5760/8838 (65%)]\tLoss: 0.866174\n",
      "Train Epoch: 17 [6400/8838 (72%)]\tLoss: 0.688002\n",
      "Train Epoch: 17 [7040/8838 (79%)]\tLoss: 0.496127\n",
      "Train Epoch: 17 [7680/8838 (86%)]\tLoss: 0.589530\n",
      "Train Epoch: 17 [8320/8838 (94%)]\tLoss: 0.984240\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/6366 (0%)]\tLoss: 2.188782\n",
      "Train Epoch: 1 [640/6366 (10%)]\tLoss: 0.869017\n",
      "Train Epoch: 1 [1280/6366 (20%)]\tLoss: 0.677004\n",
      "Train Epoch: 1 [1920/6366 (30%)]\tLoss: 0.843984\n",
      "Train Epoch: 1 [2560/6366 (40%)]\tLoss: 0.674457\n",
      "Train Epoch: 1 [3200/6366 (50%)]\tLoss: 0.478527\n",
      "Train Epoch: 1 [3840/6366 (60%)]\tLoss: 0.542282\n",
      "Train Epoch: 1 [4480/6366 (70%)]\tLoss: 0.522690\n",
      "Train Epoch: 1 [5120/6366 (80%)]\tLoss: 0.493359\n",
      "Train Epoch: 1 [5760/6366 (90%)]\tLoss: 0.826407\n",
      "Train Epoch: 2 [0/6366 (0%)]\tLoss: 0.561584\n",
      "Train Epoch: 2 [640/6366 (10%)]\tLoss: 0.712359\n",
      "Train Epoch: 2 [1280/6366 (20%)]\tLoss: 0.476741\n",
      "Train Epoch: 2 [1920/6366 (30%)]\tLoss: 0.356202\n",
      "Train Epoch: 2 [2560/6366 (40%)]\tLoss: 0.535675\n",
      "Train Epoch: 2 [3200/6366 (50%)]\tLoss: 0.777045\n",
      "Train Epoch: 2 [3840/6366 (60%)]\tLoss: 0.576890\n",
      "Train Epoch: 2 [4480/6366 (70%)]\tLoss: 0.593874\n",
      "Train Epoch: 2 [5120/6366 (80%)]\tLoss: 0.289555\n",
      "Train Epoch: 2 [5760/6366 (90%)]\tLoss: 0.713177\n",
      "Train Epoch: 3 [0/6366 (0%)]\tLoss: 0.528495\n",
      "Train Epoch: 3 [640/6366 (10%)]\tLoss: 0.472730\n",
      "Train Epoch: 3 [1280/6366 (20%)]\tLoss: 0.463912\n",
      "Train Epoch: 3 [1920/6366 (30%)]\tLoss: 0.488653\n",
      "Train Epoch: 3 [2560/6366 (40%)]\tLoss: 0.354180\n",
      "Train Epoch: 3 [3200/6366 (50%)]\tLoss: 0.555279\n",
      "Train Epoch: 3 [3840/6366 (60%)]\tLoss: 0.617779\n",
      "Train Epoch: 3 [4480/6366 (70%)]\tLoss: 0.398728\n",
      "Train Epoch: 3 [5120/6366 (80%)]\tLoss: 0.456708\n",
      "Train Epoch: 3 [5760/6366 (90%)]\tLoss: 0.532557\n",
      "Train Epoch: 4 [0/6366 (0%)]\tLoss: 0.503775\n",
      "Train Epoch: 4 [640/6366 (10%)]\tLoss: 0.343903\n",
      "Train Epoch: 4 [1280/6366 (20%)]\tLoss: 0.343567\n",
      "Train Epoch: 4 [1920/6366 (30%)]\tLoss: 0.620202\n",
      "Train Epoch: 4 [2560/6366 (40%)]\tLoss: 0.450371\n",
      "Train Epoch: 4 [3200/6366 (50%)]\tLoss: 0.457315\n",
      "Train Epoch: 4 [3840/6366 (60%)]\tLoss: 0.614674\n",
      "Train Epoch: 4 [4480/6366 (70%)]\tLoss: 0.540696\n",
      "Train Epoch: 4 [5120/6366 (80%)]\tLoss: 0.480293\n",
      "Train Epoch: 4 [5760/6366 (90%)]\tLoss: 0.650724\n",
      "Train Epoch: 5 [0/6366 (0%)]\tLoss: 0.656933\n",
      "Train Epoch: 5 [640/6366 (10%)]\tLoss: 0.517542\n",
      "Train Epoch: 5 [1280/6366 (20%)]\tLoss: 0.390440\n",
      "Train Epoch: 5 [1920/6366 (30%)]\tLoss: 0.236285\n",
      "Train Epoch: 5 [2560/6366 (40%)]\tLoss: 0.399196\n",
      "Train Epoch: 5 [3200/6366 (50%)]\tLoss: 0.671803\n",
      "Train Epoch: 5 [3840/6366 (60%)]\tLoss: 0.371319\n",
      "Train Epoch: 5 [4480/6366 (70%)]\tLoss: 0.406549\n",
      "Train Epoch: 5 [5120/6366 (80%)]\tLoss: 0.493093\n",
      "Train Epoch: 5 [5760/6366 (90%)]\tLoss: 0.473126\n",
      "Train Epoch: 6 [0/6366 (0%)]\tLoss: 0.430403\n",
      "Train Epoch: 6 [640/6366 (10%)]\tLoss: 0.361124\n",
      "Train Epoch: 6 [1280/6366 (20%)]\tLoss: 0.465690\n",
      "Train Epoch: 6 [1920/6366 (30%)]\tLoss: 0.425320\n",
      "Train Epoch: 6 [2560/6366 (40%)]\tLoss: 0.363421\n",
      "Train Epoch: 6 [3200/6366 (50%)]\tLoss: 0.683770\n",
      "Train Epoch: 6 [3840/6366 (60%)]\tLoss: 0.597051\n",
      "Train Epoch: 6 [4480/6366 (70%)]\tLoss: 0.543148\n",
      "Train Epoch: 6 [5120/6366 (80%)]\tLoss: 0.416717\n",
      "Train Epoch: 6 [5760/6366 (90%)]\tLoss: 0.392029\n",
      "Train Epoch: 7 [0/6366 (0%)]\tLoss: 0.399115\n",
      "Train Epoch: 7 [640/6366 (10%)]\tLoss: 0.429404\n",
      "Train Epoch: 7 [1280/6366 (20%)]\tLoss: 0.395132\n",
      "Train Epoch: 7 [1920/6366 (30%)]\tLoss: 0.820437\n",
      "Train Epoch: 7 [2560/6366 (40%)]\tLoss: 0.346639\n",
      "Train Epoch: 7 [3200/6366 (50%)]\tLoss: 0.246782\n",
      "Train Epoch: 7 [3840/6366 (60%)]\tLoss: 0.646891\n",
      "Train Epoch: 7 [4480/6366 (70%)]\tLoss: 0.625263\n",
      "Train Epoch: 7 [5120/6366 (80%)]\tLoss: 0.439556\n",
      "Train Epoch: 7 [5760/6366 (90%)]\tLoss: 0.371318\n",
      "Train Epoch: 8 [0/6366 (0%)]\tLoss: 0.240054\n",
      "Train Epoch: 8 [640/6366 (10%)]\tLoss: 0.604492\n",
      "Train Epoch: 8 [1280/6366 (20%)]\tLoss: 0.479500\n",
      "Train Epoch: 8 [1920/6366 (30%)]\tLoss: 0.386431\n",
      "Train Epoch: 8 [2560/6366 (40%)]\tLoss: 0.632561\n",
      "Train Epoch: 8 [3200/6366 (50%)]\tLoss: 0.321447\n",
      "Train Epoch: 8 [3840/6366 (60%)]\tLoss: 0.283043\n",
      "Train Epoch: 8 [4480/6366 (70%)]\tLoss: 0.351005\n",
      "Train Epoch: 8 [5120/6366 (80%)]\tLoss: 0.457360\n",
      "Train Epoch: 8 [5760/6366 (90%)]\tLoss: 0.554816\n",
      "Train Epoch: 9 [0/6366 (0%)]\tLoss: 0.716234\n",
      "Train Epoch: 9 [640/6366 (10%)]\tLoss: 0.368416\n",
      "Train Epoch: 9 [1280/6366 (20%)]\tLoss: 0.316545\n",
      "Train Epoch: 9 [1920/6366 (30%)]\tLoss: 0.444519\n",
      "Train Epoch: 9 [2560/6366 (40%)]\tLoss: 0.329976\n",
      "Train Epoch: 9 [3200/6366 (50%)]\tLoss: 0.423727\n",
      "Train Epoch: 9 [3840/6366 (60%)]\tLoss: 0.237992\n",
      "Train Epoch: 9 [4480/6366 (70%)]\tLoss: 0.391579\n",
      "Train Epoch: 9 [5120/6366 (80%)]\tLoss: 0.431026\n",
      "Train Epoch: 9 [5760/6366 (90%)]\tLoss: 0.581186\n",
      "Train Epoch: 10 [0/6366 (0%)]\tLoss: 0.283437\n",
      "Train Epoch: 10 [640/6366 (10%)]\tLoss: 0.470743\n",
      "Train Epoch: 10 [1280/6366 (20%)]\tLoss: 0.487710\n",
      "Train Epoch: 10 [1920/6366 (30%)]\tLoss: 0.418107\n",
      "Train Epoch: 10 [2560/6366 (40%)]\tLoss: 0.383646\n",
      "Train Epoch: 10 [3200/6366 (50%)]\tLoss: 0.618258\n",
      "Train Epoch: 10 [3840/6366 (60%)]\tLoss: 0.457908\n",
      "Train Epoch: 10 [4480/6366 (70%)]\tLoss: 0.552492\n",
      "Train Epoch: 10 [5120/6366 (80%)]\tLoss: 0.432612\n",
      "Train Epoch: 10 [5760/6366 (90%)]\tLoss: 0.250979\n",
      "Train Epoch: 11 [0/6366 (0%)]\tLoss: 0.241689\n",
      "Train Epoch: 11 [640/6366 (10%)]\tLoss: 0.291879\n",
      "Train Epoch: 11 [1280/6366 (20%)]\tLoss: 0.606406\n",
      "Train Epoch: 11 [1920/6366 (30%)]\tLoss: 0.224599\n",
      "Train Epoch: 11 [2560/6366 (40%)]\tLoss: 0.552891\n",
      "Train Epoch: 11 [3200/6366 (50%)]\tLoss: 0.563484\n",
      "Train Epoch: 11 [3840/6366 (60%)]\tLoss: 0.280102\n",
      "Train Epoch: 11 [4480/6366 (70%)]\tLoss: 0.357715\n",
      "Train Epoch: 11 [5120/6366 (80%)]\tLoss: 0.364167\n",
      "Train Epoch: 11 [5760/6366 (90%)]\tLoss: 0.317547\n",
      "Train Epoch: 12 [0/6366 (0%)]\tLoss: 0.240147\n",
      "Train Epoch: 12 [640/6366 (10%)]\tLoss: 0.328600\n",
      "Train Epoch: 12 [1280/6366 (20%)]\tLoss: 0.593532\n",
      "Train Epoch: 12 [1920/6366 (30%)]\tLoss: 0.443407\n",
      "Train Epoch: 12 [2560/6366 (40%)]\tLoss: 0.552595\n",
      "Train Epoch: 12 [3200/6366 (50%)]\tLoss: 0.325025\n",
      "Train Epoch: 12 [3840/6366 (60%)]\tLoss: 0.305865\n",
      "Train Epoch: 12 [4480/6366 (70%)]\tLoss: 0.496965\n",
      "Train Epoch: 12 [5120/6366 (80%)]\tLoss: 0.157721\n",
      "Train Epoch: 12 [5760/6366 (90%)]\tLoss: 0.404519\n",
      "Train Epoch: 13 [0/6366 (0%)]\tLoss: 0.453099\n",
      "Train Epoch: 13 [640/6366 (10%)]\tLoss: 0.259075\n",
      "Train Epoch: 13 [1280/6366 (20%)]\tLoss: 0.434150\n",
      "Train Epoch: 13 [1920/6366 (30%)]\tLoss: 0.487408\n",
      "Train Epoch: 13 [2560/6366 (40%)]\tLoss: 0.169167\n",
      "Train Epoch: 13 [3200/6366 (50%)]\tLoss: 0.452161\n",
      "Train Epoch: 13 [3840/6366 (60%)]\tLoss: 0.368574\n",
      "Train Epoch: 13 [4480/6366 (70%)]\tLoss: 0.477480\n",
      "Train Epoch: 13 [5120/6366 (80%)]\tLoss: 0.747476\n",
      "Train Epoch: 13 [5760/6366 (90%)]\tLoss: 0.554568\n",
      "Train Epoch: 14 [0/6366 (0%)]\tLoss: 0.639404\n",
      "Train Epoch: 14 [640/6366 (10%)]\tLoss: 0.726914\n",
      "Train Epoch: 14 [1280/6366 (20%)]\tLoss: 0.288894\n",
      "Train Epoch: 14 [1920/6366 (30%)]\tLoss: 0.230602\n",
      "Train Epoch: 14 [2560/6366 (40%)]\tLoss: 0.264495\n",
      "Train Epoch: 14 [3200/6366 (50%)]\tLoss: 0.423255\n",
      "Train Epoch: 14 [3840/6366 (60%)]\tLoss: 0.452130\n",
      "Train Epoch: 14 [4480/6366 (70%)]\tLoss: 0.429232\n",
      "Train Epoch: 14 [5120/6366 (80%)]\tLoss: 0.470716\n",
      "Train Epoch: 14 [5760/6366 (90%)]\tLoss: 0.522395\n",
      "Train Epoch: 15 [0/6366 (0%)]\tLoss: 0.351578\n",
      "Train Epoch: 15 [640/6366 (10%)]\tLoss: 0.390259\n",
      "Train Epoch: 15 [1280/6366 (20%)]\tLoss: 0.326589\n",
      "Train Epoch: 15 [1920/6366 (30%)]\tLoss: 0.235149\n",
      "Train Epoch: 15 [2560/6366 (40%)]\tLoss: 0.374476\n",
      "Train Epoch: 15 [3200/6366 (50%)]\tLoss: 0.334107\n",
      "Train Epoch: 15 [3840/6366 (60%)]\tLoss: 0.232558\n",
      "Train Epoch: 15 [4480/6366 (70%)]\tLoss: 0.247373\n",
      "Train Epoch: 15 [5120/6366 (80%)]\tLoss: 0.167823\n",
      "Train Epoch: 15 [5760/6366 (90%)]\tLoss: 0.242342\n",
      "Train Epoch: 16 [0/6366 (0%)]\tLoss: 0.274422\n",
      "Train Epoch: 16 [640/6366 (10%)]\tLoss: 0.257690\n",
      "Train Epoch: 16 [1280/6366 (20%)]\tLoss: 0.147257\n",
      "Train Epoch: 16 [1920/6366 (30%)]\tLoss: 0.234874\n",
      "Train Epoch: 16 [2560/6366 (40%)]\tLoss: 0.479252\n",
      "Train Epoch: 16 [3200/6366 (50%)]\tLoss: 0.405206\n",
      "Train Epoch: 16 [3840/6366 (60%)]\tLoss: 0.446919\n",
      "Train Epoch: 16 [4480/6366 (70%)]\tLoss: 0.226915\n",
      "Train Epoch: 16 [5120/6366 (80%)]\tLoss: 0.369675\n",
      "Train Epoch: 16 [5760/6366 (90%)]\tLoss: 0.274706\n",
      "Train Epoch: 17 [0/6366 (0%)]\tLoss: 0.235609\n",
      "Train Epoch: 17 [640/6366 (10%)]\tLoss: 0.282300\n",
      "Train Epoch: 17 [1280/6366 (20%)]\tLoss: 0.558994\n",
      "Train Epoch: 17 [1920/6366 (30%)]\tLoss: 0.524737\n",
      "Train Epoch: 17 [2560/6366 (40%)]\tLoss: 0.173479\n",
      "Train Epoch: 17 [3200/6366 (50%)]\tLoss: 0.478123\n",
      "Train Epoch: 17 [3840/6366 (60%)]\tLoss: 0.257484\n",
      "Train Epoch: 17 [4480/6366 (70%)]\tLoss: 0.310632\n",
      "Train Epoch: 17 [5120/6366 (80%)]\tLoss: 0.665621\n",
      "Train Epoch: 17 [5760/6366 (90%)]\tLoss: 0.482505\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/9451 (0%)]\tLoss: 2.118769\n",
      "Train Epoch: 1 [640/9451 (7%)]\tLoss: 1.497227\n",
      "Train Epoch: 1 [1280/9451 (14%)]\tLoss: 1.377949\n",
      "Train Epoch: 1 [1920/9451 (20%)]\tLoss: 1.213067\n",
      "Train Epoch: 1 [2560/9451 (27%)]\tLoss: 1.131352\n",
      "Train Epoch: 1 [3200/9451 (34%)]\tLoss: 1.111306\n",
      "Train Epoch: 1 [3840/9451 (41%)]\tLoss: 1.182685\n",
      "Train Epoch: 1 [4480/9451 (47%)]\tLoss: 1.055899\n",
      "Train Epoch: 1 [5120/9451 (54%)]\tLoss: 0.933962\n",
      "Train Epoch: 1 [5760/9451 (61%)]\tLoss: 1.016840\n",
      "Train Epoch: 1 [6400/9451 (68%)]\tLoss: 1.294321\n",
      "Train Epoch: 1 [7040/9451 (74%)]\tLoss: 1.298354\n",
      "Train Epoch: 1 [7680/9451 (81%)]\tLoss: 1.213936\n",
      "Train Epoch: 1 [8320/9451 (88%)]\tLoss: 1.027298\n",
      "Train Epoch: 1 [8960/9451 (95%)]\tLoss: 1.171785\n",
      "Train Epoch: 2 [0/9451 (0%)]\tLoss: 1.163935\n",
      "Train Epoch: 2 [640/9451 (7%)]\tLoss: 0.927642\n",
      "Train Epoch: 2 [1280/9451 (14%)]\tLoss: 1.042347\n",
      "Train Epoch: 2 [1920/9451 (20%)]\tLoss: 1.046733\n",
      "Train Epoch: 2 [2560/9451 (27%)]\tLoss: 1.265571\n",
      "Train Epoch: 2 [3200/9451 (34%)]\tLoss: 1.227509\n",
      "Train Epoch: 2 [3840/9451 (41%)]\tLoss: 1.289849\n",
      "Train Epoch: 2 [4480/9451 (47%)]\tLoss: 0.770281\n",
      "Train Epoch: 2 [5120/9451 (54%)]\tLoss: 1.156526\n",
      "Train Epoch: 2 [5760/9451 (61%)]\tLoss: 1.175093\n",
      "Train Epoch: 2 [6400/9451 (68%)]\tLoss: 0.886959\n",
      "Train Epoch: 2 [7040/9451 (74%)]\tLoss: 0.906267\n",
      "Train Epoch: 2 [7680/9451 (81%)]\tLoss: 1.170871\n",
      "Train Epoch: 2 [8320/9451 (88%)]\tLoss: 0.892943\n",
      "Train Epoch: 2 [8960/9451 (95%)]\tLoss: 1.267682\n",
      "Train Epoch: 3 [0/9451 (0%)]\tLoss: 1.066589\n",
      "Train Epoch: 3 [640/9451 (7%)]\tLoss: 0.936489\n",
      "Train Epoch: 3 [1280/9451 (14%)]\tLoss: 1.562130\n",
      "Train Epoch: 3 [1920/9451 (20%)]\tLoss: 0.935488\n",
      "Train Epoch: 3 [2560/9451 (27%)]\tLoss: 0.858810\n",
      "Train Epoch: 3 [3200/9451 (34%)]\tLoss: 0.872152\n",
      "Train Epoch: 3 [3840/9451 (41%)]\tLoss: 1.119473\n",
      "Train Epoch: 3 [4480/9451 (47%)]\tLoss: 1.130843\n",
      "Train Epoch: 3 [5120/9451 (54%)]\tLoss: 1.068080\n",
      "Train Epoch: 3 [5760/9451 (61%)]\tLoss: 0.798484\n",
      "Train Epoch: 3 [6400/9451 (68%)]\tLoss: 1.257586\n",
      "Train Epoch: 3 [7040/9451 (74%)]\tLoss: 1.129377\n",
      "Train Epoch: 3 [7680/9451 (81%)]\tLoss: 1.008425\n",
      "Train Epoch: 3 [8320/9451 (88%)]\tLoss: 0.993637\n",
      "Train Epoch: 3 [8960/9451 (95%)]\tLoss: 0.876773\n",
      "Train Epoch: 4 [0/9451 (0%)]\tLoss: 0.867248\n",
      "Train Epoch: 4 [640/9451 (7%)]\tLoss: 1.144291\n",
      "Train Epoch: 4 [1280/9451 (14%)]\tLoss: 0.864160\n",
      "Train Epoch: 4 [1920/9451 (20%)]\tLoss: 0.981360\n",
      "Train Epoch: 4 [2560/9451 (27%)]\tLoss: 0.724547\n",
      "Train Epoch: 4 [3200/9451 (34%)]\tLoss: 0.985242\n",
      "Train Epoch: 4 [3840/9451 (41%)]\tLoss: 0.692977\n",
      "Train Epoch: 4 [4480/9451 (47%)]\tLoss: 0.942655\n",
      "Train Epoch: 4 [5120/9451 (54%)]\tLoss: 1.135186\n",
      "Train Epoch: 4 [5760/9451 (61%)]\tLoss: 0.952489\n",
      "Train Epoch: 4 [6400/9451 (68%)]\tLoss: 1.011536\n",
      "Train Epoch: 4 [7040/9451 (74%)]\tLoss: 1.249337\n",
      "Train Epoch: 4 [7680/9451 (81%)]\tLoss: 0.864132\n",
      "Train Epoch: 4 [8320/9451 (88%)]\tLoss: 1.172561\n",
      "Train Epoch: 4 [8960/9451 (95%)]\tLoss: 1.312196\n",
      "Train Epoch: 5 [0/9451 (0%)]\tLoss: 1.068772\n",
      "Train Epoch: 5 [640/9451 (7%)]\tLoss: 0.871877\n",
      "Train Epoch: 5 [1280/9451 (14%)]\tLoss: 0.985854\n",
      "Train Epoch: 5 [1920/9451 (20%)]\tLoss: 1.024825\n",
      "Train Epoch: 5 [2560/9451 (27%)]\tLoss: 0.837342\n",
      "Train Epoch: 5 [3200/9451 (34%)]\tLoss: 0.861329\n",
      "Train Epoch: 5 [3840/9451 (41%)]\tLoss: 0.883449\n",
      "Train Epoch: 5 [4480/9451 (47%)]\tLoss: 0.947804\n",
      "Train Epoch: 5 [5120/9451 (54%)]\tLoss: 0.667767\n",
      "Train Epoch: 5 [5760/9451 (61%)]\tLoss: 0.939051\n",
      "Train Epoch: 5 [6400/9451 (68%)]\tLoss: 0.791935\n",
      "Train Epoch: 5 [7040/9451 (74%)]\tLoss: 0.914696\n",
      "Train Epoch: 5 [7680/9451 (81%)]\tLoss: 1.240235\n",
      "Train Epoch: 5 [8320/9451 (88%)]\tLoss: 0.917544\n",
      "Train Epoch: 5 [8960/9451 (95%)]\tLoss: 1.020906\n",
      "Train Epoch: 6 [0/9451 (0%)]\tLoss: 0.844070\n",
      "Train Epoch: 6 [640/9451 (7%)]\tLoss: 1.081892\n",
      "Train Epoch: 6 [1280/9451 (14%)]\tLoss: 0.841083\n",
      "Train Epoch: 6 [1920/9451 (20%)]\tLoss: 0.989591\n",
      "Train Epoch: 6 [2560/9451 (27%)]\tLoss: 0.950762\n",
      "Train Epoch: 6 [3200/9451 (34%)]\tLoss: 1.067796\n",
      "Train Epoch: 6 [3840/9451 (41%)]\tLoss: 1.283292\n",
      "Train Epoch: 6 [4480/9451 (47%)]\tLoss: 0.919407\n",
      "Train Epoch: 6 [5120/9451 (54%)]\tLoss: 0.978352\n",
      "Train Epoch: 6 [5760/9451 (61%)]\tLoss: 0.965523\n",
      "Train Epoch: 6 [6400/9451 (68%)]\tLoss: 1.012897\n",
      "Train Epoch: 6 [7040/9451 (74%)]\tLoss: 1.033079\n",
      "Train Epoch: 6 [7680/9451 (81%)]\tLoss: 0.836449\n",
      "Train Epoch: 6 [8320/9451 (88%)]\tLoss: 0.881619\n",
      "Train Epoch: 6 [8960/9451 (95%)]\tLoss: 0.656914\n",
      "Train Epoch: 7 [0/9451 (0%)]\tLoss: 1.101471\n",
      "Train Epoch: 7 [640/9451 (7%)]\tLoss: 1.113205\n",
      "Train Epoch: 7 [1280/9451 (14%)]\tLoss: 1.097017\n",
      "Train Epoch: 7 [1920/9451 (20%)]\tLoss: 0.898123\n",
      "Train Epoch: 7 [2560/9451 (27%)]\tLoss: 0.803484\n",
      "Train Epoch: 7 [3200/9451 (34%)]\tLoss: 0.929826\n",
      "Train Epoch: 7 [3840/9451 (41%)]\tLoss: 0.814156\n",
      "Train Epoch: 7 [4480/9451 (47%)]\tLoss: 0.881809\n",
      "Train Epoch: 7 [5120/9451 (54%)]\tLoss: 1.099248\n",
      "Train Epoch: 7 [5760/9451 (61%)]\tLoss: 0.751179\n",
      "Train Epoch: 7 [6400/9451 (68%)]\tLoss: 0.640613\n",
      "Train Epoch: 7 [7040/9451 (74%)]\tLoss: 0.818827\n",
      "Train Epoch: 7 [7680/9451 (81%)]\tLoss: 1.059038\n",
      "Train Epoch: 7 [8320/9451 (88%)]\tLoss: 1.066723\n",
      "Train Epoch: 7 [8960/9451 (95%)]\tLoss: 1.054901\n",
      "Train Epoch: 8 [0/9451 (0%)]\tLoss: 1.097484\n",
      "Train Epoch: 8 [640/9451 (7%)]\tLoss: 0.732677\n",
      "Train Epoch: 8 [1280/9451 (14%)]\tLoss: 1.084539\n",
      "Train Epoch: 8 [1920/9451 (20%)]\tLoss: 0.714516\n",
      "Train Epoch: 8 [2560/9451 (27%)]\tLoss: 0.855789\n",
      "Train Epoch: 8 [3200/9451 (34%)]\tLoss: 0.938458\n",
      "Train Epoch: 8 [3840/9451 (41%)]\tLoss: 0.884222\n",
      "Train Epoch: 8 [4480/9451 (47%)]\tLoss: 0.794320\n",
      "Train Epoch: 8 [5120/9451 (54%)]\tLoss: 0.947632\n",
      "Train Epoch: 8 [5760/9451 (61%)]\tLoss: 0.922537\n",
      "Train Epoch: 8 [6400/9451 (68%)]\tLoss: 1.069382\n",
      "Train Epoch: 8 [7040/9451 (74%)]\tLoss: 1.045088\n",
      "Train Epoch: 8 [7680/9451 (81%)]\tLoss: 0.913556\n",
      "Train Epoch: 8 [8320/9451 (88%)]\tLoss: 0.991617\n",
      "Train Epoch: 8 [8960/9451 (95%)]\tLoss: 0.877447\n",
      "Train Epoch: 9 [0/9451 (0%)]\tLoss: 0.839049\n",
      "Train Epoch: 9 [640/9451 (7%)]\tLoss: 0.762618\n",
      "Train Epoch: 9 [1280/9451 (14%)]\tLoss: 0.818621\n",
      "Train Epoch: 9 [1920/9451 (20%)]\tLoss: 0.917488\n",
      "Train Epoch: 9 [2560/9451 (27%)]\tLoss: 0.752766\n",
      "Train Epoch: 9 [3200/9451 (34%)]\tLoss: 1.065986\n",
      "Train Epoch: 9 [3840/9451 (41%)]\tLoss: 0.726632\n",
      "Train Epoch: 9 [4480/9451 (47%)]\tLoss: 1.059729\n",
      "Train Epoch: 9 [5120/9451 (54%)]\tLoss: 0.868390\n",
      "Train Epoch: 9 [5760/9451 (61%)]\tLoss: 1.225062\n",
      "Train Epoch: 9 [6400/9451 (68%)]\tLoss: 0.739088\n",
      "Train Epoch: 9 [7040/9451 (74%)]\tLoss: 0.586515\n",
      "Train Epoch: 9 [7680/9451 (81%)]\tLoss: 0.649208\n",
      "Train Epoch: 9 [8320/9451 (88%)]\tLoss: 0.871353\n",
      "Train Epoch: 9 [8960/9451 (95%)]\tLoss: 0.982008\n",
      "Train Epoch: 10 [0/9451 (0%)]\tLoss: 0.936649\n",
      "Train Epoch: 10 [640/9451 (7%)]\tLoss: 1.045264\n",
      "Train Epoch: 10 [1280/9451 (14%)]\tLoss: 0.841756\n",
      "Train Epoch: 10 [1920/9451 (20%)]\tLoss: 0.663536\n",
      "Train Epoch: 10 [2560/9451 (27%)]\tLoss: 0.752530\n",
      "Train Epoch: 10 [3200/9451 (34%)]\tLoss: 0.802900\n",
      "Train Epoch: 10 [3840/9451 (41%)]\tLoss: 0.790763\n",
      "Train Epoch: 10 [4480/9451 (47%)]\tLoss: 0.780725\n",
      "Train Epoch: 10 [5120/9451 (54%)]\tLoss: 0.698998\n",
      "Train Epoch: 10 [5760/9451 (61%)]\tLoss: 0.738280\n",
      "Train Epoch: 10 [6400/9451 (68%)]\tLoss: 0.798051\n",
      "Train Epoch: 10 [7040/9451 (74%)]\tLoss: 0.934147\n",
      "Train Epoch: 10 [7680/9451 (81%)]\tLoss: 1.121517\n",
      "Train Epoch: 10 [8320/9451 (88%)]\tLoss: 0.775250\n",
      "Train Epoch: 10 [8960/9451 (95%)]\tLoss: 0.862725\n",
      "Train Epoch: 11 [0/9451 (0%)]\tLoss: 0.861205\n",
      "Train Epoch: 11 [640/9451 (7%)]\tLoss: 0.993504\n",
      "Train Epoch: 11 [1280/9451 (14%)]\tLoss: 0.857278\n",
      "Train Epoch: 11 [1920/9451 (20%)]\tLoss: 0.716519\n",
      "Train Epoch: 11 [2560/9451 (27%)]\tLoss: 0.746861\n",
      "Train Epoch: 11 [3200/9451 (34%)]\tLoss: 0.803526\n",
      "Train Epoch: 11 [3840/9451 (41%)]\tLoss: 0.651387\n",
      "Train Epoch: 11 [4480/9451 (47%)]\tLoss: 0.714240\n",
      "Train Epoch: 11 [5120/9451 (54%)]\tLoss: 0.756515\n",
      "Train Epoch: 11 [5760/9451 (61%)]\tLoss: 0.537533\n",
      "Train Epoch: 11 [6400/9451 (68%)]\tLoss: 0.605905\n",
      "Train Epoch: 11 [7040/9451 (74%)]\tLoss: 1.014661\n",
      "Train Epoch: 11 [7680/9451 (81%)]\tLoss: 0.935946\n",
      "Train Epoch: 11 [8320/9451 (88%)]\tLoss: 0.790566\n",
      "Train Epoch: 11 [8960/9451 (95%)]\tLoss: 0.973080\n",
      "Train Epoch: 12 [0/9451 (0%)]\tLoss: 1.150287\n",
      "Train Epoch: 12 [640/9451 (7%)]\tLoss: 0.800256\n",
      "Train Epoch: 12 [1280/9451 (14%)]\tLoss: 0.643430\n",
      "Train Epoch: 12 [1920/9451 (20%)]\tLoss: 0.751707\n",
      "Train Epoch: 12 [2560/9451 (27%)]\tLoss: 0.765462\n",
      "Train Epoch: 12 [3200/9451 (34%)]\tLoss: 0.853215\n",
      "Train Epoch: 12 [3840/9451 (41%)]\tLoss: 0.846344\n",
      "Train Epoch: 12 [4480/9451 (47%)]\tLoss: 0.697159\n",
      "Train Epoch: 12 [5120/9451 (54%)]\tLoss: 1.012837\n",
      "Train Epoch: 12 [5760/9451 (61%)]\tLoss: 0.829285\n",
      "Train Epoch: 12 [6400/9451 (68%)]\tLoss: 0.958605\n",
      "Train Epoch: 12 [7040/9451 (74%)]\tLoss: 0.592990\n",
      "Train Epoch: 12 [7680/9451 (81%)]\tLoss: 0.907984\n",
      "Train Epoch: 12 [8320/9451 (88%)]\tLoss: 0.936787\n",
      "Train Epoch: 12 [8960/9451 (95%)]\tLoss: 0.920681\n",
      "Train Epoch: 13 [0/9451 (0%)]\tLoss: 0.826787\n",
      "Train Epoch: 13 [640/9451 (7%)]\tLoss: 0.689354\n",
      "Train Epoch: 13 [1280/9451 (14%)]\tLoss: 0.720319\n",
      "Train Epoch: 13 [1920/9451 (20%)]\tLoss: 0.577392\n",
      "Train Epoch: 13 [2560/9451 (27%)]\tLoss: 0.815635\n",
      "Train Epoch: 13 [3200/9451 (34%)]\tLoss: 0.863945\n",
      "Train Epoch: 13 [3840/9451 (41%)]\tLoss: 0.811647\n",
      "Train Epoch: 13 [4480/9451 (47%)]\tLoss: 1.048798\n",
      "Train Epoch: 13 [5120/9451 (54%)]\tLoss: 0.923301\n",
      "Train Epoch: 13 [5760/9451 (61%)]\tLoss: 1.075826\n",
      "Train Epoch: 13 [6400/9451 (68%)]\tLoss: 0.956933\n",
      "Train Epoch: 13 [7040/9451 (74%)]\tLoss: 0.685596\n",
      "Train Epoch: 13 [7680/9451 (81%)]\tLoss: 1.092704\n",
      "Train Epoch: 13 [8320/9451 (88%)]\tLoss: 0.775696\n",
      "Train Epoch: 13 [8960/9451 (95%)]\tLoss: 0.868330\n",
      "Train Epoch: 14 [0/9451 (0%)]\tLoss: 0.797993\n",
      "Train Epoch: 14 [640/9451 (7%)]\tLoss: 0.793474\n",
      "Train Epoch: 14 [1280/9451 (14%)]\tLoss: 0.914951\n",
      "Train Epoch: 14 [1920/9451 (20%)]\tLoss: 0.951742\n",
      "Train Epoch: 14 [2560/9451 (27%)]\tLoss: 0.891910\n",
      "Train Epoch: 14 [3200/9451 (34%)]\tLoss: 0.858353\n",
      "Train Epoch: 14 [3840/9451 (41%)]\tLoss: 0.782906\n",
      "Train Epoch: 14 [4480/9451 (47%)]\tLoss: 0.816090\n",
      "Train Epoch: 14 [5120/9451 (54%)]\tLoss: 0.777007\n",
      "Train Epoch: 14 [5760/9451 (61%)]\tLoss: 0.998303\n",
      "Train Epoch: 14 [6400/9451 (68%)]\tLoss: 0.762694\n",
      "Train Epoch: 14 [7040/9451 (74%)]\tLoss: 0.769859\n",
      "Train Epoch: 14 [7680/9451 (81%)]\tLoss: 0.814841\n",
      "Train Epoch: 14 [8320/9451 (88%)]\tLoss: 0.959389\n",
      "Train Epoch: 14 [8960/9451 (95%)]\tLoss: 0.655276\n",
      "Train Epoch: 15 [0/9451 (0%)]\tLoss: 0.839926\n",
      "Train Epoch: 15 [640/9451 (7%)]\tLoss: 0.719561\n",
      "Train Epoch: 15 [1280/9451 (14%)]\tLoss: 0.959540\n",
      "Train Epoch: 15 [1920/9451 (20%)]\tLoss: 0.780515\n",
      "Train Epoch: 15 [2560/9451 (27%)]\tLoss: 0.751590\n",
      "Train Epoch: 15 [3200/9451 (34%)]\tLoss: 0.931142\n",
      "Train Epoch: 15 [3840/9451 (41%)]\tLoss: 0.801371\n",
      "Train Epoch: 15 [4480/9451 (47%)]\tLoss: 0.849562\n",
      "Train Epoch: 15 [5120/9451 (54%)]\tLoss: 0.656674\n",
      "Train Epoch: 15 [5760/9451 (61%)]\tLoss: 0.941319\n",
      "Train Epoch: 15 [6400/9451 (68%)]\tLoss: 0.620241\n",
      "Train Epoch: 15 [7040/9451 (74%)]\tLoss: 0.645958\n",
      "Train Epoch: 15 [7680/9451 (81%)]\tLoss: 0.972378\n",
      "Train Epoch: 15 [8320/9451 (88%)]\tLoss: 0.910147\n",
      "Train Epoch: 15 [8960/9451 (95%)]\tLoss: 1.013727\n",
      "Train Epoch: 16 [0/9451 (0%)]\tLoss: 0.596323\n",
      "Train Epoch: 16 [640/9451 (7%)]\tLoss: 0.900560\n",
      "Train Epoch: 16 [1280/9451 (14%)]\tLoss: 0.717694\n",
      "Train Epoch: 16 [1920/9451 (20%)]\tLoss: 0.900267\n",
      "Train Epoch: 16 [2560/9451 (27%)]\tLoss: 0.932075\n",
      "Train Epoch: 16 [3200/9451 (34%)]\tLoss: 0.785621\n",
      "Train Epoch: 16 [3840/9451 (41%)]\tLoss: 0.973986\n",
      "Train Epoch: 16 [4480/9451 (47%)]\tLoss: 0.645910\n",
      "Train Epoch: 16 [5120/9451 (54%)]\tLoss: 0.825427\n",
      "Train Epoch: 16 [5760/9451 (61%)]\tLoss: 0.798678\n",
      "Train Epoch: 16 [6400/9451 (68%)]\tLoss: 0.836389\n",
      "Train Epoch: 16 [7040/9451 (74%)]\tLoss: 0.672599\n",
      "Train Epoch: 16 [7680/9451 (81%)]\tLoss: 0.848710\n",
      "Train Epoch: 16 [8320/9451 (88%)]\tLoss: 0.830518\n",
      "Train Epoch: 16 [8960/9451 (95%)]\tLoss: 0.694574\n",
      "Train Epoch: 17 [0/9451 (0%)]\tLoss: 1.037097\n",
      "Train Epoch: 17 [640/9451 (7%)]\tLoss: 0.796306\n",
      "Train Epoch: 17 [1280/9451 (14%)]\tLoss: 0.691153\n",
      "Train Epoch: 17 [1920/9451 (20%)]\tLoss: 0.540590\n",
      "Train Epoch: 17 [2560/9451 (27%)]\tLoss: 1.042932\n",
      "Train Epoch: 17 [3200/9451 (34%)]\tLoss: 0.703608\n",
      "Train Epoch: 17 [3840/9451 (41%)]\tLoss: 0.867925\n",
      "Train Epoch: 17 [4480/9451 (47%)]\tLoss: 0.665463\n",
      "Train Epoch: 17 [5120/9451 (54%)]\tLoss: 0.816978\n",
      "Train Epoch: 17 [5760/9451 (61%)]\tLoss: 0.835987\n",
      "Train Epoch: 17 [6400/9451 (68%)]\tLoss: 0.884505\n",
      "Train Epoch: 17 [7040/9451 (74%)]\tLoss: 0.706379\n",
      "Train Epoch: 17 [7680/9451 (81%)]\tLoss: 0.781919\n",
      "Train Epoch: 17 [8320/9451 (88%)]\tLoss: 0.993957\n",
      "Train Epoch: 17 [8960/9451 (95%)]\tLoss: 0.739072\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/1026 (0%)]\tLoss: 1.934848\n",
      "Train Epoch: 1 [640/1026 (59%)]\tLoss: 1.034457\n",
      "Train Epoch: 2 [0/1026 (0%)]\tLoss: 1.167495\n",
      "Train Epoch: 2 [640/1026 (59%)]\tLoss: 1.042920\n",
      "Train Epoch: 3 [0/1026 (0%)]\tLoss: 1.661302\n",
      "Train Epoch: 3 [640/1026 (59%)]\tLoss: 1.224959\n",
      "Train Epoch: 4 [0/1026 (0%)]\tLoss: 1.065790\n",
      "Train Epoch: 4 [640/1026 (59%)]\tLoss: 0.986033\n",
      "Train Epoch: 5 [0/1026 (0%)]\tLoss: 1.132825\n",
      "Train Epoch: 5 [640/1026 (59%)]\tLoss: 1.162577\n",
      "Train Epoch: 6 [0/1026 (0%)]\tLoss: 1.044597\n",
      "Train Epoch: 6 [640/1026 (59%)]\tLoss: 0.968462\n",
      "Train Epoch: 7 [0/1026 (0%)]\tLoss: 1.388533\n",
      "Train Epoch: 7 [640/1026 (59%)]\tLoss: 0.867537\n",
      "Train Epoch: 8 [0/1026 (0%)]\tLoss: 1.319419\n",
      "Train Epoch: 8 [640/1026 (59%)]\tLoss: 1.323348\n",
      "Train Epoch: 9 [0/1026 (0%)]\tLoss: 1.264817\n",
      "Train Epoch: 9 [640/1026 (59%)]\tLoss: 0.993617\n",
      "Train Epoch: 10 [0/1026 (0%)]\tLoss: 1.289080\n",
      "Train Epoch: 10 [640/1026 (59%)]\tLoss: 1.179788\n",
      "Train Epoch: 11 [0/1026 (0%)]\tLoss: 1.021800\n",
      "Train Epoch: 11 [640/1026 (59%)]\tLoss: 1.119793\n",
      "Train Epoch: 12 [0/1026 (0%)]\tLoss: 0.932669\n",
      "Train Epoch: 12 [640/1026 (59%)]\tLoss: 0.895044\n",
      "Train Epoch: 13 [0/1026 (0%)]\tLoss: 0.952427\n",
      "Train Epoch: 13 [640/1026 (59%)]\tLoss: 1.209627\n",
      "Train Epoch: 14 [0/1026 (0%)]\tLoss: 0.963266\n",
      "Train Epoch: 14 [640/1026 (59%)]\tLoss: 1.000423\n",
      "Train Epoch: 15 [0/1026 (0%)]\tLoss: 1.230802\n",
      "Train Epoch: 15 [640/1026 (59%)]\tLoss: 0.930242\n",
      "Train Epoch: 16 [0/1026 (0%)]\tLoss: 0.901472\n",
      "Train Epoch: 16 [640/1026 (59%)]\tLoss: 0.952227\n",
      "Train Epoch: 17 [0/1026 (0%)]\tLoss: 1.265615\n",
      "Train Epoch: 17 [640/1026 (59%)]\tLoss: 1.029138\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.8329, Accuracy: 3249/10000 (32%)\n",
      "\n",
      "Round 4/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 1.624969\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.497486\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.548951\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.440259\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.403573\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.693242\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.660585\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.356281\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.303349\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.835976\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.428095\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.386060\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.167082\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.401536\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.407679\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.282405\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.544097\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.292284\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.239811\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.318406\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.521389\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.398105\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.310397\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.395936\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.318386\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.611710\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.357178\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.438067\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.137871\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.628168\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.149955\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.509326\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.265295\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.434440\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.389326\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.686862\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.363785\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.298126\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.363563\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.219842\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.390025\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.313876\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.460923\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.522237\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.338742\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.278731\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.316964\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.320572\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.250635\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.249381\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.213760\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.473321\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.380406\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.305153\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.269363\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.287302\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.183288\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.320113\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.450240\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.160069\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.249136\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.155128\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.149029\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.551857\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.335545\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.311450\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.188926\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.588118\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.319086\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.334875\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.356338\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.253154\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.398128\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.348777\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.499621\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.416366\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.339541\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.245515\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.066244\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.401299\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.269352\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.500340\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.267755\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.200044\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.517265\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.314543\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.292310\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.354014\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.391937\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.703549\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.435478\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.153565\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.195741\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.584158\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.345175\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.156173\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.158654\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.427590\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.385052\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.247210\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.267429\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.397880\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 1.510969\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 0.832441\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 0.867901\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 0.996627\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 1.194431\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 1.052959\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 0.840749\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 1.217576\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 1.010459\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 1.135274\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 1.165393\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 0.988229\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 0.823868\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 0.798079\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 1.103113\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 0.919302\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 0.942451\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 1.680046\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 1.165582\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.131486\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 1.145227\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 1.080404\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 1.027136\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 1.001434\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 0.998252\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 0.994247\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 0.990645\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 1.130665\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 0.815934\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 0.834115\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 1.181132\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 1.035179\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 0.735743\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 0.727336\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.976881\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 0.925081\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 1.040064\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 0.819046\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 0.961934\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 0.885891\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 0.864110\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 0.966433\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 0.828680\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 0.948522\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 1.014756\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 1.012010\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.800378\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 0.909432\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.796566\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 0.962183\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 0.684300\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 0.746561\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 0.756550\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 0.698620\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 0.997664\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 0.953565\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 0.946747\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.908845\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 1.138417\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.743424\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.708146\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 0.793471\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 1.015978\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.936390\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.870820\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 1.044271\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 0.788632\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 0.852030\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 0.833126\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.964780\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 0.818006\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 0.945094\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 0.561135\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 0.646549\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 0.912862\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.781827\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.775847\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.961801\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 0.973479\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 0.921471\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 0.739150\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 0.951878\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 0.803881\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.840341\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 1.104513\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.799318\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.961892\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.929431\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 1.055014\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.912413\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 0.718962\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.733355\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 1.069375\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.751139\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.805614\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.876973\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 0.614516\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 1.017314\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.637567\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 0.787410\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 1.137567\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.705105\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 0.846793\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.789670\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.999323\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.828726\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 1.176422\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.787294\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.968701\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.807235\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.914951\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 1.229359\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.876878\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 0.947001\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 0.936007\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 0.699482\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.748994\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.703082\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.741710\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.763779\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.585002\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.821019\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.711021\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.688737\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.852668\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.651576\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 0.795236\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.737396\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.887001\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.632172\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.699411\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.923675\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.749379\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.617353\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.696301\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 0.884895\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 0.668107\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.702956\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.604264\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 0.870514\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.901712\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.749708\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 0.780010\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 1.060320\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.726724\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.776765\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.724442\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 1.102451\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 0.482807\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 1.030912\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.725971\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.920027\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.792640\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 1.746611\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.613893\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.307890\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.355848\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.405859\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.140309\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.503410\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.406661\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.547972\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.173675\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.228326\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.431745\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.343811\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.385822\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.468743\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.331746\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.204800\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.277943\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.342271\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.155857\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.403874\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.330784\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.401564\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.578709\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.344763\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.496627\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.414383\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.497477\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.310581\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.177118\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.188178\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.457233\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.336188\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.432416\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.271661\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.323445\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.294741\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.406959\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.332066\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.465395\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.477998\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.393895\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.244160\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.321650\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.423786\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.276607\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.217952\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.433778\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.132229\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.131840\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.230909\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.431134\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.344314\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.240410\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.223329\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.268684\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.206112\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.290173\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.183718\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.234897\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.282765\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.391778\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.368806\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.618801\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.191105\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.295665\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.301963\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.294081\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.272377\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.361662\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.212343\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.247509\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.167346\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.173181\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.168945\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.230952\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.322148\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.437020\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.143964\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.260997\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.248114\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.186563\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.376132\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.082444\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.276550\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.368251\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.138949\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.085891\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.347249\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.182828\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.256707\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.492833\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.172272\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.115541\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.222867\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.451339\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.106249\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.189534\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.247333\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.418222\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.306053\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.273426\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 1.781884\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 1.181933\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 0.944376\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 0.993333\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 0.870606\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 0.707855\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 1.149794\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 1.101322\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 0.817464\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 0.834920\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 1.314533\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 0.861492\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 0.808984\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 0.863429\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 0.889441\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 0.977547\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 0.964642\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 0.959962\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 0.931010\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 0.680819\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 1.083135\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 1.275828\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 0.894842\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 0.822171\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 0.883823\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.727282\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.809828\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 1.260672\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 0.758465\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 1.155469\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 0.910635\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 1.047278\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 0.808853\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.794671\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 1.023500\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.695277\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 1.084988\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 0.763765\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.901430\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 0.913067\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 1.254886\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.875812\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 0.710073\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 0.775943\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 1.034412\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.726977\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 0.791354\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.983668\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.770328\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 1.019754\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 1.019330\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 0.692148\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 0.857648\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.936565\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.987573\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.871927\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.805711\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 0.816885\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.775067\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.779168\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.863909\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.648841\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.849238\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 0.861059\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 0.871062\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.917885\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.792305\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 0.799296\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.942002\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.766644\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.994450\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 1.003189\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.848867\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 1.077831\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 0.718844\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.676976\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 1.102458\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 1.305334\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.746601\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.634399\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 0.785889\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.608350\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.657113\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.862493\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 1.157796\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 0.809560\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 0.839579\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 0.819807\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 0.696810\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 0.715143\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 0.799916\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.835802\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.940768\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.818291\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.746627\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 0.863251\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.721562\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 0.695382\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.737095\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.574974\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.895639\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 0.562810\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.932311\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.925148\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.521156\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 0.939850\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.811894\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 0.737661\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.978836\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.788424\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.909553\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.741403\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.718791\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.728442\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.739970\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.741878\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 0.589801\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.716472\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 1.100822\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.700472\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 0.920130\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 0.948739\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.575547\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.946009\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.823317\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 1.001366\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.853015\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.668580\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.692741\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.779743\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 1.011238\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.654567\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.618210\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.863994\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.815641\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.799188\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.895750\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.603377\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.789530\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.645858\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.617613\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.945250\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.675442\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.550423\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.705245\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.597529\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.993987\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.841776\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 0.852269\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.739409\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.777861\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.640324\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.589922\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.588049\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.673895\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.553065\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.570194\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.946099\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.972974\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.766478\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.819489\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.492216\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.783557\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.761600\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.918355\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.895685\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.942724\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.828252\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.666166\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 0.707973\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.936996\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.701304\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.819578\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.856852\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.895325\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.864939\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.699168\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.798653\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.677606\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.882622\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.620939\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.850719\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.568886\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.478241\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 0.727278\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.622515\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 0.739761\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.693549\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.527290\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.771189\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 1.172053\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.682815\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.765654\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.779429\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.823232\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.575216\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.740492\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.676893\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.802452\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.831384\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.881847\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.886693\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.796098\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.771000\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.789885\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.772022\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.791150\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.743014\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.739451\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 0.780548\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.700086\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.858461\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.770993\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.647783\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.939001\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.916403\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.738609\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.566326\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.637996\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.640200\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.653970\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.912942\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.602615\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.709447\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.669834\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.790130\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.787289\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.994384\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.592023\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.597255\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.622670\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.631442\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.625770\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.671993\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.799420\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.906332\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.679706\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.716865\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.755305\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.825430\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.579295\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.850739\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.942408\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.659379\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.597751\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.405565\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.756250\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.470202\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.942020\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.766372\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.704839\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.755543\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.828263\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.684196\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.866922\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.994985\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 1.103072\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 0.889184\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 0.838546\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 0.883025\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 0.839889\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 1.183764\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 0.790454\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 0.876778\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 1.036754\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.795073\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 1.042373\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 0.730546\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 0.989041\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 0.917958\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.963813\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 0.767223\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 0.640344\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.801837\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.808564\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 1.299757\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.688236\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.656282\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 0.713861\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 1.141241\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 1.187635\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 1.030659\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.650521\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 0.827367\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 0.697725\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.672566\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.657333\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 0.721757\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 0.738167\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 0.884733\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 0.632405\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 0.712797\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.817314\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.733846\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.728922\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.812990\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.774361\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.884224\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.755605\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 0.785493\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.665910\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.752071\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.737959\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.674944\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 0.654764\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.473947\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 0.681836\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.735906\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.567315\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.823664\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 1.114468\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.671562\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.819999\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.741049\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 0.888952\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.680380\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.535801\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.781008\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.661837\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.667081\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.571619\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.821870\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.735938\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.925053\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.683386\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.599561\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.689632\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.503710\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.900074\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.583375\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.688580\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 0.657391\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.670274\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.836762\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.784602\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 1.001279\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.933826\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.719468\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.911376\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.813700\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.688991\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.532501\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.712312\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.632260\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.675199\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.860719\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.696982\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.769965\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.754725\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.738693\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.570326\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.543190\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.878597\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.705473\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.679086\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.791781\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.785880\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/8838 (0%)]\tLoss: 2.486462\n",
      "Train Epoch: 1 [640/8838 (7%)]\tLoss: 1.242138\n",
      "Train Epoch: 1 [1280/8838 (14%)]\tLoss: 1.013245\n",
      "Train Epoch: 1 [1920/8838 (22%)]\tLoss: 0.727065\n",
      "Train Epoch: 1 [2560/8838 (29%)]\tLoss: 0.921293\n",
      "Train Epoch: 1 [3200/8838 (36%)]\tLoss: 0.866280\n",
      "Train Epoch: 1 [3840/8838 (43%)]\tLoss: 0.728823\n",
      "Train Epoch: 1 [4480/8838 (50%)]\tLoss: 0.820443\n",
      "Train Epoch: 1 [5120/8838 (58%)]\tLoss: 0.641122\n",
      "Train Epoch: 1 [5760/8838 (65%)]\tLoss: 0.733482\n",
      "Train Epoch: 1 [6400/8838 (72%)]\tLoss: 0.750435\n",
      "Train Epoch: 1 [7040/8838 (79%)]\tLoss: 0.850094\n",
      "Train Epoch: 1 [7680/8838 (86%)]\tLoss: 0.733531\n",
      "Train Epoch: 1 [8320/8838 (94%)]\tLoss: 0.744984\n",
      "Train Epoch: 2 [0/8838 (0%)]\tLoss: 0.816126\n",
      "Train Epoch: 2 [640/8838 (7%)]\tLoss: 0.739127\n",
      "Train Epoch: 2 [1280/8838 (14%)]\tLoss: 0.869571\n",
      "Train Epoch: 2 [1920/8838 (22%)]\tLoss: 0.811643\n",
      "Train Epoch: 2 [2560/8838 (29%)]\tLoss: 0.679172\n",
      "Train Epoch: 2 [3200/8838 (36%)]\tLoss: 0.648620\n",
      "Train Epoch: 2 [3840/8838 (43%)]\tLoss: 0.835876\n",
      "Train Epoch: 2 [4480/8838 (50%)]\tLoss: 0.895890\n",
      "Train Epoch: 2 [5120/8838 (58%)]\tLoss: 0.592846\n",
      "Train Epoch: 2 [5760/8838 (65%)]\tLoss: 0.586035\n",
      "Train Epoch: 2 [6400/8838 (72%)]\tLoss: 1.163819\n",
      "Train Epoch: 2 [7040/8838 (79%)]\tLoss: 0.573498\n",
      "Train Epoch: 2 [7680/8838 (86%)]\tLoss: 0.791660\n",
      "Train Epoch: 2 [8320/8838 (94%)]\tLoss: 0.716563\n",
      "Train Epoch: 3 [0/8838 (0%)]\tLoss: 0.780444\n",
      "Train Epoch: 3 [640/8838 (7%)]\tLoss: 0.789193\n",
      "Train Epoch: 3 [1280/8838 (14%)]\tLoss: 0.684090\n",
      "Train Epoch: 3 [1920/8838 (22%)]\tLoss: 0.646364\n",
      "Train Epoch: 3 [2560/8838 (29%)]\tLoss: 0.770131\n",
      "Train Epoch: 3 [3200/8838 (36%)]\tLoss: 1.244227\n",
      "Train Epoch: 3 [3840/8838 (43%)]\tLoss: 0.711915\n",
      "Train Epoch: 3 [4480/8838 (50%)]\tLoss: 0.621088\n",
      "Train Epoch: 3 [5120/8838 (58%)]\tLoss: 0.872149\n",
      "Train Epoch: 3 [5760/8838 (65%)]\tLoss: 0.631851\n",
      "Train Epoch: 3 [6400/8838 (72%)]\tLoss: 0.770966\n",
      "Train Epoch: 3 [7040/8838 (79%)]\tLoss: 0.620066\n",
      "Train Epoch: 3 [7680/8838 (86%)]\tLoss: 0.745930\n",
      "Train Epoch: 3 [8320/8838 (94%)]\tLoss: 0.749738\n",
      "Train Epoch: 4 [0/8838 (0%)]\tLoss: 0.537490\n",
      "Train Epoch: 4 [640/8838 (7%)]\tLoss: 0.811667\n",
      "Train Epoch: 4 [1280/8838 (14%)]\tLoss: 0.752873\n",
      "Train Epoch: 4 [1920/8838 (22%)]\tLoss: 0.772326\n",
      "Train Epoch: 4 [2560/8838 (29%)]\tLoss: 0.882254\n",
      "Train Epoch: 4 [3200/8838 (36%)]\tLoss: 0.705397\n",
      "Train Epoch: 4 [3840/8838 (43%)]\tLoss: 0.579837\n",
      "Train Epoch: 4 [4480/8838 (50%)]\tLoss: 0.967149\n",
      "Train Epoch: 4 [5120/8838 (58%)]\tLoss: 0.778899\n",
      "Train Epoch: 4 [5760/8838 (65%)]\tLoss: 0.834569\n",
      "Train Epoch: 4 [6400/8838 (72%)]\tLoss: 0.767755\n",
      "Train Epoch: 4 [7040/8838 (79%)]\tLoss: 0.679835\n",
      "Train Epoch: 4 [7680/8838 (86%)]\tLoss: 0.644493\n",
      "Train Epoch: 4 [8320/8838 (94%)]\tLoss: 0.503967\n",
      "Train Epoch: 5 [0/8838 (0%)]\tLoss: 0.781415\n",
      "Train Epoch: 5 [640/8838 (7%)]\tLoss: 0.555366\n",
      "Train Epoch: 5 [1280/8838 (14%)]\tLoss: 0.754202\n",
      "Train Epoch: 5 [1920/8838 (22%)]\tLoss: 0.638829\n",
      "Train Epoch: 5 [2560/8838 (29%)]\tLoss: 0.651090\n",
      "Train Epoch: 5 [3200/8838 (36%)]\tLoss: 0.624896\n",
      "Train Epoch: 5 [3840/8838 (43%)]\tLoss: 0.858609\n",
      "Train Epoch: 5 [4480/8838 (50%)]\tLoss: 0.778198\n",
      "Train Epoch: 5 [5120/8838 (58%)]\tLoss: 0.883549\n",
      "Train Epoch: 5 [5760/8838 (65%)]\tLoss: 0.579453\n",
      "Train Epoch: 5 [6400/8838 (72%)]\tLoss: 0.570318\n",
      "Train Epoch: 5 [7040/8838 (79%)]\tLoss: 0.645503\n",
      "Train Epoch: 5 [7680/8838 (86%)]\tLoss: 0.849672\n",
      "Train Epoch: 5 [8320/8838 (94%)]\tLoss: 0.661596\n",
      "Train Epoch: 6 [0/8838 (0%)]\tLoss: 1.094146\n",
      "Train Epoch: 6 [640/8838 (7%)]\tLoss: 0.525977\n",
      "Train Epoch: 6 [1280/8838 (14%)]\tLoss: 0.778646\n",
      "Train Epoch: 6 [1920/8838 (22%)]\tLoss: 0.759543\n",
      "Train Epoch: 6 [2560/8838 (29%)]\tLoss: 0.758437\n",
      "Train Epoch: 6 [3200/8838 (36%)]\tLoss: 0.526978\n",
      "Train Epoch: 6 [3840/8838 (43%)]\tLoss: 0.640650\n",
      "Train Epoch: 6 [4480/8838 (50%)]\tLoss: 0.551664\n",
      "Train Epoch: 6 [5120/8838 (58%)]\tLoss: 0.669887\n",
      "Train Epoch: 6 [5760/8838 (65%)]\tLoss: 0.950777\n",
      "Train Epoch: 6 [6400/8838 (72%)]\tLoss: 0.727572\n",
      "Train Epoch: 6 [7040/8838 (79%)]\tLoss: 0.731904\n",
      "Train Epoch: 6 [7680/8838 (86%)]\tLoss: 0.585926\n",
      "Train Epoch: 6 [8320/8838 (94%)]\tLoss: 0.528603\n",
      "Train Epoch: 7 [0/8838 (0%)]\tLoss: 0.737553\n",
      "Train Epoch: 7 [640/8838 (7%)]\tLoss: 0.711596\n",
      "Train Epoch: 7 [1280/8838 (14%)]\tLoss: 0.618064\n",
      "Train Epoch: 7 [1920/8838 (22%)]\tLoss: 0.705445\n",
      "Train Epoch: 7 [2560/8838 (29%)]\tLoss: 0.766858\n",
      "Train Epoch: 7 [3200/8838 (36%)]\tLoss: 0.795091\n",
      "Train Epoch: 7 [3840/8838 (43%)]\tLoss: 0.595034\n",
      "Train Epoch: 7 [4480/8838 (50%)]\tLoss: 0.522257\n",
      "Train Epoch: 7 [5120/8838 (58%)]\tLoss: 0.930696\n",
      "Train Epoch: 7 [5760/8838 (65%)]\tLoss: 0.664199\n",
      "Train Epoch: 7 [6400/8838 (72%)]\tLoss: 0.684793\n",
      "Train Epoch: 7 [7040/8838 (79%)]\tLoss: 0.702721\n",
      "Train Epoch: 7 [7680/8838 (86%)]\tLoss: 0.672498\n",
      "Train Epoch: 7 [8320/8838 (94%)]\tLoss: 0.646903\n",
      "Train Epoch: 8 [0/8838 (0%)]\tLoss: 0.661314\n",
      "Train Epoch: 8 [640/8838 (7%)]\tLoss: 0.641710\n",
      "Train Epoch: 8 [1280/8838 (14%)]\tLoss: 0.964607\n",
      "Train Epoch: 8 [1920/8838 (22%)]\tLoss: 0.555580\n",
      "Train Epoch: 8 [2560/8838 (29%)]\tLoss: 0.446746\n",
      "Train Epoch: 8 [3200/8838 (36%)]\tLoss: 0.700307\n",
      "Train Epoch: 8 [3840/8838 (43%)]\tLoss: 0.621574\n",
      "Train Epoch: 8 [4480/8838 (50%)]\tLoss: 0.794204\n",
      "Train Epoch: 8 [5120/8838 (58%)]\tLoss: 0.643902\n",
      "Train Epoch: 8 [5760/8838 (65%)]\tLoss: 0.636873\n",
      "Train Epoch: 8 [6400/8838 (72%)]\tLoss: 0.687134\n",
      "Train Epoch: 8 [7040/8838 (79%)]\tLoss: 0.745919\n",
      "Train Epoch: 8 [7680/8838 (86%)]\tLoss: 0.714479\n",
      "Train Epoch: 8 [8320/8838 (94%)]\tLoss: 0.562702\n",
      "Train Epoch: 9 [0/8838 (0%)]\tLoss: 0.748868\n",
      "Train Epoch: 9 [640/8838 (7%)]\tLoss: 0.636442\n",
      "Train Epoch: 9 [1280/8838 (14%)]\tLoss: 0.727338\n",
      "Train Epoch: 9 [1920/8838 (22%)]\tLoss: 0.919186\n",
      "Train Epoch: 9 [2560/8838 (29%)]\tLoss: 0.626954\n",
      "Train Epoch: 9 [3200/8838 (36%)]\tLoss: 0.548096\n",
      "Train Epoch: 9 [3840/8838 (43%)]\tLoss: 0.735792\n",
      "Train Epoch: 9 [4480/8838 (50%)]\tLoss: 0.641432\n",
      "Train Epoch: 9 [5120/8838 (58%)]\tLoss: 0.591842\n",
      "Train Epoch: 9 [5760/8838 (65%)]\tLoss: 0.641007\n",
      "Train Epoch: 9 [6400/8838 (72%)]\tLoss: 0.696839\n",
      "Train Epoch: 9 [7040/8838 (79%)]\tLoss: 0.636482\n",
      "Train Epoch: 9 [7680/8838 (86%)]\tLoss: 0.873507\n",
      "Train Epoch: 9 [8320/8838 (94%)]\tLoss: 0.746957\n",
      "Train Epoch: 10 [0/8838 (0%)]\tLoss: 0.659394\n",
      "Train Epoch: 10 [640/8838 (7%)]\tLoss: 0.602211\n",
      "Train Epoch: 10 [1280/8838 (14%)]\tLoss: 0.950766\n",
      "Train Epoch: 10 [1920/8838 (22%)]\tLoss: 0.497221\n",
      "Train Epoch: 10 [2560/8838 (29%)]\tLoss: 0.529689\n",
      "Train Epoch: 10 [3200/8838 (36%)]\tLoss: 0.605381\n",
      "Train Epoch: 10 [3840/8838 (43%)]\tLoss: 0.756082\n",
      "Train Epoch: 10 [4480/8838 (50%)]\tLoss: 0.865157\n",
      "Train Epoch: 10 [5120/8838 (58%)]\tLoss: 0.732167\n",
      "Train Epoch: 10 [5760/8838 (65%)]\tLoss: 0.581435\n",
      "Train Epoch: 10 [6400/8838 (72%)]\tLoss: 0.813513\n",
      "Train Epoch: 10 [7040/8838 (79%)]\tLoss: 0.595006\n",
      "Train Epoch: 10 [7680/8838 (86%)]\tLoss: 0.561871\n",
      "Train Epoch: 10 [8320/8838 (94%)]\tLoss: 0.592041\n",
      "Train Epoch: 11 [0/8838 (0%)]\tLoss: 0.732857\n",
      "Train Epoch: 11 [640/8838 (7%)]\tLoss: 0.526134\n",
      "Train Epoch: 11 [1280/8838 (14%)]\tLoss: 0.543306\n",
      "Train Epoch: 11 [1920/8838 (22%)]\tLoss: 0.838842\n",
      "Train Epoch: 11 [2560/8838 (29%)]\tLoss: 0.652220\n",
      "Train Epoch: 11 [3200/8838 (36%)]\tLoss: 0.624184\n",
      "Train Epoch: 11 [3840/8838 (43%)]\tLoss: 0.637958\n",
      "Train Epoch: 11 [4480/8838 (50%)]\tLoss: 0.811840\n",
      "Train Epoch: 11 [5120/8838 (58%)]\tLoss: 0.517600\n",
      "Train Epoch: 11 [5760/8838 (65%)]\tLoss: 0.640568\n",
      "Train Epoch: 11 [6400/8838 (72%)]\tLoss: 0.639249\n",
      "Train Epoch: 11 [7040/8838 (79%)]\tLoss: 0.602052\n",
      "Train Epoch: 11 [7680/8838 (86%)]\tLoss: 0.645979\n",
      "Train Epoch: 11 [8320/8838 (94%)]\tLoss: 0.687501\n",
      "Train Epoch: 12 [0/8838 (0%)]\tLoss: 0.567564\n",
      "Train Epoch: 12 [640/8838 (7%)]\tLoss: 0.550440\n",
      "Train Epoch: 12 [1280/8838 (14%)]\tLoss: 0.776163\n",
      "Train Epoch: 12 [1920/8838 (22%)]\tLoss: 0.531567\n",
      "Train Epoch: 12 [2560/8838 (29%)]\tLoss: 0.599297\n",
      "Train Epoch: 12 [3200/8838 (36%)]\tLoss: 0.665839\n",
      "Train Epoch: 12 [3840/8838 (43%)]\tLoss: 0.551097\n",
      "Train Epoch: 12 [4480/8838 (50%)]\tLoss: 0.583256\n",
      "Train Epoch: 12 [5120/8838 (58%)]\tLoss: 0.469920\n",
      "Train Epoch: 12 [5760/8838 (65%)]\tLoss: 0.467718\n",
      "Train Epoch: 12 [6400/8838 (72%)]\tLoss: 0.551490\n",
      "Train Epoch: 12 [7040/8838 (79%)]\tLoss: 0.655890\n",
      "Train Epoch: 12 [7680/8838 (86%)]\tLoss: 0.677248\n",
      "Train Epoch: 12 [8320/8838 (94%)]\tLoss: 0.536032\n",
      "Train Epoch: 13 [0/8838 (0%)]\tLoss: 0.629278\n",
      "Train Epoch: 13 [640/8838 (7%)]\tLoss: 0.683227\n",
      "Train Epoch: 13 [1280/8838 (14%)]\tLoss: 0.534756\n",
      "Train Epoch: 13 [1920/8838 (22%)]\tLoss: 0.548317\n",
      "Train Epoch: 13 [2560/8838 (29%)]\tLoss: 0.707384\n",
      "Train Epoch: 13 [3200/8838 (36%)]\tLoss: 0.714342\n",
      "Train Epoch: 13 [3840/8838 (43%)]\tLoss: 0.809638\n",
      "Train Epoch: 13 [4480/8838 (50%)]\tLoss: 0.676258\n",
      "Train Epoch: 13 [5120/8838 (58%)]\tLoss: 0.651493\n",
      "Train Epoch: 13 [5760/8838 (65%)]\tLoss: 0.733168\n",
      "Train Epoch: 13 [6400/8838 (72%)]\tLoss: 0.740053\n",
      "Train Epoch: 13 [7040/8838 (79%)]\tLoss: 0.550048\n",
      "Train Epoch: 13 [7680/8838 (86%)]\tLoss: 0.571680\n",
      "Train Epoch: 13 [8320/8838 (94%)]\tLoss: 0.661997\n",
      "Train Epoch: 14 [0/8838 (0%)]\tLoss: 0.522211\n",
      "Train Epoch: 14 [640/8838 (7%)]\tLoss: 0.519075\n",
      "Train Epoch: 14 [1280/8838 (14%)]\tLoss: 0.638483\n",
      "Train Epoch: 14 [1920/8838 (22%)]\tLoss: 0.514159\n",
      "Train Epoch: 14 [2560/8838 (29%)]\tLoss: 0.622034\n",
      "Train Epoch: 14 [3200/8838 (36%)]\tLoss: 1.070929\n",
      "Train Epoch: 14 [3840/8838 (43%)]\tLoss: 0.656031\n",
      "Train Epoch: 14 [4480/8838 (50%)]\tLoss: 0.473620\n",
      "Train Epoch: 14 [5120/8838 (58%)]\tLoss: 0.758505\n",
      "Train Epoch: 14 [5760/8838 (65%)]\tLoss: 0.844999\n",
      "Train Epoch: 14 [6400/8838 (72%)]\tLoss: 0.570691\n",
      "Train Epoch: 14 [7040/8838 (79%)]\tLoss: 0.626576\n",
      "Train Epoch: 14 [7680/8838 (86%)]\tLoss: 0.600393\n",
      "Train Epoch: 14 [8320/8838 (94%)]\tLoss: 0.542690\n",
      "Train Epoch: 15 [0/8838 (0%)]\tLoss: 0.536551\n",
      "Train Epoch: 15 [640/8838 (7%)]\tLoss: 0.600965\n",
      "Train Epoch: 15 [1280/8838 (14%)]\tLoss: 0.606082\n",
      "Train Epoch: 15 [1920/8838 (22%)]\tLoss: 0.800632\n",
      "Train Epoch: 15 [2560/8838 (29%)]\tLoss: 0.521533\n",
      "Train Epoch: 15 [3200/8838 (36%)]\tLoss: 0.554913\n",
      "Train Epoch: 15 [3840/8838 (43%)]\tLoss: 0.755073\n",
      "Train Epoch: 15 [4480/8838 (50%)]\tLoss: 0.839589\n",
      "Train Epoch: 15 [5120/8838 (58%)]\tLoss: 0.507048\n",
      "Train Epoch: 15 [5760/8838 (65%)]\tLoss: 0.724021\n",
      "Train Epoch: 15 [6400/8838 (72%)]\tLoss: 0.819695\n",
      "Train Epoch: 15 [7040/8838 (79%)]\tLoss: 0.420639\n",
      "Train Epoch: 15 [7680/8838 (86%)]\tLoss: 0.453505\n",
      "Train Epoch: 15 [8320/8838 (94%)]\tLoss: 0.581646\n",
      "Train Epoch: 16 [0/8838 (0%)]\tLoss: 0.418323\n",
      "Train Epoch: 16 [640/8838 (7%)]\tLoss: 0.554092\n",
      "Train Epoch: 16 [1280/8838 (14%)]\tLoss: 0.638470\n",
      "Train Epoch: 16 [1920/8838 (22%)]\tLoss: 0.586447\n",
      "Train Epoch: 16 [2560/8838 (29%)]\tLoss: 0.459847\n",
      "Train Epoch: 16 [3200/8838 (36%)]\tLoss: 0.615269\n",
      "Train Epoch: 16 [3840/8838 (43%)]\tLoss: 0.460113\n",
      "Train Epoch: 16 [4480/8838 (50%)]\tLoss: 0.828028\n",
      "Train Epoch: 16 [5120/8838 (58%)]\tLoss: 0.743336\n",
      "Train Epoch: 16 [5760/8838 (65%)]\tLoss: 0.614983\n",
      "Train Epoch: 16 [6400/8838 (72%)]\tLoss: 0.781898\n",
      "Train Epoch: 16 [7040/8838 (79%)]\tLoss: 0.502774\n",
      "Train Epoch: 16 [7680/8838 (86%)]\tLoss: 0.584859\n",
      "Train Epoch: 16 [8320/8838 (94%)]\tLoss: 0.466952\n",
      "Train Epoch: 17 [0/8838 (0%)]\tLoss: 0.751882\n",
      "Train Epoch: 17 [640/8838 (7%)]\tLoss: 0.739918\n",
      "Train Epoch: 17 [1280/8838 (14%)]\tLoss: 0.620004\n",
      "Train Epoch: 17 [1920/8838 (22%)]\tLoss: 0.559187\n",
      "Train Epoch: 17 [2560/8838 (29%)]\tLoss: 0.642671\n",
      "Train Epoch: 17 [3200/8838 (36%)]\tLoss: 0.733996\n",
      "Train Epoch: 17 [3840/8838 (43%)]\tLoss: 0.541036\n",
      "Train Epoch: 17 [4480/8838 (50%)]\tLoss: 0.775140\n",
      "Train Epoch: 17 [5120/8838 (58%)]\tLoss: 0.638774\n",
      "Train Epoch: 17 [5760/8838 (65%)]\tLoss: 0.763951\n",
      "Train Epoch: 17 [6400/8838 (72%)]\tLoss: 0.574745\n",
      "Train Epoch: 17 [7040/8838 (79%)]\tLoss: 0.624501\n",
      "Train Epoch: 17 [7680/8838 (86%)]\tLoss: 0.647819\n",
      "Train Epoch: 17 [8320/8838 (94%)]\tLoss: 0.770648\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/6366 (0%)]\tLoss: 2.051721\n",
      "Train Epoch: 1 [640/6366 (10%)]\tLoss: 0.463077\n",
      "Train Epoch: 1 [1280/6366 (20%)]\tLoss: 0.619700\n",
      "Train Epoch: 1 [1920/6366 (30%)]\tLoss: 1.039946\n",
      "Train Epoch: 1 [2560/6366 (40%)]\tLoss: 0.484288\n",
      "Train Epoch: 1 [3200/6366 (50%)]\tLoss: 0.499487\n",
      "Train Epoch: 1 [3840/6366 (60%)]\tLoss: 0.609944\n",
      "Train Epoch: 1 [4480/6366 (70%)]\tLoss: 0.513361\n",
      "Train Epoch: 1 [5120/6366 (80%)]\tLoss: 0.823790\n",
      "Train Epoch: 1 [5760/6366 (90%)]\tLoss: 0.632812\n",
      "Train Epoch: 2 [0/6366 (0%)]\tLoss: 0.487608\n",
      "Train Epoch: 2 [640/6366 (10%)]\tLoss: 0.455932\n",
      "Train Epoch: 2 [1280/6366 (20%)]\tLoss: 0.421462\n",
      "Train Epoch: 2 [1920/6366 (30%)]\tLoss: 0.547988\n",
      "Train Epoch: 2 [2560/6366 (40%)]\tLoss: 0.659800\n",
      "Train Epoch: 2 [3200/6366 (50%)]\tLoss: 0.648902\n",
      "Train Epoch: 2 [3840/6366 (60%)]\tLoss: 0.368114\n",
      "Train Epoch: 2 [4480/6366 (70%)]\tLoss: 0.453215\n",
      "Train Epoch: 2 [5120/6366 (80%)]\tLoss: 0.793959\n",
      "Train Epoch: 2 [5760/6366 (90%)]\tLoss: 0.488446\n",
      "Train Epoch: 3 [0/6366 (0%)]\tLoss: 0.481126\n",
      "Train Epoch: 3 [640/6366 (10%)]\tLoss: 0.412111\n",
      "Train Epoch: 3 [1280/6366 (20%)]\tLoss: 0.359790\n",
      "Train Epoch: 3 [1920/6366 (30%)]\tLoss: 0.778949\n",
      "Train Epoch: 3 [2560/6366 (40%)]\tLoss: 0.414103\n",
      "Train Epoch: 3 [3200/6366 (50%)]\tLoss: 0.287605\n",
      "Train Epoch: 3 [3840/6366 (60%)]\tLoss: 0.453951\n",
      "Train Epoch: 3 [4480/6366 (70%)]\tLoss: 0.804735\n",
      "Train Epoch: 3 [5120/6366 (80%)]\tLoss: 0.504281\n",
      "Train Epoch: 3 [5760/6366 (90%)]\tLoss: 0.595991\n",
      "Train Epoch: 4 [0/6366 (0%)]\tLoss: 0.327717\n",
      "Train Epoch: 4 [640/6366 (10%)]\tLoss: 0.326487\n",
      "Train Epoch: 4 [1280/6366 (20%)]\tLoss: 0.381125\n",
      "Train Epoch: 4 [1920/6366 (30%)]\tLoss: 0.557307\n",
      "Train Epoch: 4 [2560/6366 (40%)]\tLoss: 0.665209\n",
      "Train Epoch: 4 [3200/6366 (50%)]\tLoss: 0.571047\n",
      "Train Epoch: 4 [3840/6366 (60%)]\tLoss: 0.447851\n",
      "Train Epoch: 4 [4480/6366 (70%)]\tLoss: 0.442262\n",
      "Train Epoch: 4 [5120/6366 (80%)]\tLoss: 0.464549\n",
      "Train Epoch: 4 [5760/6366 (90%)]\tLoss: 0.640307\n",
      "Train Epoch: 5 [0/6366 (0%)]\tLoss: 0.439247\n",
      "Train Epoch: 5 [640/6366 (10%)]\tLoss: 0.578786\n",
      "Train Epoch: 5 [1280/6366 (20%)]\tLoss: 0.632685\n",
      "Train Epoch: 5 [1920/6366 (30%)]\tLoss: 0.497292\n",
      "Train Epoch: 5 [2560/6366 (40%)]\tLoss: 0.546490\n",
      "Train Epoch: 5 [3200/6366 (50%)]\tLoss: 0.423092\n",
      "Train Epoch: 5 [3840/6366 (60%)]\tLoss: 0.604973\n",
      "Train Epoch: 5 [4480/6366 (70%)]\tLoss: 0.446889\n",
      "Train Epoch: 5 [5120/6366 (80%)]\tLoss: 0.395081\n",
      "Train Epoch: 5 [5760/6366 (90%)]\tLoss: 0.357233\n",
      "Train Epoch: 6 [0/6366 (0%)]\tLoss: 0.518287\n",
      "Train Epoch: 6 [640/6366 (10%)]\tLoss: 0.451767\n",
      "Train Epoch: 6 [1280/6366 (20%)]\tLoss: 0.373369\n",
      "Train Epoch: 6 [1920/6366 (30%)]\tLoss: 0.608425\n",
      "Train Epoch: 6 [2560/6366 (40%)]\tLoss: 0.259304\n",
      "Train Epoch: 6 [3200/6366 (50%)]\tLoss: 0.243167\n",
      "Train Epoch: 6 [3840/6366 (60%)]\tLoss: 0.659388\n",
      "Train Epoch: 6 [4480/6366 (70%)]\tLoss: 0.432718\n",
      "Train Epoch: 6 [5120/6366 (80%)]\tLoss: 0.612632\n",
      "Train Epoch: 6 [5760/6366 (90%)]\tLoss: 0.286715\n",
      "Train Epoch: 7 [0/6366 (0%)]\tLoss: 0.584727\n",
      "Train Epoch: 7 [640/6366 (10%)]\tLoss: 0.512297\n",
      "Train Epoch: 7 [1280/6366 (20%)]\tLoss: 0.279857\n",
      "Train Epoch: 7 [1920/6366 (30%)]\tLoss: 0.351684\n",
      "Train Epoch: 7 [2560/6366 (40%)]\tLoss: 0.175748\n",
      "Train Epoch: 7 [3200/6366 (50%)]\tLoss: 0.494605\n",
      "Train Epoch: 7 [3840/6366 (60%)]\tLoss: 0.379847\n",
      "Train Epoch: 7 [4480/6366 (70%)]\tLoss: 0.858091\n",
      "Train Epoch: 7 [5120/6366 (80%)]\tLoss: 0.309406\n",
      "Train Epoch: 7 [5760/6366 (90%)]\tLoss: 0.191804\n",
      "Train Epoch: 8 [0/6366 (0%)]\tLoss: 0.272005\n",
      "Train Epoch: 8 [640/6366 (10%)]\tLoss: 0.503459\n",
      "Train Epoch: 8 [1280/6366 (20%)]\tLoss: 0.297574\n",
      "Train Epoch: 8 [1920/6366 (30%)]\tLoss: 0.537227\n",
      "Train Epoch: 8 [2560/6366 (40%)]\tLoss: 0.492414\n",
      "Train Epoch: 8 [3200/6366 (50%)]\tLoss: 0.425946\n",
      "Train Epoch: 8 [3840/6366 (60%)]\tLoss: 0.379696\n",
      "Train Epoch: 8 [4480/6366 (70%)]\tLoss: 0.247487\n",
      "Train Epoch: 8 [5120/6366 (80%)]\tLoss: 0.393031\n",
      "Train Epoch: 8 [5760/6366 (90%)]\tLoss: 0.500156\n",
      "Train Epoch: 9 [0/6366 (0%)]\tLoss: 0.309863\n",
      "Train Epoch: 9 [640/6366 (10%)]\tLoss: 0.509362\n",
      "Train Epoch: 9 [1280/6366 (20%)]\tLoss: 0.324275\n",
      "Train Epoch: 9 [1920/6366 (30%)]\tLoss: 0.274841\n",
      "Train Epoch: 9 [2560/6366 (40%)]\tLoss: 0.391199\n",
      "Train Epoch: 9 [3200/6366 (50%)]\tLoss: 0.465449\n",
      "Train Epoch: 9 [3840/6366 (60%)]\tLoss: 0.568549\n",
      "Train Epoch: 9 [4480/6366 (70%)]\tLoss: 0.464644\n",
      "Train Epoch: 9 [5120/6366 (80%)]\tLoss: 0.336515\n",
      "Train Epoch: 9 [5760/6366 (90%)]\tLoss: 0.498682\n",
      "Train Epoch: 10 [0/6366 (0%)]\tLoss: 0.393557\n",
      "Train Epoch: 10 [640/6366 (10%)]\tLoss: 0.330517\n",
      "Train Epoch: 10 [1280/6366 (20%)]\tLoss: 0.439888\n",
      "Train Epoch: 10 [1920/6366 (30%)]\tLoss: 0.369053\n",
      "Train Epoch: 10 [2560/6366 (40%)]\tLoss: 0.366945\n",
      "Train Epoch: 10 [3200/6366 (50%)]\tLoss: 0.452683\n",
      "Train Epoch: 10 [3840/6366 (60%)]\tLoss: 0.309013\n",
      "Train Epoch: 10 [4480/6366 (70%)]\tLoss: 0.318879\n",
      "Train Epoch: 10 [5120/6366 (80%)]\tLoss: 0.207322\n",
      "Train Epoch: 10 [5760/6366 (90%)]\tLoss: 0.474626\n",
      "Train Epoch: 11 [0/6366 (0%)]\tLoss: 0.499468\n",
      "Train Epoch: 11 [640/6366 (10%)]\tLoss: 0.247243\n",
      "Train Epoch: 11 [1280/6366 (20%)]\tLoss: 0.433882\n",
      "Train Epoch: 11 [1920/6366 (30%)]\tLoss: 0.367444\n",
      "Train Epoch: 11 [2560/6366 (40%)]\tLoss: 0.246813\n",
      "Train Epoch: 11 [3200/6366 (50%)]\tLoss: 0.382129\n",
      "Train Epoch: 11 [3840/6366 (60%)]\tLoss: 0.474135\n",
      "Train Epoch: 11 [4480/6366 (70%)]\tLoss: 0.558735\n",
      "Train Epoch: 11 [5120/6366 (80%)]\tLoss: 0.252922\n",
      "Train Epoch: 11 [5760/6366 (90%)]\tLoss: 0.541122\n",
      "Train Epoch: 12 [0/6366 (0%)]\tLoss: 0.181598\n",
      "Train Epoch: 12 [640/6366 (10%)]\tLoss: 0.318125\n",
      "Train Epoch: 12 [1280/6366 (20%)]\tLoss: 0.227569\n",
      "Train Epoch: 12 [1920/6366 (30%)]\tLoss: 0.412702\n",
      "Train Epoch: 12 [2560/6366 (40%)]\tLoss: 0.307701\n",
      "Train Epoch: 12 [3200/6366 (50%)]\tLoss: 0.575217\n",
      "Train Epoch: 12 [3840/6366 (60%)]\tLoss: 0.547772\n",
      "Train Epoch: 12 [4480/6366 (70%)]\tLoss: 0.206782\n",
      "Train Epoch: 12 [5120/6366 (80%)]\tLoss: 0.403070\n",
      "Train Epoch: 12 [5760/6366 (90%)]\tLoss: 0.275313\n",
      "Train Epoch: 13 [0/6366 (0%)]\tLoss: 0.389465\n",
      "Train Epoch: 13 [640/6366 (10%)]\tLoss: 0.260890\n",
      "Train Epoch: 13 [1280/6366 (20%)]\tLoss: 0.347548\n",
      "Train Epoch: 13 [1920/6366 (30%)]\tLoss: 0.317295\n",
      "Train Epoch: 13 [2560/6366 (40%)]\tLoss: 0.313768\n",
      "Train Epoch: 13 [3200/6366 (50%)]\tLoss: 0.191570\n",
      "Train Epoch: 13 [3840/6366 (60%)]\tLoss: 0.457519\n",
      "Train Epoch: 13 [4480/6366 (70%)]\tLoss: 0.564303\n",
      "Train Epoch: 13 [5120/6366 (80%)]\tLoss: 0.514680\n",
      "Train Epoch: 13 [5760/6366 (90%)]\tLoss: 0.487299\n",
      "Train Epoch: 14 [0/6366 (0%)]\tLoss: 0.480003\n",
      "Train Epoch: 14 [640/6366 (10%)]\tLoss: 0.172054\n",
      "Train Epoch: 14 [1280/6366 (20%)]\tLoss: 0.347935\n",
      "Train Epoch: 14 [1920/6366 (30%)]\tLoss: 0.265877\n",
      "Train Epoch: 14 [2560/6366 (40%)]\tLoss: 0.402000\n",
      "Train Epoch: 14 [3200/6366 (50%)]\tLoss: 0.249852\n",
      "Train Epoch: 14 [3840/6366 (60%)]\tLoss: 0.194379\n",
      "Train Epoch: 14 [4480/6366 (70%)]\tLoss: 0.197663\n",
      "Train Epoch: 14 [5120/6366 (80%)]\tLoss: 0.438973\n",
      "Train Epoch: 14 [5760/6366 (90%)]\tLoss: 0.327902\n",
      "Train Epoch: 15 [0/6366 (0%)]\tLoss: 0.526253\n",
      "Train Epoch: 15 [640/6366 (10%)]\tLoss: 0.780053\n",
      "Train Epoch: 15 [1280/6366 (20%)]\tLoss: 0.359951\n",
      "Train Epoch: 15 [1920/6366 (30%)]\tLoss: 0.428032\n",
      "Train Epoch: 15 [2560/6366 (40%)]\tLoss: 0.285421\n",
      "Train Epoch: 15 [3200/6366 (50%)]\tLoss: 0.380505\n",
      "Train Epoch: 15 [3840/6366 (60%)]\tLoss: 0.247271\n",
      "Train Epoch: 15 [4480/6366 (70%)]\tLoss: 0.212049\n",
      "Train Epoch: 15 [5120/6366 (80%)]\tLoss: 0.151974\n",
      "Train Epoch: 15 [5760/6366 (90%)]\tLoss: 0.344475\n",
      "Train Epoch: 16 [0/6366 (0%)]\tLoss: 0.291075\n",
      "Train Epoch: 16 [640/6366 (10%)]\tLoss: 0.672660\n",
      "Train Epoch: 16 [1280/6366 (20%)]\tLoss: 0.554389\n",
      "Train Epoch: 16 [1920/6366 (30%)]\tLoss: 0.454667\n",
      "Train Epoch: 16 [2560/6366 (40%)]\tLoss: 0.469253\n",
      "Train Epoch: 16 [3200/6366 (50%)]\tLoss: 0.470776\n",
      "Train Epoch: 16 [3840/6366 (60%)]\tLoss: 0.361853\n",
      "Train Epoch: 16 [4480/6366 (70%)]\tLoss: 0.390111\n",
      "Train Epoch: 16 [5120/6366 (80%)]\tLoss: 0.420616\n",
      "Train Epoch: 16 [5760/6366 (90%)]\tLoss: 0.387768\n",
      "Train Epoch: 17 [0/6366 (0%)]\tLoss: 0.314474\n",
      "Train Epoch: 17 [640/6366 (10%)]\tLoss: 0.213475\n",
      "Train Epoch: 17 [1280/6366 (20%)]\tLoss: 0.358966\n",
      "Train Epoch: 17 [1920/6366 (30%)]\tLoss: 0.215226\n",
      "Train Epoch: 17 [2560/6366 (40%)]\tLoss: 0.335212\n",
      "Train Epoch: 17 [3200/6366 (50%)]\tLoss: 0.445819\n",
      "Train Epoch: 17 [3840/6366 (60%)]\tLoss: 0.400396\n",
      "Train Epoch: 17 [4480/6366 (70%)]\tLoss: 0.437684\n",
      "Train Epoch: 17 [5120/6366 (80%)]\tLoss: 0.263039\n",
      "Train Epoch: 17 [5760/6366 (90%)]\tLoss: 0.401893\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/9451 (0%)]\tLoss: 2.020692\n",
      "Train Epoch: 1 [640/9451 (7%)]\tLoss: 0.834032\n",
      "Train Epoch: 1 [1280/9451 (14%)]\tLoss: 1.222395\n",
      "Train Epoch: 1 [1920/9451 (20%)]\tLoss: 1.165409\n",
      "Train Epoch: 1 [2560/9451 (27%)]\tLoss: 1.065770\n",
      "Train Epoch: 1 [3200/9451 (34%)]\tLoss: 1.455786\n",
      "Train Epoch: 1 [3840/9451 (41%)]\tLoss: 1.025177\n",
      "Train Epoch: 1 [4480/9451 (47%)]\tLoss: 1.108737\n",
      "Train Epoch: 1 [5120/9451 (54%)]\tLoss: 1.100670\n",
      "Train Epoch: 1 [5760/9451 (61%)]\tLoss: 1.333617\n",
      "Train Epoch: 1 [6400/9451 (68%)]\tLoss: 0.946994\n",
      "Train Epoch: 1 [7040/9451 (74%)]\tLoss: 1.010068\n",
      "Train Epoch: 1 [7680/9451 (81%)]\tLoss: 1.030391\n",
      "Train Epoch: 1 [8320/9451 (88%)]\tLoss: 0.978793\n",
      "Train Epoch: 1 [8960/9451 (95%)]\tLoss: 1.285993\n",
      "Train Epoch: 2 [0/9451 (0%)]\tLoss: 1.021102\n",
      "Train Epoch: 2 [640/9451 (7%)]\tLoss: 1.184518\n",
      "Train Epoch: 2 [1280/9451 (14%)]\tLoss: 1.007209\n",
      "Train Epoch: 2 [1920/9451 (20%)]\tLoss: 0.748334\n",
      "Train Epoch: 2 [2560/9451 (27%)]\tLoss: 1.167245\n",
      "Train Epoch: 2 [3200/9451 (34%)]\tLoss: 0.974757\n",
      "Train Epoch: 2 [3840/9451 (41%)]\tLoss: 0.867060\n",
      "Train Epoch: 2 [4480/9451 (47%)]\tLoss: 1.118865\n",
      "Train Epoch: 2 [5120/9451 (54%)]\tLoss: 1.023903\n",
      "Train Epoch: 2 [5760/9451 (61%)]\tLoss: 0.909679\n",
      "Train Epoch: 2 [6400/9451 (68%)]\tLoss: 1.000237\n",
      "Train Epoch: 2 [7040/9451 (74%)]\tLoss: 1.254203\n",
      "Train Epoch: 2 [7680/9451 (81%)]\tLoss: 1.046646\n",
      "Train Epoch: 2 [8320/9451 (88%)]\tLoss: 0.920066\n",
      "Train Epoch: 2 [8960/9451 (95%)]\tLoss: 1.332789\n",
      "Train Epoch: 3 [0/9451 (0%)]\tLoss: 1.010457\n",
      "Train Epoch: 3 [640/9451 (7%)]\tLoss: 0.897929\n",
      "Train Epoch: 3 [1280/9451 (14%)]\tLoss: 0.973750\n",
      "Train Epoch: 3 [1920/9451 (20%)]\tLoss: 0.952412\n",
      "Train Epoch: 3 [2560/9451 (27%)]\tLoss: 1.105428\n",
      "Train Epoch: 3 [3200/9451 (34%)]\tLoss: 0.924755\n",
      "Train Epoch: 3 [3840/9451 (41%)]\tLoss: 1.091295\n",
      "Train Epoch: 3 [4480/9451 (47%)]\tLoss: 1.260571\n",
      "Train Epoch: 3 [5120/9451 (54%)]\tLoss: 0.840392\n",
      "Train Epoch: 3 [5760/9451 (61%)]\tLoss: 0.985412\n",
      "Train Epoch: 3 [6400/9451 (68%)]\tLoss: 1.049896\n",
      "Train Epoch: 3 [7040/9451 (74%)]\tLoss: 0.906994\n",
      "Train Epoch: 3 [7680/9451 (81%)]\tLoss: 1.084816\n",
      "Train Epoch: 3 [8320/9451 (88%)]\tLoss: 1.040069\n",
      "Train Epoch: 3 [8960/9451 (95%)]\tLoss: 0.993242\n",
      "Train Epoch: 4 [0/9451 (0%)]\tLoss: 0.864879\n",
      "Train Epoch: 4 [640/9451 (7%)]\tLoss: 0.931759\n",
      "Train Epoch: 4 [1280/9451 (14%)]\tLoss: 0.835840\n",
      "Train Epoch: 4 [1920/9451 (20%)]\tLoss: 0.775359\n",
      "Train Epoch: 4 [2560/9451 (27%)]\tLoss: 0.895259\n",
      "Train Epoch: 4 [3200/9451 (34%)]\tLoss: 0.985908\n",
      "Train Epoch: 4 [3840/9451 (41%)]\tLoss: 0.765248\n",
      "Train Epoch: 4 [4480/9451 (47%)]\tLoss: 1.033024\n",
      "Train Epoch: 4 [5120/9451 (54%)]\tLoss: 1.032836\n",
      "Train Epoch: 4 [5760/9451 (61%)]\tLoss: 0.872988\n",
      "Train Epoch: 4 [6400/9451 (68%)]\tLoss: 0.968514\n",
      "Train Epoch: 4 [7040/9451 (74%)]\tLoss: 0.830790\n",
      "Train Epoch: 4 [7680/9451 (81%)]\tLoss: 0.844003\n",
      "Train Epoch: 4 [8320/9451 (88%)]\tLoss: 1.075284\n",
      "Train Epoch: 4 [8960/9451 (95%)]\tLoss: 0.871639\n",
      "Train Epoch: 5 [0/9451 (0%)]\tLoss: 0.807272\n",
      "Train Epoch: 5 [640/9451 (7%)]\tLoss: 1.038801\n",
      "Train Epoch: 5 [1280/9451 (14%)]\tLoss: 0.824808\n",
      "Train Epoch: 5 [1920/9451 (20%)]\tLoss: 1.178751\n",
      "Train Epoch: 5 [2560/9451 (27%)]\tLoss: 1.077522\n",
      "Train Epoch: 5 [3200/9451 (34%)]\tLoss: 1.175449\n",
      "Train Epoch: 5 [3840/9451 (41%)]\tLoss: 0.820953\n",
      "Train Epoch: 5 [4480/9451 (47%)]\tLoss: 1.093677\n",
      "Train Epoch: 5 [5120/9451 (54%)]\tLoss: 0.864624\n",
      "Train Epoch: 5 [5760/9451 (61%)]\tLoss: 1.022313\n",
      "Train Epoch: 5 [6400/9451 (68%)]\tLoss: 0.837680\n",
      "Train Epoch: 5 [7040/9451 (74%)]\tLoss: 1.012961\n",
      "Train Epoch: 5 [7680/9451 (81%)]\tLoss: 0.858729\n",
      "Train Epoch: 5 [8320/9451 (88%)]\tLoss: 0.876518\n",
      "Train Epoch: 5 [8960/9451 (95%)]\tLoss: 0.768308\n",
      "Train Epoch: 6 [0/9451 (0%)]\tLoss: 0.885539\n",
      "Train Epoch: 6 [640/9451 (7%)]\tLoss: 0.839398\n",
      "Train Epoch: 6 [1280/9451 (14%)]\tLoss: 0.875970\n",
      "Train Epoch: 6 [1920/9451 (20%)]\tLoss: 0.625282\n",
      "Train Epoch: 6 [2560/9451 (27%)]\tLoss: 0.867322\n",
      "Train Epoch: 6 [3200/9451 (34%)]\tLoss: 0.899841\n",
      "Train Epoch: 6 [3840/9451 (41%)]\tLoss: 1.083084\n",
      "Train Epoch: 6 [4480/9451 (47%)]\tLoss: 0.860820\n",
      "Train Epoch: 6 [5120/9451 (54%)]\tLoss: 0.807408\n",
      "Train Epoch: 6 [5760/9451 (61%)]\tLoss: 0.952321\n",
      "Train Epoch: 6 [6400/9451 (68%)]\tLoss: 0.985926\n",
      "Train Epoch: 6 [7040/9451 (74%)]\tLoss: 0.870674\n",
      "Train Epoch: 6 [7680/9451 (81%)]\tLoss: 1.104834\n",
      "Train Epoch: 6 [8320/9451 (88%)]\tLoss: 1.111128\n",
      "Train Epoch: 6 [8960/9451 (95%)]\tLoss: 0.836411\n",
      "Train Epoch: 7 [0/9451 (0%)]\tLoss: 0.862112\n",
      "Train Epoch: 7 [640/9451 (7%)]\tLoss: 0.762435\n",
      "Train Epoch: 7 [1280/9451 (14%)]\tLoss: 0.916732\n",
      "Train Epoch: 7 [1920/9451 (20%)]\tLoss: 0.874846\n",
      "Train Epoch: 7 [2560/9451 (27%)]\tLoss: 1.208620\n",
      "Train Epoch: 7 [3200/9451 (34%)]\tLoss: 0.839129\n",
      "Train Epoch: 7 [3840/9451 (41%)]\tLoss: 1.276480\n",
      "Train Epoch: 7 [4480/9451 (47%)]\tLoss: 0.870761\n",
      "Train Epoch: 7 [5120/9451 (54%)]\tLoss: 0.912085\n",
      "Train Epoch: 7 [5760/9451 (61%)]\tLoss: 1.224133\n",
      "Train Epoch: 7 [6400/9451 (68%)]\tLoss: 0.980923\n",
      "Train Epoch: 7 [7040/9451 (74%)]\tLoss: 0.964613\n",
      "Train Epoch: 7 [7680/9451 (81%)]\tLoss: 1.035592\n",
      "Train Epoch: 7 [8320/9451 (88%)]\tLoss: 1.180998\n",
      "Train Epoch: 7 [8960/9451 (95%)]\tLoss: 0.900884\n",
      "Train Epoch: 8 [0/9451 (0%)]\tLoss: 0.984777\n",
      "Train Epoch: 8 [640/9451 (7%)]\tLoss: 0.941737\n",
      "Train Epoch: 8 [1280/9451 (14%)]\tLoss: 0.873864\n",
      "Train Epoch: 8 [1920/9451 (20%)]\tLoss: 0.797805\n",
      "Train Epoch: 8 [2560/9451 (27%)]\tLoss: 0.804113\n",
      "Train Epoch: 8 [3200/9451 (34%)]\tLoss: 0.852839\n",
      "Train Epoch: 8 [3840/9451 (41%)]\tLoss: 0.733895\n",
      "Train Epoch: 8 [4480/9451 (47%)]\tLoss: 0.896097\n",
      "Train Epoch: 8 [5120/9451 (54%)]\tLoss: 0.795362\n",
      "Train Epoch: 8 [5760/9451 (61%)]\tLoss: 1.315111\n",
      "Train Epoch: 8 [6400/9451 (68%)]\tLoss: 0.872227\n",
      "Train Epoch: 8 [7040/9451 (74%)]\tLoss: 0.788983\n",
      "Train Epoch: 8 [7680/9451 (81%)]\tLoss: 0.792325\n",
      "Train Epoch: 8 [8320/9451 (88%)]\tLoss: 0.681807\n",
      "Train Epoch: 8 [8960/9451 (95%)]\tLoss: 0.881369\n",
      "Train Epoch: 9 [0/9451 (0%)]\tLoss: 1.051234\n",
      "Train Epoch: 9 [640/9451 (7%)]\tLoss: 0.744976\n",
      "Train Epoch: 9 [1280/9451 (14%)]\tLoss: 0.896390\n",
      "Train Epoch: 9 [1920/9451 (20%)]\tLoss: 0.639978\n",
      "Train Epoch: 9 [2560/9451 (27%)]\tLoss: 0.911442\n",
      "Train Epoch: 9 [3200/9451 (34%)]\tLoss: 0.857247\n",
      "Train Epoch: 9 [3840/9451 (41%)]\tLoss: 0.825950\n",
      "Train Epoch: 9 [4480/9451 (47%)]\tLoss: 0.835077\n",
      "Train Epoch: 9 [5120/9451 (54%)]\tLoss: 0.792154\n",
      "Train Epoch: 9 [5760/9451 (61%)]\tLoss: 0.773846\n",
      "Train Epoch: 9 [6400/9451 (68%)]\tLoss: 0.701023\n",
      "Train Epoch: 9 [7040/9451 (74%)]\tLoss: 1.127362\n",
      "Train Epoch: 9 [7680/9451 (81%)]\tLoss: 0.733953\n",
      "Train Epoch: 9 [8320/9451 (88%)]\tLoss: 1.029346\n",
      "Train Epoch: 9 [8960/9451 (95%)]\tLoss: 0.889839\n",
      "Train Epoch: 10 [0/9451 (0%)]\tLoss: 1.123745\n",
      "Train Epoch: 10 [640/9451 (7%)]\tLoss: 0.651551\n",
      "Train Epoch: 10 [1280/9451 (14%)]\tLoss: 0.750163\n",
      "Train Epoch: 10 [1920/9451 (20%)]\tLoss: 0.821125\n",
      "Train Epoch: 10 [2560/9451 (27%)]\tLoss: 0.830779\n",
      "Train Epoch: 10 [3200/9451 (34%)]\tLoss: 0.954186\n",
      "Train Epoch: 10 [3840/9451 (41%)]\tLoss: 0.906533\n",
      "Train Epoch: 10 [4480/9451 (47%)]\tLoss: 1.088195\n",
      "Train Epoch: 10 [5120/9451 (54%)]\tLoss: 0.944990\n",
      "Train Epoch: 10 [5760/9451 (61%)]\tLoss: 0.748417\n",
      "Train Epoch: 10 [6400/9451 (68%)]\tLoss: 0.997335\n",
      "Train Epoch: 10 [7040/9451 (74%)]\tLoss: 0.829723\n",
      "Train Epoch: 10 [7680/9451 (81%)]\tLoss: 0.736487\n",
      "Train Epoch: 10 [8320/9451 (88%)]\tLoss: 0.696804\n",
      "Train Epoch: 10 [8960/9451 (95%)]\tLoss: 0.718144\n",
      "Train Epoch: 11 [0/9451 (0%)]\tLoss: 0.731267\n",
      "Train Epoch: 11 [640/9451 (7%)]\tLoss: 0.901213\n",
      "Train Epoch: 11 [1280/9451 (14%)]\tLoss: 0.797750\n",
      "Train Epoch: 11 [1920/9451 (20%)]\tLoss: 0.807747\n",
      "Train Epoch: 11 [2560/9451 (27%)]\tLoss: 0.754569\n",
      "Train Epoch: 11 [3200/9451 (34%)]\tLoss: 0.842043\n",
      "Train Epoch: 11 [3840/9451 (41%)]\tLoss: 1.067950\n",
      "Train Epoch: 11 [4480/9451 (47%)]\tLoss: 0.829553\n",
      "Train Epoch: 11 [5120/9451 (54%)]\tLoss: 0.858390\n",
      "Train Epoch: 11 [5760/9451 (61%)]\tLoss: 0.819596\n",
      "Train Epoch: 11 [6400/9451 (68%)]\tLoss: 0.754371\n",
      "Train Epoch: 11 [7040/9451 (74%)]\tLoss: 0.647317\n",
      "Train Epoch: 11 [7680/9451 (81%)]\tLoss: 0.987373\n",
      "Train Epoch: 11 [8320/9451 (88%)]\tLoss: 0.845777\n",
      "Train Epoch: 11 [8960/9451 (95%)]\tLoss: 1.035668\n",
      "Train Epoch: 12 [0/9451 (0%)]\tLoss: 0.677462\n",
      "Train Epoch: 12 [640/9451 (7%)]\tLoss: 0.884841\n",
      "Train Epoch: 12 [1280/9451 (14%)]\tLoss: 0.783599\n",
      "Train Epoch: 12 [1920/9451 (20%)]\tLoss: 1.106139\n",
      "Train Epoch: 12 [2560/9451 (27%)]\tLoss: 0.857892\n",
      "Train Epoch: 12 [3200/9451 (34%)]\tLoss: 1.083497\n",
      "Train Epoch: 12 [3840/9451 (41%)]\tLoss: 0.728319\n",
      "Train Epoch: 12 [4480/9451 (47%)]\tLoss: 0.969848\n",
      "Train Epoch: 12 [5120/9451 (54%)]\tLoss: 0.726160\n",
      "Train Epoch: 12 [5760/9451 (61%)]\tLoss: 0.770930\n",
      "Train Epoch: 12 [6400/9451 (68%)]\tLoss: 0.991300\n",
      "Train Epoch: 12 [7040/9451 (74%)]\tLoss: 0.963111\n",
      "Train Epoch: 12 [7680/9451 (81%)]\tLoss: 0.673023\n",
      "Train Epoch: 12 [8320/9451 (88%)]\tLoss: 0.860959\n",
      "Train Epoch: 12 [8960/9451 (95%)]\tLoss: 1.008179\n",
      "Train Epoch: 13 [0/9451 (0%)]\tLoss: 1.069320\n",
      "Train Epoch: 13 [640/9451 (7%)]\tLoss: 0.682704\n",
      "Train Epoch: 13 [1280/9451 (14%)]\tLoss: 1.006160\n",
      "Train Epoch: 13 [1920/9451 (20%)]\tLoss: 0.804696\n",
      "Train Epoch: 13 [2560/9451 (27%)]\tLoss: 0.729532\n",
      "Train Epoch: 13 [3200/9451 (34%)]\tLoss: 0.515215\n",
      "Train Epoch: 13 [3840/9451 (41%)]\tLoss: 0.765634\n",
      "Train Epoch: 13 [4480/9451 (47%)]\tLoss: 0.726425\n",
      "Train Epoch: 13 [5120/9451 (54%)]\tLoss: 0.859984\n",
      "Train Epoch: 13 [5760/9451 (61%)]\tLoss: 0.875838\n",
      "Train Epoch: 13 [6400/9451 (68%)]\tLoss: 0.765722\n",
      "Train Epoch: 13 [7040/9451 (74%)]\tLoss: 0.640744\n",
      "Train Epoch: 13 [7680/9451 (81%)]\tLoss: 0.703891\n",
      "Train Epoch: 13 [8320/9451 (88%)]\tLoss: 1.051350\n",
      "Train Epoch: 13 [8960/9451 (95%)]\tLoss: 0.629941\n",
      "Train Epoch: 14 [0/9451 (0%)]\tLoss: 0.683346\n",
      "Train Epoch: 14 [640/9451 (7%)]\tLoss: 0.900095\n",
      "Train Epoch: 14 [1280/9451 (14%)]\tLoss: 0.789946\n",
      "Train Epoch: 14 [1920/9451 (20%)]\tLoss: 0.722586\n",
      "Train Epoch: 14 [2560/9451 (27%)]\tLoss: 0.936359\n",
      "Train Epoch: 14 [3200/9451 (34%)]\tLoss: 0.909171\n",
      "Train Epoch: 14 [3840/9451 (41%)]\tLoss: 0.706408\n",
      "Train Epoch: 14 [4480/9451 (47%)]\tLoss: 0.872965\n",
      "Train Epoch: 14 [5120/9451 (54%)]\tLoss: 0.699422\n",
      "Train Epoch: 14 [5760/9451 (61%)]\tLoss: 0.980740\n",
      "Train Epoch: 14 [6400/9451 (68%)]\tLoss: 0.955207\n",
      "Train Epoch: 14 [7040/9451 (74%)]\tLoss: 0.892419\n",
      "Train Epoch: 14 [7680/9451 (81%)]\tLoss: 0.793425\n",
      "Train Epoch: 14 [8320/9451 (88%)]\tLoss: 0.804138\n",
      "Train Epoch: 14 [8960/9451 (95%)]\tLoss: 0.866458\n",
      "Train Epoch: 15 [0/9451 (0%)]\tLoss: 0.722524\n",
      "Train Epoch: 15 [640/9451 (7%)]\tLoss: 0.822212\n",
      "Train Epoch: 15 [1280/9451 (14%)]\tLoss: 0.829388\n",
      "Train Epoch: 15 [1920/9451 (20%)]\tLoss: 0.843570\n",
      "Train Epoch: 15 [2560/9451 (27%)]\tLoss: 0.691392\n",
      "Train Epoch: 15 [3200/9451 (34%)]\tLoss: 0.732683\n",
      "Train Epoch: 15 [3840/9451 (41%)]\tLoss: 0.806513\n",
      "Train Epoch: 15 [4480/9451 (47%)]\tLoss: 0.529088\n",
      "Train Epoch: 15 [5120/9451 (54%)]\tLoss: 0.965069\n",
      "Train Epoch: 15 [5760/9451 (61%)]\tLoss: 0.763976\n",
      "Train Epoch: 15 [6400/9451 (68%)]\tLoss: 0.999381\n",
      "Train Epoch: 15 [7040/9451 (74%)]\tLoss: 0.606063\n",
      "Train Epoch: 15 [7680/9451 (81%)]\tLoss: 0.869888\n",
      "Train Epoch: 15 [8320/9451 (88%)]\tLoss: 0.819545\n",
      "Train Epoch: 15 [8960/9451 (95%)]\tLoss: 0.871459\n",
      "Train Epoch: 16 [0/9451 (0%)]\tLoss: 0.915437\n",
      "Train Epoch: 16 [640/9451 (7%)]\tLoss: 0.735924\n",
      "Train Epoch: 16 [1280/9451 (14%)]\tLoss: 0.890737\n",
      "Train Epoch: 16 [1920/9451 (20%)]\tLoss: 0.658941\n",
      "Train Epoch: 16 [2560/9451 (27%)]\tLoss: 1.030874\n",
      "Train Epoch: 16 [3200/9451 (34%)]\tLoss: 0.690198\n",
      "Train Epoch: 16 [3840/9451 (41%)]\tLoss: 0.724557\n",
      "Train Epoch: 16 [4480/9451 (47%)]\tLoss: 0.923609\n",
      "Train Epoch: 16 [5120/9451 (54%)]\tLoss: 0.974872\n",
      "Train Epoch: 16 [5760/9451 (61%)]\tLoss: 0.504984\n",
      "Train Epoch: 16 [6400/9451 (68%)]\tLoss: 0.694283\n",
      "Train Epoch: 16 [7040/9451 (74%)]\tLoss: 0.611968\n",
      "Train Epoch: 16 [7680/9451 (81%)]\tLoss: 0.823525\n",
      "Train Epoch: 16 [8320/9451 (88%)]\tLoss: 0.726396\n",
      "Train Epoch: 16 [8960/9451 (95%)]\tLoss: 1.086617\n",
      "Train Epoch: 17 [0/9451 (0%)]\tLoss: 0.935199\n",
      "Train Epoch: 17 [640/9451 (7%)]\tLoss: 1.020764\n",
      "Train Epoch: 17 [1280/9451 (14%)]\tLoss: 0.783451\n",
      "Train Epoch: 17 [1920/9451 (20%)]\tLoss: 0.816290\n",
      "Train Epoch: 17 [2560/9451 (27%)]\tLoss: 0.755341\n",
      "Train Epoch: 17 [3200/9451 (34%)]\tLoss: 0.921257\n",
      "Train Epoch: 17 [3840/9451 (41%)]\tLoss: 0.834991\n",
      "Train Epoch: 17 [4480/9451 (47%)]\tLoss: 0.754295\n",
      "Train Epoch: 17 [5120/9451 (54%)]\tLoss: 0.794435\n",
      "Train Epoch: 17 [5760/9451 (61%)]\tLoss: 0.743726\n",
      "Train Epoch: 17 [6400/9451 (68%)]\tLoss: 0.661662\n",
      "Train Epoch: 17 [7040/9451 (74%)]\tLoss: 0.823867\n",
      "Train Epoch: 17 [7680/9451 (81%)]\tLoss: 0.767312\n",
      "Train Epoch: 17 [8320/9451 (88%)]\tLoss: 0.804260\n",
      "Train Epoch: 17 [8960/9451 (95%)]\tLoss: 0.812713\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/1026 (0%)]\tLoss: 1.667361\n",
      "Train Epoch: 1 [640/1026 (59%)]\tLoss: 1.336630\n",
      "Train Epoch: 2 [0/1026 (0%)]\tLoss: 1.104215\n",
      "Train Epoch: 2 [640/1026 (59%)]\tLoss: 1.271256\n",
      "Train Epoch: 3 [0/1026 (0%)]\tLoss: 1.370769\n",
      "Train Epoch: 3 [640/1026 (59%)]\tLoss: 1.159791\n",
      "Train Epoch: 4 [0/1026 (0%)]\tLoss: 1.028453\n",
      "Train Epoch: 4 [640/1026 (59%)]\tLoss: 0.968959\n",
      "Train Epoch: 5 [0/1026 (0%)]\tLoss: 1.089866\n",
      "Train Epoch: 5 [640/1026 (59%)]\tLoss: 1.034642\n",
      "Train Epoch: 6 [0/1026 (0%)]\tLoss: 1.061990\n",
      "Train Epoch: 6 [640/1026 (59%)]\tLoss: 0.977976\n",
      "Train Epoch: 7 [0/1026 (0%)]\tLoss: 0.972857\n",
      "Train Epoch: 7 [640/1026 (59%)]\tLoss: 0.972522\n",
      "Train Epoch: 8 [0/1026 (0%)]\tLoss: 0.844491\n",
      "Train Epoch: 8 [640/1026 (59%)]\tLoss: 1.097093\n",
      "Train Epoch: 9 [0/1026 (0%)]\tLoss: 1.204776\n",
      "Train Epoch: 9 [640/1026 (59%)]\tLoss: 1.330349\n",
      "Train Epoch: 10 [0/1026 (0%)]\tLoss: 1.172690\n",
      "Train Epoch: 10 [640/1026 (59%)]\tLoss: 1.180649\n",
      "Train Epoch: 11 [0/1026 (0%)]\tLoss: 1.030288\n",
      "Train Epoch: 11 [640/1026 (59%)]\tLoss: 0.807305\n",
      "Train Epoch: 12 [0/1026 (0%)]\tLoss: 1.236276\n",
      "Train Epoch: 12 [640/1026 (59%)]\tLoss: 1.015761\n",
      "Train Epoch: 13 [0/1026 (0%)]\tLoss: 1.110352\n",
      "Train Epoch: 13 [640/1026 (59%)]\tLoss: 1.191703\n",
      "Train Epoch: 14 [0/1026 (0%)]\tLoss: 1.055933\n",
      "Train Epoch: 14 [640/1026 (59%)]\tLoss: 0.966658\n",
      "Train Epoch: 15 [0/1026 (0%)]\tLoss: 0.988962\n",
      "Train Epoch: 15 [640/1026 (59%)]\tLoss: 0.984674\n",
      "Train Epoch: 16 [0/1026 (0%)]\tLoss: 0.916092\n",
      "Train Epoch: 16 [640/1026 (59%)]\tLoss: 1.072482\n",
      "Train Epoch: 17 [0/1026 (0%)]\tLoss: 1.034616\n",
      "Train Epoch: 17 [640/1026 (59%)]\tLoss: 0.910254\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.7345, Accuracy: 3686/10000 (37%)\n",
      "\n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 1.481464\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.408649\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.576879\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.293031\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.650278\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.473424\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.538725\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.221289\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.449339\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.534276\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.491269\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.349687\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.457964\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.307737\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.387273\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.423491\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.836786\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.262035\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.378051\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.759005\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.317983\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.447734\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.328739\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.392436\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.341260\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.449082\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.418319\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.196597\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.319180\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.252650\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.151158\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.360483\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.478109\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.226303\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.334151\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.522991\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.282308\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.607397\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.311603\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.318866\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.214815\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.640096\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.308055\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.193509\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.234279\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.379040\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.326858\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.197726\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.289620\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.191033\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.538470\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.235437\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.314212\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.341517\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.252891\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.094060\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.439606\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.296192\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.273890\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.376428\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.321547\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.306723\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.467467\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.131927\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.285324\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.448279\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.322128\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.245464\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.101557\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.272046\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.564150\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.410157\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.387712\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.499764\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.497949\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.395323\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.464428\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.310643\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.371085\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.234699\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.293377\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.490148\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.376028\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.150603\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.446674\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.332185\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.485408\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.312497\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.199954\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.337167\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.310195\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.223315\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.289441\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.443493\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.271911\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.327459\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.313605\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.107522\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.169580\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.252425\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.337111\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.373521\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 1.542737\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 1.111140\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 0.947291\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 0.997294\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 0.961073\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 1.035480\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 1.003325\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 0.607161\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 0.900150\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 1.037291\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 0.938789\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 0.815539\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 0.934086\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 1.015650\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 0.881653\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 0.815364\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 0.855544\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 1.633637\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 0.991063\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.001920\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 1.207671\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 0.977019\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 0.928802\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 0.955160\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 0.899216\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 0.853364\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 0.979367\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 0.796105\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 0.802794\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 0.734183\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 0.893888\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 0.899453\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 0.841462\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 1.043319\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.940729\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 1.057563\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 0.806561\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 0.915425\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 0.866053\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 0.730533\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 1.221298\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 0.627340\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 0.810421\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 0.691849\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 0.839654\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 0.718009\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.749037\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 0.847597\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.786936\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 0.914624\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 0.849203\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 0.681111\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 0.800290\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 0.809882\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 0.924613\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 0.909373\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 0.672411\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.714212\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 0.962803\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.920040\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.804738\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 0.656320\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 0.853280\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.656497\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.697935\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 0.817221\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 0.745541\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 0.800077\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 0.887879\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.918565\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 0.816105\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 0.817001\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 0.789411\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 0.799436\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 0.714811\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.921875\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 1.002467\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.858732\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 0.793232\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 0.773535\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 0.739178\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 0.870536\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 1.077857\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.810364\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 0.863934\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.932405\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.778465\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.693089\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 0.643335\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.796861\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 1.012529\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.646740\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 0.867010\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.718050\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.589768\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.786046\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 0.680907\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 0.801678\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.825005\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 0.625553\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.963262\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.683004\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 0.771563\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.768194\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.739413\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.584044\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 0.953973\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.567102\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.726886\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.795894\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.930307\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.601256\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.999091\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 0.815482\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 0.713135\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 0.797240\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.854063\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.625685\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.720004\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.889953\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.909024\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.690841\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.701279\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.693478\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 1.027779\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 1.021871\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 0.674463\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.750591\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.712331\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.835455\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.933560\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.731541\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.778487\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 1.020882\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.641106\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 0.613427\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 0.904921\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.910386\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.437626\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 0.899704\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.659370\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.726952\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 0.601102\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 0.656224\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.538907\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.710696\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.847464\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.857808\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 0.819317\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.880909\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.876711\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.903066\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.837314\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 1.467872\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.434158\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.464954\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.295948\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.116721\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.520491\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.296578\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.412837\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.300379\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.276137\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.406190\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.342450\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.303657\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.320673\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.260795\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.280650\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.520580\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.477701\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.290813\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.375843\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.163291\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.416897\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.219750\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.530195\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.561852\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.387002\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.330124\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.283251\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.092405\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.298705\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.545531\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.303057\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.435078\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.217973\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.421398\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.591300\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.568728\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.318541\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.121400\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.232924\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.459407\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.479160\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.186773\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.305352\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.208063\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.159738\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.364342\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.259536\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.481775\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.163001\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.129550\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.300191\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.339424\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.323595\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.228006\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.315300\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.280682\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.289800\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.274011\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.484505\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.411126\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.265230\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.113832\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.238221\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.315386\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.281555\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.193905\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.120893\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.342460\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.396200\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.278201\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.376632\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.276702\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.168427\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.436138\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.283906\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.257500\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.161488\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.156129\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.122680\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.150329\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.373996\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.312837\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.197088\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.359490\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.275513\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.101351\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.322012\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.330576\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.160738\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.281238\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.274789\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.224318\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.210390\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.307880\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.435774\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.357747\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.297666\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.243371\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.146976\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.378407\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.354331\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 2.016258\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 1.017125\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 0.735582\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 1.018809\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 1.121377\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 0.955909\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 0.906810\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 0.648441\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 0.968713\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 0.772789\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 0.615732\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 0.827917\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 0.862203\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 0.708753\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 0.881640\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 1.028733\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 0.915143\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 1.042329\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 1.076741\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 0.886413\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 0.911764\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 0.820386\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 0.693370\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 0.859512\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 0.911750\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.683719\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.820920\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 1.150108\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 0.869943\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 0.865924\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 1.171227\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 0.477475\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 0.741707\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.787159\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 0.612272\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.884287\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 0.441382\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 0.894102\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.720226\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 1.010541\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 0.702752\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.813815\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 1.028600\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 0.792463\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 0.846060\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 1.125155\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 0.844250\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.811144\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.998138\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 0.715904\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 1.111776\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 0.864406\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 0.744504\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.878572\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.843521\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.730397\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.847939\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 0.738905\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 1.222284\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.774682\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.825307\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 1.058377\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.781066\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 1.002491\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 0.799065\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.884741\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.722678\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 0.778495\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.521014\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.850156\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.655734\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 0.983709\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.733553\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 0.786734\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 1.059904\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.700697\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.728514\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 0.800029\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.778148\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.857287\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 0.731767\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.907730\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.835412\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.858972\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 0.791738\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 1.025819\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 0.479178\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 0.887829\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 0.652986\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 0.703705\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 0.724388\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.864777\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.913488\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.739795\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.795804\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 0.897257\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 1.039752\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 0.416630\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.915922\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.776084\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.880523\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 1.050091\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.416711\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.731546\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.827592\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 1.035001\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.717054\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 0.704248\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.705265\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.584237\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.760244\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.892968\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.620102\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.702957\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.659341\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.790820\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 0.571539\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.576176\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 0.677067\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.721735\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 0.787175\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 1.057234\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.781814\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.666619\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.935160\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.880587\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.524999\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.795132\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.902677\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.932329\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 1.079744\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.995636\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.848528\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.795930\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.891957\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.769390\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.903540\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.668746\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.650714\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.922740\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.626913\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.698229\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.787303\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 1.013056\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.833564\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.597913\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.778196\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.739129\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 0.770289\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.599481\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.702651\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.706566\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.641044\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.641434\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.836696\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.989797\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.502367\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.665801\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.916016\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.690440\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.544190\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.868768\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.955641\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.937900\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.789383\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.738055\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.668327\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.792026\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.541118\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 0.784858\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.835430\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.854378\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.503767\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.670945\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.747445\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.733769\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.827318\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.495174\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.843424\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.693318\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.661639\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.925111\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.690001\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.465245\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 0.751111\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.563866\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 0.692889\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.492491\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 1.063882\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.664727\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.625380\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.737221\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.654315\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.666848\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.684923\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.721780\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.490763\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.605564\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.589542\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.690730\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.838232\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.564507\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.928992\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.737473\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.958725\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.727020\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.822957\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.702585\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.912782\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 0.639400\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.466968\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.453050\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.739036\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.570956\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.674552\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.650692\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.769307\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.819724\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.697900\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.552735\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.780539\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.667554\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.886714\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.861723\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.661637\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.546923\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.713437\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.503136\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.658249\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.580327\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.568028\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.546712\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.703847\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.939703\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.712524\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.815638\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.632153\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.784776\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.737095\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.698897\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.736290\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.563388\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.640200\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.542749\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.716004\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.745341\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.753720\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.743461\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.632558\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.701300\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.476974\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.675409\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.880444\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.582408\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.740765\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.648986\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 0.910001\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 0.887559\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 0.765116\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 1.039765\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 1.048328\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 1.029344\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 0.779157\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 0.807431\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 0.829692\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.633718\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 1.049432\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 1.019292\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 0.704060\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 0.508852\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.861761\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 1.111471\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 0.746453\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.832996\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.529352\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.724548\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.605389\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.703855\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 0.624328\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 0.805022\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 0.797852\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 0.686785\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.898869\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 1.000454\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 1.012332\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.872261\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.799216\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 0.743913\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 0.772801\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 0.620154\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 0.539377\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 0.714547\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.567597\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.741308\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.951501\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.681852\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.755884\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.598378\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.692947\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 0.734047\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.796369\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.635319\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.793953\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.685794\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 0.664624\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.563002\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 0.743997\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.676104\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.573612\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.821690\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 0.529761\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.726837\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.936221\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.640489\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 0.508669\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.652122\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.809689\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.697169\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.427628\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.774110\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.645509\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.563856\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.771702\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.716033\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.717604\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.622512\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.537961\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.717271\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.704412\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.848215\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.687307\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 0.585566\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.692502\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.639849\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.769034\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.670347\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.575697\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.627348\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.675723\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.802075\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.677633\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.871722\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.915314\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.645547\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 1.045553\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.628122\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.744976\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.637948\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.639267\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.749999\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.580939\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.412430\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.502648\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.397455\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.627367\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.724190\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.877102\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.8207, Accuracy: 3719/10000 (37%)\n",
      "\n",
      "Round 2/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 1.107196\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.463866\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.603790\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.313376\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.391509\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.489044\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.645099\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.271811\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.241980\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.253795\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.493996\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.279344\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.206378\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.451566\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.164714\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.338369\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.376614\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.594929\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.241084\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.490562\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.311315\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.444674\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.419550\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.181212\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.316078\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.182324\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.208024\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.446204\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.257871\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.469422\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.427173\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.317609\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.310684\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.605446\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.282680\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.475667\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.153908\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.272510\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.376797\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.441769\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.362813\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.387908\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.261371\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.299324\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.421754\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.178159\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.164208\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.513382\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.219384\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.259352\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.132006\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.291013\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.170021\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.108077\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.454842\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.239304\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.304208\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.221951\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.480879\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.407770\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.139004\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.408281\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.355981\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.261921\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.420874\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.152187\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.497831\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.369195\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.317104\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.260761\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.348262\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.259603\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.302459\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.357172\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.283191\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.345640\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.112494\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.314868\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.421773\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.271124\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.159118\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.211064\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.250781\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.508893\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.224665\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.198825\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.150993\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.180733\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.168158\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.174447\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.207635\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.136831\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.191409\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.235211\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.283345\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.362324\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.245978\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.257223\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.196973\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.205015\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.176539\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.389408\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 1.559251\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 1.182975\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 0.993317\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 1.112619\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 0.886336\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 1.018396\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 1.179557\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 1.081254\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 0.997478\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 1.046688\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 0.814438\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 0.925326\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 0.913815\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 0.942104\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 0.906352\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 0.792282\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 0.790410\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 1.283842\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 0.811019\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 0.865498\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 1.004420\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 0.958315\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 0.815081\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 1.076437\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 0.873865\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 0.866651\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 0.986732\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 0.728602\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 0.808066\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 0.950512\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 0.878530\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 0.845667\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 0.645136\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 1.040971\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.829574\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 0.879614\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 0.918626\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 0.518709\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 0.709011\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 0.796027\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 0.716830\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 1.067736\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 0.837662\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 0.873789\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 0.982122\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 0.796906\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.718128\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 0.568373\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.684158\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 0.495143\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 0.744183\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 0.806069\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 0.909730\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 1.036705\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 0.758418\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 0.607810\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 0.933043\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.742925\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 0.889727\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.596206\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.785378\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 0.788590\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 0.855339\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.910733\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.817171\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 0.571247\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 0.635580\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 0.784908\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 0.741721\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.867541\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 0.538612\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 0.956227\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 0.704274\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 0.719164\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 1.024176\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.803292\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.580904\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.821449\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 0.572085\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 0.870749\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 0.827798\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 0.921332\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 0.811669\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.853201\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 0.703646\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.935060\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.575474\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.923535\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 0.719201\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.935529\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 0.803822\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.824964\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 0.780261\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.758389\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.610779\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.681164\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 0.491828\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 0.845756\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.651445\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 0.934133\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.894904\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.883783\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 0.614261\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.646754\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.771241\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.554683\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 0.839223\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.869411\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.461397\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.541234\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.708964\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.704847\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.920251\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 1.104299\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 0.731074\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 0.850302\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.765184\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.677675\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.635221\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.761991\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.923449\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.795226\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.613699\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.633269\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.682243\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.892054\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 0.820667\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.831173\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.867256\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.845017\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.615018\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.849769\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.804191\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.810816\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.942713\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 0.780347\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 0.915324\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.601791\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.695760\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 0.746019\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.824771\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.483889\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 0.903716\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 0.813246\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.713471\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.559803\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.619422\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.860473\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 0.835630\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.864794\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.592879\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.494669\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.704281\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 1.204200\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.411883\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.210755\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.462283\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.271597\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.447985\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.440252\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.666874\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.266069\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.404974\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.267320\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.359693\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.210959\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.240972\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.389440\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.395153\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.414383\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.264885\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.189062\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.342325\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.297517\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.328369\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.063541\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.375994\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.140129\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.301906\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.317493\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.439498\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.101323\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.288520\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.243760\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.332691\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.221023\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.286873\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.079767\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.287511\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.200157\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.365010\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.138161\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.141845\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.221040\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.245545\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.153529\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.400164\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.313136\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.208184\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.391884\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.185042\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.086929\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.314814\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.535568\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.154259\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.577958\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.661359\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.301738\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.321094\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.548872\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.170316\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.348768\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.196771\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.220912\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.318881\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.156791\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.290292\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.317859\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.191293\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.263844\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.273120\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.374973\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.401605\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.398258\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.138253\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.217994\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.251433\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.139063\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.140853\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.074757\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.182100\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.166032\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.296411\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.300334\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.237032\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.210230\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.135701\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.157636\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.218796\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.243944\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.400577\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.256474\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.112982\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.267300\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.230221\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.161296\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.388809\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.162468\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.062639\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.209751\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.115534\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.363312\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.145556\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.314183\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.143250\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 1.709797\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 1.097677\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 0.965190\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 0.975312\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 0.759472\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 1.067887\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 0.692651\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 0.909466\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 0.830763\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 0.922755\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 1.289909\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 0.929142\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 0.700284\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 0.855865\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 0.880523\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 0.893590\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 1.055326\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 0.901702\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 0.869970\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 0.789576\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 0.738458\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 0.786117\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 0.592585\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 0.817200\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 1.048759\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.801098\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.975096\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 0.994505\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 0.933740\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 0.867414\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 1.038771\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 0.739065\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 0.759897\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.789062\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 0.848040\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.565779\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 0.899604\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 0.681010\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.883270\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 0.605318\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 0.852149\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.958478\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 0.806901\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 0.684788\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 1.000308\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.879161\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 0.982371\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.764362\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.888410\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 0.880092\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 0.689849\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 1.002276\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 0.664950\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.818662\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.625671\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.653795\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.911377\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 0.809283\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.767709\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.853765\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.778216\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.630170\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.724446\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 0.770697\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 0.619837\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.756667\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.669679\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 0.637369\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.763091\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.883250\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.720005\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 0.658631\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.973851\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 0.720119\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 0.922930\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.647716\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.634421\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 0.746327\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.916394\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.907030\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 0.762274\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.810447\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.564710\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.727631\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 0.884043\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 0.655425\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 0.939227\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 1.132919\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 0.785081\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 1.003013\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 1.005360\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.738471\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.585128\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.656666\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 1.057654\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 0.703655\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.521324\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 0.645009\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.639647\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.608851\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.818571\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 0.598938\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.474883\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.685850\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.575391\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 0.720065\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.749635\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 0.707644\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.690777\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.985613\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.831229\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.438399\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.757281\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.713598\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.549588\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.791744\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 0.654217\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.794147\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 0.672502\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.789288\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 1.038378\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 0.807722\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.707744\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.547531\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.736565\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.778271\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.598625\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.832997\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.753790\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.914096\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 0.801220\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.889630\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.653750\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.652322\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.746350\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.807738\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.787347\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.684921\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.681738\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.601723\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.867509\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.719926\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.825718\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.650164\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.754861\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.744784\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.701063\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.714423\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 0.670418\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.803831\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.562609\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.692620\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.652250\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.615957\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.772224\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.733918\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.619556\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.709333\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.525179\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.713980\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.732136\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.787325\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.746094\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.580221\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.841094\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.594510\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.754683\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.819534\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.788501\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 0.788715\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.748186\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.777024\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.937523\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.784484\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.722790\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.708159\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.735902\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.676336\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.674156\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.874254\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.835327\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.972616\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.616543\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.594480\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 0.683061\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.715728\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 0.563965\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.660056\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.689702\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.631186\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.996496\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.642751\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.885672\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.721073\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.903799\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.707892\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.800767\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.462106\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.670859\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.780167\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.521459\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.927444\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.517017\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.770824\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.730747\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.692084\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.664533\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.808513\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.772749\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 0.885773\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.593914\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.550235\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.761078\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.649794\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.838507\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.866035\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.641751\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.825877\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.582424\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.908025\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.565942\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.636155\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.745947\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.821405\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.656394\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.800066\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.540239\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.718908\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 1.024682\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.823780\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.715782\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.646725\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.763447\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.825972\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.560368\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.541306\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.465752\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.603844\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.772041\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.527681\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.623887\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.871190\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.732645\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.714135\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.530972\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.956626\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.537361\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.535912\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.777106\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.981820\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.666407\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.583411\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.636352\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.961003\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.505566\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.293995\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 1.194890\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 1.039350\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 0.633928\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 0.876658\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 0.740192\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 0.988383\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 0.948180\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 1.067538\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 0.693127\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.648718\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 0.670798\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 0.902708\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 0.679833\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 0.732432\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.634627\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 0.732470\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 0.539167\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.875221\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.810484\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.609743\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.692383\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.674687\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 0.445131\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 0.942452\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 0.733906\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 0.847726\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 1.133914\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 0.841593\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 0.651367\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.555422\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.622912\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 1.160321\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 1.025860\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 0.870462\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 0.638380\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 0.603842\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.782045\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.594743\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.605598\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.677304\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.567004\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.601181\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.943019\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 0.744420\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.672684\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.787695\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.487730\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.579301\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 1.004778\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.609573\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 0.864968\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.690356\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.654968\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.463733\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 0.807912\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.757330\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.925915\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.628301\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 0.630560\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.620318\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.942336\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.652644\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.511052\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.868320\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.552619\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.655199\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.901986\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.815821\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.745170\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.737569\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.695149\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.879678\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.843885\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.860940\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.844763\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 0.731061\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.596541\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.807908\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.841731\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.570184\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.650122\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.673045\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.774129\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.507301\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.780852\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.784527\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.682720\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.690564\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.540481\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.560927\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.588386\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.824358\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.770873\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.545758\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.710011\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.532898\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.554687\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.423905\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.730723\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.497352\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.742229\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.8115, Accuracy: 3860/10000 (39%)\n",
      "\n",
      "Round 3/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 0.960688\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.407169\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.326565\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.183076\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.217788\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.400039\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.600928\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.412047\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.316983\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.258630\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.179691\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.442781\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.222661\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.376154\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.210635\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.403183\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.367241\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.296460\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.359301\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.322749\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.375830\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.340669\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.296346\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.292004\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.426411\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.168218\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.421515\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.338623\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.541273\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.373928\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.364638\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.342885\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.137841\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.203634\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.248281\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.244632\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.270044\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.305135\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.301781\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.286371\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.191261\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.437656\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.256349\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.297952\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.558516\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.273919\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.401066\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.081801\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.247272\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.385755\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.164938\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.137818\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.385970\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.372435\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.350919\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.226504\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.129212\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.290339\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.406692\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.655400\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.189065\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.390219\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.450044\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.164013\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.256112\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.258958\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.304912\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.218476\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.387600\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.166470\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.341061\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.310435\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.073288\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.204607\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.189707\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.377711\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.291152\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.227444\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.353318\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.409434\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.188326\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.227548\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.291784\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.252926\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.281882\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.134092\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.222206\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.287724\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.325357\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.336084\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.399365\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.098605\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.192392\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.380547\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.280963\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.266814\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.462121\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.390796\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.277829\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.232625\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.368553\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.119355\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 1.225416\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 1.122905\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 0.929738\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 0.982775\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 0.975027\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 0.811395\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 0.868673\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 0.775093\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 0.920279\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 1.034431\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 0.917296\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 0.712035\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 0.638621\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 0.864708\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 0.817881\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 0.848847\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 0.940306\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 1.399947\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 0.831756\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.334068\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 0.858601\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 1.200557\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 1.169116\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 0.923288\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 0.833668\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 0.800177\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 0.900794\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 0.689109\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 1.113651\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 1.033945\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 0.661251\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 0.546228\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 1.082017\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 0.749027\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.834488\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 0.728272\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 0.750684\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 0.663480\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 0.672961\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 0.794314\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 0.735968\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 1.015195\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 0.754399\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 0.588298\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 0.776825\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 0.664413\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.613817\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 0.915090\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.686161\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 0.770552\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 0.824115\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 0.790031\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 0.481242\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 0.783678\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 0.785563\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 0.766789\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 0.819156\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.714837\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 0.964182\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.625263\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.870040\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 0.478913\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 0.566966\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.902936\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.902777\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 0.814631\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 0.834447\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 1.039950\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 0.638902\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.980761\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 1.066492\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 0.687868\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 0.518800\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 0.720114\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 0.698975\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.824722\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.754620\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.649447\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 0.694644\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 0.773542\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 0.590629\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 0.681107\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 1.141019\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.820794\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 0.643296\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.627177\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.712048\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.957622\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 0.775915\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.753833\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 1.096225\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.641926\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 0.754088\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.574053\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.709997\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.845225\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 0.733525\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 0.739314\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.527762\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 0.603712\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.864649\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.707369\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 0.779783\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 1.007101\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.641173\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.597451\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 0.810552\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.742999\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.829097\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.785790\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 1.032708\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.639915\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.628178\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 0.954581\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 0.670765\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 0.808263\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.720568\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.934520\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.835198\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.835730\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.709238\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.513631\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.821855\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.848954\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.707819\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.558466\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 0.550742\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.668712\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.804334\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.747465\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.692734\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.859920\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.693537\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.661988\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.769006\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 0.813083\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 0.954908\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.505337\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.748012\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 0.919865\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.631729\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.623768\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 0.716856\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 0.677661\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.711876\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.761784\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.666247\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.738183\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 0.831975\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.810917\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.594535\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.678005\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.692285\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 1.319013\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.348005\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.173190\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.412467\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.171398\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.335293\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.116213\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.460756\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.170188\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.383875\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.327705\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.372568\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.225231\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.270591\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.156087\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.513311\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.248559\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.273343\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.300963\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.497308\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.183793\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.200646\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.130480\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.420453\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.142678\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.605323\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.145944\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.386765\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.117956\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.453425\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.141901\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.184288\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.302004\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.116343\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.381121\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.307034\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.384537\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.266588\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.530593\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.276723\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.174276\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.140372\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.371338\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.260999\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.416189\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.281934\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.336009\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.259205\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.211601\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.214199\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.381484\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.218258\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.370065\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.191356\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.202156\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.176494\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.203515\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.240471\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.251658\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.513309\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.271425\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.121762\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.562667\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.161268\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.256040\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.075475\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.171402\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.306962\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.244752\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.281636\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.248449\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.183444\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.166045\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.206520\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.232177\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.456124\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.454561\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.203528\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.146743\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.217172\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.218414\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.244623\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.124929\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.250943\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.099615\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.157061\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.200129\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.293406\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.129153\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.264054\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.181695\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.215120\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.252031\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.186433\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.164830\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.086598\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.172145\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.187105\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.227265\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.331951\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.148975\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.024480\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 1.701928\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 0.851564\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 0.858221\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 0.875246\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 0.830636\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 0.749606\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 0.651735\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 1.127481\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 0.718574\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 0.763652\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 0.759492\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 0.806386\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 1.153761\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 0.782131\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 0.963233\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 0.821045\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 1.030279\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 0.616475\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 0.820724\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 0.708494\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 0.588214\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 0.776556\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 0.715740\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 0.640019\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 0.684997\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.820232\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.740051\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 0.957876\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 0.757440\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 0.718255\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 0.685704\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 0.782116\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 0.651047\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.641716\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 0.987819\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.550442\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 0.725600\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 0.670310\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.892762\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 0.891976\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 1.124889\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.605388\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 0.937849\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 1.008683\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 0.740665\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.800727\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 0.652730\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.768444\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.726851\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 0.544052\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 0.957291\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 0.667732\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 0.909870\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.906448\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.777584\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.592685\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.753657\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 0.776925\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.703772\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.667749\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.706010\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.799754\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.706975\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 1.032343\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 0.595568\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.776152\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.849420\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 0.959606\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.790123\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.558827\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.788491\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 0.658401\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.938089\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 0.671316\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 0.973126\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.552219\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.646704\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 0.777513\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.711576\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.556540\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 0.684130\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.703702\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.809507\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.758047\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 0.665872\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 0.784119\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 0.604932\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 0.732154\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 0.827514\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 0.539812\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 0.682561\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.578084\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.710602\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.787364\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.655277\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 0.560519\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.744577\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 0.949925\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.703246\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.699536\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.639305\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 0.634516\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.566656\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.797106\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.693509\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 0.694720\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.801109\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 0.574458\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.733780\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.678951\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.497578\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.928350\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.892790\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.788748\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.674472\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.840721\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 0.648057\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.657657\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 1.116403\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.736383\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 0.869325\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 0.758927\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.781221\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.678383\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.709332\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.603910\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.543582\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.632781\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.669854\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.756263\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 0.855204\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.693950\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 1.059215\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.586760\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.853511\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.633762\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.576501\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.855025\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.744796\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.794905\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.466125\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.864141\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.798996\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.862307\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.592127\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.640218\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.731554\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.580905\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 0.400009\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.866313\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.626982\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.682285\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.893152\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.655788\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.850694\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.466980\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.770341\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.647400\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.744536\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.557793\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.832833\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.649800\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.918951\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.799600\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.806644\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.778544\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.727665\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.655577\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.571912\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 0.824226\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.886404\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.848245\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.755117\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.586991\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.572447\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.786456\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.681687\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.602199\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.737134\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.704125\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.827192\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.798540\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.778118\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.679438\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 0.484350\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.819161\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 1.098179\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.638976\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.963146\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.945827\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.726261\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.639257\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.670121\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.627261\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.535180\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.718086\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.588504\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.867083\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.793750\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.623038\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.663027\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.685059\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.490477\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.571155\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.594233\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.907309\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 1.005992\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 1.022471\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.650193\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 0.686997\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.728893\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.730140\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.572467\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.488709\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.804077\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 1.098294\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.774356\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.428543\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.568593\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.556298\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 0.705421\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.763689\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.741162\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.666933\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.561944\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.694613\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.645580\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.554360\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.767322\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.702265\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.474134\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.788918\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.722430\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.748705\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.628404\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.585279\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.564055\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.742406\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.680208\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 0.572631\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.528006\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.567273\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.777908\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.910905\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.626755\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.733215\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.658066\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.812303\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.722727\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.625919\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.581859\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.972603\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.607686\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.849887\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.735719\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.523710\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 0.755596\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 0.907517\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 0.934087\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 0.911306\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 0.823981\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 0.761381\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 1.002665\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 0.740960\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 0.906252\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.688347\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 0.789826\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 0.845784\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 1.130860\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 0.448035\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.560893\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 0.902040\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 0.726175\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.814861\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.648913\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.820816\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.424460\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.844369\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 0.601934\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 0.689145\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 0.690800\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 0.862122\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.757641\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 0.879363\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 0.741658\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.698335\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.634288\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 0.647220\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 0.807680\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 1.119581\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 0.625118\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 0.599161\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.593333\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.617519\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.540104\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.725381\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.686639\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.495330\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.686956\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 0.871165\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.571549\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.498264\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.780938\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.744502\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 0.629856\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.520122\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 0.820988\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.838872\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.525436\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.643931\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 0.652299\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.516900\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.521374\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.657384\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 0.560080\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.796235\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.623097\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.644553\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.826373\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.768784\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.773430\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.690268\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.655563\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.881825\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.546462\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.453153\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.621813\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.726078\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.443950\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.625352\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.732345\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 0.611903\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.597046\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.497796\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.695078\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.594081\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.668625\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.560327\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.501179\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.576739\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.483416\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.725025\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.705037\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.608360\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.710124\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.616165\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.842572\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.443688\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.550958\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.570729\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.510283\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.571957\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.640406\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.781773\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.908481\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 1.017616\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.772298\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.7925, Accuracy: 3983/10000 (40%)\n",
      "\n",
      "Round 4/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3701 (0%)]\tLoss: 1.032388\n",
      "Train Epoch: 1 [640/3701 (17%)]\tLoss: 0.350133\n",
      "Train Epoch: 1 [1280/3701 (34%)]\tLoss: 0.508776\n",
      "Train Epoch: 1 [1920/3701 (52%)]\tLoss: 0.315723\n",
      "Train Epoch: 1 [2560/3701 (69%)]\tLoss: 0.225777\n",
      "Train Epoch: 1 [3200/3701 (86%)]\tLoss: 0.318478\n",
      "Train Epoch: 2 [0/3701 (0%)]\tLoss: 0.372507\n",
      "Train Epoch: 2 [640/3701 (17%)]\tLoss: 0.491235\n",
      "Train Epoch: 2 [1280/3701 (34%)]\tLoss: 0.429053\n",
      "Train Epoch: 2 [1920/3701 (52%)]\tLoss: 0.229123\n",
      "Train Epoch: 2 [2560/3701 (69%)]\tLoss: 0.438048\n",
      "Train Epoch: 2 [3200/3701 (86%)]\tLoss: 0.295388\n",
      "Train Epoch: 3 [0/3701 (0%)]\tLoss: 0.212918\n",
      "Train Epoch: 3 [640/3701 (17%)]\tLoss: 0.357257\n",
      "Train Epoch: 3 [1280/3701 (34%)]\tLoss: 0.342528\n",
      "Train Epoch: 3 [1920/3701 (52%)]\tLoss: 0.286917\n",
      "Train Epoch: 3 [2560/3701 (69%)]\tLoss: 0.425402\n",
      "Train Epoch: 3 [3200/3701 (86%)]\tLoss: 0.292254\n",
      "Train Epoch: 4 [0/3701 (0%)]\tLoss: 0.349483\n",
      "Train Epoch: 4 [640/3701 (17%)]\tLoss: 0.238049\n",
      "Train Epoch: 4 [1280/3701 (34%)]\tLoss: 0.296935\n",
      "Train Epoch: 4 [1920/3701 (52%)]\tLoss: 0.270569\n",
      "Train Epoch: 4 [2560/3701 (69%)]\tLoss: 0.489484\n",
      "Train Epoch: 4 [3200/3701 (86%)]\tLoss: 0.202482\n",
      "Train Epoch: 5 [0/3701 (0%)]\tLoss: 0.299627\n",
      "Train Epoch: 5 [640/3701 (17%)]\tLoss: 0.286804\n",
      "Train Epoch: 5 [1280/3701 (34%)]\tLoss: 0.359284\n",
      "Train Epoch: 5 [1920/3701 (52%)]\tLoss: 0.371815\n",
      "Train Epoch: 5 [2560/3701 (69%)]\tLoss: 0.275596\n",
      "Train Epoch: 5 [3200/3701 (86%)]\tLoss: 0.199664\n",
      "Train Epoch: 6 [0/3701 (0%)]\tLoss: 0.191184\n",
      "Train Epoch: 6 [640/3701 (17%)]\tLoss: 0.413930\n",
      "Train Epoch: 6 [1280/3701 (34%)]\tLoss: 0.486116\n",
      "Train Epoch: 6 [1920/3701 (52%)]\tLoss: 0.274386\n",
      "Train Epoch: 6 [2560/3701 (69%)]\tLoss: 0.288989\n",
      "Train Epoch: 6 [3200/3701 (86%)]\tLoss: 0.417873\n",
      "Train Epoch: 7 [0/3701 (0%)]\tLoss: 0.235895\n",
      "Train Epoch: 7 [640/3701 (17%)]\tLoss: 0.268005\n",
      "Train Epoch: 7 [1280/3701 (34%)]\tLoss: 0.340523\n",
      "Train Epoch: 7 [1920/3701 (52%)]\tLoss: 0.321557\n",
      "Train Epoch: 7 [2560/3701 (69%)]\tLoss: 0.228195\n",
      "Train Epoch: 7 [3200/3701 (86%)]\tLoss: 0.198904\n",
      "Train Epoch: 8 [0/3701 (0%)]\tLoss: 0.169416\n",
      "Train Epoch: 8 [640/3701 (17%)]\tLoss: 0.468498\n",
      "Train Epoch: 8 [1280/3701 (34%)]\tLoss: 0.302992\n",
      "Train Epoch: 8 [1920/3701 (52%)]\tLoss: 0.580420\n",
      "Train Epoch: 8 [2560/3701 (69%)]\tLoss: 0.320625\n",
      "Train Epoch: 8 [3200/3701 (86%)]\tLoss: 0.274638\n",
      "Train Epoch: 9 [0/3701 (0%)]\tLoss: 0.179056\n",
      "Train Epoch: 9 [640/3701 (17%)]\tLoss: 0.238119\n",
      "Train Epoch: 9 [1280/3701 (34%)]\tLoss: 0.256110\n",
      "Train Epoch: 9 [1920/3701 (52%)]\tLoss: 0.140790\n",
      "Train Epoch: 9 [2560/3701 (69%)]\tLoss: 0.314182\n",
      "Train Epoch: 9 [3200/3701 (86%)]\tLoss: 0.465834\n",
      "Train Epoch: 10 [0/3701 (0%)]\tLoss: 0.564849\n",
      "Train Epoch: 10 [640/3701 (17%)]\tLoss: 0.318996\n",
      "Train Epoch: 10 [1280/3701 (34%)]\tLoss: 0.328335\n",
      "Train Epoch: 10 [1920/3701 (52%)]\tLoss: 0.346197\n",
      "Train Epoch: 10 [2560/3701 (69%)]\tLoss: 0.272615\n",
      "Train Epoch: 10 [3200/3701 (86%)]\tLoss: 0.231335\n",
      "Train Epoch: 11 [0/3701 (0%)]\tLoss: 0.148243\n",
      "Train Epoch: 11 [640/3701 (17%)]\tLoss: 0.197678\n",
      "Train Epoch: 11 [1280/3701 (34%)]\tLoss: 0.266726\n",
      "Train Epoch: 11 [1920/3701 (52%)]\tLoss: 0.142893\n",
      "Train Epoch: 11 [2560/3701 (69%)]\tLoss: 0.364270\n",
      "Train Epoch: 11 [3200/3701 (86%)]\tLoss: 0.330308\n",
      "Train Epoch: 12 [0/3701 (0%)]\tLoss: 0.291296\n",
      "Train Epoch: 12 [640/3701 (17%)]\tLoss: 0.267196\n",
      "Train Epoch: 12 [1280/3701 (34%)]\tLoss: 0.307820\n",
      "Train Epoch: 12 [1920/3701 (52%)]\tLoss: 0.296767\n",
      "Train Epoch: 12 [2560/3701 (69%)]\tLoss: 0.536303\n",
      "Train Epoch: 12 [3200/3701 (86%)]\tLoss: 0.189270\n",
      "Train Epoch: 13 [0/3701 (0%)]\tLoss: 0.248052\n",
      "Train Epoch: 13 [640/3701 (17%)]\tLoss: 0.129172\n",
      "Train Epoch: 13 [1280/3701 (34%)]\tLoss: 0.131805\n",
      "Train Epoch: 13 [1920/3701 (52%)]\tLoss: 0.269638\n",
      "Train Epoch: 13 [2560/3701 (69%)]\tLoss: 0.100022\n",
      "Train Epoch: 13 [3200/3701 (86%)]\tLoss: 0.326538\n",
      "Train Epoch: 14 [0/3701 (0%)]\tLoss: 0.379149\n",
      "Train Epoch: 14 [640/3701 (17%)]\tLoss: 0.285540\n",
      "Train Epoch: 14 [1280/3701 (34%)]\tLoss: 0.134820\n",
      "Train Epoch: 14 [1920/3701 (52%)]\tLoss: 0.257965\n",
      "Train Epoch: 14 [2560/3701 (69%)]\tLoss: 0.149692\n",
      "Train Epoch: 14 [3200/3701 (86%)]\tLoss: 0.154256\n",
      "Train Epoch: 15 [0/3701 (0%)]\tLoss: 0.185427\n",
      "Train Epoch: 15 [640/3701 (17%)]\tLoss: 0.141653\n",
      "Train Epoch: 15 [1280/3701 (34%)]\tLoss: 0.260383\n",
      "Train Epoch: 15 [1920/3701 (52%)]\tLoss: 0.252906\n",
      "Train Epoch: 15 [2560/3701 (69%)]\tLoss: 0.288727\n",
      "Train Epoch: 15 [3200/3701 (86%)]\tLoss: 0.401060\n",
      "Train Epoch: 16 [0/3701 (0%)]\tLoss: 0.138432\n",
      "Train Epoch: 16 [640/3701 (17%)]\tLoss: 0.324990\n",
      "Train Epoch: 16 [1280/3701 (34%)]\tLoss: 0.173024\n",
      "Train Epoch: 16 [1920/3701 (52%)]\tLoss: 0.158296\n",
      "Train Epoch: 16 [2560/3701 (69%)]\tLoss: 0.334021\n",
      "Train Epoch: 16 [3200/3701 (86%)]\tLoss: 0.160434\n",
      "Train Epoch: 17 [0/3701 (0%)]\tLoss: 0.293402\n",
      "Train Epoch: 17 [640/3701 (17%)]\tLoss: 0.227428\n",
      "Train Epoch: 17 [1280/3701 (34%)]\tLoss: 0.256686\n",
      "Train Epoch: 17 [1920/3701 (52%)]\tLoss: 0.212298\n",
      "Train Epoch: 17 [2560/3701 (69%)]\tLoss: 0.120122\n",
      "Train Epoch: 17 [3200/3701 (86%)]\tLoss: 0.180191\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/464 (0%)]\tLoss: 1.409487\n",
      "Train Epoch: 2 [0/464 (0%)]\tLoss: 1.218587\n",
      "Train Epoch: 3 [0/464 (0%)]\tLoss: 0.857288\n",
      "Train Epoch: 4 [0/464 (0%)]\tLoss: 0.896425\n",
      "Train Epoch: 5 [0/464 (0%)]\tLoss: 1.202422\n",
      "Train Epoch: 6 [0/464 (0%)]\tLoss: 0.674050\n",
      "Train Epoch: 7 [0/464 (0%)]\tLoss: 0.744259\n",
      "Train Epoch: 8 [0/464 (0%)]\tLoss: 0.780423\n",
      "Train Epoch: 9 [0/464 (0%)]\tLoss: 1.065739\n",
      "Train Epoch: 10 [0/464 (0%)]\tLoss: 0.906574\n",
      "Train Epoch: 11 [0/464 (0%)]\tLoss: 0.620559\n",
      "Train Epoch: 12 [0/464 (0%)]\tLoss: 0.706338\n",
      "Train Epoch: 13 [0/464 (0%)]\tLoss: 0.890113\n",
      "Train Epoch: 14 [0/464 (0%)]\tLoss: 0.960628\n",
      "Train Epoch: 15 [0/464 (0%)]\tLoss: 0.741641\n",
      "Train Epoch: 16 [0/464 (0%)]\tLoss: 0.716623\n",
      "Train Epoch: 17 [0/464 (0%)]\tLoss: 0.690921\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/4580 (0%)]\tLoss: 1.294100\n",
      "Train Epoch: 1 [640/4580 (14%)]\tLoss: 0.956183\n",
      "Train Epoch: 1 [1280/4580 (28%)]\tLoss: 1.008029\n",
      "Train Epoch: 1 [1920/4580 (42%)]\tLoss: 0.883189\n",
      "Train Epoch: 1 [2560/4580 (56%)]\tLoss: 1.000219\n",
      "Train Epoch: 1 [3200/4580 (69%)]\tLoss: 0.936318\n",
      "Train Epoch: 1 [3840/4580 (83%)]\tLoss: 0.813566\n",
      "Train Epoch: 1 [4480/4580 (97%)]\tLoss: 0.882140\n",
      "Train Epoch: 2 [0/4580 (0%)]\tLoss: 0.827404\n",
      "Train Epoch: 2 [640/4580 (14%)]\tLoss: 0.849851\n",
      "Train Epoch: 2 [1280/4580 (28%)]\tLoss: 0.753178\n",
      "Train Epoch: 2 [1920/4580 (42%)]\tLoss: 0.689747\n",
      "Train Epoch: 2 [2560/4580 (56%)]\tLoss: 0.716071\n",
      "Train Epoch: 2 [3200/4580 (69%)]\tLoss: 0.739630\n",
      "Train Epoch: 2 [3840/4580 (83%)]\tLoss: 0.888242\n",
      "Train Epoch: 2 [4480/4580 (97%)]\tLoss: 0.782531\n",
      "Train Epoch: 3 [0/4580 (0%)]\tLoss: 1.079410\n",
      "Train Epoch: 3 [640/4580 (14%)]\tLoss: 0.760541\n",
      "Train Epoch: 3 [1280/4580 (28%)]\tLoss: 0.817742\n",
      "Train Epoch: 3 [1920/4580 (42%)]\tLoss: 0.677210\n",
      "Train Epoch: 3 [2560/4580 (56%)]\tLoss: 0.944990\n",
      "Train Epoch: 3 [3200/4580 (69%)]\tLoss: 1.019410\n",
      "Train Epoch: 3 [3840/4580 (83%)]\tLoss: 0.707355\n",
      "Train Epoch: 3 [4480/4580 (97%)]\tLoss: 0.545448\n",
      "Train Epoch: 4 [0/4580 (0%)]\tLoss: 0.756247\n",
      "Train Epoch: 4 [640/4580 (14%)]\tLoss: 0.705072\n",
      "Train Epoch: 4 [1280/4580 (28%)]\tLoss: 0.654567\n",
      "Train Epoch: 4 [1920/4580 (42%)]\tLoss: 0.781328\n",
      "Train Epoch: 4 [2560/4580 (56%)]\tLoss: 0.767069\n",
      "Train Epoch: 4 [3200/4580 (69%)]\tLoss: 0.546149\n",
      "Train Epoch: 4 [3840/4580 (83%)]\tLoss: 1.116066\n",
      "Train Epoch: 4 [4480/4580 (97%)]\tLoss: 0.781971\n",
      "Train Epoch: 5 [0/4580 (0%)]\tLoss: 0.656453\n",
      "Train Epoch: 5 [640/4580 (14%)]\tLoss: 0.994121\n",
      "Train Epoch: 5 [1280/4580 (28%)]\tLoss: 0.714426\n",
      "Train Epoch: 5 [1920/4580 (42%)]\tLoss: 0.646290\n",
      "Train Epoch: 5 [2560/4580 (56%)]\tLoss: 0.928202\n",
      "Train Epoch: 5 [3200/4580 (69%)]\tLoss: 0.635750\n",
      "Train Epoch: 5 [3840/4580 (83%)]\tLoss: 0.532914\n",
      "Train Epoch: 5 [4480/4580 (97%)]\tLoss: 0.680195\n",
      "Train Epoch: 6 [0/4580 (0%)]\tLoss: 0.920460\n",
      "Train Epoch: 6 [640/4580 (14%)]\tLoss: 0.749073\n",
      "Train Epoch: 6 [1280/4580 (28%)]\tLoss: 0.921255\n",
      "Train Epoch: 6 [1920/4580 (42%)]\tLoss: 0.503661\n",
      "Train Epoch: 6 [2560/4580 (56%)]\tLoss: 0.621314\n",
      "Train Epoch: 6 [3200/4580 (69%)]\tLoss: 0.620590\n",
      "Train Epoch: 6 [3840/4580 (83%)]\tLoss: 0.725602\n",
      "Train Epoch: 6 [4480/4580 (97%)]\tLoss: 0.777365\n",
      "Train Epoch: 7 [0/4580 (0%)]\tLoss: 0.633715\n",
      "Train Epoch: 7 [640/4580 (14%)]\tLoss: 0.647114\n",
      "Train Epoch: 7 [1280/4580 (28%)]\tLoss: 0.865306\n",
      "Train Epoch: 7 [1920/4580 (42%)]\tLoss: 0.752290\n",
      "Train Epoch: 7 [2560/4580 (56%)]\tLoss: 0.956165\n",
      "Train Epoch: 7 [3200/4580 (69%)]\tLoss: 0.657310\n",
      "Train Epoch: 7 [3840/4580 (83%)]\tLoss: 0.663103\n",
      "Train Epoch: 7 [4480/4580 (97%)]\tLoss: 1.021356\n",
      "Train Epoch: 8 [0/4580 (0%)]\tLoss: 0.699228\n",
      "Train Epoch: 8 [640/4580 (14%)]\tLoss: 0.755477\n",
      "Train Epoch: 8 [1280/4580 (28%)]\tLoss: 0.624568\n",
      "Train Epoch: 8 [1920/4580 (42%)]\tLoss: 0.734072\n",
      "Train Epoch: 8 [2560/4580 (56%)]\tLoss: 0.694771\n",
      "Train Epoch: 8 [3200/4580 (69%)]\tLoss: 0.689691\n",
      "Train Epoch: 8 [3840/4580 (83%)]\tLoss: 0.756641\n",
      "Train Epoch: 8 [4480/4580 (97%)]\tLoss: 0.699753\n",
      "Train Epoch: 9 [0/4580 (0%)]\tLoss: 0.768312\n",
      "Train Epoch: 9 [640/4580 (14%)]\tLoss: 1.050017\n",
      "Train Epoch: 9 [1280/4580 (28%)]\tLoss: 0.663161\n",
      "Train Epoch: 9 [1920/4580 (42%)]\tLoss: 0.781470\n",
      "Train Epoch: 9 [2560/4580 (56%)]\tLoss: 0.547643\n",
      "Train Epoch: 9 [3200/4580 (69%)]\tLoss: 0.760501\n",
      "Train Epoch: 9 [3840/4580 (83%)]\tLoss: 0.889956\n",
      "Train Epoch: 9 [4480/4580 (97%)]\tLoss: 0.862499\n",
      "Train Epoch: 10 [0/4580 (0%)]\tLoss: 0.716801\n",
      "Train Epoch: 10 [640/4580 (14%)]\tLoss: 0.749553\n",
      "Train Epoch: 10 [1280/4580 (28%)]\tLoss: 0.687569\n",
      "Train Epoch: 10 [1920/4580 (42%)]\tLoss: 0.626117\n",
      "Train Epoch: 10 [2560/4580 (56%)]\tLoss: 0.902759\n",
      "Train Epoch: 10 [3200/4580 (69%)]\tLoss: 0.798702\n",
      "Train Epoch: 10 [3840/4580 (83%)]\tLoss: 0.786305\n",
      "Train Epoch: 10 [4480/4580 (97%)]\tLoss: 0.516981\n",
      "Train Epoch: 11 [0/4580 (0%)]\tLoss: 0.648347\n",
      "Train Epoch: 11 [640/4580 (14%)]\tLoss: 0.595748\n",
      "Train Epoch: 11 [1280/4580 (28%)]\tLoss: 0.648626\n",
      "Train Epoch: 11 [1920/4580 (42%)]\tLoss: 0.826289\n",
      "Train Epoch: 11 [2560/4580 (56%)]\tLoss: 0.830143\n",
      "Train Epoch: 11 [3200/4580 (69%)]\tLoss: 0.680974\n",
      "Train Epoch: 11 [3840/4580 (83%)]\tLoss: 0.973278\n",
      "Train Epoch: 11 [4480/4580 (97%)]\tLoss: 0.590121\n",
      "Train Epoch: 12 [0/4580 (0%)]\tLoss: 0.682252\n",
      "Train Epoch: 12 [640/4580 (14%)]\tLoss: 0.560300\n",
      "Train Epoch: 12 [1280/4580 (28%)]\tLoss: 0.920345\n",
      "Train Epoch: 12 [1920/4580 (42%)]\tLoss: 0.941722\n",
      "Train Epoch: 12 [2560/4580 (56%)]\tLoss: 0.911721\n",
      "Train Epoch: 12 [3200/4580 (69%)]\tLoss: 0.739875\n",
      "Train Epoch: 12 [3840/4580 (83%)]\tLoss: 0.840402\n",
      "Train Epoch: 12 [4480/4580 (97%)]\tLoss: 0.574307\n",
      "Train Epoch: 13 [0/4580 (0%)]\tLoss: 0.586414\n",
      "Train Epoch: 13 [640/4580 (14%)]\tLoss: 0.495350\n",
      "Train Epoch: 13 [1280/4580 (28%)]\tLoss: 0.817394\n",
      "Train Epoch: 13 [1920/4580 (42%)]\tLoss: 0.761925\n",
      "Train Epoch: 13 [2560/4580 (56%)]\tLoss: 0.710582\n",
      "Train Epoch: 13 [3200/4580 (69%)]\tLoss: 0.779042\n",
      "Train Epoch: 13 [3840/4580 (83%)]\tLoss: 0.505592\n",
      "Train Epoch: 13 [4480/4580 (97%)]\tLoss: 0.729483\n",
      "Train Epoch: 14 [0/4580 (0%)]\tLoss: 0.597402\n",
      "Train Epoch: 14 [640/4580 (14%)]\tLoss: 0.573105\n",
      "Train Epoch: 14 [1280/4580 (28%)]\tLoss: 0.669008\n",
      "Train Epoch: 14 [1920/4580 (42%)]\tLoss: 0.488421\n",
      "Train Epoch: 14 [2560/4580 (56%)]\tLoss: 0.638099\n",
      "Train Epoch: 14 [3200/4580 (69%)]\tLoss: 0.852710\n",
      "Train Epoch: 14 [3840/4580 (83%)]\tLoss: 0.836904\n",
      "Train Epoch: 14 [4480/4580 (97%)]\tLoss: 0.866277\n",
      "Train Epoch: 15 [0/4580 (0%)]\tLoss: 0.879584\n",
      "Train Epoch: 15 [640/4580 (14%)]\tLoss: 0.738862\n",
      "Train Epoch: 15 [1280/4580 (28%)]\tLoss: 0.792358\n",
      "Train Epoch: 15 [1920/4580 (42%)]\tLoss: 0.559029\n",
      "Train Epoch: 15 [2560/4580 (56%)]\tLoss: 0.755116\n",
      "Train Epoch: 15 [3200/4580 (69%)]\tLoss: 0.748648\n",
      "Train Epoch: 15 [3840/4580 (83%)]\tLoss: 0.810335\n",
      "Train Epoch: 15 [4480/4580 (97%)]\tLoss: 0.668247\n",
      "Train Epoch: 16 [0/4580 (0%)]\tLoss: 0.787755\n",
      "Train Epoch: 16 [640/4580 (14%)]\tLoss: 0.986270\n",
      "Train Epoch: 16 [1280/4580 (28%)]\tLoss: 1.005974\n",
      "Train Epoch: 16 [1920/4580 (42%)]\tLoss: 0.688607\n",
      "Train Epoch: 16 [2560/4580 (56%)]\tLoss: 0.679762\n",
      "Train Epoch: 16 [3200/4580 (69%)]\tLoss: 0.578132\n",
      "Train Epoch: 16 [3840/4580 (83%)]\tLoss: 0.433925\n",
      "Train Epoch: 16 [4480/4580 (97%)]\tLoss: 0.819380\n",
      "Train Epoch: 17 [0/4580 (0%)]\tLoss: 0.809562\n",
      "Train Epoch: 17 [640/4580 (14%)]\tLoss: 0.845345\n",
      "Train Epoch: 17 [1280/4580 (28%)]\tLoss: 0.698570\n",
      "Train Epoch: 17 [1920/4580 (42%)]\tLoss: 0.663712\n",
      "Train Epoch: 17 [2560/4580 (56%)]\tLoss: 0.509805\n",
      "Train Epoch: 17 [3200/4580 (69%)]\tLoss: 0.633384\n",
      "Train Epoch: 17 [3840/4580 (83%)]\tLoss: 0.714028\n",
      "Train Epoch: 17 [4480/4580 (97%)]\tLoss: 0.781558\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3239 (0%)]\tLoss: 1.167542\n",
      "Train Epoch: 1 [640/3239 (20%)]\tLoss: 0.350260\n",
      "Train Epoch: 1 [1280/3239 (39%)]\tLoss: 0.192613\n",
      "Train Epoch: 1 [1920/3239 (59%)]\tLoss: 0.354291\n",
      "Train Epoch: 1 [2560/3239 (78%)]\tLoss: 0.263319\n",
      "Train Epoch: 1 [1950/3239 (98%)]\tLoss: 0.159044\n",
      "Train Epoch: 2 [0/3239 (0%)]\tLoss: 0.263386\n",
      "Train Epoch: 2 [640/3239 (20%)]\tLoss: 0.290137\n",
      "Train Epoch: 2 [1280/3239 (39%)]\tLoss: 0.304815\n",
      "Train Epoch: 2 [1920/3239 (59%)]\tLoss: 0.225071\n",
      "Train Epoch: 2 [2560/3239 (78%)]\tLoss: 0.324438\n",
      "Train Epoch: 2 [1950/3239 (98%)]\tLoss: 0.119108\n",
      "Train Epoch: 3 [0/3239 (0%)]\tLoss: 0.457981\n",
      "Train Epoch: 3 [640/3239 (20%)]\tLoss: 0.348066\n",
      "Train Epoch: 3 [1280/3239 (39%)]\tLoss: 0.333683\n",
      "Train Epoch: 3 [1920/3239 (59%)]\tLoss: 0.237254\n",
      "Train Epoch: 3 [2560/3239 (78%)]\tLoss: 0.193534\n",
      "Train Epoch: 3 [1950/3239 (98%)]\tLoss: 0.469056\n",
      "Train Epoch: 4 [0/3239 (0%)]\tLoss: 0.252380\n",
      "Train Epoch: 4 [640/3239 (20%)]\tLoss: 0.437687\n",
      "Train Epoch: 4 [1280/3239 (39%)]\tLoss: 0.253165\n",
      "Train Epoch: 4 [1920/3239 (59%)]\tLoss: 0.122317\n",
      "Train Epoch: 4 [2560/3239 (78%)]\tLoss: 0.219910\n",
      "Train Epoch: 4 [1950/3239 (98%)]\tLoss: 0.453170\n",
      "Train Epoch: 5 [0/3239 (0%)]\tLoss: 0.439210\n",
      "Train Epoch: 5 [640/3239 (20%)]\tLoss: 0.172525\n",
      "Train Epoch: 5 [1280/3239 (39%)]\tLoss: 0.631209\n",
      "Train Epoch: 5 [1920/3239 (59%)]\tLoss: 0.092033\n",
      "Train Epoch: 5 [2560/3239 (78%)]\tLoss: 0.232068\n",
      "Train Epoch: 5 [1950/3239 (98%)]\tLoss: 0.221616\n",
      "Train Epoch: 6 [0/3239 (0%)]\tLoss: 0.373067\n",
      "Train Epoch: 6 [640/3239 (20%)]\tLoss: 0.239131\n",
      "Train Epoch: 6 [1280/3239 (39%)]\tLoss: 0.276938\n",
      "Train Epoch: 6 [1920/3239 (59%)]\tLoss: 0.307734\n",
      "Train Epoch: 6 [2560/3239 (78%)]\tLoss: 0.120329\n",
      "Train Epoch: 6 [1950/3239 (98%)]\tLoss: 0.267196\n",
      "Train Epoch: 7 [0/3239 (0%)]\tLoss: 0.185529\n",
      "Train Epoch: 7 [640/3239 (20%)]\tLoss: 0.252766\n",
      "Train Epoch: 7 [1280/3239 (39%)]\tLoss: 0.264926\n",
      "Train Epoch: 7 [1920/3239 (59%)]\tLoss: 0.381069\n",
      "Train Epoch: 7 [2560/3239 (78%)]\tLoss: 0.139493\n",
      "Train Epoch: 7 [1950/3239 (98%)]\tLoss: 0.096434\n",
      "Train Epoch: 8 [0/3239 (0%)]\tLoss: 0.114761\n",
      "Train Epoch: 8 [640/3239 (20%)]\tLoss: 0.481172\n",
      "Train Epoch: 8 [1280/3239 (39%)]\tLoss: 0.353596\n",
      "Train Epoch: 8 [1920/3239 (59%)]\tLoss: 0.056717\n",
      "Train Epoch: 8 [2560/3239 (78%)]\tLoss: 0.333182\n",
      "Train Epoch: 8 [1950/3239 (98%)]\tLoss: 0.083413\n",
      "Train Epoch: 9 [0/3239 (0%)]\tLoss: 0.352771\n",
      "Train Epoch: 9 [640/3239 (20%)]\tLoss: 0.417666\n",
      "Train Epoch: 9 [1280/3239 (39%)]\tLoss: 0.062636\n",
      "Train Epoch: 9 [1920/3239 (59%)]\tLoss: 0.187896\n",
      "Train Epoch: 9 [2560/3239 (78%)]\tLoss: 0.179948\n",
      "Train Epoch: 9 [1950/3239 (98%)]\tLoss: 0.327163\n",
      "Train Epoch: 10 [0/3239 (0%)]\tLoss: 0.159603\n",
      "Train Epoch: 10 [640/3239 (20%)]\tLoss: 0.158320\n",
      "Train Epoch: 10 [1280/3239 (39%)]\tLoss: 0.241372\n",
      "Train Epoch: 10 [1920/3239 (59%)]\tLoss: 0.110535\n",
      "Train Epoch: 10 [2560/3239 (78%)]\tLoss: 0.321933\n",
      "Train Epoch: 10 [1950/3239 (98%)]\tLoss: 0.357214\n",
      "Train Epoch: 11 [0/3239 (0%)]\tLoss: 0.321579\n",
      "Train Epoch: 11 [640/3239 (20%)]\tLoss: 0.156677\n",
      "Train Epoch: 11 [1280/3239 (39%)]\tLoss: 0.202374\n",
      "Train Epoch: 11 [1920/3239 (59%)]\tLoss: 0.254828\n",
      "Train Epoch: 11 [2560/3239 (78%)]\tLoss: 0.219540\n",
      "Train Epoch: 11 [1950/3239 (98%)]\tLoss: 0.145829\n",
      "Train Epoch: 12 [0/3239 (0%)]\tLoss: 0.270224\n",
      "Train Epoch: 12 [640/3239 (20%)]\tLoss: 0.217454\n",
      "Train Epoch: 12 [1280/3239 (39%)]\tLoss: 0.554862\n",
      "Train Epoch: 12 [1920/3239 (59%)]\tLoss: 0.556898\n",
      "Train Epoch: 12 [2560/3239 (78%)]\tLoss: 0.171338\n",
      "Train Epoch: 12 [1950/3239 (98%)]\tLoss: 0.106895\n",
      "Train Epoch: 13 [0/3239 (0%)]\tLoss: 0.112096\n",
      "Train Epoch: 13 [640/3239 (20%)]\tLoss: 0.174142\n",
      "Train Epoch: 13 [1280/3239 (39%)]\tLoss: 0.259537\n",
      "Train Epoch: 13 [1920/3239 (59%)]\tLoss: 0.171266\n",
      "Train Epoch: 13 [2560/3239 (78%)]\tLoss: 0.182592\n",
      "Train Epoch: 13 [1950/3239 (98%)]\tLoss: 0.118032\n",
      "Train Epoch: 14 [0/3239 (0%)]\tLoss: 0.389247\n",
      "Train Epoch: 14 [640/3239 (20%)]\tLoss: 0.197004\n",
      "Train Epoch: 14 [1280/3239 (39%)]\tLoss: 0.150109\n",
      "Train Epoch: 14 [1920/3239 (59%)]\tLoss: 0.258171\n",
      "Train Epoch: 14 [2560/3239 (78%)]\tLoss: 0.229391\n",
      "Train Epoch: 14 [1950/3239 (98%)]\tLoss: 0.253872\n",
      "Train Epoch: 15 [0/3239 (0%)]\tLoss: 0.209487\n",
      "Train Epoch: 15 [640/3239 (20%)]\tLoss: 0.178040\n",
      "Train Epoch: 15 [1280/3239 (39%)]\tLoss: 0.381125\n",
      "Train Epoch: 15 [1920/3239 (59%)]\tLoss: 0.275622\n",
      "Train Epoch: 15 [2560/3239 (78%)]\tLoss: 0.093425\n",
      "Train Epoch: 15 [1950/3239 (98%)]\tLoss: 0.444531\n",
      "Train Epoch: 16 [0/3239 (0%)]\tLoss: 0.308299\n",
      "Train Epoch: 16 [640/3239 (20%)]\tLoss: 0.390060\n",
      "Train Epoch: 16 [1280/3239 (39%)]\tLoss: 0.189224\n",
      "Train Epoch: 16 [1920/3239 (59%)]\tLoss: 0.371326\n",
      "Train Epoch: 16 [2560/3239 (78%)]\tLoss: 0.201592\n",
      "Train Epoch: 16 [1950/3239 (98%)]\tLoss: 0.487981\n",
      "Train Epoch: 17 [0/3239 (0%)]\tLoss: 0.142695\n",
      "Train Epoch: 17 [640/3239 (20%)]\tLoss: 0.261062\n",
      "Train Epoch: 17 [1280/3239 (39%)]\tLoss: 0.207925\n",
      "Train Epoch: 17 [1920/3239 (59%)]\tLoss: 0.171114\n",
      "Train Epoch: 17 [2560/3239 (78%)]\tLoss: 0.236007\n",
      "Train Epoch: 17 [1950/3239 (98%)]\tLoss: 0.471591\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/9051 (0%)]\tLoss: 1.562686\n",
      "Train Epoch: 1 [640/9051 (7%)]\tLoss: 0.862052\n",
      "Train Epoch: 1 [1280/9051 (14%)]\tLoss: 0.919087\n",
      "Train Epoch: 1 [1920/9051 (21%)]\tLoss: 0.981026\n",
      "Train Epoch: 1 [2560/9051 (28%)]\tLoss: 0.777902\n",
      "Train Epoch: 1 [3200/9051 (35%)]\tLoss: 0.970314\n",
      "Train Epoch: 1 [3840/9051 (42%)]\tLoss: 1.100132\n",
      "Train Epoch: 1 [4480/9051 (49%)]\tLoss: 0.697838\n",
      "Train Epoch: 1 [5120/9051 (56%)]\tLoss: 0.644234\n",
      "Train Epoch: 1 [5760/9051 (63%)]\tLoss: 1.101006\n",
      "Train Epoch: 1 [6400/9051 (70%)]\tLoss: 0.719576\n",
      "Train Epoch: 1 [7040/9051 (77%)]\tLoss: 0.896030\n",
      "Train Epoch: 1 [7680/9051 (85%)]\tLoss: 0.691309\n",
      "Train Epoch: 1 [8320/9051 (92%)]\tLoss: 0.899619\n",
      "Train Epoch: 1 [8960/9051 (99%)]\tLoss: 0.871937\n",
      "Train Epoch: 2 [0/9051 (0%)]\tLoss: 0.802897\n",
      "Train Epoch: 2 [640/9051 (7%)]\tLoss: 0.656273\n",
      "Train Epoch: 2 [1280/9051 (14%)]\tLoss: 0.741913\n",
      "Train Epoch: 2 [1920/9051 (21%)]\tLoss: 0.698866\n",
      "Train Epoch: 2 [2560/9051 (28%)]\tLoss: 0.742750\n",
      "Train Epoch: 2 [3200/9051 (35%)]\tLoss: 1.053029\n",
      "Train Epoch: 2 [3840/9051 (42%)]\tLoss: 0.780459\n",
      "Train Epoch: 2 [4480/9051 (49%)]\tLoss: 0.662061\n",
      "Train Epoch: 2 [5120/9051 (56%)]\tLoss: 0.823087\n",
      "Train Epoch: 2 [5760/9051 (63%)]\tLoss: 0.733152\n",
      "Train Epoch: 2 [6400/9051 (70%)]\tLoss: 0.680547\n",
      "Train Epoch: 2 [7040/9051 (77%)]\tLoss: 0.851387\n",
      "Train Epoch: 2 [7680/9051 (85%)]\tLoss: 0.822287\n",
      "Train Epoch: 2 [8320/9051 (92%)]\tLoss: 0.665774\n",
      "Train Epoch: 2 [8960/9051 (99%)]\tLoss: 0.778150\n",
      "Train Epoch: 3 [0/9051 (0%)]\tLoss: 0.665197\n",
      "Train Epoch: 3 [640/9051 (7%)]\tLoss: 0.823379\n",
      "Train Epoch: 3 [1280/9051 (14%)]\tLoss: 0.586030\n",
      "Train Epoch: 3 [1920/9051 (21%)]\tLoss: 0.894718\n",
      "Train Epoch: 3 [2560/9051 (28%)]\tLoss: 0.685269\n",
      "Train Epoch: 3 [3200/9051 (35%)]\tLoss: 0.878095\n",
      "Train Epoch: 3 [3840/9051 (42%)]\tLoss: 0.752247\n",
      "Train Epoch: 3 [4480/9051 (49%)]\tLoss: 0.617231\n",
      "Train Epoch: 3 [5120/9051 (56%)]\tLoss: 0.756672\n",
      "Train Epoch: 3 [5760/9051 (63%)]\tLoss: 0.780551\n",
      "Train Epoch: 3 [6400/9051 (70%)]\tLoss: 0.872922\n",
      "Train Epoch: 3 [7040/9051 (77%)]\tLoss: 0.908873\n",
      "Train Epoch: 3 [7680/9051 (85%)]\tLoss: 0.661294\n",
      "Train Epoch: 3 [8320/9051 (92%)]\tLoss: 0.742335\n",
      "Train Epoch: 3 [8960/9051 (99%)]\tLoss: 0.795111\n",
      "Train Epoch: 4 [0/9051 (0%)]\tLoss: 0.842308\n",
      "Train Epoch: 4 [640/9051 (7%)]\tLoss: 1.053149\n",
      "Train Epoch: 4 [1280/9051 (14%)]\tLoss: 0.803292\n",
      "Train Epoch: 4 [1920/9051 (21%)]\tLoss: 0.903519\n",
      "Train Epoch: 4 [2560/9051 (28%)]\tLoss: 0.798558\n",
      "Train Epoch: 4 [3200/9051 (35%)]\tLoss: 0.940104\n",
      "Train Epoch: 4 [3840/9051 (42%)]\tLoss: 0.789463\n",
      "Train Epoch: 4 [4480/9051 (49%)]\tLoss: 0.516264\n",
      "Train Epoch: 4 [5120/9051 (56%)]\tLoss: 0.654669\n",
      "Train Epoch: 4 [5760/9051 (63%)]\tLoss: 0.848824\n",
      "Train Epoch: 4 [6400/9051 (70%)]\tLoss: 0.869840\n",
      "Train Epoch: 4 [7040/9051 (77%)]\tLoss: 0.702881\n",
      "Train Epoch: 4 [7680/9051 (85%)]\tLoss: 0.640914\n",
      "Train Epoch: 4 [8320/9051 (92%)]\tLoss: 0.483620\n",
      "Train Epoch: 4 [8960/9051 (99%)]\tLoss: 0.638751\n",
      "Train Epoch: 5 [0/9051 (0%)]\tLoss: 0.706546\n",
      "Train Epoch: 5 [640/9051 (7%)]\tLoss: 0.749707\n",
      "Train Epoch: 5 [1280/9051 (14%)]\tLoss: 0.704255\n",
      "Train Epoch: 5 [1920/9051 (21%)]\tLoss: 0.667299\n",
      "Train Epoch: 5 [2560/9051 (28%)]\tLoss: 0.728768\n",
      "Train Epoch: 5 [3200/9051 (35%)]\tLoss: 0.755598\n",
      "Train Epoch: 5 [3840/9051 (42%)]\tLoss: 0.758644\n",
      "Train Epoch: 5 [4480/9051 (49%)]\tLoss: 0.691810\n",
      "Train Epoch: 5 [5120/9051 (56%)]\tLoss: 0.939605\n",
      "Train Epoch: 5 [5760/9051 (63%)]\tLoss: 0.760580\n",
      "Train Epoch: 5 [6400/9051 (70%)]\tLoss: 0.746665\n",
      "Train Epoch: 5 [7040/9051 (77%)]\tLoss: 0.633764\n",
      "Train Epoch: 5 [7680/9051 (85%)]\tLoss: 0.852882\n",
      "Train Epoch: 5 [8320/9051 (92%)]\tLoss: 0.566237\n",
      "Train Epoch: 5 [8960/9051 (99%)]\tLoss: 0.581495\n",
      "Train Epoch: 6 [0/9051 (0%)]\tLoss: 0.468906\n",
      "Train Epoch: 6 [640/9051 (7%)]\tLoss: 0.705670\n",
      "Train Epoch: 6 [1280/9051 (14%)]\tLoss: 0.764304\n",
      "Train Epoch: 6 [1920/9051 (21%)]\tLoss: 0.536694\n",
      "Train Epoch: 6 [2560/9051 (28%)]\tLoss: 0.783715\n",
      "Train Epoch: 6 [3200/9051 (35%)]\tLoss: 1.007836\n",
      "Train Epoch: 6 [3840/9051 (42%)]\tLoss: 0.393357\n",
      "Train Epoch: 6 [4480/9051 (49%)]\tLoss: 0.733698\n",
      "Train Epoch: 6 [5120/9051 (56%)]\tLoss: 0.777192\n",
      "Train Epoch: 6 [5760/9051 (63%)]\tLoss: 0.714012\n",
      "Train Epoch: 6 [6400/9051 (70%)]\tLoss: 0.971937\n",
      "Train Epoch: 6 [7040/9051 (77%)]\tLoss: 0.744853\n",
      "Train Epoch: 6 [7680/9051 (85%)]\tLoss: 0.833650\n",
      "Train Epoch: 6 [8320/9051 (92%)]\tLoss: 0.827038\n",
      "Train Epoch: 6 [8960/9051 (99%)]\tLoss: 0.788052\n",
      "Train Epoch: 7 [0/9051 (0%)]\tLoss: 0.494318\n",
      "Train Epoch: 7 [640/9051 (7%)]\tLoss: 0.673281\n",
      "Train Epoch: 7 [1280/9051 (14%)]\tLoss: 0.478811\n",
      "Train Epoch: 7 [1920/9051 (21%)]\tLoss: 0.790798\n",
      "Train Epoch: 7 [2560/9051 (28%)]\tLoss: 0.695480\n",
      "Train Epoch: 7 [3200/9051 (35%)]\tLoss: 1.186230\n",
      "Train Epoch: 7 [3840/9051 (42%)]\tLoss: 0.658875\n",
      "Train Epoch: 7 [4480/9051 (49%)]\tLoss: 0.653819\n",
      "Train Epoch: 7 [5120/9051 (56%)]\tLoss: 0.656087\n",
      "Train Epoch: 7 [5760/9051 (63%)]\tLoss: 0.521549\n",
      "Train Epoch: 7 [6400/9051 (70%)]\tLoss: 0.607824\n",
      "Train Epoch: 7 [7040/9051 (77%)]\tLoss: 0.649270\n",
      "Train Epoch: 7 [7680/9051 (85%)]\tLoss: 0.584484\n",
      "Train Epoch: 7 [8320/9051 (92%)]\tLoss: 0.649782\n",
      "Train Epoch: 7 [8960/9051 (99%)]\tLoss: 0.753674\n",
      "Train Epoch: 8 [0/9051 (0%)]\tLoss: 0.742689\n",
      "Train Epoch: 8 [640/9051 (7%)]\tLoss: 0.744676\n",
      "Train Epoch: 8 [1280/9051 (14%)]\tLoss: 0.802991\n",
      "Train Epoch: 8 [1920/9051 (21%)]\tLoss: 0.874280\n",
      "Train Epoch: 8 [2560/9051 (28%)]\tLoss: 0.598623\n",
      "Train Epoch: 8 [3200/9051 (35%)]\tLoss: 0.862409\n",
      "Train Epoch: 8 [3840/9051 (42%)]\tLoss: 0.603909\n",
      "Train Epoch: 8 [4480/9051 (49%)]\tLoss: 0.464919\n",
      "Train Epoch: 8 [5120/9051 (56%)]\tLoss: 0.707234\n",
      "Train Epoch: 8 [5760/9051 (63%)]\tLoss: 0.572358\n",
      "Train Epoch: 8 [6400/9051 (70%)]\tLoss: 0.710573\n",
      "Train Epoch: 8 [7040/9051 (77%)]\tLoss: 0.927637\n",
      "Train Epoch: 8 [7680/9051 (85%)]\tLoss: 0.610432\n",
      "Train Epoch: 8 [8320/9051 (92%)]\tLoss: 0.496870\n",
      "Train Epoch: 8 [8960/9051 (99%)]\tLoss: 0.774666\n",
      "Train Epoch: 9 [0/9051 (0%)]\tLoss: 0.566735\n",
      "Train Epoch: 9 [640/9051 (7%)]\tLoss: 0.788822\n",
      "Train Epoch: 9 [1280/9051 (14%)]\tLoss: 0.624176\n",
      "Train Epoch: 9 [1920/9051 (21%)]\tLoss: 0.920226\n",
      "Train Epoch: 9 [2560/9051 (28%)]\tLoss: 0.648517\n",
      "Train Epoch: 9 [3200/9051 (35%)]\tLoss: 0.671004\n",
      "Train Epoch: 9 [3840/9051 (42%)]\tLoss: 0.571575\n",
      "Train Epoch: 9 [4480/9051 (49%)]\tLoss: 0.710485\n",
      "Train Epoch: 9 [5120/9051 (56%)]\tLoss: 0.631342\n",
      "Train Epoch: 9 [5760/9051 (63%)]\tLoss: 0.691675\n",
      "Train Epoch: 9 [6400/9051 (70%)]\tLoss: 0.606923\n",
      "Train Epoch: 9 [7040/9051 (77%)]\tLoss: 0.654841\n",
      "Train Epoch: 9 [7680/9051 (85%)]\tLoss: 0.733359\n",
      "Train Epoch: 9 [8320/9051 (92%)]\tLoss: 0.714261\n",
      "Train Epoch: 9 [8960/9051 (99%)]\tLoss: 0.697839\n",
      "Train Epoch: 10 [0/9051 (0%)]\tLoss: 0.948480\n",
      "Train Epoch: 10 [640/9051 (7%)]\tLoss: 0.662324\n",
      "Train Epoch: 10 [1280/9051 (14%)]\tLoss: 0.671111\n",
      "Train Epoch: 10 [1920/9051 (21%)]\tLoss: 0.640428\n",
      "Train Epoch: 10 [2560/9051 (28%)]\tLoss: 0.740148\n",
      "Train Epoch: 10 [3200/9051 (35%)]\tLoss: 0.829361\n",
      "Train Epoch: 10 [3840/9051 (42%)]\tLoss: 0.774698\n",
      "Train Epoch: 10 [4480/9051 (49%)]\tLoss: 0.552617\n",
      "Train Epoch: 10 [5120/9051 (56%)]\tLoss: 0.665427\n",
      "Train Epoch: 10 [5760/9051 (63%)]\tLoss: 0.481580\n",
      "Train Epoch: 10 [6400/9051 (70%)]\tLoss: 0.928183\n",
      "Train Epoch: 10 [7040/9051 (77%)]\tLoss: 0.460878\n",
      "Train Epoch: 10 [7680/9051 (85%)]\tLoss: 0.652192\n",
      "Train Epoch: 10 [8320/9051 (92%)]\tLoss: 0.851052\n",
      "Train Epoch: 10 [8960/9051 (99%)]\tLoss: 0.846493\n",
      "Train Epoch: 11 [0/9051 (0%)]\tLoss: 0.565260\n",
      "Train Epoch: 11 [640/9051 (7%)]\tLoss: 0.536613\n",
      "Train Epoch: 11 [1280/9051 (14%)]\tLoss: 0.695193\n",
      "Train Epoch: 11 [1920/9051 (21%)]\tLoss: 0.541403\n",
      "Train Epoch: 11 [2560/9051 (28%)]\tLoss: 0.920115\n",
      "Train Epoch: 11 [3200/9051 (35%)]\tLoss: 0.828014\n",
      "Train Epoch: 11 [3840/9051 (42%)]\tLoss: 0.670557\n",
      "Train Epoch: 11 [4480/9051 (49%)]\tLoss: 0.544185\n",
      "Train Epoch: 11 [5120/9051 (56%)]\tLoss: 0.615409\n",
      "Train Epoch: 11 [5760/9051 (63%)]\tLoss: 0.562614\n",
      "Train Epoch: 11 [6400/9051 (70%)]\tLoss: 0.794801\n",
      "Train Epoch: 11 [7040/9051 (77%)]\tLoss: 0.683330\n",
      "Train Epoch: 11 [7680/9051 (85%)]\tLoss: 0.761000\n",
      "Train Epoch: 11 [8320/9051 (92%)]\tLoss: 0.589892\n",
      "Train Epoch: 11 [8960/9051 (99%)]\tLoss: 0.766331\n",
      "Train Epoch: 12 [0/9051 (0%)]\tLoss: 0.684793\n",
      "Train Epoch: 12 [640/9051 (7%)]\tLoss: 0.717716\n",
      "Train Epoch: 12 [1280/9051 (14%)]\tLoss: 0.802495\n",
      "Train Epoch: 12 [1920/9051 (21%)]\tLoss: 0.507799\n",
      "Train Epoch: 12 [2560/9051 (28%)]\tLoss: 0.532410\n",
      "Train Epoch: 12 [3200/9051 (35%)]\tLoss: 0.499682\n",
      "Train Epoch: 12 [3840/9051 (42%)]\tLoss: 0.598911\n",
      "Train Epoch: 12 [4480/9051 (49%)]\tLoss: 0.466629\n",
      "Train Epoch: 12 [5120/9051 (56%)]\tLoss: 0.670160\n",
      "Train Epoch: 12 [5760/9051 (63%)]\tLoss: 0.794934\n",
      "Train Epoch: 12 [6400/9051 (70%)]\tLoss: 0.550585\n",
      "Train Epoch: 12 [7040/9051 (77%)]\tLoss: 0.597602\n",
      "Train Epoch: 12 [7680/9051 (85%)]\tLoss: 0.857090\n",
      "Train Epoch: 12 [8320/9051 (92%)]\tLoss: 0.917034\n",
      "Train Epoch: 12 [8960/9051 (99%)]\tLoss: 0.840194\n",
      "Train Epoch: 13 [0/9051 (0%)]\tLoss: 0.771580\n",
      "Train Epoch: 13 [640/9051 (7%)]\tLoss: 0.731778\n",
      "Train Epoch: 13 [1280/9051 (14%)]\tLoss: 0.573523\n",
      "Train Epoch: 13 [1920/9051 (21%)]\tLoss: 0.699828\n",
      "Train Epoch: 13 [2560/9051 (28%)]\tLoss: 0.554536\n",
      "Train Epoch: 13 [3200/9051 (35%)]\tLoss: 0.587018\n",
      "Train Epoch: 13 [3840/9051 (42%)]\tLoss: 0.502711\n",
      "Train Epoch: 13 [4480/9051 (49%)]\tLoss: 0.656830\n",
      "Train Epoch: 13 [5120/9051 (56%)]\tLoss: 0.720494\n",
      "Train Epoch: 13 [5760/9051 (63%)]\tLoss: 0.577052\n",
      "Train Epoch: 13 [6400/9051 (70%)]\tLoss: 0.719028\n",
      "Train Epoch: 13 [7040/9051 (77%)]\tLoss: 0.624131\n",
      "Train Epoch: 13 [7680/9051 (85%)]\tLoss: 0.927780\n",
      "Train Epoch: 13 [8320/9051 (92%)]\tLoss: 0.559846\n",
      "Train Epoch: 13 [8960/9051 (99%)]\tLoss: 0.729128\n",
      "Train Epoch: 14 [0/9051 (0%)]\tLoss: 0.471358\n",
      "Train Epoch: 14 [640/9051 (7%)]\tLoss: 0.694844\n",
      "Train Epoch: 14 [1280/9051 (14%)]\tLoss: 0.655550\n",
      "Train Epoch: 14 [1920/9051 (21%)]\tLoss: 0.572253\n",
      "Train Epoch: 14 [2560/9051 (28%)]\tLoss: 0.913310\n",
      "Train Epoch: 14 [3200/9051 (35%)]\tLoss: 0.557488\n",
      "Train Epoch: 14 [3840/9051 (42%)]\tLoss: 0.597674\n",
      "Train Epoch: 14 [4480/9051 (49%)]\tLoss: 0.758773\n",
      "Train Epoch: 14 [5120/9051 (56%)]\tLoss: 0.676955\n",
      "Train Epoch: 14 [5760/9051 (63%)]\tLoss: 0.787901\n",
      "Train Epoch: 14 [6400/9051 (70%)]\tLoss: 0.563620\n",
      "Train Epoch: 14 [7040/9051 (77%)]\tLoss: 0.844222\n",
      "Train Epoch: 14 [7680/9051 (85%)]\tLoss: 0.642290\n",
      "Train Epoch: 14 [8320/9051 (92%)]\tLoss: 0.517704\n",
      "Train Epoch: 14 [8960/9051 (99%)]\tLoss: 0.509559\n",
      "Train Epoch: 15 [0/9051 (0%)]\tLoss: 0.637862\n",
      "Train Epoch: 15 [640/9051 (7%)]\tLoss: 0.767376\n",
      "Train Epoch: 15 [1280/9051 (14%)]\tLoss: 0.661872\n",
      "Train Epoch: 15 [1920/9051 (21%)]\tLoss: 0.636653\n",
      "Train Epoch: 15 [2560/9051 (28%)]\tLoss: 0.557464\n",
      "Train Epoch: 15 [3200/9051 (35%)]\tLoss: 0.765070\n",
      "Train Epoch: 15 [3840/9051 (42%)]\tLoss: 0.599021\n",
      "Train Epoch: 15 [4480/9051 (49%)]\tLoss: 0.752109\n",
      "Train Epoch: 15 [5120/9051 (56%)]\tLoss: 0.513144\n",
      "Train Epoch: 15 [5760/9051 (63%)]\tLoss: 0.749094\n",
      "Train Epoch: 15 [6400/9051 (70%)]\tLoss: 1.009742\n",
      "Train Epoch: 15 [7040/9051 (77%)]\tLoss: 0.824796\n",
      "Train Epoch: 15 [7680/9051 (85%)]\tLoss: 0.655538\n",
      "Train Epoch: 15 [8320/9051 (92%)]\tLoss: 0.680793\n",
      "Train Epoch: 15 [8960/9051 (99%)]\tLoss: 0.750724\n",
      "Train Epoch: 16 [0/9051 (0%)]\tLoss: 0.472640\n",
      "Train Epoch: 16 [640/9051 (7%)]\tLoss: 0.603272\n",
      "Train Epoch: 16 [1280/9051 (14%)]\tLoss: 0.746004\n",
      "Train Epoch: 16 [1920/9051 (21%)]\tLoss: 0.553526\n",
      "Train Epoch: 16 [2560/9051 (28%)]\tLoss: 0.706826\n",
      "Train Epoch: 16 [3200/9051 (35%)]\tLoss: 0.765881\n",
      "Train Epoch: 16 [3840/9051 (42%)]\tLoss: 0.690975\n",
      "Train Epoch: 16 [4480/9051 (49%)]\tLoss: 0.661245\n",
      "Train Epoch: 16 [5120/9051 (56%)]\tLoss: 0.683963\n",
      "Train Epoch: 16 [5760/9051 (63%)]\tLoss: 0.576999\n",
      "Train Epoch: 16 [6400/9051 (70%)]\tLoss: 0.727943\n",
      "Train Epoch: 16 [7040/9051 (77%)]\tLoss: 0.627373\n",
      "Train Epoch: 16 [7680/9051 (85%)]\tLoss: 0.665831\n",
      "Train Epoch: 16 [8320/9051 (92%)]\tLoss: 0.534362\n",
      "Train Epoch: 16 [8960/9051 (99%)]\tLoss: 1.091742\n",
      "Train Epoch: 17 [0/9051 (0%)]\tLoss: 0.741140\n",
      "Train Epoch: 17 [640/9051 (7%)]\tLoss: 0.627700\n",
      "Train Epoch: 17 [1280/9051 (14%)]\tLoss: 0.609814\n",
      "Train Epoch: 17 [1920/9051 (21%)]\tLoss: 0.804356\n",
      "Train Epoch: 17 [2560/9051 (28%)]\tLoss: 0.756649\n",
      "Train Epoch: 17 [3200/9051 (35%)]\tLoss: 0.886266\n",
      "Train Epoch: 17 [3840/9051 (42%)]\tLoss: 0.615810\n",
      "Train Epoch: 17 [4480/9051 (49%)]\tLoss: 0.558759\n",
      "Train Epoch: 17 [5120/9051 (56%)]\tLoss: 0.662764\n",
      "Train Epoch: 17 [5760/9051 (63%)]\tLoss: 0.706937\n",
      "Train Epoch: 17 [6400/9051 (70%)]\tLoss: 0.690419\n",
      "Train Epoch: 17 [7040/9051 (77%)]\tLoss: 0.590030\n",
      "Train Epoch: 17 [7680/9051 (85%)]\tLoss: 0.747117\n",
      "Train Epoch: 17 [8320/9051 (92%)]\tLoss: 0.630874\n",
      "Train Epoch: 17 [8960/9051 (99%)]\tLoss: 0.632216\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3284 (0%)]\tLoss: 1.239345\n",
      "Train Epoch: 1 [640/3284 (19%)]\tLoss: 0.676102\n",
      "Train Epoch: 1 [1280/3284 (38%)]\tLoss: 0.912365\n",
      "Train Epoch: 1 [1920/3284 (58%)]\tLoss: 0.950664\n",
      "Train Epoch: 1 [2560/3284 (77%)]\tLoss: 0.703194\n",
      "Train Epoch: 1 [3200/3284 (96%)]\tLoss: 0.879580\n",
      "Train Epoch: 2 [0/3284 (0%)]\tLoss: 0.642833\n",
      "Train Epoch: 2 [640/3284 (19%)]\tLoss: 0.634295\n",
      "Train Epoch: 2 [1280/3284 (38%)]\tLoss: 0.712750\n",
      "Train Epoch: 2 [1920/3284 (58%)]\tLoss: 0.672239\n",
      "Train Epoch: 2 [2560/3284 (77%)]\tLoss: 0.908671\n",
      "Train Epoch: 2 [3200/3284 (96%)]\tLoss: 0.738697\n",
      "Train Epoch: 3 [0/3284 (0%)]\tLoss: 0.643835\n",
      "Train Epoch: 3 [640/3284 (19%)]\tLoss: 0.527538\n",
      "Train Epoch: 3 [1280/3284 (38%)]\tLoss: 0.743778\n",
      "Train Epoch: 3 [1920/3284 (58%)]\tLoss: 0.805560\n",
      "Train Epoch: 3 [2560/3284 (77%)]\tLoss: 0.789879\n",
      "Train Epoch: 3 [3200/3284 (96%)]\tLoss: 0.452073\n",
      "Train Epoch: 4 [0/3284 (0%)]\tLoss: 0.530681\n",
      "Train Epoch: 4 [640/3284 (19%)]\tLoss: 0.739598\n",
      "Train Epoch: 4 [1280/3284 (38%)]\tLoss: 0.426634\n",
      "Train Epoch: 4 [1920/3284 (58%)]\tLoss: 0.578264\n",
      "Train Epoch: 4 [2560/3284 (77%)]\tLoss: 0.681241\n",
      "Train Epoch: 4 [3200/3284 (96%)]\tLoss: 0.644653\n",
      "Train Epoch: 5 [0/3284 (0%)]\tLoss: 0.980106\n",
      "Train Epoch: 5 [640/3284 (19%)]\tLoss: 0.919029\n",
      "Train Epoch: 5 [1280/3284 (38%)]\tLoss: 0.695088\n",
      "Train Epoch: 5 [1920/3284 (58%)]\tLoss: 0.819660\n",
      "Train Epoch: 5 [2560/3284 (77%)]\tLoss: 0.685408\n",
      "Train Epoch: 5 [3200/3284 (96%)]\tLoss: 0.744236\n",
      "Train Epoch: 6 [0/3284 (0%)]\tLoss: 0.661516\n",
      "Train Epoch: 6 [640/3284 (19%)]\tLoss: 0.838709\n",
      "Train Epoch: 6 [1280/3284 (38%)]\tLoss: 0.799860\n",
      "Train Epoch: 6 [1920/3284 (58%)]\tLoss: 0.755339\n",
      "Train Epoch: 6 [2560/3284 (77%)]\tLoss: 0.724551\n",
      "Train Epoch: 6 [3200/3284 (96%)]\tLoss: 0.591108\n",
      "Train Epoch: 7 [0/3284 (0%)]\tLoss: 0.697424\n",
      "Train Epoch: 7 [640/3284 (19%)]\tLoss: 0.813552\n",
      "Train Epoch: 7 [1280/3284 (38%)]\tLoss: 0.713701\n",
      "Train Epoch: 7 [1920/3284 (58%)]\tLoss: 0.624092\n",
      "Train Epoch: 7 [2560/3284 (77%)]\tLoss: 0.797333\n",
      "Train Epoch: 7 [3200/3284 (96%)]\tLoss: 0.556261\n",
      "Train Epoch: 8 [0/3284 (0%)]\tLoss: 0.571586\n",
      "Train Epoch: 8 [640/3284 (19%)]\tLoss: 0.766747\n",
      "Train Epoch: 8 [1280/3284 (38%)]\tLoss: 0.653948\n",
      "Train Epoch: 8 [1920/3284 (58%)]\tLoss: 0.824054\n",
      "Train Epoch: 8 [2560/3284 (77%)]\tLoss: 0.538251\n",
      "Train Epoch: 8 [3200/3284 (96%)]\tLoss: 0.590566\n",
      "Train Epoch: 9 [0/3284 (0%)]\tLoss: 0.656017\n",
      "Train Epoch: 9 [640/3284 (19%)]\tLoss: 0.611164\n",
      "Train Epoch: 9 [1280/3284 (38%)]\tLoss: 0.813422\n",
      "Train Epoch: 9 [1920/3284 (58%)]\tLoss: 0.387004\n",
      "Train Epoch: 9 [2560/3284 (77%)]\tLoss: 0.735056\n",
      "Train Epoch: 9 [3200/3284 (96%)]\tLoss: 0.792022\n",
      "Train Epoch: 10 [0/3284 (0%)]\tLoss: 0.645783\n",
      "Train Epoch: 10 [640/3284 (19%)]\tLoss: 0.636147\n",
      "Train Epoch: 10 [1280/3284 (38%)]\tLoss: 0.578192\n",
      "Train Epoch: 10 [1920/3284 (58%)]\tLoss: 0.755606\n",
      "Train Epoch: 10 [2560/3284 (77%)]\tLoss: 0.734450\n",
      "Train Epoch: 10 [3200/3284 (96%)]\tLoss: 0.584755\n",
      "Train Epoch: 11 [0/3284 (0%)]\tLoss: 0.588386\n",
      "Train Epoch: 11 [640/3284 (19%)]\tLoss: 0.691051\n",
      "Train Epoch: 11 [1280/3284 (38%)]\tLoss: 0.647354\n",
      "Train Epoch: 11 [1920/3284 (58%)]\tLoss: 0.513863\n",
      "Train Epoch: 11 [2560/3284 (77%)]\tLoss: 0.602084\n",
      "Train Epoch: 11 [3200/3284 (96%)]\tLoss: 0.597008\n",
      "Train Epoch: 12 [0/3284 (0%)]\tLoss: 0.472495\n",
      "Train Epoch: 12 [640/3284 (19%)]\tLoss: 0.572085\n",
      "Train Epoch: 12 [1280/3284 (38%)]\tLoss: 0.778117\n",
      "Train Epoch: 12 [1920/3284 (58%)]\tLoss: 0.628646\n",
      "Train Epoch: 12 [2560/3284 (77%)]\tLoss: 0.797883\n",
      "Train Epoch: 12 [3200/3284 (96%)]\tLoss: 0.565087\n",
      "Train Epoch: 13 [0/3284 (0%)]\tLoss: 0.630709\n",
      "Train Epoch: 13 [640/3284 (19%)]\tLoss: 0.482077\n",
      "Train Epoch: 13 [1280/3284 (38%)]\tLoss: 0.479879\n",
      "Train Epoch: 13 [1920/3284 (58%)]\tLoss: 0.430767\n",
      "Train Epoch: 13 [2560/3284 (77%)]\tLoss: 0.611486\n",
      "Train Epoch: 13 [3200/3284 (96%)]\tLoss: 0.715899\n",
      "Train Epoch: 14 [0/3284 (0%)]\tLoss: 0.540831\n",
      "Train Epoch: 14 [640/3284 (19%)]\tLoss: 0.876523\n",
      "Train Epoch: 14 [1280/3284 (38%)]\tLoss: 0.704613\n",
      "Train Epoch: 14 [1920/3284 (58%)]\tLoss: 0.705039\n",
      "Train Epoch: 14 [2560/3284 (77%)]\tLoss: 0.543391\n",
      "Train Epoch: 14 [3200/3284 (96%)]\tLoss: 0.351545\n",
      "Train Epoch: 15 [0/3284 (0%)]\tLoss: 0.705354\n",
      "Train Epoch: 15 [640/3284 (19%)]\tLoss: 0.762006\n",
      "Train Epoch: 15 [1280/3284 (38%)]\tLoss: 0.789177\n",
      "Train Epoch: 15 [1920/3284 (58%)]\tLoss: 0.752861\n",
      "Train Epoch: 15 [2560/3284 (77%)]\tLoss: 0.588333\n",
      "Train Epoch: 15 [3200/3284 (96%)]\tLoss: 0.743667\n",
      "Train Epoch: 16 [0/3284 (0%)]\tLoss: 0.420263\n",
      "Train Epoch: 16 [640/3284 (19%)]\tLoss: 0.586520\n",
      "Train Epoch: 16 [1280/3284 (38%)]\tLoss: 0.693123\n",
      "Train Epoch: 16 [1920/3284 (58%)]\tLoss: 0.507461\n",
      "Train Epoch: 16 [2560/3284 (77%)]\tLoss: 0.744021\n",
      "Train Epoch: 16 [3200/3284 (96%)]\tLoss: 0.444255\n",
      "Train Epoch: 17 [0/3284 (0%)]\tLoss: 0.518964\n",
      "Train Epoch: 17 [640/3284 (19%)]\tLoss: 0.679309\n",
      "Train Epoch: 17 [1280/3284 (38%)]\tLoss: 0.495236\n",
      "Train Epoch: 17 [1920/3284 (58%)]\tLoss: 0.577889\n",
      "Train Epoch: 17 [2560/3284 (77%)]\tLoss: 0.594582\n",
      "Train Epoch: 17 [3200/3284 (96%)]\tLoss: 0.603906\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.7640, Accuracy: 4055/10000 (41%)\n",
      "\n",
      "Running experiment with alpha: 0.5 \n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 2.050592\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.772411\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.697055\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.489106\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.448043\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.330304\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.590572\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.419865\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.612969\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.477138\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.498917\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.450101\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.216798\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.436702\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.630676\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.114650\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.260152\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.197477\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.174895\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.516824\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.643084\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.238099\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.135357\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.458036\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.285321\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.179645\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.306912\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.378265\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.308228\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.434217\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.548463\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.096450\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 1.391834\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.391484\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.279980\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.288635\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.412004\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.409705\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.186135\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.450397\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.598222\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.224398\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.325647\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.443664\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.453270\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.396793\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.222000\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.484560\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.318046\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.217076\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.311082\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.235980\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.337554\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.330578\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.355419\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.307562\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.162557\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.375880\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.442117\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.299161\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.127566\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.319448\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.273917\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.092342\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.281196\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.183278\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.362763\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.363885\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.197404\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.137250\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.298787\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.214130\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.452894\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.261270\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.226706\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.214120\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.493868\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.276006\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 1.658084\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.380953\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.208001\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.356366\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.155588\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.242924\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.270209\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.439028\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.191287\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.317674\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 1.184441\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.265987\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.303831\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.360084\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.554189\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.258314\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.245182\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.472487\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.362950\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.249971\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.187498\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 1.021428\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.114886\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.425301\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.767916\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.603542\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.565782\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.273943\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.650636\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.305433\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.331735\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.354374\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.318583\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.433584\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.325304\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.490443\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.482540\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.501185\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.425951\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.201397\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.420597\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.402141\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.335337\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.142846\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.419353\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.433659\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.447396\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.409042\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.368884\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.361935\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.305124\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.369549\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.413572\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.202771\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.419805\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.120898\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.195040\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.169824\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.228115\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.397150\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.213179\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.634719\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.299394\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.560782\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.207211\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 1.515844\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.339815\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.401234\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.522360\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.298376\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.574451\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.534151\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.237870\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.193058\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.376543\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.383485\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.221105\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.168067\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.266543\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.358626\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.300850\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.032334\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.499870\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.532606\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 1.304640\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.402272\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.307171\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.253205\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.131376\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.526340\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.197538\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.439132\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.205660\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.354651\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.109035\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.416293\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.072018\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.460934\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.077178\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.253186\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.224388\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.099020\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 1.151603\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.178041\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.149972\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.118217\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.087002\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.282960\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.249069\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.334216\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 1.213121\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.231905\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.124193\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.120481\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.043446\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.482512\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.254635\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.478320\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.198854\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.213500\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.463954\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.221954\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.271227\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.183088\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.260785\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.338131\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.851449\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.410083\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.285880\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.385386\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.316364\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 1.550008\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 1.188999\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.083660\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.245329\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 1.057695\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 0.948621\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.420748\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 1.507820\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.276556\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 1.190141\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.026638\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 1.254805\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 1.320271\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.164631\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.205881\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 1.109726\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 1.121987\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.450017\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.022151\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.329561\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.440732\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 1.166108\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.041766\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 1.326123\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 1.334016\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 0.877661\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 1.084678\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 0.926995\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 1.408063\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 1.296071\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 0.901352\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.282283\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.140203\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 1.100762\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 1.100144\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 1.081573\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 1.174712\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 1.078861\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 1.050817\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 1.167669\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 0.918413\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 1.092477\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 1.037668\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 1.025512\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 1.561116\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 1.015804\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.095087\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 1.138121\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 0.987245\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 1.264468\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 1.050396\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 1.008034\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 1.072250\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 1.085076\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 1.225957\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 0.840400\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 1.111749\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 1.127327\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 1.173600\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 1.216984\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 1.185014\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.236241\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 1.199094\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.896742\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 1.100676\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 1.144936\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 1.126143\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.164613\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 0.897881\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.339015\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 0.902944\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.025370\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 1.245385\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 1.239213\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 1.168874\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 1.257188\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 1.126836\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 1.192184\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 0.938462\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 1.199061\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 1.071907\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 1.019778\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 1.421684\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 1.098118\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 0.834868\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 0.993548\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 1.029232\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 0.915689\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 1.289925\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 0.876373\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 1.187162\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 0.965091\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.070904\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 1.150785\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 1.046328\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 0.740452\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 1.390054\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 0.902590\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 1.060215\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 1.226742\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 1.159855\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 0.949023\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 1.102604\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 0.936503\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 1.334196\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 1.153672\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 1.008261\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.070164\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 1.029416\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 0.786314\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.953770\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 0.931832\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 1.100278\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 0.774610\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 0.875872\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 0.884910\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 1.054873\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 1.087528\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 1.155679\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 1.070003\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 1.138479\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 0.978856\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 0.811885\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 0.903971\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 1.285087\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 1.147419\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 1.116597\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 1.154457\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 0.891678\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 1.004292\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 0.945539\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 1.216971\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 0.964947\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 0.981333\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 1.014533\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 0.990279\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 1.103950\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 1.142803\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 1.188749\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 0.759383\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 1.159478\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 1.103116\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 0.978451\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 0.940106\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 1.078654\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 1.061620\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 0.936526\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 1.168993\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 1.013524\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 1.089843\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.959791\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 1.255102\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 0.977175\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 0.918213\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.130301\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 0.904398\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 0.964856\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 0.918113\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 0.984698\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 0.958869\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 1.175820\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.941042\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.890796\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.865870\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 0.973243\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.254455\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.247229\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 1.285578\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 1.214547\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.453574\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.125918\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 1.223104\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.012155\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 0.826416\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.021021\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 1.090397\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.982429\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 0.922010\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 1.158614\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.098711\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.041283\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 1.264311\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 1.377851\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 0.830733\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 1.270384\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 1.144474\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 1.148928\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 0.957390\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 1.052193\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 1.057899\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 1.404459\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 1.041645\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 1.247076\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 1.197977\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 1.171242\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 1.046224\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 0.947402\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 1.010359\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 1.112122\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 1.247369\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 0.890367\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 1.425433\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 0.825374\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 1.121279\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 1.303300\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 0.954789\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 1.241114\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 1.225432\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 0.911972\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 1.446247\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 1.230278\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 1.187580\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 1.137497\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 1.220219\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 1.151542\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 1.336766\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 1.018434\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 0.970032\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 1.009535\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 1.166793\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.850285\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 1.177041\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 1.118586\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 1.201423\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 1.094240\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 1.079595\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 1.139549\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 0.913760\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 1.434870\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.052081\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 0.839877\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 1.060850\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 1.072626\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 1.091159\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 1.279214\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 1.161495\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 1.001949\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 1.001008\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 0.960990\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 1.289174\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 1.232381\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 0.960406\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 1.107768\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 1.046415\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 1.090576\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 1.329550\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.872272\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 1.147859\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 1.116004\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 1.165878\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 1.271245\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 0.797938\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.858880\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 0.978772\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 1.011352\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 0.959892\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 0.933048\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 1.085940\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 0.983221\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 1.135501\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 0.841715\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 0.995589\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.873918\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 1.079352\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.996224\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 1.132053\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 1.030119\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 1.102267\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 1.091244\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 0.863360\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 0.967202\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 0.900705\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 1.050339\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 1.110395\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 1.053647\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 0.825227\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.751372\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 0.891790\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.942838\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 0.919927\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 0.905991\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.987920\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 1.102022\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 1.319803\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 1.241069\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 1.053690\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 0.945203\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 0.885219\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 1.062731\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 1.056892\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 1.089162\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.908513\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 1.024209\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 0.990090\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 1.174960\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.952372\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 0.805525\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 1.121630\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 1.058272\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.968194\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 1.036941\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 1.045290\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 0.980373\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.962476\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 1.156384\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 0.941443\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 1.060118\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 1.000817\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 1.189746\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.973978\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 0.807632\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 0.900435\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 1.038440\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 1.072195\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.082977\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.993774\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 1.016043\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 0.976438\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 1.051010\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 0.842630\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 0.702976\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 0.944515\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 0.981765\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 1.215214\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.971920\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.977626\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 0.916736\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 1.156407\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 1.032036\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 0.979939\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 1.109021\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 0.964963\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 0.925566\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.974204\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 1.082590\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 0.994621\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 0.894793\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.874715\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.830874\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 1.037197\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.772890\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 0.967231\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 1.078858\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 1.018666\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 1.029636\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.756079\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.843532\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 0.999255\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 1.069222\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 1.016750\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 0.851746\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 1.004672\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.852595\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.566721\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.770479\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.380724\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.470756\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.490849\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.431997\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.554805\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.320021\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.523137\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.323487\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.339187\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.601746\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.540879\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.269415\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.551133\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.476218\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.282481\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.407685\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.358928\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.512824\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.533045\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.399271\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.309253\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.249772\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.448107\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.162736\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.247538\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.417462\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.392672\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.308908\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.489107\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.299736\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.204342\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.181522\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.150156\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.231281\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.397764\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.290842\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.160858\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.380359\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 1.428600\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.343494\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.293867\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.628336\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.287392\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.246326\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.236934\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.076041\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.189332\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.224097\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.119291\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.447823\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.360213\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.131154\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 1.486662\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.385482\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.337904\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.523085\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.354389\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.035374\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.242614\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.170033\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.390614\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.448720\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.409940\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.357413\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.496444\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.147936\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 0.681308\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.196696\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.242987\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 1.184610\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.290036\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.478772\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.304079\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.149074\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.282960\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.053305\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.470852\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 1.389957\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.454543\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.244841\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 0.980752\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.440899\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.336472\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.180763\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.267640\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.179644\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.469467\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.253595\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.347761\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.372626\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.038499\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.476700\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.316256\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.611674\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 1.243658\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.330366\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.448375\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.290406\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.187561\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.478167\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.109316\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.267266\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.307080\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.201417\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.083995\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.204984\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.256703\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.320789\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 1.110767\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.082662\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.169000\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.178588\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.220062\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.369162\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.282300\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.396895\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.342840\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.353811\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.067565\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.312346\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.334890\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.037154\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 1.045030\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 1.253144\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.205847\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.231468\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.374114\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.347864\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.310486\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.158235\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.180625\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.224348\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.177181\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.317694\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.379513\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.153152\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 1.554332\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.443760\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.338052\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.290179\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.058679\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.386586\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.302556\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.482590\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.389749\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.373729\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.228240\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.180402\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.379834\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 1.368116\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 0.706617\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.209100\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.195404\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 1.288113\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 0.956655\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 1.294970\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 1.576729\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.285762\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.320177\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 1.547774\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.078819\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.208086\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.146572\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.392108\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 1.028519\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.277419\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.235724\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.312620\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.279172\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.299418\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.017852\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 1.216228\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.140940\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 1.137455\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.532470\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.247645\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.475861\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.155237\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 1.267308\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 0.958432\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.165288\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.331124\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 1.223410\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 0.987907\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 1.216446\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.448946\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.181876\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.224056\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.093469\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 1.131605\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.033319\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.140302\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 0.890680\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.513975\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.464134\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.113955\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.216800\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 1.390011\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.111808\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.346169\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 1.313232\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.144841\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.211708\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 1.247153\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.090820\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 1.240619\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 2.242115\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 1.120009\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.263961\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.447636\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.243756\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.061829\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.135659\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.175819\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.145862\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 1.380853\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 1.304300\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.382405\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.114599\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.590314\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 0.912739\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.179059\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.229479\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 1.105777\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.051496\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.194577\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.152020\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.069885\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.172469\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.161818\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.406358\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.214855\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.271228\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.292479\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 1.267264\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 2.851260\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.901667\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.728580\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.512272\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.394090\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.594404\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.567125\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.625408\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.556061\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.580657\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.485465\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.437609\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.202357\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.676745\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.544187\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.390967\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.463566\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.650681\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.288693\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.773872\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.471586\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.460719\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.225871\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.527792\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.668233\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.410262\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.406387\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.256769\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.439636\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.440278\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.391262\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.504547\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.621091\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.453288\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.469729\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.359059\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.458765\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.312955\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.477203\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.467035\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.686096\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.242280\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.239068\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.292932\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.480344\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.401483\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.277835\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.544451\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.317761\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.298253\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.419434\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.347055\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.358637\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.335776\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.323316\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.353715\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.251935\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.393255\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.326715\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.331281\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.179669\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.293577\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.304877\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.355008\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.217052\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 1.263834\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.340157\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.423363\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 1.540743\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.302741\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.262825\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.252902\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.243853\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.160955\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 1.379000\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 1.270562\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 1.313802\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.477064\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.503423\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.328047\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.357903\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.148774\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.146655\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.372379\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.593346\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/9397 (0%)]\tLoss: 2.451112\n",
      "Train Epoch: 1 [640/9397 (7%)]\tLoss: 1.956404\n",
      "Train Epoch: 1 [1280/9397 (14%)]\tLoss: 1.410493\n",
      "Train Epoch: 1 [1920/9397 (20%)]\tLoss: 1.674780\n",
      "Train Epoch: 1 [2560/9397 (27%)]\tLoss: 1.669843\n",
      "Train Epoch: 1 [3200/9397 (34%)]\tLoss: 1.490681\n",
      "Train Epoch: 1 [3840/9397 (41%)]\tLoss: 1.533534\n",
      "Train Epoch: 1 [4480/9397 (48%)]\tLoss: 1.422329\n",
      "Train Epoch: 1 [5120/9397 (54%)]\tLoss: 1.482800\n",
      "Train Epoch: 1 [5760/9397 (61%)]\tLoss: 1.533175\n",
      "Train Epoch: 1 [6400/9397 (68%)]\tLoss: 1.388563\n",
      "Train Epoch: 1 [7040/9397 (75%)]\tLoss: 1.418606\n",
      "Train Epoch: 1 [7680/9397 (82%)]\tLoss: 1.627258\n",
      "Train Epoch: 1 [8320/9397 (88%)]\tLoss: 1.477434\n",
      "Train Epoch: 1 [8960/9397 (95%)]\tLoss: 1.199522\n",
      "Train Epoch: 2 [0/9397 (0%)]\tLoss: 1.560438\n",
      "Train Epoch: 2 [640/9397 (7%)]\tLoss: 1.371638\n",
      "Train Epoch: 2 [1280/9397 (14%)]\tLoss: 1.374774\n",
      "Train Epoch: 2 [1920/9397 (20%)]\tLoss: 1.243643\n",
      "Train Epoch: 2 [2560/9397 (27%)]\tLoss: 1.141904\n",
      "Train Epoch: 2 [3200/9397 (34%)]\tLoss: 1.515956\n",
      "Train Epoch: 2 [3840/9397 (41%)]\tLoss: 1.492011\n",
      "Train Epoch: 2 [4480/9397 (48%)]\tLoss: 1.433584\n",
      "Train Epoch: 2 [5120/9397 (54%)]\tLoss: 1.280362\n",
      "Train Epoch: 2 [5760/9397 (61%)]\tLoss: 1.254981\n",
      "Train Epoch: 2 [6400/9397 (68%)]\tLoss: 1.440479\n",
      "Train Epoch: 2 [7040/9397 (75%)]\tLoss: 1.443424\n",
      "Train Epoch: 2 [7680/9397 (82%)]\tLoss: 1.472899\n",
      "Train Epoch: 2 [8320/9397 (88%)]\tLoss: 1.456950\n",
      "Train Epoch: 2 [8960/9397 (95%)]\tLoss: 1.466930\n",
      "Train Epoch: 3 [0/9397 (0%)]\tLoss: 1.412348\n",
      "Train Epoch: 3 [640/9397 (7%)]\tLoss: 1.423759\n",
      "Train Epoch: 3 [1280/9397 (14%)]\tLoss: 1.252341\n",
      "Train Epoch: 3 [1920/9397 (20%)]\tLoss: 1.223835\n",
      "Train Epoch: 3 [2560/9397 (27%)]\tLoss: 1.271446\n",
      "Train Epoch: 3 [3200/9397 (34%)]\tLoss: 1.565040\n",
      "Train Epoch: 3 [3840/9397 (41%)]\tLoss: 1.404756\n",
      "Train Epoch: 3 [4480/9397 (48%)]\tLoss: 1.226326\n",
      "Train Epoch: 3 [5120/9397 (54%)]\tLoss: 1.311580\n",
      "Train Epoch: 3 [5760/9397 (61%)]\tLoss: 1.261693\n",
      "Train Epoch: 3 [6400/9397 (68%)]\tLoss: 1.282842\n",
      "Train Epoch: 3 [7040/9397 (75%)]\tLoss: 1.138337\n",
      "Train Epoch: 3 [7680/9397 (82%)]\tLoss: 1.398975\n",
      "Train Epoch: 3 [8320/9397 (88%)]\tLoss: 1.206779\n",
      "Train Epoch: 3 [8960/9397 (95%)]\tLoss: 1.277932\n",
      "Train Epoch: 4 [0/9397 (0%)]\tLoss: 1.168838\n",
      "Train Epoch: 4 [640/9397 (7%)]\tLoss: 1.264094\n",
      "Train Epoch: 4 [1280/9397 (14%)]\tLoss: 1.170395\n",
      "Train Epoch: 4 [1920/9397 (20%)]\tLoss: 1.110023\n",
      "Train Epoch: 4 [2560/9397 (27%)]\tLoss: 1.698236\n",
      "Train Epoch: 4 [3200/9397 (34%)]\tLoss: 1.218546\n",
      "Train Epoch: 4 [3840/9397 (41%)]\tLoss: 1.292059\n",
      "Train Epoch: 4 [4480/9397 (48%)]\tLoss: 1.240212\n",
      "Train Epoch: 4 [5120/9397 (54%)]\tLoss: 1.284853\n",
      "Train Epoch: 4 [5760/9397 (61%)]\tLoss: 0.947107\n",
      "Train Epoch: 4 [6400/9397 (68%)]\tLoss: 1.049563\n",
      "Train Epoch: 4 [7040/9397 (75%)]\tLoss: 1.207030\n",
      "Train Epoch: 4 [7680/9397 (82%)]\tLoss: 1.174566\n",
      "Train Epoch: 4 [8320/9397 (88%)]\tLoss: 1.075342\n",
      "Train Epoch: 4 [8960/9397 (95%)]\tLoss: 1.463374\n",
      "Train Epoch: 5 [0/9397 (0%)]\tLoss: 1.282871\n",
      "Train Epoch: 5 [640/9397 (7%)]\tLoss: 1.258813\n",
      "Train Epoch: 5 [1280/9397 (14%)]\tLoss: 1.301861\n",
      "Train Epoch: 5 [1920/9397 (20%)]\tLoss: 1.396700\n",
      "Train Epoch: 5 [2560/9397 (27%)]\tLoss: 1.411799\n",
      "Train Epoch: 5 [3200/9397 (34%)]\tLoss: 1.275977\n",
      "Train Epoch: 5 [3840/9397 (41%)]\tLoss: 1.141869\n",
      "Train Epoch: 5 [4480/9397 (48%)]\tLoss: 1.377876\n",
      "Train Epoch: 5 [5120/9397 (54%)]\tLoss: 1.152626\n",
      "Train Epoch: 5 [5760/9397 (61%)]\tLoss: 1.176581\n",
      "Train Epoch: 5 [6400/9397 (68%)]\tLoss: 1.202255\n",
      "Train Epoch: 5 [7040/9397 (75%)]\tLoss: 1.081410\n",
      "Train Epoch: 5 [7680/9397 (82%)]\tLoss: 1.533415\n",
      "Train Epoch: 5 [8320/9397 (88%)]\tLoss: 1.211155\n",
      "Train Epoch: 5 [8960/9397 (95%)]\tLoss: 1.405048\n",
      "Train Epoch: 6 [0/9397 (0%)]\tLoss: 1.277494\n",
      "Train Epoch: 6 [640/9397 (7%)]\tLoss: 1.479561\n",
      "Train Epoch: 6 [1280/9397 (14%)]\tLoss: 1.060486\n",
      "Train Epoch: 6 [1920/9397 (20%)]\tLoss: 1.294048\n",
      "Train Epoch: 6 [2560/9397 (27%)]\tLoss: 1.344944\n",
      "Train Epoch: 6 [3200/9397 (34%)]\tLoss: 1.372943\n",
      "Train Epoch: 6 [3840/9397 (41%)]\tLoss: 1.244091\n",
      "Train Epoch: 6 [4480/9397 (48%)]\tLoss: 1.222872\n",
      "Train Epoch: 6 [5120/9397 (54%)]\tLoss: 1.194914\n",
      "Train Epoch: 6 [5760/9397 (61%)]\tLoss: 1.127134\n",
      "Train Epoch: 6 [6400/9397 (68%)]\tLoss: 1.341740\n",
      "Train Epoch: 6 [7040/9397 (75%)]\tLoss: 1.225474\n",
      "Train Epoch: 6 [7680/9397 (82%)]\tLoss: 1.146016\n",
      "Train Epoch: 6 [8320/9397 (88%)]\tLoss: 1.270462\n",
      "Train Epoch: 6 [8960/9397 (95%)]\tLoss: 1.157652\n",
      "Train Epoch: 7 [0/9397 (0%)]\tLoss: 1.266799\n",
      "Train Epoch: 7 [640/9397 (7%)]\tLoss: 1.039088\n",
      "Train Epoch: 7 [1280/9397 (14%)]\tLoss: 1.055992\n",
      "Train Epoch: 7 [1920/9397 (20%)]\tLoss: 1.358122\n",
      "Train Epoch: 7 [2560/9397 (27%)]\tLoss: 1.086068\n",
      "Train Epoch: 7 [3200/9397 (34%)]\tLoss: 1.339173\n",
      "Train Epoch: 7 [3840/9397 (41%)]\tLoss: 1.267362\n",
      "Train Epoch: 7 [4480/9397 (48%)]\tLoss: 1.437289\n",
      "Train Epoch: 7 [5120/9397 (54%)]\tLoss: 1.303170\n",
      "Train Epoch: 7 [5760/9397 (61%)]\tLoss: 1.423123\n",
      "Train Epoch: 7 [6400/9397 (68%)]\tLoss: 1.183632\n",
      "Train Epoch: 7 [7040/9397 (75%)]\tLoss: 1.191831\n",
      "Train Epoch: 7 [7680/9397 (82%)]\tLoss: 1.329220\n",
      "Train Epoch: 7 [8320/9397 (88%)]\tLoss: 1.256002\n",
      "Train Epoch: 7 [8960/9397 (95%)]\tLoss: 1.341352\n",
      "Train Epoch: 8 [0/9397 (0%)]\tLoss: 0.998984\n",
      "Train Epoch: 8 [640/9397 (7%)]\tLoss: 1.128990\n",
      "Train Epoch: 8 [1280/9397 (14%)]\tLoss: 1.222795\n",
      "Train Epoch: 8 [1920/9397 (20%)]\tLoss: 1.656862\n",
      "Train Epoch: 8 [2560/9397 (27%)]\tLoss: 1.278409\n",
      "Train Epoch: 8 [3200/9397 (34%)]\tLoss: 1.067697\n",
      "Train Epoch: 8 [3840/9397 (41%)]\tLoss: 1.503829\n",
      "Train Epoch: 8 [4480/9397 (48%)]\tLoss: 1.219576\n",
      "Train Epoch: 8 [5120/9397 (54%)]\tLoss: 1.023432\n",
      "Train Epoch: 8 [5760/9397 (61%)]\tLoss: 1.177632\n",
      "Train Epoch: 8 [6400/9397 (68%)]\tLoss: 1.147408\n",
      "Train Epoch: 8 [7040/9397 (75%)]\tLoss: 1.387060\n",
      "Train Epoch: 8 [7680/9397 (82%)]\tLoss: 1.059160\n",
      "Train Epoch: 8 [8320/9397 (88%)]\tLoss: 1.060442\n",
      "Train Epoch: 8 [8960/9397 (95%)]\tLoss: 1.079076\n",
      "Train Epoch: 9 [0/9397 (0%)]\tLoss: 1.247240\n",
      "Train Epoch: 9 [640/9397 (7%)]\tLoss: 1.138681\n",
      "Train Epoch: 9 [1280/9397 (14%)]\tLoss: 1.258040\n",
      "Train Epoch: 9 [1920/9397 (20%)]\tLoss: 0.947755\n",
      "Train Epoch: 9 [2560/9397 (27%)]\tLoss: 1.035484\n",
      "Train Epoch: 9 [3200/9397 (34%)]\tLoss: 1.016580\n",
      "Train Epoch: 9 [3840/9397 (41%)]\tLoss: 1.108466\n",
      "Train Epoch: 9 [4480/9397 (48%)]\tLoss: 1.016929\n",
      "Train Epoch: 9 [5120/9397 (54%)]\tLoss: 1.026824\n",
      "Train Epoch: 9 [5760/9397 (61%)]\tLoss: 1.161456\n",
      "Train Epoch: 9 [6400/9397 (68%)]\tLoss: 1.413581\n",
      "Train Epoch: 9 [7040/9397 (75%)]\tLoss: 1.084044\n",
      "Train Epoch: 9 [7680/9397 (82%)]\tLoss: 1.082344\n",
      "Train Epoch: 9 [8320/9397 (88%)]\tLoss: 1.363519\n",
      "Train Epoch: 9 [8960/9397 (95%)]\tLoss: 1.185193\n",
      "Train Epoch: 10 [0/9397 (0%)]\tLoss: 0.977554\n",
      "Train Epoch: 10 [640/9397 (7%)]\tLoss: 1.164099\n",
      "Train Epoch: 10 [1280/9397 (14%)]\tLoss: 1.259734\n",
      "Train Epoch: 10 [1920/9397 (20%)]\tLoss: 1.332269\n",
      "Train Epoch: 10 [2560/9397 (27%)]\tLoss: 1.091541\n",
      "Train Epoch: 10 [3200/9397 (34%)]\tLoss: 1.279520\n",
      "Train Epoch: 10 [3840/9397 (41%)]\tLoss: 1.239740\n",
      "Train Epoch: 10 [4480/9397 (48%)]\tLoss: 1.499580\n",
      "Train Epoch: 10 [5120/9397 (54%)]\tLoss: 1.257325\n",
      "Train Epoch: 10 [5760/9397 (61%)]\tLoss: 1.035722\n",
      "Train Epoch: 10 [6400/9397 (68%)]\tLoss: 1.328537\n",
      "Train Epoch: 10 [7040/9397 (75%)]\tLoss: 1.104660\n",
      "Train Epoch: 10 [7680/9397 (82%)]\tLoss: 1.266859\n",
      "Train Epoch: 10 [8320/9397 (88%)]\tLoss: 1.146901\n",
      "Train Epoch: 10 [8960/9397 (95%)]\tLoss: 1.147173\n",
      "Train Epoch: 11 [0/9397 (0%)]\tLoss: 1.144919\n",
      "Train Epoch: 11 [640/9397 (7%)]\tLoss: 1.280298\n",
      "Train Epoch: 11 [1280/9397 (14%)]\tLoss: 1.163444\n",
      "Train Epoch: 11 [1920/9397 (20%)]\tLoss: 1.333880\n",
      "Train Epoch: 11 [2560/9397 (27%)]\tLoss: 1.548032\n",
      "Train Epoch: 11 [3200/9397 (34%)]\tLoss: 1.197522\n",
      "Train Epoch: 11 [3840/9397 (41%)]\tLoss: 1.093897\n",
      "Train Epoch: 11 [4480/9397 (48%)]\tLoss: 1.377146\n",
      "Train Epoch: 11 [5120/9397 (54%)]\tLoss: 1.285249\n",
      "Train Epoch: 11 [5760/9397 (61%)]\tLoss: 1.110744\n",
      "Train Epoch: 11 [6400/9397 (68%)]\tLoss: 1.355055\n",
      "Train Epoch: 11 [7040/9397 (75%)]\tLoss: 1.277060\n",
      "Train Epoch: 11 [7680/9397 (82%)]\tLoss: 1.124701\n",
      "Train Epoch: 11 [8320/9397 (88%)]\tLoss: 1.078823\n",
      "Train Epoch: 11 [8960/9397 (95%)]\tLoss: 1.289008\n",
      "Train Epoch: 12 [0/9397 (0%)]\tLoss: 1.146221\n",
      "Train Epoch: 12 [640/9397 (7%)]\tLoss: 1.115958\n",
      "Train Epoch: 12 [1280/9397 (14%)]\tLoss: 1.051880\n",
      "Train Epoch: 12 [1920/9397 (20%)]\tLoss: 1.210731\n",
      "Train Epoch: 12 [2560/9397 (27%)]\tLoss: 0.979234\n",
      "Train Epoch: 12 [3200/9397 (34%)]\tLoss: 1.270690\n",
      "Train Epoch: 12 [3840/9397 (41%)]\tLoss: 1.142176\n",
      "Train Epoch: 12 [4480/9397 (48%)]\tLoss: 1.251281\n",
      "Train Epoch: 12 [5120/9397 (54%)]\tLoss: 1.212494\n",
      "Train Epoch: 12 [5760/9397 (61%)]\tLoss: 1.313372\n",
      "Train Epoch: 12 [6400/9397 (68%)]\tLoss: 1.014633\n",
      "Train Epoch: 12 [7040/9397 (75%)]\tLoss: 1.169264\n",
      "Train Epoch: 12 [7680/9397 (82%)]\tLoss: 1.360281\n",
      "Train Epoch: 12 [8320/9397 (88%)]\tLoss: 1.207991\n",
      "Train Epoch: 12 [8960/9397 (95%)]\tLoss: 1.526032\n",
      "Train Epoch: 13 [0/9397 (0%)]\tLoss: 1.238956\n",
      "Train Epoch: 13 [640/9397 (7%)]\tLoss: 1.143299\n",
      "Train Epoch: 13 [1280/9397 (14%)]\tLoss: 1.258334\n",
      "Train Epoch: 13 [1920/9397 (20%)]\tLoss: 1.111010\n",
      "Train Epoch: 13 [2560/9397 (27%)]\tLoss: 1.025452\n",
      "Train Epoch: 13 [3200/9397 (34%)]\tLoss: 0.983120\n",
      "Train Epoch: 13 [3840/9397 (41%)]\tLoss: 1.257116\n",
      "Train Epoch: 13 [4480/9397 (48%)]\tLoss: 1.322111\n",
      "Train Epoch: 13 [5120/9397 (54%)]\tLoss: 1.313984\n",
      "Train Epoch: 13 [5760/9397 (61%)]\tLoss: 1.134108\n",
      "Train Epoch: 13 [6400/9397 (68%)]\tLoss: 1.342141\n",
      "Train Epoch: 13 [7040/9397 (75%)]\tLoss: 1.310347\n",
      "Train Epoch: 13 [7680/9397 (82%)]\tLoss: 1.122374\n",
      "Train Epoch: 13 [8320/9397 (88%)]\tLoss: 1.154530\n",
      "Train Epoch: 13 [8960/9397 (95%)]\tLoss: 1.211502\n",
      "Train Epoch: 14 [0/9397 (0%)]\tLoss: 0.885634\n",
      "Train Epoch: 14 [640/9397 (7%)]\tLoss: 1.433010\n",
      "Train Epoch: 14 [1280/9397 (14%)]\tLoss: 1.495149\n",
      "Train Epoch: 14 [1920/9397 (20%)]\tLoss: 1.464508\n",
      "Train Epoch: 14 [2560/9397 (27%)]\tLoss: 1.292179\n",
      "Train Epoch: 14 [3200/9397 (34%)]\tLoss: 1.421929\n",
      "Train Epoch: 14 [3840/9397 (41%)]\tLoss: 0.971474\n",
      "Train Epoch: 14 [4480/9397 (48%)]\tLoss: 0.962463\n",
      "Train Epoch: 14 [5120/9397 (54%)]\tLoss: 1.287367\n",
      "Train Epoch: 14 [5760/9397 (61%)]\tLoss: 1.280160\n",
      "Train Epoch: 14 [6400/9397 (68%)]\tLoss: 1.337387\n",
      "Train Epoch: 14 [7040/9397 (75%)]\tLoss: 1.136444\n",
      "Train Epoch: 14 [7680/9397 (82%)]\tLoss: 1.215280\n",
      "Train Epoch: 14 [8320/9397 (88%)]\tLoss: 1.160616\n",
      "Train Epoch: 14 [8960/9397 (95%)]\tLoss: 1.154439\n",
      "Train Epoch: 15 [0/9397 (0%)]\tLoss: 1.209144\n",
      "Train Epoch: 15 [640/9397 (7%)]\tLoss: 1.200630\n",
      "Train Epoch: 15 [1280/9397 (14%)]\tLoss: 1.314419\n",
      "Train Epoch: 15 [1920/9397 (20%)]\tLoss: 1.340956\n",
      "Train Epoch: 15 [2560/9397 (27%)]\tLoss: 0.974922\n",
      "Train Epoch: 15 [3200/9397 (34%)]\tLoss: 1.275828\n",
      "Train Epoch: 15 [3840/9397 (41%)]\tLoss: 1.059344\n",
      "Train Epoch: 15 [4480/9397 (48%)]\tLoss: 1.313722\n",
      "Train Epoch: 15 [5120/9397 (54%)]\tLoss: 1.020051\n",
      "Train Epoch: 15 [5760/9397 (61%)]\tLoss: 1.204767\n",
      "Train Epoch: 15 [6400/9397 (68%)]\tLoss: 0.948410\n",
      "Train Epoch: 15 [7040/9397 (75%)]\tLoss: 1.161538\n",
      "Train Epoch: 15 [7680/9397 (82%)]\tLoss: 1.222012\n",
      "Train Epoch: 15 [8320/9397 (88%)]\tLoss: 1.189436\n",
      "Train Epoch: 15 [8960/9397 (95%)]\tLoss: 1.222106\n",
      "Train Epoch: 16 [0/9397 (0%)]\tLoss: 1.281778\n",
      "Train Epoch: 16 [640/9397 (7%)]\tLoss: 1.302655\n",
      "Train Epoch: 16 [1280/9397 (14%)]\tLoss: 1.345624\n",
      "Train Epoch: 16 [1920/9397 (20%)]\tLoss: 1.329160\n",
      "Train Epoch: 16 [2560/9397 (27%)]\tLoss: 0.979648\n",
      "Train Epoch: 16 [3200/9397 (34%)]\tLoss: 1.224213\n",
      "Train Epoch: 16 [3840/9397 (41%)]\tLoss: 0.987167\n",
      "Train Epoch: 16 [4480/9397 (48%)]\tLoss: 0.970127\n",
      "Train Epoch: 16 [5120/9397 (54%)]\tLoss: 1.343779\n",
      "Train Epoch: 16 [5760/9397 (61%)]\tLoss: 1.156719\n",
      "Train Epoch: 16 [6400/9397 (68%)]\tLoss: 1.157373\n",
      "Train Epoch: 16 [7040/9397 (75%)]\tLoss: 1.094921\n",
      "Train Epoch: 16 [7680/9397 (82%)]\tLoss: 1.181556\n",
      "Train Epoch: 16 [8320/9397 (88%)]\tLoss: 1.210968\n",
      "Train Epoch: 16 [8960/9397 (95%)]\tLoss: 1.244243\n",
      "Train Epoch: 17 [0/9397 (0%)]\tLoss: 1.041423\n",
      "Train Epoch: 17 [640/9397 (7%)]\tLoss: 1.014693\n",
      "Train Epoch: 17 [1280/9397 (14%)]\tLoss: 1.324680\n",
      "Train Epoch: 17 [1920/9397 (20%)]\tLoss: 0.967417\n",
      "Train Epoch: 17 [2560/9397 (27%)]\tLoss: 1.102322\n",
      "Train Epoch: 17 [3200/9397 (34%)]\tLoss: 1.115638\n",
      "Train Epoch: 17 [3840/9397 (41%)]\tLoss: 1.014540\n",
      "Train Epoch: 17 [4480/9397 (48%)]\tLoss: 1.389670\n",
      "Train Epoch: 17 [5120/9397 (54%)]\tLoss: 0.987528\n",
      "Train Epoch: 17 [5760/9397 (61%)]\tLoss: 1.118724\n",
      "Train Epoch: 17 [6400/9397 (68%)]\tLoss: 1.079773\n",
      "Train Epoch: 17 [7040/9397 (75%)]\tLoss: 1.291977\n",
      "Train Epoch: 17 [7680/9397 (82%)]\tLoss: 1.175728\n",
      "Train Epoch: 17 [8320/9397 (88%)]\tLoss: 1.070558\n",
      "Train Epoch: 17 [8960/9397 (95%)]\tLoss: 1.583848\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/1887 (0%)]\tLoss: 1.847284\n",
      "Train Epoch: 1 [640/1887 (33%)]\tLoss: 1.666285\n",
      "Train Epoch: 1 [1280/1887 (67%)]\tLoss: 1.851624\n",
      "Train Epoch: 2 [0/1887 (0%)]\tLoss: 1.582159\n",
      "Train Epoch: 2 [640/1887 (33%)]\tLoss: 1.663355\n",
      "Train Epoch: 2 [1280/1887 (67%)]\tLoss: 1.349793\n",
      "Train Epoch: 3 [0/1887 (0%)]\tLoss: 1.380245\n",
      "Train Epoch: 3 [640/1887 (33%)]\tLoss: 1.466345\n",
      "Train Epoch: 3 [1280/1887 (67%)]\tLoss: 1.533658\n",
      "Train Epoch: 4 [0/1887 (0%)]\tLoss: 1.428509\n",
      "Train Epoch: 4 [640/1887 (33%)]\tLoss: 1.511077\n",
      "Train Epoch: 4 [1280/1887 (67%)]\tLoss: 1.087012\n",
      "Train Epoch: 5 [0/1887 (0%)]\tLoss: 1.162809\n",
      "Train Epoch: 5 [640/1887 (33%)]\tLoss: 1.303918\n",
      "Train Epoch: 5 [1280/1887 (67%)]\tLoss: 1.171211\n",
      "Train Epoch: 6 [0/1887 (0%)]\tLoss: 1.305703\n",
      "Train Epoch: 6 [640/1887 (33%)]\tLoss: 1.219853\n",
      "Train Epoch: 6 [1280/1887 (67%)]\tLoss: 1.435132\n",
      "Train Epoch: 7 [0/1887 (0%)]\tLoss: 1.293042\n",
      "Train Epoch: 7 [640/1887 (33%)]\tLoss: 1.371629\n",
      "Train Epoch: 7 [1280/1887 (67%)]\tLoss: 1.289043\n",
      "Train Epoch: 8 [0/1887 (0%)]\tLoss: 1.642271\n",
      "Train Epoch: 8 [640/1887 (33%)]\tLoss: 1.176390\n",
      "Train Epoch: 8 [1280/1887 (67%)]\tLoss: 1.375322\n",
      "Train Epoch: 9 [0/1887 (0%)]\tLoss: 1.545043\n",
      "Train Epoch: 9 [640/1887 (33%)]\tLoss: 1.209134\n",
      "Train Epoch: 9 [1280/1887 (67%)]\tLoss: 1.347799\n",
      "Train Epoch: 10 [0/1887 (0%)]\tLoss: 1.347476\n",
      "Train Epoch: 10 [640/1887 (33%)]\tLoss: 1.186896\n",
      "Train Epoch: 10 [1280/1887 (67%)]\tLoss: 1.184749\n",
      "Train Epoch: 11 [0/1887 (0%)]\tLoss: 1.289459\n",
      "Train Epoch: 11 [640/1887 (33%)]\tLoss: 1.293310\n",
      "Train Epoch: 11 [1280/1887 (67%)]\tLoss: 1.200541\n",
      "Train Epoch: 12 [0/1887 (0%)]\tLoss: 1.111247\n",
      "Train Epoch: 12 [640/1887 (33%)]\tLoss: 1.386914\n",
      "Train Epoch: 12 [1280/1887 (67%)]\tLoss: 1.278702\n",
      "Train Epoch: 13 [0/1887 (0%)]\tLoss: 1.341997\n",
      "Train Epoch: 13 [640/1887 (33%)]\tLoss: 1.313210\n",
      "Train Epoch: 13 [1280/1887 (67%)]\tLoss: 1.318568\n",
      "Train Epoch: 14 [0/1887 (0%)]\tLoss: 0.980241\n",
      "Train Epoch: 14 [640/1887 (33%)]\tLoss: 1.216153\n",
      "Train Epoch: 14 [1280/1887 (67%)]\tLoss: 1.218750\n",
      "Train Epoch: 15 [0/1887 (0%)]\tLoss: 1.063565\n",
      "Train Epoch: 15 [640/1887 (33%)]\tLoss: 1.199498\n",
      "Train Epoch: 15 [1280/1887 (67%)]\tLoss: 1.260283\n",
      "Train Epoch: 16 [0/1887 (0%)]\tLoss: 1.185213\n",
      "Train Epoch: 16 [640/1887 (33%)]\tLoss: 1.021175\n",
      "Train Epoch: 16 [1280/1887 (67%)]\tLoss: 1.059569\n",
      "Train Epoch: 17 [0/1887 (0%)]\tLoss: 1.255008\n",
      "Train Epoch: 17 [640/1887 (33%)]\tLoss: 1.156726\n",
      "Train Epoch: 17 [1280/1887 (67%)]\tLoss: 1.308324\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/5490 (0%)]\tLoss: 1.792598\n",
      "Train Epoch: 1 [640/5490 (12%)]\tLoss: 1.669476\n",
      "Train Epoch: 1 [1280/5490 (23%)]\tLoss: 1.126402\n",
      "Train Epoch: 1 [1920/5490 (35%)]\tLoss: 1.442908\n",
      "Train Epoch: 1 [2560/5490 (47%)]\tLoss: 1.545745\n",
      "Train Epoch: 1 [3200/5490 (58%)]\tLoss: 1.301555\n",
      "Train Epoch: 1 [3840/5490 (70%)]\tLoss: 1.547486\n",
      "Train Epoch: 1 [4480/5490 (81%)]\tLoss: 1.420001\n",
      "Train Epoch: 1 [5120/5490 (93%)]\tLoss: 1.090286\n",
      "Train Epoch: 2 [0/5490 (0%)]\tLoss: 1.278340\n",
      "Train Epoch: 2 [640/5490 (12%)]\tLoss: 1.332532\n",
      "Train Epoch: 2 [1280/5490 (23%)]\tLoss: 1.534945\n",
      "Train Epoch: 2 [1920/5490 (35%)]\tLoss: 1.064957\n",
      "Train Epoch: 2 [2560/5490 (47%)]\tLoss: 1.381843\n",
      "Train Epoch: 2 [3200/5490 (58%)]\tLoss: 1.332255\n",
      "Train Epoch: 2 [3840/5490 (70%)]\tLoss: 1.389184\n",
      "Train Epoch: 2 [4480/5490 (81%)]\tLoss: 1.012324\n",
      "Train Epoch: 2 [5120/5490 (93%)]\tLoss: 1.089462\n",
      "Train Epoch: 3 [0/5490 (0%)]\tLoss: 1.239584\n",
      "Train Epoch: 3 [640/5490 (12%)]\tLoss: 1.414198\n",
      "Train Epoch: 3 [1280/5490 (23%)]\tLoss: 1.284155\n",
      "Train Epoch: 3 [1920/5490 (35%)]\tLoss: 1.119412\n",
      "Train Epoch: 3 [2560/5490 (47%)]\tLoss: 1.325534\n",
      "Train Epoch: 3 [3200/5490 (58%)]\tLoss: 1.037542\n",
      "Train Epoch: 3 [3840/5490 (70%)]\tLoss: 1.099245\n",
      "Train Epoch: 3 [4480/5490 (81%)]\tLoss: 1.384564\n",
      "Train Epoch: 3 [5120/5490 (93%)]\tLoss: 1.071293\n",
      "Train Epoch: 4 [0/5490 (0%)]\tLoss: 1.067847\n",
      "Train Epoch: 4 [640/5490 (12%)]\tLoss: 1.225972\n",
      "Train Epoch: 4 [1280/5490 (23%)]\tLoss: 1.181229\n",
      "Train Epoch: 4 [1920/5490 (35%)]\tLoss: 1.223793\n",
      "Train Epoch: 4 [2560/5490 (47%)]\tLoss: 1.261673\n",
      "Train Epoch: 4 [3200/5490 (58%)]\tLoss: 1.156669\n",
      "Train Epoch: 4 [3840/5490 (70%)]\tLoss: 1.333051\n",
      "Train Epoch: 4 [4480/5490 (81%)]\tLoss: 1.309654\n",
      "Train Epoch: 4 [5120/5490 (93%)]\tLoss: 0.997502\n",
      "Train Epoch: 5 [0/5490 (0%)]\tLoss: 0.973237\n",
      "Train Epoch: 5 [640/5490 (12%)]\tLoss: 1.137587\n",
      "Train Epoch: 5 [1280/5490 (23%)]\tLoss: 1.165833\n",
      "Train Epoch: 5 [1920/5490 (35%)]\tLoss: 1.132414\n",
      "Train Epoch: 5 [2560/5490 (47%)]\tLoss: 1.305414\n",
      "Train Epoch: 5 [3200/5490 (58%)]\tLoss: 1.067290\n",
      "Train Epoch: 5 [3840/5490 (70%)]\tLoss: 1.278462\n",
      "Train Epoch: 5 [4480/5490 (81%)]\tLoss: 0.918235\n",
      "Train Epoch: 5 [5120/5490 (93%)]\tLoss: 1.170944\n",
      "Train Epoch: 6 [0/5490 (0%)]\tLoss: 1.074108\n",
      "Train Epoch: 6 [640/5490 (12%)]\tLoss: 1.420354\n",
      "Train Epoch: 6 [1280/5490 (23%)]\tLoss: 1.193841\n",
      "Train Epoch: 6 [1920/5490 (35%)]\tLoss: 1.094444\n",
      "Train Epoch: 6 [2560/5490 (47%)]\tLoss: 1.175505\n",
      "Train Epoch: 6 [3200/5490 (58%)]\tLoss: 1.198956\n",
      "Train Epoch: 6 [3840/5490 (70%)]\tLoss: 1.266874\n",
      "Train Epoch: 6 [4480/5490 (81%)]\tLoss: 1.119319\n",
      "Train Epoch: 6 [5120/5490 (93%)]\tLoss: 1.430722\n",
      "Train Epoch: 7 [0/5490 (0%)]\tLoss: 1.130283\n",
      "Train Epoch: 7 [640/5490 (12%)]\tLoss: 1.250473\n",
      "Train Epoch: 7 [1280/5490 (23%)]\tLoss: 1.015708\n",
      "Train Epoch: 7 [1920/5490 (35%)]\tLoss: 1.187534\n",
      "Train Epoch: 7 [2560/5490 (47%)]\tLoss: 1.191168\n",
      "Train Epoch: 7 [3200/5490 (58%)]\tLoss: 1.152015\n",
      "Train Epoch: 7 [3840/5490 (70%)]\tLoss: 1.150243\n",
      "Train Epoch: 7 [4480/5490 (81%)]\tLoss: 0.995579\n",
      "Train Epoch: 7 [5120/5490 (93%)]\tLoss: 1.379069\n",
      "Train Epoch: 8 [0/5490 (0%)]\tLoss: 1.274016\n",
      "Train Epoch: 8 [640/5490 (12%)]\tLoss: 1.329599\n",
      "Train Epoch: 8 [1280/5490 (23%)]\tLoss: 1.522876\n",
      "Train Epoch: 8 [1920/5490 (35%)]\tLoss: 1.262244\n",
      "Train Epoch: 8 [2560/5490 (47%)]\tLoss: 1.061321\n",
      "Train Epoch: 8 [3200/5490 (58%)]\tLoss: 1.184132\n",
      "Train Epoch: 8 [3840/5490 (70%)]\tLoss: 1.103273\n",
      "Train Epoch: 8 [4480/5490 (81%)]\tLoss: 1.071836\n",
      "Train Epoch: 8 [5120/5490 (93%)]\tLoss: 1.356771\n",
      "Train Epoch: 9 [0/5490 (0%)]\tLoss: 1.426652\n",
      "Train Epoch: 9 [640/5490 (12%)]\tLoss: 1.145820\n",
      "Train Epoch: 9 [1280/5490 (23%)]\tLoss: 1.176834\n",
      "Train Epoch: 9 [1920/5490 (35%)]\tLoss: 1.182713\n",
      "Train Epoch: 9 [2560/5490 (47%)]\tLoss: 1.229376\n",
      "Train Epoch: 9 [3200/5490 (58%)]\tLoss: 1.441585\n",
      "Train Epoch: 9 [3840/5490 (70%)]\tLoss: 0.919128\n",
      "Train Epoch: 9 [4480/5490 (81%)]\tLoss: 1.211301\n",
      "Train Epoch: 9 [5120/5490 (93%)]\tLoss: 1.215159\n",
      "Train Epoch: 10 [0/5490 (0%)]\tLoss: 1.222769\n",
      "Train Epoch: 10 [640/5490 (12%)]\tLoss: 1.127112\n",
      "Train Epoch: 10 [1280/5490 (23%)]\tLoss: 1.111393\n",
      "Train Epoch: 10 [1920/5490 (35%)]\tLoss: 1.017419\n",
      "Train Epoch: 10 [2560/5490 (47%)]\tLoss: 1.131334\n",
      "Train Epoch: 10 [3200/5490 (58%)]\tLoss: 1.006601\n",
      "Train Epoch: 10 [3840/5490 (70%)]\tLoss: 0.905587\n",
      "Train Epoch: 10 [4480/5490 (81%)]\tLoss: 1.384695\n",
      "Train Epoch: 10 [5120/5490 (93%)]\tLoss: 0.969281\n",
      "Train Epoch: 11 [0/5490 (0%)]\tLoss: 0.974847\n",
      "Train Epoch: 11 [640/5490 (12%)]\tLoss: 1.251030\n",
      "Train Epoch: 11 [1280/5490 (23%)]\tLoss: 1.270245\n",
      "Train Epoch: 11 [1920/5490 (35%)]\tLoss: 1.070869\n",
      "Train Epoch: 11 [2560/5490 (47%)]\tLoss: 1.242142\n",
      "Train Epoch: 11 [3200/5490 (58%)]\tLoss: 0.905361\n",
      "Train Epoch: 11 [3840/5490 (70%)]\tLoss: 1.169017\n",
      "Train Epoch: 11 [4480/5490 (81%)]\tLoss: 1.101871\n",
      "Train Epoch: 11 [5120/5490 (93%)]\tLoss: 1.273700\n",
      "Train Epoch: 12 [0/5490 (0%)]\tLoss: 1.198598\n",
      "Train Epoch: 12 [640/5490 (12%)]\tLoss: 0.878111\n",
      "Train Epoch: 12 [1280/5490 (23%)]\tLoss: 0.835079\n",
      "Train Epoch: 12 [1920/5490 (35%)]\tLoss: 1.099080\n",
      "Train Epoch: 12 [2560/5490 (47%)]\tLoss: 1.123608\n",
      "Train Epoch: 12 [3200/5490 (58%)]\tLoss: 1.306085\n",
      "Train Epoch: 12 [3840/5490 (70%)]\tLoss: 1.095339\n",
      "Train Epoch: 12 [4480/5490 (81%)]\tLoss: 1.260849\n",
      "Train Epoch: 12 [5120/5490 (93%)]\tLoss: 1.143200\n",
      "Train Epoch: 13 [0/5490 (0%)]\tLoss: 1.578473\n",
      "Train Epoch: 13 [640/5490 (12%)]\tLoss: 1.155525\n",
      "Train Epoch: 13 [1280/5490 (23%)]\tLoss: 1.009712\n",
      "Train Epoch: 13 [1920/5490 (35%)]\tLoss: 1.061780\n",
      "Train Epoch: 13 [2560/5490 (47%)]\tLoss: 1.116719\n",
      "Train Epoch: 13 [3200/5490 (58%)]\tLoss: 1.069035\n",
      "Train Epoch: 13 [3840/5490 (70%)]\tLoss: 1.244987\n",
      "Train Epoch: 13 [4480/5490 (81%)]\tLoss: 1.091864\n",
      "Train Epoch: 13 [5120/5490 (93%)]\tLoss: 1.102944\n",
      "Train Epoch: 14 [0/5490 (0%)]\tLoss: 1.128848\n",
      "Train Epoch: 14 [640/5490 (12%)]\tLoss: 0.979746\n",
      "Train Epoch: 14 [1280/5490 (23%)]\tLoss: 1.372381\n",
      "Train Epoch: 14 [1920/5490 (35%)]\tLoss: 1.303738\n",
      "Train Epoch: 14 [2560/5490 (47%)]\tLoss: 1.010254\n",
      "Train Epoch: 14 [3200/5490 (58%)]\tLoss: 1.138016\n",
      "Train Epoch: 14 [3840/5490 (70%)]\tLoss: 0.920270\n",
      "Train Epoch: 14 [4480/5490 (81%)]\tLoss: 0.913237\n",
      "Train Epoch: 14 [5120/5490 (93%)]\tLoss: 0.874576\n",
      "Train Epoch: 15 [0/5490 (0%)]\tLoss: 1.027504\n",
      "Train Epoch: 15 [640/5490 (12%)]\tLoss: 0.976167\n",
      "Train Epoch: 15 [1280/5490 (23%)]\tLoss: 0.966097\n",
      "Train Epoch: 15 [1920/5490 (35%)]\tLoss: 0.808742\n",
      "Train Epoch: 15 [2560/5490 (47%)]\tLoss: 0.857719\n",
      "Train Epoch: 15 [3200/5490 (58%)]\tLoss: 1.201186\n",
      "Train Epoch: 15 [3840/5490 (70%)]\tLoss: 1.080795\n",
      "Train Epoch: 15 [4480/5490 (81%)]\tLoss: 1.151305\n",
      "Train Epoch: 15 [5120/5490 (93%)]\tLoss: 0.941486\n",
      "Train Epoch: 16 [0/5490 (0%)]\tLoss: 1.103308\n",
      "Train Epoch: 16 [640/5490 (12%)]\tLoss: 1.054156\n",
      "Train Epoch: 16 [1280/5490 (23%)]\tLoss: 1.179703\n",
      "Train Epoch: 16 [1920/5490 (35%)]\tLoss: 1.040370\n",
      "Train Epoch: 16 [2560/5490 (47%)]\tLoss: 0.950334\n",
      "Train Epoch: 16 [3200/5490 (58%)]\tLoss: 0.913242\n",
      "Train Epoch: 16 [3840/5490 (70%)]\tLoss: 0.972832\n",
      "Train Epoch: 16 [4480/5490 (81%)]\tLoss: 0.945363\n",
      "Train Epoch: 16 [5120/5490 (93%)]\tLoss: 1.012781\n",
      "Train Epoch: 17 [0/5490 (0%)]\tLoss: 0.908971\n",
      "Train Epoch: 17 [640/5490 (12%)]\tLoss: 1.112385\n",
      "Train Epoch: 17 [1280/5490 (23%)]\tLoss: 1.060459\n",
      "Train Epoch: 17 [1920/5490 (35%)]\tLoss: 1.175927\n",
      "Train Epoch: 17 [2560/5490 (47%)]\tLoss: 1.319481\n",
      "Train Epoch: 17 [3200/5490 (58%)]\tLoss: 0.949224\n",
      "Train Epoch: 17 [3840/5490 (70%)]\tLoss: 1.286841\n",
      "Train Epoch: 17 [4480/5490 (81%)]\tLoss: 1.105240\n",
      "Train Epoch: 17 [5120/5490 (93%)]\tLoss: 1.280098\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/2468 (0%)]\tLoss: 1.956116\n",
      "Train Epoch: 1 [640/2468 (26%)]\tLoss: 1.421108\n",
      "Train Epoch: 1 [1280/2468 (51%)]\tLoss: 1.288055\n",
      "Train Epoch: 1 [1920/2468 (77%)]\tLoss: 1.276129\n",
      "Train Epoch: 2 [0/2468 (0%)]\tLoss: 1.126382\n",
      "Train Epoch: 2 [640/2468 (26%)]\tLoss: 1.607392\n",
      "Train Epoch: 2 [1280/2468 (51%)]\tLoss: 1.231795\n",
      "Train Epoch: 2 [1920/2468 (77%)]\tLoss: 1.182726\n",
      "Train Epoch: 3 [0/2468 (0%)]\tLoss: 1.279699\n",
      "Train Epoch: 3 [640/2468 (26%)]\tLoss: 1.148169\n",
      "Train Epoch: 3 [1280/2468 (51%)]\tLoss: 1.088104\n",
      "Train Epoch: 3 [1920/2468 (77%)]\tLoss: 1.303734\n",
      "Train Epoch: 4 [0/2468 (0%)]\tLoss: 1.138249\n",
      "Train Epoch: 4 [640/2468 (26%)]\tLoss: 1.405649\n",
      "Train Epoch: 4 [1280/2468 (51%)]\tLoss: 1.328127\n",
      "Train Epoch: 4 [1920/2468 (77%)]\tLoss: 1.376759\n",
      "Train Epoch: 5 [0/2468 (0%)]\tLoss: 1.022803\n",
      "Train Epoch: 5 [640/2468 (26%)]\tLoss: 1.385474\n",
      "Train Epoch: 5 [1280/2468 (51%)]\tLoss: 1.293896\n",
      "Train Epoch: 5 [1920/2468 (77%)]\tLoss: 1.382829\n",
      "Train Epoch: 6 [0/2468 (0%)]\tLoss: 1.070186\n",
      "Train Epoch: 6 [640/2468 (26%)]\tLoss: 1.282711\n",
      "Train Epoch: 6 [1280/2468 (51%)]\tLoss: 1.112685\n",
      "Train Epoch: 6 [1920/2468 (77%)]\tLoss: 0.984503\n",
      "Train Epoch: 7 [0/2468 (0%)]\tLoss: 1.179595\n",
      "Train Epoch: 7 [640/2468 (26%)]\tLoss: 1.327072\n",
      "Train Epoch: 7 [1280/2468 (51%)]\tLoss: 1.407342\n",
      "Train Epoch: 7 [1920/2468 (77%)]\tLoss: 1.298884\n",
      "Train Epoch: 8 [0/2468 (0%)]\tLoss: 1.248161\n",
      "Train Epoch: 8 [640/2468 (26%)]\tLoss: 1.227083\n",
      "Train Epoch: 8 [1280/2468 (51%)]\tLoss: 1.324875\n",
      "Train Epoch: 8 [1920/2468 (77%)]\tLoss: 1.296045\n",
      "Train Epoch: 9 [0/2468 (0%)]\tLoss: 1.223859\n",
      "Train Epoch: 9 [640/2468 (26%)]\tLoss: 1.102827\n",
      "Train Epoch: 9 [1280/2468 (51%)]\tLoss: 1.031125\n",
      "Train Epoch: 9 [1920/2468 (77%)]\tLoss: 1.127779\n",
      "Train Epoch: 10 [0/2468 (0%)]\tLoss: 1.154280\n",
      "Train Epoch: 10 [640/2468 (26%)]\tLoss: 1.252148\n",
      "Train Epoch: 10 [1280/2468 (51%)]\tLoss: 1.481573\n",
      "Train Epoch: 10 [1920/2468 (77%)]\tLoss: 1.206053\n",
      "Train Epoch: 11 [0/2468 (0%)]\tLoss: 1.107710\n",
      "Train Epoch: 11 [640/2468 (26%)]\tLoss: 1.069694\n",
      "Train Epoch: 11 [1280/2468 (51%)]\tLoss: 0.966703\n",
      "Train Epoch: 11 [1920/2468 (77%)]\tLoss: 1.126022\n",
      "Train Epoch: 12 [0/2468 (0%)]\tLoss: 1.048838\n",
      "Train Epoch: 12 [640/2468 (26%)]\tLoss: 1.105444\n",
      "Train Epoch: 12 [1280/2468 (51%)]\tLoss: 1.102935\n",
      "Train Epoch: 12 [1920/2468 (77%)]\tLoss: 1.278802\n",
      "Train Epoch: 13 [0/2468 (0%)]\tLoss: 1.060377\n",
      "Train Epoch: 13 [640/2468 (26%)]\tLoss: 1.074161\n",
      "Train Epoch: 13 [1280/2468 (51%)]\tLoss: 1.214273\n",
      "Train Epoch: 13 [1920/2468 (77%)]\tLoss: 1.228164\n",
      "Train Epoch: 14 [0/2468 (0%)]\tLoss: 0.948052\n",
      "Train Epoch: 14 [640/2468 (26%)]\tLoss: 1.071562\n",
      "Train Epoch: 14 [1280/2468 (51%)]\tLoss: 1.101767\n",
      "Train Epoch: 14 [1920/2468 (77%)]\tLoss: 1.129259\n",
      "Train Epoch: 15 [0/2468 (0%)]\tLoss: 1.175335\n",
      "Train Epoch: 15 [640/2468 (26%)]\tLoss: 1.071328\n",
      "Train Epoch: 15 [1280/2468 (51%)]\tLoss: 1.160755\n",
      "Train Epoch: 15 [1920/2468 (77%)]\tLoss: 1.150788\n",
      "Train Epoch: 16 [0/2468 (0%)]\tLoss: 1.246298\n",
      "Train Epoch: 16 [640/2468 (26%)]\tLoss: 1.098821\n",
      "Train Epoch: 16 [1280/2468 (51%)]\tLoss: 1.401202\n",
      "Train Epoch: 16 [1920/2468 (77%)]\tLoss: 1.045683\n",
      "Train Epoch: 17 [0/2468 (0%)]\tLoss: 1.027289\n",
      "Train Epoch: 17 [640/2468 (26%)]\tLoss: 1.120508\n",
      "Train Epoch: 17 [1280/2468 (51%)]\tLoss: 0.994428\n",
      "Train Epoch: 17 [1920/2468 (77%)]\tLoss: 1.070686\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.4051, Accuracy: 4924/10000 (49%)\n",
      "\n",
      "Round 2/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.468770\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.419918\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.457780\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.300133\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.403502\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.224346\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.500317\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.372590\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.373124\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.280425\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.090397\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.481977\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.406195\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.458312\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.407980\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.301199\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.419335\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.244678\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.179498\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.462952\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.257797\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.298022\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.428632\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.384330\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.313980\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.144865\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.395617\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.280371\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.350193\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.407633\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.110112\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.206969\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 1.358597\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.233729\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.313528\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.227034\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.350471\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.155265\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.112943\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.194943\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.252975\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.274425\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.174278\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.394277\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.237606\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.374542\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.337671\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.162212\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.137430\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.201992\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.137012\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.328207\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.305933\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.265707\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.301638\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.547538\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.352231\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.069653\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.324118\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.347117\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.356873\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.206146\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.091697\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.236941\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.296404\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.327378\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.266432\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.285775\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.096663\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.305555\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.200875\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.229758\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.280623\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.143410\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.416250\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.227699\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.473326\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.178447\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 1.083338\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.289475\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.116116\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.171983\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.219566\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.171047\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.252572\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.142676\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.342166\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.132583\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 0.952087\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.210701\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.364337\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.208305\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.218389\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.106322\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.241950\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.287906\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.286783\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.215097\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.222405\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 1.150248\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.218139\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.152512\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.678624\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.499170\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.606016\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.320576\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.140220\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.292243\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.486018\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.349539\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.350191\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.327935\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.229834\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.325897\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.664870\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.235845\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.302563\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.457554\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.293646\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.316188\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.253240\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.519456\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.195884\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.300952\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.111291\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.352500\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.185613\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.434441\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.488963\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.158855\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.404464\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.167859\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.218850\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.252582\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.216581\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.268490\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.612172\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.462903\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.274839\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.591627\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.128829\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.438856\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.185170\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 1.539986\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.224213\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.404713\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.294392\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.242866\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.250989\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.215923\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.333878\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.553413\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.160832\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.400811\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.165217\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.361189\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.200044\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.058683\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.536367\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.452352\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.283828\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.196420\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 1.246447\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.202404\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.058982\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.142278\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.184105\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.418353\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.242979\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.152772\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.389454\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.304903\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.142723\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.050771\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.216293\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.208146\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.127337\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.174431\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.393032\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.280584\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 1.263643\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.271209\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.275087\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.091623\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.317352\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.105538\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.224699\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.286038\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 1.309108\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.140106\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.442268\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.146937\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.251626\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.252998\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.126444\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.257814\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.411365\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.026713\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.288905\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.100587\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.116102\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.180512\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.372838\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.067595\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.435211\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.150355\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 0.986238\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.223194\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.268013\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 0.979743\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 1.428239\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.205417\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.230255\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 1.230549\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 1.150655\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.236385\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 1.085096\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.119322\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 0.951608\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.349102\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 1.212435\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 1.087429\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.220786\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.347546\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 0.895067\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 0.949442\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.141214\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.259274\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.183661\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.241968\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 1.211078\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 0.836190\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 1.030041\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 1.046982\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 1.373547\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 1.247045\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 0.981917\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 1.068759\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 1.191361\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 1.171272\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.184828\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.023282\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 1.092114\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 1.074526\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 1.206582\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 0.924449\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 1.072688\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 0.914341\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 1.152417\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 1.009281\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 1.058630\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 0.958158\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 1.152194\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 0.916604\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 1.140794\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.001370\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 1.083896\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 1.006101\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 0.924702\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 0.943501\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 1.217242\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 0.929568\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 1.148261\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 1.062664\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 1.031321\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 0.925849\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 1.087763\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 1.171393\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 1.125083\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 0.907510\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.038513\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 1.105213\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.995926\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 1.177398\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 0.901648\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 0.938346\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.186063\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 0.964532\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.014080\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 1.048467\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.105702\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 1.196938\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 1.163642\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 0.932145\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 1.147769\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 1.016120\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 1.075877\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 0.909931\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 1.070035\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 1.081468\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 1.035258\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 1.163304\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 1.529479\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 1.096682\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 1.138167\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 0.924425\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 1.158046\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 1.014518\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 1.223379\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 1.087192\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 0.887617\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.092474\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 1.158399\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 0.890936\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 0.819865\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 1.047714\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 0.877090\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 1.172664\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 0.770434\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 1.166816\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 1.083833\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 0.929700\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 0.989994\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 1.106232\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 0.816250\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 1.277205\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.187646\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 0.929048\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 0.882265\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 1.090090\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 1.134841\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 0.773969\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 1.216711\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 1.139099\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 1.011915\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 1.134667\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 0.937046\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 0.777722\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 1.080450\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 0.960930\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 0.915118\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 0.800779\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 1.044474\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 0.993612\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 0.774411\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 1.191360\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 1.045579\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 1.138867\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 1.002987\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 0.938917\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 1.058823\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 1.076703\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 0.899028\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 1.255027\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 1.082559\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 1.080751\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.861787\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 1.046309\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 1.001190\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 1.194147\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 0.821511\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 0.949350\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 1.248159\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 1.119923\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 1.010448\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 1.002720\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 0.964427\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 1.078092\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 0.934352\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.913985\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 0.895625\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 0.972185\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 0.897733\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.107990\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 1.186855\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 1.152708\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 1.169320\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 1.136607\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 0.884995\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 0.788724\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.965640\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 1.048466\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.976390\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 1.073507\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.378978\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.091960\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 1.109480\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 1.203610\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.198010\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.083616\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 1.262340\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.202679\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 0.962092\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.182640\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 1.036532\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.941664\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 1.124950\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 1.064480\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.271149\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.105534\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 1.163717\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 1.090216\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 1.069090\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 1.115210\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 1.115899\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 1.064329\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 1.015208\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 0.836019\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 1.023009\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 1.150972\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 1.205040\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 0.845272\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 1.275418\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 1.041298\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 1.084102\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 1.060980\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 1.112320\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 0.931421\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 1.040640\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 1.222483\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 1.031565\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 0.936590\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 0.991958\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 1.000624\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 1.123632\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 1.284052\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 1.089285\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 1.257823\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 0.860648\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 1.058622\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 1.143245\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 0.883252\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 0.972179\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 0.875965\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 1.279417\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 1.009279\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 0.939597\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 1.015113\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 0.959749\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.892798\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 0.922161\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 1.009373\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 1.010106\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 0.991097\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 1.128029\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 1.081622\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 1.423183\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 1.067255\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 0.994033\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 0.856958\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 1.105508\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 1.002660\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 1.056669\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 1.022484\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 1.000468\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 0.944638\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 1.032300\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 1.073227\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 0.972735\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 1.034103\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 0.798853\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 0.996897\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 1.130267\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 1.101318\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 0.856956\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.948997\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 1.025754\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.980661\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 1.011963\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 1.172791\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 0.845479\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.792866\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 0.769638\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 0.944308\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 0.871966\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 0.971325\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 0.985136\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 0.910794\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 0.919840\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 1.051269\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 0.920539\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.800255\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 0.967761\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 1.176447\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 0.965156\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.923735\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 0.845475\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 1.107023\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 1.024179\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 1.090014\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 0.926352\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 0.762683\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 1.289767\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 1.011313\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 1.061478\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.889310\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 1.201287\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 1.117005\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 0.967121\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 0.877200\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.952125\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 1.047767\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 1.118815\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 1.208564\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 0.885570\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 0.929031\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 0.867098\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 0.912612\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 1.121322\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.737579\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.882019\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 0.915238\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 0.864947\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 0.894539\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.889124\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 1.114648\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 0.947695\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 1.173017\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.915413\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 0.833343\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 1.013068\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 1.216833\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.794248\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 0.877268\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 1.013284\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 0.949814\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 0.973629\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 0.917505\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.943296\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 1.077396\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 1.001775\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 1.130517\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 0.854429\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.130180\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.892621\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 1.076327\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 1.209633\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.824772\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 1.293848\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 1.131022\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 1.120722\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 0.805081\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 0.957601\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.926310\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.954918\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 0.733296\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 0.991645\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.762847\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 1.140541\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 1.176577\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 1.196677\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 1.102305\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.896407\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 1.093863\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 1.036524\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 1.095715\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.898737\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.999095\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 1.100903\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 1.116510\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 1.079774\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.852898\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 0.944176\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 0.852523\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.993667\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.945974\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 1.120579\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.908097\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 1.003928\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 1.027070\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 0.784984\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.577008\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.525444\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.464741\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.378585\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.366920\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.265976\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.068197\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.357362\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.246557\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.255130\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.371121\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.402285\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.564614\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.709358\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.318810\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.400003\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.307177\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.512855\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.433969\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.444792\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.393703\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.229534\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.329449\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.203750\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.199450\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.463329\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.052630\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.563966\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.183641\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.490310\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.317634\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.258336\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.097879\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.328792\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.218572\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.389358\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.268402\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.389509\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.319775\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.402064\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.433644\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 1.147317\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.326166\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.331722\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.196186\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.223904\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.233548\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.264162\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.371456\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.325176\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.167108\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.336066\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.238225\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.255645\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.345714\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 1.369870\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.429430\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.389496\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.618079\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.415285\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.551212\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.514116\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.261304\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.137955\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.338586\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.429441\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.055592\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.189870\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.450397\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 1.423720\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.418545\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.382588\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 1.141179\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.132464\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.169927\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.140299\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.321595\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.142069\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 0.994588\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.302419\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 1.731434\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.419649\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.164069\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 1.259012\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.087986\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.324484\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.200053\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.125793\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.145097\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.573370\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.298703\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.369309\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.299257\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.291996\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 0.999834\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.203087\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.294217\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 1.664606\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.106905\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.225531\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.171672\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.245822\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.337299\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.128708\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.312681\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.124882\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.353674\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.521149\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.234538\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.173753\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.129677\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 1.182685\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.522350\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.006107\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.330249\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.018776\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.224685\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.257412\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.160178\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.093896\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.217311\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.352207\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.251933\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.357626\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.301016\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 1.323060\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 1.287172\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.060582\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.208700\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.147776\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.147098\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.172566\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.051332\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.172480\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.429166\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.279541\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.381339\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.172110\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.094215\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 1.185050\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.285583\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.211100\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.121523\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.137895\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.299257\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.357056\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.360847\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.109276\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.328464\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.402456\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.066998\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.313993\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 1.291773\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 0.994342\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.207049\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.063673\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 1.370790\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 1.120669\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 1.271143\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 1.347666\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.149406\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.256551\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 0.987272\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.455405\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.404016\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.116861\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.052319\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 1.356398\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.194206\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.090594\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.047311\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.071968\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.214157\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 0.941152\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 1.279133\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.114457\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 1.114630\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.362604\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.346148\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.375816\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.094970\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 0.940920\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 1.244769\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.248273\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.297696\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 1.028060\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 1.374264\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 1.407410\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.453065\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.159378\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.175580\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.094374\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 1.052955\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.219700\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.124536\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 0.821743\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.176637\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.187927\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.199469\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.261908\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 1.252960\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.180466\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.023114\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 1.299909\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.214804\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.032500\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 1.059606\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.057675\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 1.300729\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 0.845056\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 1.342272\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.138666\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.338573\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.094440\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.240018\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.203831\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.083088\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.240220\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 1.343591\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 1.110014\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.185319\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.047438\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.044299\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 0.748017\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.230158\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.019441\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 1.076299\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 0.970763\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.447613\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.112861\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.263644\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.069055\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.108675\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.250134\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.094831\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.459610\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.222208\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 2.401609\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.862662\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.328613\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.485443\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.346590\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.236245\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.308435\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.542632\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.325524\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.686313\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.363071\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.197334\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.413315\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.220735\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.456655\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.214853\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.537476\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.560604\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.219800\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.194636\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.372915\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.222623\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.249037\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.342867\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.247034\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.330732\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.410360\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.358579\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.538440\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.111221\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.350529\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.241868\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.191184\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.242109\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.316110\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.327576\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.392416\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.189901\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.367812\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.324272\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.563033\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.222680\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.473708\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.379236\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.308977\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.229838\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.480174\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.563667\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.394314\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.209807\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.194804\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.125100\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.146484\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.250849\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.467140\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.364531\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.254471\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.338971\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.414565\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.204589\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.534617\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.173640\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.363918\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.407115\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.414342\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.144868\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 1.372064\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.207162\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.151311\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 1.342002\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.261372\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.369129\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.384610\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.270758\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.289713\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 1.430473\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 1.052351\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 1.121535\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.389271\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.225827\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.253793\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.418559\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.189992\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.355322\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.217835\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.253653\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/9397 (0%)]\tLoss: 1.557088\n",
      "Train Epoch: 1 [640/9397 (7%)]\tLoss: 1.324668\n",
      "Train Epoch: 1 [1280/9397 (14%)]\tLoss: 1.209478\n",
      "Train Epoch: 1 [1920/9397 (20%)]\tLoss: 1.493770\n",
      "Train Epoch: 1 [2560/9397 (27%)]\tLoss: 1.111332\n",
      "Train Epoch: 1 [3200/9397 (34%)]\tLoss: 1.310563\n",
      "Train Epoch: 1 [3840/9397 (41%)]\tLoss: 1.436101\n",
      "Train Epoch: 1 [4480/9397 (48%)]\tLoss: 1.239166\n",
      "Train Epoch: 1 [5120/9397 (54%)]\tLoss: 1.435753\n",
      "Train Epoch: 1 [5760/9397 (61%)]\tLoss: 1.300939\n",
      "Train Epoch: 1 [6400/9397 (68%)]\tLoss: 1.406397\n",
      "Train Epoch: 1 [7040/9397 (75%)]\tLoss: 1.487830\n",
      "Train Epoch: 1 [7680/9397 (82%)]\tLoss: 1.214867\n",
      "Train Epoch: 1 [8320/9397 (88%)]\tLoss: 1.345485\n",
      "Train Epoch: 1 [8960/9397 (95%)]\tLoss: 1.246192\n",
      "Train Epoch: 2 [0/9397 (0%)]\tLoss: 1.376616\n",
      "Train Epoch: 2 [640/9397 (7%)]\tLoss: 1.053350\n",
      "Train Epoch: 2 [1280/9397 (14%)]\tLoss: 1.231226\n",
      "Train Epoch: 2 [1920/9397 (20%)]\tLoss: 1.322703\n",
      "Train Epoch: 2 [2560/9397 (27%)]\tLoss: 1.234381\n",
      "Train Epoch: 2 [3200/9397 (34%)]\tLoss: 1.401720\n",
      "Train Epoch: 2 [3840/9397 (41%)]\tLoss: 1.402699\n",
      "Train Epoch: 2 [4480/9397 (48%)]\tLoss: 1.161196\n",
      "Train Epoch: 2 [5120/9397 (54%)]\tLoss: 1.354345\n",
      "Train Epoch: 2 [5760/9397 (61%)]\tLoss: 1.226225\n",
      "Train Epoch: 2 [6400/9397 (68%)]\tLoss: 1.207947\n",
      "Train Epoch: 2 [7040/9397 (75%)]\tLoss: 1.051229\n",
      "Train Epoch: 2 [7680/9397 (82%)]\tLoss: 1.212577\n",
      "Train Epoch: 2 [8320/9397 (88%)]\tLoss: 1.191073\n",
      "Train Epoch: 2 [8960/9397 (95%)]\tLoss: 1.471743\n",
      "Train Epoch: 3 [0/9397 (0%)]\tLoss: 1.459779\n",
      "Train Epoch: 3 [640/9397 (7%)]\tLoss: 1.307220\n",
      "Train Epoch: 3 [1280/9397 (14%)]\tLoss: 1.120100\n",
      "Train Epoch: 3 [1920/9397 (20%)]\tLoss: 1.706976\n",
      "Train Epoch: 3 [2560/9397 (27%)]\tLoss: 1.315292\n",
      "Train Epoch: 3 [3200/9397 (34%)]\tLoss: 1.181815\n",
      "Train Epoch: 3 [3840/9397 (41%)]\tLoss: 1.142631\n",
      "Train Epoch: 3 [4480/9397 (48%)]\tLoss: 1.363221\n",
      "Train Epoch: 3 [5120/9397 (54%)]\tLoss: 1.208051\n",
      "Train Epoch: 3 [5760/9397 (61%)]\tLoss: 1.135419\n",
      "Train Epoch: 3 [6400/9397 (68%)]\tLoss: 1.196672\n",
      "Train Epoch: 3 [7040/9397 (75%)]\tLoss: 1.255658\n",
      "Train Epoch: 3 [7680/9397 (82%)]\tLoss: 1.197698\n",
      "Train Epoch: 3 [8320/9397 (88%)]\tLoss: 1.111853\n",
      "Train Epoch: 3 [8960/9397 (95%)]\tLoss: 1.056439\n",
      "Train Epoch: 4 [0/9397 (0%)]\tLoss: 1.151324\n",
      "Train Epoch: 4 [640/9397 (7%)]\tLoss: 1.108703\n",
      "Train Epoch: 4 [1280/9397 (14%)]\tLoss: 1.221444\n",
      "Train Epoch: 4 [1920/9397 (20%)]\tLoss: 1.236437\n",
      "Train Epoch: 4 [2560/9397 (27%)]\tLoss: 1.398696\n",
      "Train Epoch: 4 [3200/9397 (34%)]\tLoss: 1.187644\n",
      "Train Epoch: 4 [3840/9397 (41%)]\tLoss: 1.135172\n",
      "Train Epoch: 4 [4480/9397 (48%)]\tLoss: 1.468209\n",
      "Train Epoch: 4 [5120/9397 (54%)]\tLoss: 1.328462\n",
      "Train Epoch: 4 [5760/9397 (61%)]\tLoss: 1.305981\n",
      "Train Epoch: 4 [6400/9397 (68%)]\tLoss: 1.233112\n",
      "Train Epoch: 4 [7040/9397 (75%)]\tLoss: 1.192024\n",
      "Train Epoch: 4 [7680/9397 (82%)]\tLoss: 1.292699\n",
      "Train Epoch: 4 [8320/9397 (88%)]\tLoss: 1.242446\n",
      "Train Epoch: 4 [8960/9397 (95%)]\tLoss: 1.253400\n",
      "Train Epoch: 5 [0/9397 (0%)]\tLoss: 1.344169\n",
      "Train Epoch: 5 [640/9397 (7%)]\tLoss: 1.111503\n",
      "Train Epoch: 5 [1280/9397 (14%)]\tLoss: 1.162792\n",
      "Train Epoch: 5 [1920/9397 (20%)]\tLoss: 1.338861\n",
      "Train Epoch: 5 [2560/9397 (27%)]\tLoss: 1.251182\n",
      "Train Epoch: 5 [3200/9397 (34%)]\tLoss: 1.186158\n",
      "Train Epoch: 5 [3840/9397 (41%)]\tLoss: 1.172872\n",
      "Train Epoch: 5 [4480/9397 (48%)]\tLoss: 1.093889\n",
      "Train Epoch: 5 [5120/9397 (54%)]\tLoss: 1.220022\n",
      "Train Epoch: 5 [5760/9397 (61%)]\tLoss: 1.127032\n",
      "Train Epoch: 5 [6400/9397 (68%)]\tLoss: 1.032473\n",
      "Train Epoch: 5 [7040/9397 (75%)]\tLoss: 1.224114\n",
      "Train Epoch: 5 [7680/9397 (82%)]\tLoss: 1.420883\n",
      "Train Epoch: 5 [8320/9397 (88%)]\tLoss: 1.432432\n",
      "Train Epoch: 5 [8960/9397 (95%)]\tLoss: 1.182552\n",
      "Train Epoch: 6 [0/9397 (0%)]\tLoss: 1.319240\n",
      "Train Epoch: 6 [640/9397 (7%)]\tLoss: 1.116910\n",
      "Train Epoch: 6 [1280/9397 (14%)]\tLoss: 1.149629\n",
      "Train Epoch: 6 [1920/9397 (20%)]\tLoss: 1.180422\n",
      "Train Epoch: 6 [2560/9397 (27%)]\tLoss: 1.222510\n",
      "Train Epoch: 6 [3200/9397 (34%)]\tLoss: 1.346450\n",
      "Train Epoch: 6 [3840/9397 (41%)]\tLoss: 1.139213\n",
      "Train Epoch: 6 [4480/9397 (48%)]\tLoss: 1.393991\n",
      "Train Epoch: 6 [5120/9397 (54%)]\tLoss: 1.180542\n",
      "Train Epoch: 6 [5760/9397 (61%)]\tLoss: 1.205923\n",
      "Train Epoch: 6 [6400/9397 (68%)]\tLoss: 1.087878\n",
      "Train Epoch: 6 [7040/9397 (75%)]\tLoss: 1.246421\n",
      "Train Epoch: 6 [7680/9397 (82%)]\tLoss: 1.386838\n",
      "Train Epoch: 6 [8320/9397 (88%)]\tLoss: 1.040378\n",
      "Train Epoch: 6 [8960/9397 (95%)]\tLoss: 1.132080\n",
      "Train Epoch: 7 [0/9397 (0%)]\tLoss: 1.179794\n",
      "Train Epoch: 7 [640/9397 (7%)]\tLoss: 1.059795\n",
      "Train Epoch: 7 [1280/9397 (14%)]\tLoss: 1.276051\n",
      "Train Epoch: 7 [1920/9397 (20%)]\tLoss: 1.015432\n",
      "Train Epoch: 7 [2560/9397 (27%)]\tLoss: 0.979599\n",
      "Train Epoch: 7 [3200/9397 (34%)]\tLoss: 1.127752\n",
      "Train Epoch: 7 [3840/9397 (41%)]\tLoss: 1.123674\n",
      "Train Epoch: 7 [4480/9397 (48%)]\tLoss: 1.132051\n",
      "Train Epoch: 7 [5120/9397 (54%)]\tLoss: 1.197450\n",
      "Train Epoch: 7 [5760/9397 (61%)]\tLoss: 1.353238\n",
      "Train Epoch: 7 [6400/9397 (68%)]\tLoss: 1.255311\n",
      "Train Epoch: 7 [7040/9397 (75%)]\tLoss: 1.166243\n",
      "Train Epoch: 7 [7680/9397 (82%)]\tLoss: 1.260967\n",
      "Train Epoch: 7 [8320/9397 (88%)]\tLoss: 1.052086\n",
      "Train Epoch: 7 [8960/9397 (95%)]\tLoss: 1.373748\n",
      "Train Epoch: 8 [0/9397 (0%)]\tLoss: 1.220611\n",
      "Train Epoch: 8 [640/9397 (7%)]\tLoss: 1.036901\n",
      "Train Epoch: 8 [1280/9397 (14%)]\tLoss: 0.940469\n",
      "Train Epoch: 8 [1920/9397 (20%)]\tLoss: 1.220483\n",
      "Train Epoch: 8 [2560/9397 (27%)]\tLoss: 1.347883\n",
      "Train Epoch: 8 [3200/9397 (34%)]\tLoss: 1.236106\n",
      "Train Epoch: 8 [3840/9397 (41%)]\tLoss: 1.050274\n",
      "Train Epoch: 8 [4480/9397 (48%)]\tLoss: 1.057110\n",
      "Train Epoch: 8 [5120/9397 (54%)]\tLoss: 1.007311\n",
      "Train Epoch: 8 [5760/9397 (61%)]\tLoss: 1.340681\n",
      "Train Epoch: 8 [6400/9397 (68%)]\tLoss: 1.121122\n",
      "Train Epoch: 8 [7040/9397 (75%)]\tLoss: 1.224287\n",
      "Train Epoch: 8 [7680/9397 (82%)]\tLoss: 1.385241\n",
      "Train Epoch: 8 [8320/9397 (88%)]\tLoss: 1.356127\n",
      "Train Epoch: 8 [8960/9397 (95%)]\tLoss: 1.196207\n",
      "Train Epoch: 9 [0/9397 (0%)]\tLoss: 1.208556\n",
      "Train Epoch: 9 [640/9397 (7%)]\tLoss: 1.242284\n",
      "Train Epoch: 9 [1280/9397 (14%)]\tLoss: 1.066030\n",
      "Train Epoch: 9 [1920/9397 (20%)]\tLoss: 0.991089\n",
      "Train Epoch: 9 [2560/9397 (27%)]\tLoss: 1.177116\n",
      "Train Epoch: 9 [3200/9397 (34%)]\tLoss: 1.274212\n",
      "Train Epoch: 9 [3840/9397 (41%)]\tLoss: 1.148987\n",
      "Train Epoch: 9 [4480/9397 (48%)]\tLoss: 1.321566\n",
      "Train Epoch: 9 [5120/9397 (54%)]\tLoss: 1.378497\n",
      "Train Epoch: 9 [5760/9397 (61%)]\tLoss: 1.265089\n",
      "Train Epoch: 9 [6400/9397 (68%)]\tLoss: 1.082695\n",
      "Train Epoch: 9 [7040/9397 (75%)]\tLoss: 1.202616\n",
      "Train Epoch: 9 [7680/9397 (82%)]\tLoss: 1.506284\n",
      "Train Epoch: 9 [8320/9397 (88%)]\tLoss: 1.033777\n",
      "Train Epoch: 9 [8960/9397 (95%)]\tLoss: 1.403268\n",
      "Train Epoch: 10 [0/9397 (0%)]\tLoss: 1.417915\n",
      "Train Epoch: 10 [640/9397 (7%)]\tLoss: 1.124782\n",
      "Train Epoch: 10 [1280/9397 (14%)]\tLoss: 1.432228\n",
      "Train Epoch: 10 [1920/9397 (20%)]\tLoss: 1.272585\n",
      "Train Epoch: 10 [2560/9397 (27%)]\tLoss: 1.223372\n",
      "Train Epoch: 10 [3200/9397 (34%)]\tLoss: 1.356045\n",
      "Train Epoch: 10 [3840/9397 (41%)]\tLoss: 1.052621\n",
      "Train Epoch: 10 [4480/9397 (48%)]\tLoss: 1.095897\n",
      "Train Epoch: 10 [5120/9397 (54%)]\tLoss: 1.257441\n",
      "Train Epoch: 10 [5760/9397 (61%)]\tLoss: 1.275313\n",
      "Train Epoch: 10 [6400/9397 (68%)]\tLoss: 1.233530\n",
      "Train Epoch: 10 [7040/9397 (75%)]\tLoss: 1.287800\n",
      "Train Epoch: 10 [7680/9397 (82%)]\tLoss: 1.185373\n",
      "Train Epoch: 10 [8320/9397 (88%)]\tLoss: 1.104356\n",
      "Train Epoch: 10 [8960/9397 (95%)]\tLoss: 1.164119\n",
      "Train Epoch: 11 [0/9397 (0%)]\tLoss: 1.106403\n",
      "Train Epoch: 11 [640/9397 (7%)]\tLoss: 1.302573\n",
      "Train Epoch: 11 [1280/9397 (14%)]\tLoss: 1.144896\n",
      "Train Epoch: 11 [1920/9397 (20%)]\tLoss: 1.198060\n",
      "Train Epoch: 11 [2560/9397 (27%)]\tLoss: 1.201520\n",
      "Train Epoch: 11 [3200/9397 (34%)]\tLoss: 1.172662\n",
      "Train Epoch: 11 [3840/9397 (41%)]\tLoss: 1.141283\n",
      "Train Epoch: 11 [4480/9397 (48%)]\tLoss: 1.072935\n",
      "Train Epoch: 11 [5120/9397 (54%)]\tLoss: 1.295383\n",
      "Train Epoch: 11 [5760/9397 (61%)]\tLoss: 1.017843\n",
      "Train Epoch: 11 [6400/9397 (68%)]\tLoss: 1.190012\n",
      "Train Epoch: 11 [7040/9397 (75%)]\tLoss: 1.121123\n",
      "Train Epoch: 11 [7680/9397 (82%)]\tLoss: 0.993850\n",
      "Train Epoch: 11 [8320/9397 (88%)]\tLoss: 1.261462\n",
      "Train Epoch: 11 [8960/9397 (95%)]\tLoss: 1.017679\n",
      "Train Epoch: 12 [0/9397 (0%)]\tLoss: 1.053084\n",
      "Train Epoch: 12 [640/9397 (7%)]\tLoss: 1.147386\n",
      "Train Epoch: 12 [1280/9397 (14%)]\tLoss: 0.993288\n",
      "Train Epoch: 12 [1920/9397 (20%)]\tLoss: 1.282969\n",
      "Train Epoch: 12 [2560/9397 (27%)]\tLoss: 1.080788\n",
      "Train Epoch: 12 [3200/9397 (34%)]\tLoss: 1.285890\n",
      "Train Epoch: 12 [3840/9397 (41%)]\tLoss: 1.295180\n",
      "Train Epoch: 12 [4480/9397 (48%)]\tLoss: 1.153119\n",
      "Train Epoch: 12 [5120/9397 (54%)]\tLoss: 0.972073\n",
      "Train Epoch: 12 [5760/9397 (61%)]\tLoss: 1.097169\n",
      "Train Epoch: 12 [6400/9397 (68%)]\tLoss: 1.231474\n",
      "Train Epoch: 12 [7040/9397 (75%)]\tLoss: 1.220417\n",
      "Train Epoch: 12 [7680/9397 (82%)]\tLoss: 1.366959\n",
      "Train Epoch: 12 [8320/9397 (88%)]\tLoss: 1.460017\n",
      "Train Epoch: 12 [8960/9397 (95%)]\tLoss: 1.203912\n",
      "Train Epoch: 13 [0/9397 (0%)]\tLoss: 1.027397\n",
      "Train Epoch: 13 [640/9397 (7%)]\tLoss: 1.071344\n",
      "Train Epoch: 13 [1280/9397 (14%)]\tLoss: 1.195223\n",
      "Train Epoch: 13 [1920/9397 (20%)]\tLoss: 1.170750\n",
      "Train Epoch: 13 [2560/9397 (27%)]\tLoss: 1.000344\n",
      "Train Epoch: 13 [3200/9397 (34%)]\tLoss: 0.842517\n",
      "Train Epoch: 13 [3840/9397 (41%)]\tLoss: 1.224895\n",
      "Train Epoch: 13 [4480/9397 (48%)]\tLoss: 0.986133\n",
      "Train Epoch: 13 [5120/9397 (54%)]\tLoss: 1.069648\n",
      "Train Epoch: 13 [5760/9397 (61%)]\tLoss: 1.196057\n",
      "Train Epoch: 13 [6400/9397 (68%)]\tLoss: 1.258914\n",
      "Train Epoch: 13 [7040/9397 (75%)]\tLoss: 1.101529\n",
      "Train Epoch: 13 [7680/9397 (82%)]\tLoss: 1.312615\n",
      "Train Epoch: 13 [8320/9397 (88%)]\tLoss: 1.010903\n",
      "Train Epoch: 13 [8960/9397 (95%)]\tLoss: 1.155379\n",
      "Train Epoch: 14 [0/9397 (0%)]\tLoss: 1.143917\n",
      "Train Epoch: 14 [640/9397 (7%)]\tLoss: 1.207257\n",
      "Train Epoch: 14 [1280/9397 (14%)]\tLoss: 1.243071\n",
      "Train Epoch: 14 [1920/9397 (20%)]\tLoss: 1.104494\n",
      "Train Epoch: 14 [2560/9397 (27%)]\tLoss: 1.168981\n",
      "Train Epoch: 14 [3200/9397 (34%)]\tLoss: 1.194627\n",
      "Train Epoch: 14 [3840/9397 (41%)]\tLoss: 1.348746\n",
      "Train Epoch: 14 [4480/9397 (48%)]\tLoss: 1.040888\n",
      "Train Epoch: 14 [5120/9397 (54%)]\tLoss: 1.142410\n",
      "Train Epoch: 14 [5760/9397 (61%)]\tLoss: 1.159224\n",
      "Train Epoch: 14 [6400/9397 (68%)]\tLoss: 1.261851\n",
      "Train Epoch: 14 [7040/9397 (75%)]\tLoss: 1.236564\n",
      "Train Epoch: 14 [7680/9397 (82%)]\tLoss: 1.179263\n",
      "Train Epoch: 14 [8320/9397 (88%)]\tLoss: 1.163863\n",
      "Train Epoch: 14 [8960/9397 (95%)]\tLoss: 1.008505\n",
      "Train Epoch: 15 [0/9397 (0%)]\tLoss: 0.978303\n",
      "Train Epoch: 15 [640/9397 (7%)]\tLoss: 1.039173\n",
      "Train Epoch: 15 [1280/9397 (14%)]\tLoss: 1.194900\n",
      "Train Epoch: 15 [1920/9397 (20%)]\tLoss: 0.991079\n",
      "Train Epoch: 15 [2560/9397 (27%)]\tLoss: 0.987451\n",
      "Train Epoch: 15 [3200/9397 (34%)]\tLoss: 0.984695\n",
      "Train Epoch: 15 [3840/9397 (41%)]\tLoss: 1.232781\n",
      "Train Epoch: 15 [4480/9397 (48%)]\tLoss: 1.158363\n",
      "Train Epoch: 15 [5120/9397 (54%)]\tLoss: 1.091186\n",
      "Train Epoch: 15 [5760/9397 (61%)]\tLoss: 1.008910\n",
      "Train Epoch: 15 [6400/9397 (68%)]\tLoss: 1.139318\n",
      "Train Epoch: 15 [7040/9397 (75%)]\tLoss: 1.185925\n",
      "Train Epoch: 15 [7680/9397 (82%)]\tLoss: 0.936215\n",
      "Train Epoch: 15 [8320/9397 (88%)]\tLoss: 1.122249\n",
      "Train Epoch: 15 [8960/9397 (95%)]\tLoss: 1.115072\n",
      "Train Epoch: 16 [0/9397 (0%)]\tLoss: 1.172012\n",
      "Train Epoch: 16 [640/9397 (7%)]\tLoss: 0.811664\n",
      "Train Epoch: 16 [1280/9397 (14%)]\tLoss: 1.138755\n",
      "Train Epoch: 16 [1920/9397 (20%)]\tLoss: 1.430163\n",
      "Train Epoch: 16 [2560/9397 (27%)]\tLoss: 1.056045\n",
      "Train Epoch: 16 [3200/9397 (34%)]\tLoss: 1.227283\n",
      "Train Epoch: 16 [3840/9397 (41%)]\tLoss: 1.092334\n",
      "Train Epoch: 16 [4480/9397 (48%)]\tLoss: 0.913815\n",
      "Train Epoch: 16 [5120/9397 (54%)]\tLoss: 0.989625\n",
      "Train Epoch: 16 [5760/9397 (61%)]\tLoss: 1.131038\n",
      "Train Epoch: 16 [6400/9397 (68%)]\tLoss: 1.164043\n",
      "Train Epoch: 16 [7040/9397 (75%)]\tLoss: 1.051906\n",
      "Train Epoch: 16 [7680/9397 (82%)]\tLoss: 1.190952\n",
      "Train Epoch: 16 [8320/9397 (88%)]\tLoss: 1.336483\n",
      "Train Epoch: 16 [8960/9397 (95%)]\tLoss: 1.188198\n",
      "Train Epoch: 17 [0/9397 (0%)]\tLoss: 1.448258\n",
      "Train Epoch: 17 [640/9397 (7%)]\tLoss: 1.365351\n",
      "Train Epoch: 17 [1280/9397 (14%)]\tLoss: 1.374011\n",
      "Train Epoch: 17 [1920/9397 (20%)]\tLoss: 1.112475\n",
      "Train Epoch: 17 [2560/9397 (27%)]\tLoss: 0.902003\n",
      "Train Epoch: 17 [3200/9397 (34%)]\tLoss: 1.103753\n",
      "Train Epoch: 17 [3840/9397 (41%)]\tLoss: 1.196993\n",
      "Train Epoch: 17 [4480/9397 (48%)]\tLoss: 1.125881\n",
      "Train Epoch: 17 [5120/9397 (54%)]\tLoss: 1.380350\n",
      "Train Epoch: 17 [5760/9397 (61%)]\tLoss: 0.938825\n",
      "Train Epoch: 17 [6400/9397 (68%)]\tLoss: 1.129884\n",
      "Train Epoch: 17 [7040/9397 (75%)]\tLoss: 1.260777\n",
      "Train Epoch: 17 [7680/9397 (82%)]\tLoss: 0.871130\n",
      "Train Epoch: 17 [8320/9397 (88%)]\tLoss: 1.536900\n",
      "Train Epoch: 17 [8960/9397 (95%)]\tLoss: 1.246304\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/1887 (0%)]\tLoss: 1.460695\n",
      "Train Epoch: 1 [640/1887 (33%)]\tLoss: 1.354853\n",
      "Train Epoch: 1 [1280/1887 (67%)]\tLoss: 1.326792\n",
      "Train Epoch: 2 [0/1887 (0%)]\tLoss: 1.317736\n",
      "Train Epoch: 2 [640/1887 (33%)]\tLoss: 1.273131\n",
      "Train Epoch: 2 [1280/1887 (67%)]\tLoss: 1.288797\n",
      "Train Epoch: 3 [0/1887 (0%)]\tLoss: 1.309946\n",
      "Train Epoch: 3 [640/1887 (33%)]\tLoss: 1.244290\n",
      "Train Epoch: 3 [1280/1887 (67%)]\tLoss: 1.246215\n",
      "Train Epoch: 4 [0/1887 (0%)]\tLoss: 1.233164\n",
      "Train Epoch: 4 [640/1887 (33%)]\tLoss: 1.318632\n",
      "Train Epoch: 4 [1280/1887 (67%)]\tLoss: 1.214898\n",
      "Train Epoch: 5 [0/1887 (0%)]\tLoss: 1.091112\n",
      "Train Epoch: 5 [640/1887 (33%)]\tLoss: 1.169802\n",
      "Train Epoch: 5 [1280/1887 (67%)]\tLoss: 1.352909\n",
      "Train Epoch: 6 [0/1887 (0%)]\tLoss: 1.341823\n",
      "Train Epoch: 6 [640/1887 (33%)]\tLoss: 1.204916\n",
      "Train Epoch: 6 [1280/1887 (67%)]\tLoss: 1.054528\n",
      "Train Epoch: 7 [0/1887 (0%)]\tLoss: 1.234884\n",
      "Train Epoch: 7 [640/1887 (33%)]\tLoss: 1.182955\n",
      "Train Epoch: 7 [1280/1887 (67%)]\tLoss: 1.415806\n",
      "Train Epoch: 8 [0/1887 (0%)]\tLoss: 1.258277\n",
      "Train Epoch: 8 [640/1887 (33%)]\tLoss: 1.222903\n",
      "Train Epoch: 8 [1280/1887 (67%)]\tLoss: 1.023910\n",
      "Train Epoch: 9 [0/1887 (0%)]\tLoss: 1.334592\n",
      "Train Epoch: 9 [640/1887 (33%)]\tLoss: 1.314285\n",
      "Train Epoch: 9 [1280/1887 (67%)]\tLoss: 1.312006\n",
      "Train Epoch: 10 [0/1887 (0%)]\tLoss: 1.173377\n",
      "Train Epoch: 10 [640/1887 (33%)]\tLoss: 1.414940\n",
      "Train Epoch: 10 [1280/1887 (67%)]\tLoss: 1.253933\n",
      "Train Epoch: 11 [0/1887 (0%)]\tLoss: 1.224284\n",
      "Train Epoch: 11 [640/1887 (33%)]\tLoss: 1.073049\n",
      "Train Epoch: 11 [1280/1887 (67%)]\tLoss: 1.352426\n",
      "Train Epoch: 12 [0/1887 (0%)]\tLoss: 1.129693\n",
      "Train Epoch: 12 [640/1887 (33%)]\tLoss: 1.198825\n",
      "Train Epoch: 12 [1280/1887 (67%)]\tLoss: 1.161804\n",
      "Train Epoch: 13 [0/1887 (0%)]\tLoss: 1.128855\n",
      "Train Epoch: 13 [640/1887 (33%)]\tLoss: 1.347023\n",
      "Train Epoch: 13 [1280/1887 (67%)]\tLoss: 1.150978\n",
      "Train Epoch: 14 [0/1887 (0%)]\tLoss: 1.364278\n",
      "Train Epoch: 14 [640/1887 (33%)]\tLoss: 1.325281\n",
      "Train Epoch: 14 [1280/1887 (67%)]\tLoss: 1.065393\n",
      "Train Epoch: 15 [0/1887 (0%)]\tLoss: 1.030735\n",
      "Train Epoch: 15 [640/1887 (33%)]\tLoss: 1.042904\n",
      "Train Epoch: 15 [1280/1887 (67%)]\tLoss: 1.262386\n",
      "Train Epoch: 16 [0/1887 (0%)]\tLoss: 1.071931\n",
      "Train Epoch: 16 [640/1887 (33%)]\tLoss: 1.082168\n",
      "Train Epoch: 16 [1280/1887 (67%)]\tLoss: 1.340525\n",
      "Train Epoch: 17 [0/1887 (0%)]\tLoss: 1.007629\n",
      "Train Epoch: 17 [640/1887 (33%)]\tLoss: 1.163050\n",
      "Train Epoch: 17 [1280/1887 (67%)]\tLoss: 1.309385\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/5490 (0%)]\tLoss: 1.438667\n",
      "Train Epoch: 1 [640/5490 (12%)]\tLoss: 1.409133\n",
      "Train Epoch: 1 [1280/5490 (23%)]\tLoss: 1.219225\n",
      "Train Epoch: 1 [1920/5490 (35%)]\tLoss: 1.579430\n",
      "Train Epoch: 1 [2560/5490 (47%)]\tLoss: 1.546695\n",
      "Train Epoch: 1 [3200/5490 (58%)]\tLoss: 1.228745\n",
      "Train Epoch: 1 [3840/5490 (70%)]\tLoss: 1.261343\n",
      "Train Epoch: 1 [4480/5490 (81%)]\tLoss: 1.244741\n",
      "Train Epoch: 1 [5120/5490 (93%)]\tLoss: 1.155900\n",
      "Train Epoch: 2 [0/5490 (0%)]\tLoss: 1.180224\n",
      "Train Epoch: 2 [640/5490 (12%)]\tLoss: 1.157258\n",
      "Train Epoch: 2 [1280/5490 (23%)]\tLoss: 1.485689\n",
      "Train Epoch: 2 [1920/5490 (35%)]\tLoss: 1.233090\n",
      "Train Epoch: 2 [2560/5490 (47%)]\tLoss: 1.100313\n",
      "Train Epoch: 2 [3200/5490 (58%)]\tLoss: 1.253441\n",
      "Train Epoch: 2 [3840/5490 (70%)]\tLoss: 1.240474\n",
      "Train Epoch: 2 [4480/5490 (81%)]\tLoss: 1.000037\n",
      "Train Epoch: 2 [5120/5490 (93%)]\tLoss: 1.124267\n",
      "Train Epoch: 3 [0/5490 (0%)]\tLoss: 1.371518\n",
      "Train Epoch: 3 [640/5490 (12%)]\tLoss: 1.122959\n",
      "Train Epoch: 3 [1280/5490 (23%)]\tLoss: 1.215076\n",
      "Train Epoch: 3 [1920/5490 (35%)]\tLoss: 1.219252\n",
      "Train Epoch: 3 [2560/5490 (47%)]\tLoss: 1.339015\n",
      "Train Epoch: 3 [3200/5490 (58%)]\tLoss: 1.129417\n",
      "Train Epoch: 3 [3840/5490 (70%)]\tLoss: 1.243711\n",
      "Train Epoch: 3 [4480/5490 (81%)]\tLoss: 1.034015\n",
      "Train Epoch: 3 [5120/5490 (93%)]\tLoss: 1.235631\n",
      "Train Epoch: 4 [0/5490 (0%)]\tLoss: 0.868678\n",
      "Train Epoch: 4 [640/5490 (12%)]\tLoss: 1.103502\n",
      "Train Epoch: 4 [1280/5490 (23%)]\tLoss: 1.176957\n",
      "Train Epoch: 4 [1920/5490 (35%)]\tLoss: 1.030369\n",
      "Train Epoch: 4 [2560/5490 (47%)]\tLoss: 1.158922\n",
      "Train Epoch: 4 [3200/5490 (58%)]\tLoss: 1.153446\n",
      "Train Epoch: 4 [3840/5490 (70%)]\tLoss: 1.509422\n",
      "Train Epoch: 4 [4480/5490 (81%)]\tLoss: 1.092682\n",
      "Train Epoch: 4 [5120/5490 (93%)]\tLoss: 1.134224\n",
      "Train Epoch: 5 [0/5490 (0%)]\tLoss: 1.006085\n",
      "Train Epoch: 5 [640/5490 (12%)]\tLoss: 1.217180\n",
      "Train Epoch: 5 [1280/5490 (23%)]\tLoss: 1.125025\n",
      "Train Epoch: 5 [1920/5490 (35%)]\tLoss: 0.962623\n",
      "Train Epoch: 5 [2560/5490 (47%)]\tLoss: 1.145796\n",
      "Train Epoch: 5 [3200/5490 (58%)]\tLoss: 1.143646\n",
      "Train Epoch: 5 [3840/5490 (70%)]\tLoss: 1.191350\n",
      "Train Epoch: 5 [4480/5490 (81%)]\tLoss: 1.190636\n",
      "Train Epoch: 5 [5120/5490 (93%)]\tLoss: 1.169425\n",
      "Train Epoch: 6 [0/5490 (0%)]\tLoss: 1.234396\n",
      "Train Epoch: 6 [640/5490 (12%)]\tLoss: 1.237959\n",
      "Train Epoch: 6 [1280/5490 (23%)]\tLoss: 1.055803\n",
      "Train Epoch: 6 [1920/5490 (35%)]\tLoss: 1.166099\n",
      "Train Epoch: 6 [2560/5490 (47%)]\tLoss: 0.992169\n",
      "Train Epoch: 6 [3200/5490 (58%)]\tLoss: 1.241306\n",
      "Train Epoch: 6 [3840/5490 (70%)]\tLoss: 1.225183\n",
      "Train Epoch: 6 [4480/5490 (81%)]\tLoss: 1.208262\n",
      "Train Epoch: 6 [5120/5490 (93%)]\tLoss: 1.197952\n",
      "Train Epoch: 7 [0/5490 (0%)]\tLoss: 0.843118\n",
      "Train Epoch: 7 [640/5490 (12%)]\tLoss: 0.915242\n",
      "Train Epoch: 7 [1280/5490 (23%)]\tLoss: 0.981272\n",
      "Train Epoch: 7 [1920/5490 (35%)]\tLoss: 1.447475\n",
      "Train Epoch: 7 [2560/5490 (47%)]\tLoss: 1.209828\n",
      "Train Epoch: 7 [3200/5490 (58%)]\tLoss: 1.248304\n",
      "Train Epoch: 7 [3840/5490 (70%)]\tLoss: 1.162390\n",
      "Train Epoch: 7 [4480/5490 (81%)]\tLoss: 1.233218\n",
      "Train Epoch: 7 [5120/5490 (93%)]\tLoss: 1.046028\n",
      "Train Epoch: 8 [0/5490 (0%)]\tLoss: 1.292213\n",
      "Train Epoch: 8 [640/5490 (12%)]\tLoss: 0.999949\n",
      "Train Epoch: 8 [1280/5490 (23%)]\tLoss: 1.066115\n",
      "Train Epoch: 8 [1920/5490 (35%)]\tLoss: 0.973655\n",
      "Train Epoch: 8 [2560/5490 (47%)]\tLoss: 1.150643\n",
      "Train Epoch: 8 [3200/5490 (58%)]\tLoss: 0.790182\n",
      "Train Epoch: 8 [3840/5490 (70%)]\tLoss: 1.315015\n",
      "Train Epoch: 8 [4480/5490 (81%)]\tLoss: 1.414149\n",
      "Train Epoch: 8 [5120/5490 (93%)]\tLoss: 1.050286\n",
      "Train Epoch: 9 [0/5490 (0%)]\tLoss: 0.979828\n",
      "Train Epoch: 9 [640/5490 (12%)]\tLoss: 1.231355\n",
      "Train Epoch: 9 [1280/5490 (23%)]\tLoss: 1.054240\n",
      "Train Epoch: 9 [1920/5490 (35%)]\tLoss: 1.106314\n",
      "Train Epoch: 9 [2560/5490 (47%)]\tLoss: 1.134944\n",
      "Train Epoch: 9 [3200/5490 (58%)]\tLoss: 1.242508\n",
      "Train Epoch: 9 [3840/5490 (70%)]\tLoss: 1.003777\n",
      "Train Epoch: 9 [4480/5490 (81%)]\tLoss: 0.957023\n",
      "Train Epoch: 9 [5120/5490 (93%)]\tLoss: 1.047915\n",
      "Train Epoch: 10 [0/5490 (0%)]\tLoss: 1.200778\n",
      "Train Epoch: 10 [640/5490 (12%)]\tLoss: 1.189089\n",
      "Train Epoch: 10 [1280/5490 (23%)]\tLoss: 1.056513\n",
      "Train Epoch: 10 [1920/5490 (35%)]\tLoss: 1.151466\n",
      "Train Epoch: 10 [2560/5490 (47%)]\tLoss: 1.407617\n",
      "Train Epoch: 10 [3200/5490 (58%)]\tLoss: 0.898418\n",
      "Train Epoch: 10 [3840/5490 (70%)]\tLoss: 0.982141\n",
      "Train Epoch: 10 [4480/5490 (81%)]\tLoss: 0.843114\n",
      "Train Epoch: 10 [5120/5490 (93%)]\tLoss: 1.068137\n",
      "Train Epoch: 11 [0/5490 (0%)]\tLoss: 1.466266\n",
      "Train Epoch: 11 [640/5490 (12%)]\tLoss: 1.003441\n",
      "Train Epoch: 11 [1280/5490 (23%)]\tLoss: 0.836100\n",
      "Train Epoch: 11 [1920/5490 (35%)]\tLoss: 0.843353\n",
      "Train Epoch: 11 [2560/5490 (47%)]\tLoss: 0.974429\n",
      "Train Epoch: 11 [3200/5490 (58%)]\tLoss: 0.866320\n",
      "Train Epoch: 11 [3840/5490 (70%)]\tLoss: 0.970397\n",
      "Train Epoch: 11 [4480/5490 (81%)]\tLoss: 1.188230\n",
      "Train Epoch: 11 [5120/5490 (93%)]\tLoss: 1.003400\n",
      "Train Epoch: 12 [0/5490 (0%)]\tLoss: 0.848163\n",
      "Train Epoch: 12 [640/5490 (12%)]\tLoss: 0.963427\n",
      "Train Epoch: 12 [1280/5490 (23%)]\tLoss: 0.887824\n",
      "Train Epoch: 12 [1920/5490 (35%)]\tLoss: 1.257960\n",
      "Train Epoch: 12 [2560/5490 (47%)]\tLoss: 1.280723\n",
      "Train Epoch: 12 [3200/5490 (58%)]\tLoss: 0.899703\n",
      "Train Epoch: 12 [3840/5490 (70%)]\tLoss: 0.920470\n",
      "Train Epoch: 12 [4480/5490 (81%)]\tLoss: 1.120507\n",
      "Train Epoch: 12 [5120/5490 (93%)]\tLoss: 1.099222\n",
      "Train Epoch: 13 [0/5490 (0%)]\tLoss: 0.868286\n",
      "Train Epoch: 13 [640/5490 (12%)]\tLoss: 0.851474\n",
      "Train Epoch: 13 [1280/5490 (23%)]\tLoss: 0.900502\n",
      "Train Epoch: 13 [1920/5490 (35%)]\tLoss: 1.143640\n",
      "Train Epoch: 13 [2560/5490 (47%)]\tLoss: 1.266070\n",
      "Train Epoch: 13 [3200/5490 (58%)]\tLoss: 1.257932\n",
      "Train Epoch: 13 [3840/5490 (70%)]\tLoss: 1.046217\n",
      "Train Epoch: 13 [4480/5490 (81%)]\tLoss: 1.129935\n",
      "Train Epoch: 13 [5120/5490 (93%)]\tLoss: 0.956128\n",
      "Train Epoch: 14 [0/5490 (0%)]\tLoss: 0.798081\n",
      "Train Epoch: 14 [640/5490 (12%)]\tLoss: 0.906050\n",
      "Train Epoch: 14 [1280/5490 (23%)]\tLoss: 1.134323\n",
      "Train Epoch: 14 [1920/5490 (35%)]\tLoss: 1.102846\n",
      "Train Epoch: 14 [2560/5490 (47%)]\tLoss: 1.136315\n",
      "Train Epoch: 14 [3200/5490 (58%)]\tLoss: 0.967667\n",
      "Train Epoch: 14 [3840/5490 (70%)]\tLoss: 1.236070\n",
      "Train Epoch: 14 [4480/5490 (81%)]\tLoss: 1.010489\n",
      "Train Epoch: 14 [5120/5490 (93%)]\tLoss: 1.108012\n",
      "Train Epoch: 15 [0/5490 (0%)]\tLoss: 1.161574\n",
      "Train Epoch: 15 [640/5490 (12%)]\tLoss: 1.080289\n",
      "Train Epoch: 15 [1280/5490 (23%)]\tLoss: 1.238980\n",
      "Train Epoch: 15 [1920/5490 (35%)]\tLoss: 1.027808\n",
      "Train Epoch: 15 [2560/5490 (47%)]\tLoss: 0.910967\n",
      "Train Epoch: 15 [3200/5490 (58%)]\tLoss: 1.166215\n",
      "Train Epoch: 15 [3840/5490 (70%)]\tLoss: 1.237564\n",
      "Train Epoch: 15 [4480/5490 (81%)]\tLoss: 1.077596\n",
      "Train Epoch: 15 [5120/5490 (93%)]\tLoss: 0.854180\n",
      "Train Epoch: 16 [0/5490 (0%)]\tLoss: 1.104416\n",
      "Train Epoch: 16 [640/5490 (12%)]\tLoss: 1.162619\n",
      "Train Epoch: 16 [1280/5490 (23%)]\tLoss: 1.114055\n",
      "Train Epoch: 16 [1920/5490 (35%)]\tLoss: 1.084232\n",
      "Train Epoch: 16 [2560/5490 (47%)]\tLoss: 1.104570\n",
      "Train Epoch: 16 [3200/5490 (58%)]\tLoss: 0.922170\n",
      "Train Epoch: 16 [3840/5490 (70%)]\tLoss: 0.649702\n",
      "Train Epoch: 16 [4480/5490 (81%)]\tLoss: 1.148435\n",
      "Train Epoch: 16 [5120/5490 (93%)]\tLoss: 1.179326\n",
      "Train Epoch: 17 [0/5490 (0%)]\tLoss: 1.248234\n",
      "Train Epoch: 17 [640/5490 (12%)]\tLoss: 1.302840\n",
      "Train Epoch: 17 [1280/5490 (23%)]\tLoss: 0.928215\n",
      "Train Epoch: 17 [1920/5490 (35%)]\tLoss: 0.945820\n",
      "Train Epoch: 17 [2560/5490 (47%)]\tLoss: 1.130680\n",
      "Train Epoch: 17 [3200/5490 (58%)]\tLoss: 1.090507\n",
      "Train Epoch: 17 [3840/5490 (70%)]\tLoss: 1.088363\n",
      "Train Epoch: 17 [4480/5490 (81%)]\tLoss: 0.959417\n",
      "Train Epoch: 17 [5120/5490 (93%)]\tLoss: 0.849133\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/2468 (0%)]\tLoss: 1.729065\n",
      "Train Epoch: 1 [640/2468 (26%)]\tLoss: 1.461381\n",
      "Train Epoch: 1 [1280/2468 (51%)]\tLoss: 1.347132\n",
      "Train Epoch: 1 [1920/2468 (77%)]\tLoss: 1.249531\n",
      "Train Epoch: 2 [0/2468 (0%)]\tLoss: 1.215589\n",
      "Train Epoch: 2 [640/2468 (26%)]\tLoss: 1.386178\n",
      "Train Epoch: 2 [1280/2468 (51%)]\tLoss: 1.336718\n",
      "Train Epoch: 2 [1920/2468 (77%)]\tLoss: 1.329384\n",
      "Train Epoch: 3 [0/2468 (0%)]\tLoss: 1.244982\n",
      "Train Epoch: 3 [640/2468 (26%)]\tLoss: 0.940561\n",
      "Train Epoch: 3 [1280/2468 (51%)]\tLoss: 1.036241\n",
      "Train Epoch: 3 [1920/2468 (77%)]\tLoss: 1.365512\n",
      "Train Epoch: 4 [0/2468 (0%)]\tLoss: 1.268168\n",
      "Train Epoch: 4 [640/2468 (26%)]\tLoss: 1.220779\n",
      "Train Epoch: 4 [1280/2468 (51%)]\tLoss: 1.087348\n",
      "Train Epoch: 4 [1920/2468 (77%)]\tLoss: 1.598934\n",
      "Train Epoch: 5 [0/2468 (0%)]\tLoss: 1.293809\n",
      "Train Epoch: 5 [640/2468 (26%)]\tLoss: 1.280742\n",
      "Train Epoch: 5 [1280/2468 (51%)]\tLoss: 1.244228\n",
      "Train Epoch: 5 [1920/2468 (77%)]\tLoss: 1.246515\n",
      "Train Epoch: 6 [0/2468 (0%)]\tLoss: 1.002053\n",
      "Train Epoch: 6 [640/2468 (26%)]\tLoss: 1.298604\n",
      "Train Epoch: 6 [1280/2468 (51%)]\tLoss: 1.130798\n",
      "Train Epoch: 6 [1920/2468 (77%)]\tLoss: 1.287146\n",
      "Train Epoch: 7 [0/2468 (0%)]\tLoss: 1.129394\n",
      "Train Epoch: 7 [640/2468 (26%)]\tLoss: 1.053180\n",
      "Train Epoch: 7 [1280/2468 (51%)]\tLoss: 1.424429\n",
      "Train Epoch: 7 [1920/2468 (77%)]\tLoss: 1.240122\n",
      "Train Epoch: 8 [0/2468 (0%)]\tLoss: 0.812395\n",
      "Train Epoch: 8 [640/2468 (26%)]\tLoss: 0.957840\n",
      "Train Epoch: 8 [1280/2468 (51%)]\tLoss: 0.942221\n",
      "Train Epoch: 8 [1920/2468 (77%)]\tLoss: 1.453111\n",
      "Train Epoch: 9 [0/2468 (0%)]\tLoss: 1.195013\n",
      "Train Epoch: 9 [640/2468 (26%)]\tLoss: 1.293907\n",
      "Train Epoch: 9 [1280/2468 (51%)]\tLoss: 1.058711\n",
      "Train Epoch: 9 [1920/2468 (77%)]\tLoss: 1.299739\n",
      "Train Epoch: 10 [0/2468 (0%)]\tLoss: 1.493057\n",
      "Train Epoch: 10 [640/2468 (26%)]\tLoss: 0.942993\n",
      "Train Epoch: 10 [1280/2468 (51%)]\tLoss: 1.017474\n",
      "Train Epoch: 10 [1920/2468 (77%)]\tLoss: 1.208099\n",
      "Train Epoch: 11 [0/2468 (0%)]\tLoss: 1.153283\n",
      "Train Epoch: 11 [640/2468 (26%)]\tLoss: 1.190255\n",
      "Train Epoch: 11 [1280/2468 (51%)]\tLoss: 1.093440\n",
      "Train Epoch: 11 [1920/2468 (77%)]\tLoss: 1.348392\n",
      "Train Epoch: 12 [0/2468 (0%)]\tLoss: 1.338324\n",
      "Train Epoch: 12 [640/2468 (26%)]\tLoss: 0.975029\n",
      "Train Epoch: 12 [1280/2468 (51%)]\tLoss: 1.083760\n",
      "Train Epoch: 12 [1920/2468 (77%)]\tLoss: 0.986905\n",
      "Train Epoch: 13 [0/2468 (0%)]\tLoss: 1.023301\n",
      "Train Epoch: 13 [640/2468 (26%)]\tLoss: 1.230424\n",
      "Train Epoch: 13 [1280/2468 (51%)]\tLoss: 1.271013\n",
      "Train Epoch: 13 [1920/2468 (77%)]\tLoss: 0.968478\n",
      "Train Epoch: 14 [0/2468 (0%)]\tLoss: 0.911592\n",
      "Train Epoch: 14 [640/2468 (26%)]\tLoss: 1.386042\n",
      "Train Epoch: 14 [1280/2468 (51%)]\tLoss: 0.921469\n",
      "Train Epoch: 14 [1920/2468 (77%)]\tLoss: 1.185788\n",
      "Train Epoch: 15 [0/2468 (0%)]\tLoss: 1.169633\n",
      "Train Epoch: 15 [640/2468 (26%)]\tLoss: 0.965894\n",
      "Train Epoch: 15 [1280/2468 (51%)]\tLoss: 1.172001\n",
      "Train Epoch: 15 [1920/2468 (77%)]\tLoss: 1.124961\n",
      "Train Epoch: 16 [0/2468 (0%)]\tLoss: 1.200495\n",
      "Train Epoch: 16 [640/2468 (26%)]\tLoss: 1.031568\n",
      "Train Epoch: 16 [1280/2468 (51%)]\tLoss: 1.118770\n",
      "Train Epoch: 16 [1920/2468 (77%)]\tLoss: 1.119459\n",
      "Train Epoch: 17 [0/2468 (0%)]\tLoss: 1.235369\n",
      "Train Epoch: 17 [640/2468 (26%)]\tLoss: 1.125294\n",
      "Train Epoch: 17 [1280/2468 (51%)]\tLoss: 0.888913\n",
      "Train Epoch: 17 [1920/2468 (77%)]\tLoss: 1.171550\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.3527, Accuracy: 5128/10000 (51%)\n",
      "\n",
      "Round 3/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.297254\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.460934\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.384210\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.400899\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.406622\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.418892\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.211934\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.205890\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.270957\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.046300\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.417674\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.409999\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.496904\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.352725\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.239032\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.243396\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.314723\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.338121\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.276787\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.378677\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.214844\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.401897\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.180360\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.588899\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.690337\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.021531\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.311674\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.375077\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.160590\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.266600\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 0.832826\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.150361\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 0.952039\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.091241\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.200127\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.099674\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.449503\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.193499\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.274934\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.085047\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.378632\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.278450\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.152712\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.329661\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.042305\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.090270\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.269452\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.286022\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.214636\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.236660\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.149467\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.249163\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.200527\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.431703\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.330799\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.149209\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.055937\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.230306\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.392564\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.045657\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.153969\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.166090\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.171893\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.268504\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.431916\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.118970\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.145316\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 0.996063\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.354015\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.179187\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.565616\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.159249\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.075549\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.358158\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.111237\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.115098\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.230852\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.277942\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 1.154812\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.045962\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.196044\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.246801\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.356709\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.223215\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.109432\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 0.865645\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.188932\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.263170\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 1.154767\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.135683\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.278142\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.218350\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.444052\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.280659\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.141276\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.261377\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.200038\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.202019\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.261655\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 1.014957\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.102169\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.165472\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.408898\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.631236\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.344756\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.406141\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.588639\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.214933\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.877972\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.591511\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.416958\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.197010\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.384311\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.269000\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.365602\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.351924\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.235493\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.428135\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.411084\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.170360\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.136228\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.301861\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.310751\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.163120\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.470534\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.464937\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.317583\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.429724\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.238947\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.166411\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.333679\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.241803\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.386924\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.227346\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.458914\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.069014\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.224326\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.185762\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.218817\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.108477\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.193394\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.392626\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.155083\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 1.344881\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.170048\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.260972\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.336138\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.058079\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.376671\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.170186\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.179607\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.444490\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.070729\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.288481\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.318943\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 0.976925\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.102201\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.039660\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.278791\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.402440\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.211048\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.132839\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 1.114184\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.206352\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.344051\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.385517\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.351135\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.207962\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.200452\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.205916\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.358134\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 0.971181\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.054726\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.185660\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.305285\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.041337\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.441604\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.057941\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.161974\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.220443\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 1.169443\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.169158\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.153737\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.083459\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.044963\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.298702\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.152666\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.366110\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 1.276792\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.284439\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.079885\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.421642\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.053599\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.271105\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.081525\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.183094\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.171482\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.317407\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 0.984929\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.279143\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.256084\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.252835\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.033615\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.135002\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.595508\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.182252\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.393848\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.085231\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.118249\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 0.975298\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 1.278712\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.128593\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.054716\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 0.925906\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 1.192108\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.145304\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 1.088390\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.200340\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 1.195479\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.050321\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 1.215532\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 1.077273\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.006343\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.056443\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 1.075760\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 0.922304\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.139228\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.218189\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.038725\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.008519\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 0.948404\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.018214\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 0.930222\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 1.306095\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 1.254130\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 0.953796\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 0.806678\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 1.042818\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 0.970210\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 0.946657\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.040536\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.153356\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 1.034795\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 1.204513\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 1.139503\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 1.225070\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 0.823922\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 1.238909\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 0.893833\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 0.893658\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 1.129865\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 1.153764\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 1.129822\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 1.085869\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 1.035365\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.105964\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 1.107843\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 0.949123\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 1.081107\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 0.930777\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 1.205302\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 1.102823\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 0.898147\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 0.937362\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 1.135643\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 1.168199\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 1.020995\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 1.098624\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 1.326277\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 1.182865\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.131911\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 1.118138\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.992575\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 0.994880\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 1.088303\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 0.943031\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.195873\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 1.136860\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.239506\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 1.056889\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.061674\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 1.255608\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 0.903999\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 0.858150\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 0.983979\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 1.155036\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 1.042981\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 1.113065\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 1.328122\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 1.030323\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 1.139480\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 0.963629\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 1.231844\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 1.498287\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 0.818009\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 1.131671\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 0.935695\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 0.810923\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 1.168070\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 0.859994\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 1.024108\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.115651\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 1.153340\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 0.911754\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 0.919436\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 0.751970\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 1.035944\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 1.140663\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 1.184355\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 1.329250\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 1.002796\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 0.858857\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 0.957464\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 0.729091\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 0.922755\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 1.033697\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.286185\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 1.106589\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 1.403099\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.985165\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 1.030925\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 1.176727\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 1.019164\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 0.949098\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 1.175079\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 1.210953\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 1.051533\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 1.044929\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 1.170948\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 1.046627\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 0.996511\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 0.876104\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 1.047448\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 1.091684\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 0.985897\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 1.111146\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 1.124831\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 0.892653\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 1.073683\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 1.107904\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 0.934766\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 0.941459\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 1.002851\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 1.099989\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 0.976681\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 0.911866\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.998853\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 1.045694\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 0.829098\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 1.071921\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 1.012243\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 0.839968\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 0.856851\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 0.922941\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 0.820166\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 0.917543\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 1.003782\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 0.870024\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 0.921104\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.839855\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 1.140179\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 1.184077\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 1.186283\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.073141\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 1.144566\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 0.925800\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 0.746056\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 1.020143\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 0.921154\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 0.924672\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.887937\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.881405\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.922299\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 1.100663\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.238248\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.415548\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 1.025998\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 1.114130\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.325807\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.032024\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 1.159174\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.019125\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 1.066529\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.094725\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 1.024929\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.977386\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 0.953783\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 1.402916\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.032956\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.073154\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 1.145812\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 0.990810\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 1.039311\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 1.316074\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 0.831972\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 1.275773\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 1.229432\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 1.216326\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 0.971905\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 0.987092\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 1.091858\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 1.230529\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 1.258680\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 1.047600\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 1.115660\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 0.894877\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 1.092285\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 0.988426\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 0.950447\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 1.057356\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 0.880337\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 1.215510\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 1.003414\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 1.104973\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 0.742405\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 1.196297\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 0.734765\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 1.085463\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 1.211226\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 1.202934\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 0.937991\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 1.024991\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 0.949775\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 0.865035\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 0.868251\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 0.935171\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 0.931159\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 1.183271\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 1.341930\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.863487\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 1.115719\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 1.008569\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 0.761318\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 0.848764\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 1.050472\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 0.861886\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 0.897478\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 0.922403\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.202364\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 1.034157\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 1.018055\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 1.203690\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 0.926299\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 1.040145\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 1.167076\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 0.885732\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 1.041592\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 1.113742\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 0.923272\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 1.075964\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 1.014194\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 1.029182\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 1.004758\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 0.963020\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 1.063393\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.969391\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 1.104876\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.877073\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 1.016883\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 1.141966\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 1.001953\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.826081\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 1.004597\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 0.947145\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 1.025818\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 1.149199\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 0.984935\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 1.007849\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 1.124633\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 0.905231\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 0.974009\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.861677\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 1.030445\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.863932\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 0.897659\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.759037\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 0.747416\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 1.059601\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 1.223081\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 0.862644\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 1.116453\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 0.806523\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 0.948031\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 0.758806\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 0.922002\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.904695\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 0.986385\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.925887\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 1.249921\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 1.021676\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 1.174964\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 1.181892\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 1.138920\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 1.002428\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 1.009215\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 1.049246\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 0.957919\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 1.046651\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 0.875873\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.931602\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.794962\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 0.850463\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 1.008000\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 0.931100\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.969974\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 0.874163\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 1.068152\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 0.800319\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.982503\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 1.227002\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 0.971053\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 0.820538\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.765390\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 0.952507\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 1.068473\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 0.844485\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 0.868127\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 1.129260\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.991553\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 0.927119\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 0.923194\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 1.056260\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 0.904627\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.138522\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.883600\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 0.885086\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 0.868354\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.946870\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 0.812144\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 1.070610\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 1.094519\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 1.310327\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 1.036434\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.863014\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.817812\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 1.300671\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 0.835403\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.770609\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 1.023669\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 0.896473\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 0.892492\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 0.813277\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.734806\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 1.053671\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 1.027804\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 1.033163\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.936726\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 1.008229\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 0.914099\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.921921\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 1.150698\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.884607\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 1.085025\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 0.932898\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 1.017773\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.918568\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 0.842017\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.984981\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 1.032618\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 0.916167\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 1.049757\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.752770\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.338138\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.272208\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.120887\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.489598\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.202224\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.165604\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.298355\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.265620\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.438897\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.193441\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.229179\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.323454\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.737820\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.601688\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.254370\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.411838\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.310775\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.280126\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.404240\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.162557\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.022066\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.203482\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.361355\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.549239\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.453327\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.253663\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.027890\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.371279\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.286285\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.120964\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.343219\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.233853\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.504825\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.196081\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.399829\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.501461\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.291735\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.099132\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.127335\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.318127\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 1.226422\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.021400\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.173345\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.013422\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.216148\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.438985\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.387609\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.205929\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.322913\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.247122\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.331537\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.162730\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.254766\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.131358\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 1.406415\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.207083\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.318396\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.415474\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.206977\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.233384\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.378955\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.113413\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.260082\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.233640\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.335117\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.202040\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.310420\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.387577\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 1.099435\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.261826\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.193659\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 1.150277\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.400758\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.074639\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.068243\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.126764\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.231444\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.277759\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.414483\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 1.095293\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.186467\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.157417\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 1.200496\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.151149\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.154538\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.283230\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.203396\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.453776\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.231075\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.032030\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.301087\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.445305\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.082180\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.195919\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.286088\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.169208\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 1.244597\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.505271\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.377693\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.398623\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.116604\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.246587\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.186564\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.215093\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.218094\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.245720\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.246044\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.425360\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.351154\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.251410\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 0.814668\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 0.993928\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.280371\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.294804\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.254570\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.386370\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.393679\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.239539\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.094312\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.398906\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.304641\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.437000\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.016201\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.325790\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 1.115360\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 1.251494\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.205403\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.308891\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.211963\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.258895\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.193435\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.267706\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.388736\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.173567\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.105597\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.045739\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.354819\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.173165\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 1.406780\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.424482\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.182833\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.161710\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.016906\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.335769\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 0.958687\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.330012\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.150132\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.132310\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.361343\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.299118\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.425531\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 1.272604\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 0.802564\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.177989\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.099105\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 0.990151\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 1.365444\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 1.326466\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 1.237238\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 0.976465\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.354642\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 1.343887\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.370539\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.308082\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.090171\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.073941\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 0.783713\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.268544\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.094199\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.168170\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 0.962658\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.048016\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.380754\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 1.104315\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.116694\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 1.199840\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.327476\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.182020\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.152174\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.150353\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 1.039325\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 1.255444\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.104835\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.371077\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 1.167062\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 1.088380\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 1.080963\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.111493\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.160146\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.305931\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.295424\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 1.189640\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.154846\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.064369\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 0.904560\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.107236\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.145700\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.550566\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.006337\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 0.859431\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.204551\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.214652\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 1.318663\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.200428\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.243533\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 0.866499\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.162645\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 1.242667\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 1.408930\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 1.304616\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.265746\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.117575\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.302320\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.173797\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.519902\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.123820\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.182878\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 1.034099\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 1.020269\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.150901\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.001477\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.162661\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 1.050505\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.454851\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.321697\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 1.245731\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.204277\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.245345\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.120501\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.219795\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.184035\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.042915\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.016587\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.217473\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.155525\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.173065\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 0.937391\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.525512\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.520860\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.639414\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.531758\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.241080\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.611053\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.473932\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.252457\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.488207\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.124742\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.220756\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.525368\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.431636\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.360563\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.426496\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.398283\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.319066\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.219665\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.388282\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.063304\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.418542\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.293067\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.237660\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.322252\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.288980\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.069589\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.365833\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.331449\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.304331\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.587069\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.427263\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.171280\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.561738\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.281446\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.201798\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.297730\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.406555\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.141966\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.400612\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.221280\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.397434\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.407116\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.257370\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.203860\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.358209\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.381688\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.359542\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.115408\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.411722\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.485136\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.345797\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.088820\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.400026\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.018660\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.330012\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.182030\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.183016\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.287971\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.260313\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.282516\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.223904\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.215313\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.298811\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.162145\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.397317\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 1.261134\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.128729\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.414530\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 1.176762\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.394285\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.024913\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.244323\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.040697\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.190594\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 1.301590\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 0.904583\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 1.186210\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.229229\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.247676\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.327102\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.352978\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.188474\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.070508\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.414957\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.446683\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/9397 (0%)]\tLoss: 1.533020\n",
      "Train Epoch: 1 [640/9397 (7%)]\tLoss: 1.431006\n",
      "Train Epoch: 1 [1280/9397 (14%)]\tLoss: 1.223978\n",
      "Train Epoch: 1 [1920/9397 (20%)]\tLoss: 1.591882\n",
      "Train Epoch: 1 [2560/9397 (27%)]\tLoss: 1.281416\n",
      "Train Epoch: 1 [3200/9397 (34%)]\tLoss: 1.603182\n",
      "Train Epoch: 1 [3840/9397 (41%)]\tLoss: 1.288125\n",
      "Train Epoch: 1 [4480/9397 (48%)]\tLoss: 1.010684\n",
      "Train Epoch: 1 [5120/9397 (54%)]\tLoss: 1.244095\n",
      "Train Epoch: 1 [5760/9397 (61%)]\tLoss: 1.287360\n",
      "Train Epoch: 1 [6400/9397 (68%)]\tLoss: 1.258897\n",
      "Train Epoch: 1 [7040/9397 (75%)]\tLoss: 1.231429\n",
      "Train Epoch: 1 [7680/9397 (82%)]\tLoss: 1.258958\n",
      "Train Epoch: 1 [8320/9397 (88%)]\tLoss: 1.252974\n",
      "Train Epoch: 1 [8960/9397 (95%)]\tLoss: 1.149637\n",
      "Train Epoch: 2 [0/9397 (0%)]\tLoss: 1.236852\n",
      "Train Epoch: 2 [640/9397 (7%)]\tLoss: 1.338660\n",
      "Train Epoch: 2 [1280/9397 (14%)]\tLoss: 1.112769\n",
      "Train Epoch: 2 [1920/9397 (20%)]\tLoss: 1.187875\n",
      "Train Epoch: 2 [2560/9397 (27%)]\tLoss: 1.336867\n",
      "Train Epoch: 2 [3200/9397 (34%)]\tLoss: 1.386129\n",
      "Train Epoch: 2 [3840/9397 (41%)]\tLoss: 1.152564\n",
      "Train Epoch: 2 [4480/9397 (48%)]\tLoss: 1.388905\n",
      "Train Epoch: 2 [5120/9397 (54%)]\tLoss: 1.153223\n",
      "Train Epoch: 2 [5760/9397 (61%)]\tLoss: 1.315619\n",
      "Train Epoch: 2 [6400/9397 (68%)]\tLoss: 1.086380\n",
      "Train Epoch: 2 [7040/9397 (75%)]\tLoss: 1.501478\n",
      "Train Epoch: 2 [7680/9397 (82%)]\tLoss: 1.418929\n",
      "Train Epoch: 2 [8320/9397 (88%)]\tLoss: 1.175088\n",
      "Train Epoch: 2 [8960/9397 (95%)]\tLoss: 1.236683\n",
      "Train Epoch: 3 [0/9397 (0%)]\tLoss: 1.184780\n",
      "Train Epoch: 3 [640/9397 (7%)]\tLoss: 1.208706\n",
      "Train Epoch: 3 [1280/9397 (14%)]\tLoss: 1.243477\n",
      "Train Epoch: 3 [1920/9397 (20%)]\tLoss: 1.426789\n",
      "Train Epoch: 3 [2560/9397 (27%)]\tLoss: 0.965529\n",
      "Train Epoch: 3 [3200/9397 (34%)]\tLoss: 1.164364\n",
      "Train Epoch: 3 [3840/9397 (41%)]\tLoss: 1.236565\n",
      "Train Epoch: 3 [4480/9397 (48%)]\tLoss: 1.494739\n",
      "Train Epoch: 3 [5120/9397 (54%)]\tLoss: 1.001264\n",
      "Train Epoch: 3 [5760/9397 (61%)]\tLoss: 1.167961\n",
      "Train Epoch: 3 [6400/9397 (68%)]\tLoss: 1.383339\n",
      "Train Epoch: 3 [7040/9397 (75%)]\tLoss: 1.321870\n",
      "Train Epoch: 3 [7680/9397 (82%)]\tLoss: 1.125922\n",
      "Train Epoch: 3 [8320/9397 (88%)]\tLoss: 1.100915\n",
      "Train Epoch: 3 [8960/9397 (95%)]\tLoss: 1.217743\n",
      "Train Epoch: 4 [0/9397 (0%)]\tLoss: 1.318308\n",
      "Train Epoch: 4 [640/9397 (7%)]\tLoss: 1.019690\n",
      "Train Epoch: 4 [1280/9397 (14%)]\tLoss: 1.132834\n",
      "Train Epoch: 4 [1920/9397 (20%)]\tLoss: 1.667479\n",
      "Train Epoch: 4 [2560/9397 (27%)]\tLoss: 1.422028\n",
      "Train Epoch: 4 [3200/9397 (34%)]\tLoss: 1.451150\n",
      "Train Epoch: 4 [3840/9397 (41%)]\tLoss: 1.091477\n",
      "Train Epoch: 4 [4480/9397 (48%)]\tLoss: 1.259537\n",
      "Train Epoch: 4 [5120/9397 (54%)]\tLoss: 1.402216\n",
      "Train Epoch: 4 [5760/9397 (61%)]\tLoss: 1.019631\n",
      "Train Epoch: 4 [6400/9397 (68%)]\tLoss: 1.156624\n",
      "Train Epoch: 4 [7040/9397 (75%)]\tLoss: 1.128060\n",
      "Train Epoch: 4 [7680/9397 (82%)]\tLoss: 1.185750\n",
      "Train Epoch: 4 [8320/9397 (88%)]\tLoss: 1.171926\n",
      "Train Epoch: 4 [8960/9397 (95%)]\tLoss: 1.131010\n",
      "Train Epoch: 5 [0/9397 (0%)]\tLoss: 1.126770\n",
      "Train Epoch: 5 [640/9397 (7%)]\tLoss: 1.170879\n",
      "Train Epoch: 5 [1280/9397 (14%)]\tLoss: 1.359698\n",
      "Train Epoch: 5 [1920/9397 (20%)]\tLoss: 1.310216\n",
      "Train Epoch: 5 [2560/9397 (27%)]\tLoss: 1.141812\n",
      "Train Epoch: 5 [3200/9397 (34%)]\tLoss: 1.006143\n",
      "Train Epoch: 5 [3840/9397 (41%)]\tLoss: 1.018244\n",
      "Train Epoch: 5 [4480/9397 (48%)]\tLoss: 1.180436\n",
      "Train Epoch: 5 [5120/9397 (54%)]\tLoss: 1.434985\n",
      "Train Epoch: 5 [5760/9397 (61%)]\tLoss: 1.076858\n",
      "Train Epoch: 5 [6400/9397 (68%)]\tLoss: 1.161160\n",
      "Train Epoch: 5 [7040/9397 (75%)]\tLoss: 1.098601\n",
      "Train Epoch: 5 [7680/9397 (82%)]\tLoss: 1.022981\n",
      "Train Epoch: 5 [8320/9397 (88%)]\tLoss: 1.298296\n",
      "Train Epoch: 5 [8960/9397 (95%)]\tLoss: 1.117068\n",
      "Train Epoch: 6 [0/9397 (0%)]\tLoss: 0.947449\n",
      "Train Epoch: 6 [640/9397 (7%)]\tLoss: 0.971706\n",
      "Train Epoch: 6 [1280/9397 (14%)]\tLoss: 1.097845\n",
      "Train Epoch: 6 [1920/9397 (20%)]\tLoss: 1.221103\n",
      "Train Epoch: 6 [2560/9397 (27%)]\tLoss: 1.361071\n",
      "Train Epoch: 6 [3200/9397 (34%)]\tLoss: 1.151793\n",
      "Train Epoch: 6 [3840/9397 (41%)]\tLoss: 1.141716\n",
      "Train Epoch: 6 [4480/9397 (48%)]\tLoss: 1.295017\n",
      "Train Epoch: 6 [5120/9397 (54%)]\tLoss: 1.107188\n",
      "Train Epoch: 6 [5760/9397 (61%)]\tLoss: 1.209063\n",
      "Train Epoch: 6 [6400/9397 (68%)]\tLoss: 1.056622\n",
      "Train Epoch: 6 [7040/9397 (75%)]\tLoss: 1.030613\n",
      "Train Epoch: 6 [7680/9397 (82%)]\tLoss: 1.271614\n",
      "Train Epoch: 6 [8320/9397 (88%)]\tLoss: 1.148206\n",
      "Train Epoch: 6 [8960/9397 (95%)]\tLoss: 1.128054\n",
      "Train Epoch: 7 [0/9397 (0%)]\tLoss: 1.071023\n",
      "Train Epoch: 7 [640/9397 (7%)]\tLoss: 1.145169\n",
      "Train Epoch: 7 [1280/9397 (14%)]\tLoss: 1.412617\n",
      "Train Epoch: 7 [1920/9397 (20%)]\tLoss: 1.296158\n",
      "Train Epoch: 7 [2560/9397 (27%)]\tLoss: 1.234595\n",
      "Train Epoch: 7 [3200/9397 (34%)]\tLoss: 1.159713\n",
      "Train Epoch: 7 [3840/9397 (41%)]\tLoss: 1.092741\n",
      "Train Epoch: 7 [4480/9397 (48%)]\tLoss: 1.220400\n",
      "Train Epoch: 7 [5120/9397 (54%)]\tLoss: 1.095215\n",
      "Train Epoch: 7 [5760/9397 (61%)]\tLoss: 1.202196\n",
      "Train Epoch: 7 [6400/9397 (68%)]\tLoss: 1.035204\n",
      "Train Epoch: 7 [7040/9397 (75%)]\tLoss: 1.196844\n",
      "Train Epoch: 7 [7680/9397 (82%)]\tLoss: 1.148959\n",
      "Train Epoch: 7 [8320/9397 (88%)]\tLoss: 0.968959\n",
      "Train Epoch: 7 [8960/9397 (95%)]\tLoss: 1.164405\n",
      "Train Epoch: 8 [0/9397 (0%)]\tLoss: 1.066749\n",
      "Train Epoch: 8 [640/9397 (7%)]\tLoss: 1.154568\n",
      "Train Epoch: 8 [1280/9397 (14%)]\tLoss: 1.031965\n",
      "Train Epoch: 8 [1920/9397 (20%)]\tLoss: 1.145899\n",
      "Train Epoch: 8 [2560/9397 (27%)]\tLoss: 1.038728\n",
      "Train Epoch: 8 [3200/9397 (34%)]\tLoss: 1.161769\n",
      "Train Epoch: 8 [3840/9397 (41%)]\tLoss: 1.297781\n",
      "Train Epoch: 8 [4480/9397 (48%)]\tLoss: 1.003627\n",
      "Train Epoch: 8 [5120/9397 (54%)]\tLoss: 1.527396\n",
      "Train Epoch: 8 [5760/9397 (61%)]\tLoss: 1.213269\n",
      "Train Epoch: 8 [6400/9397 (68%)]\tLoss: 1.236513\n",
      "Train Epoch: 8 [7040/9397 (75%)]\tLoss: 1.038007\n",
      "Train Epoch: 8 [7680/9397 (82%)]\tLoss: 1.033756\n",
      "Train Epoch: 8 [8320/9397 (88%)]\tLoss: 1.216865\n",
      "Train Epoch: 8 [8960/9397 (95%)]\tLoss: 0.986805\n",
      "Train Epoch: 9 [0/9397 (0%)]\tLoss: 1.105623\n",
      "Train Epoch: 9 [640/9397 (7%)]\tLoss: 1.280146\n",
      "Train Epoch: 9 [1280/9397 (14%)]\tLoss: 1.307351\n",
      "Train Epoch: 9 [1920/9397 (20%)]\tLoss: 1.236150\n",
      "Train Epoch: 9 [2560/9397 (27%)]\tLoss: 1.175732\n",
      "Train Epoch: 9 [3200/9397 (34%)]\tLoss: 1.096750\n",
      "Train Epoch: 9 [3840/9397 (41%)]\tLoss: 1.043741\n",
      "Train Epoch: 9 [4480/9397 (48%)]\tLoss: 1.124490\n",
      "Train Epoch: 9 [5120/9397 (54%)]\tLoss: 1.146448\n",
      "Train Epoch: 9 [5760/9397 (61%)]\tLoss: 0.919925\n",
      "Train Epoch: 9 [6400/9397 (68%)]\tLoss: 1.312836\n",
      "Train Epoch: 9 [7040/9397 (75%)]\tLoss: 1.240893\n",
      "Train Epoch: 9 [7680/9397 (82%)]\tLoss: 1.411319\n",
      "Train Epoch: 9 [8320/9397 (88%)]\tLoss: 1.246076\n",
      "Train Epoch: 9 [8960/9397 (95%)]\tLoss: 1.019117\n",
      "Train Epoch: 10 [0/9397 (0%)]\tLoss: 1.259897\n",
      "Train Epoch: 10 [640/9397 (7%)]\tLoss: 1.256950\n",
      "Train Epoch: 10 [1280/9397 (14%)]\tLoss: 1.014988\n",
      "Train Epoch: 10 [1920/9397 (20%)]\tLoss: 1.168920\n",
      "Train Epoch: 10 [2560/9397 (27%)]\tLoss: 0.943502\n",
      "Train Epoch: 10 [3200/9397 (34%)]\tLoss: 1.105095\n",
      "Train Epoch: 10 [3840/9397 (41%)]\tLoss: 1.045700\n",
      "Train Epoch: 10 [4480/9397 (48%)]\tLoss: 0.943959\n",
      "Train Epoch: 10 [5120/9397 (54%)]\tLoss: 1.114395\n",
      "Train Epoch: 10 [5760/9397 (61%)]\tLoss: 1.271853\n",
      "Train Epoch: 10 [6400/9397 (68%)]\tLoss: 1.107839\n",
      "Train Epoch: 10 [7040/9397 (75%)]\tLoss: 1.338118\n",
      "Train Epoch: 10 [7680/9397 (82%)]\tLoss: 1.414155\n",
      "Train Epoch: 10 [8320/9397 (88%)]\tLoss: 1.278861\n",
      "Train Epoch: 10 [8960/9397 (95%)]\tLoss: 1.020443\n",
      "Train Epoch: 11 [0/9397 (0%)]\tLoss: 1.024327\n",
      "Train Epoch: 11 [640/9397 (7%)]\tLoss: 1.259964\n",
      "Train Epoch: 11 [1280/9397 (14%)]\tLoss: 1.213066\n",
      "Train Epoch: 11 [1920/9397 (20%)]\tLoss: 1.222090\n",
      "Train Epoch: 11 [2560/9397 (27%)]\tLoss: 1.302355\n",
      "Train Epoch: 11 [3200/9397 (34%)]\tLoss: 1.067545\n",
      "Train Epoch: 11 [3840/9397 (41%)]\tLoss: 1.290102\n",
      "Train Epoch: 11 [4480/9397 (48%)]\tLoss: 1.182017\n",
      "Train Epoch: 11 [5120/9397 (54%)]\tLoss: 1.304628\n",
      "Train Epoch: 11 [5760/9397 (61%)]\tLoss: 1.005154\n",
      "Train Epoch: 11 [6400/9397 (68%)]\tLoss: 1.162040\n",
      "Train Epoch: 11 [7040/9397 (75%)]\tLoss: 0.992849\n",
      "Train Epoch: 11 [7680/9397 (82%)]\tLoss: 1.003638\n",
      "Train Epoch: 11 [8320/9397 (88%)]\tLoss: 1.094772\n",
      "Train Epoch: 11 [8960/9397 (95%)]\tLoss: 1.032393\n",
      "Train Epoch: 12 [0/9397 (0%)]\tLoss: 1.125335\n",
      "Train Epoch: 12 [640/9397 (7%)]\tLoss: 1.304307\n",
      "Train Epoch: 12 [1280/9397 (14%)]\tLoss: 1.106924\n",
      "Train Epoch: 12 [1920/9397 (20%)]\tLoss: 0.949120\n",
      "Train Epoch: 12 [2560/9397 (27%)]\tLoss: 1.165619\n",
      "Train Epoch: 12 [3200/9397 (34%)]\tLoss: 0.950887\n",
      "Train Epoch: 12 [3840/9397 (41%)]\tLoss: 1.081872\n",
      "Train Epoch: 12 [4480/9397 (48%)]\tLoss: 1.182918\n",
      "Train Epoch: 12 [5120/9397 (54%)]\tLoss: 1.322944\n",
      "Train Epoch: 12 [5760/9397 (61%)]\tLoss: 1.201287\n",
      "Train Epoch: 12 [6400/9397 (68%)]\tLoss: 1.223811\n",
      "Train Epoch: 12 [7040/9397 (75%)]\tLoss: 1.254954\n",
      "Train Epoch: 12 [7680/9397 (82%)]\tLoss: 0.981131\n",
      "Train Epoch: 12 [8320/9397 (88%)]\tLoss: 1.226642\n",
      "Train Epoch: 12 [8960/9397 (95%)]\tLoss: 1.133699\n",
      "Train Epoch: 13 [0/9397 (0%)]\tLoss: 1.265114\n",
      "Train Epoch: 13 [640/9397 (7%)]\tLoss: 1.112166\n",
      "Train Epoch: 13 [1280/9397 (14%)]\tLoss: 1.336428\n",
      "Train Epoch: 13 [1920/9397 (20%)]\tLoss: 1.088767\n",
      "Train Epoch: 13 [2560/9397 (27%)]\tLoss: 1.032939\n",
      "Train Epoch: 13 [3200/9397 (34%)]\tLoss: 1.255105\n",
      "Train Epoch: 13 [3840/9397 (41%)]\tLoss: 1.314823\n",
      "Train Epoch: 13 [4480/9397 (48%)]\tLoss: 1.185390\n",
      "Train Epoch: 13 [5120/9397 (54%)]\tLoss: 1.201118\n",
      "Train Epoch: 13 [5760/9397 (61%)]\tLoss: 1.139313\n",
      "Train Epoch: 13 [6400/9397 (68%)]\tLoss: 1.023917\n",
      "Train Epoch: 13 [7040/9397 (75%)]\tLoss: 0.933247\n",
      "Train Epoch: 13 [7680/9397 (82%)]\tLoss: 1.291878\n",
      "Train Epoch: 13 [8320/9397 (88%)]\tLoss: 1.134894\n",
      "Train Epoch: 13 [8960/9397 (95%)]\tLoss: 1.312155\n",
      "Train Epoch: 14 [0/9397 (0%)]\tLoss: 1.212504\n",
      "Train Epoch: 14 [640/9397 (7%)]\tLoss: 1.103489\n",
      "Train Epoch: 14 [1280/9397 (14%)]\tLoss: 1.072433\n",
      "Train Epoch: 14 [1920/9397 (20%)]\tLoss: 1.192178\n",
      "Train Epoch: 14 [2560/9397 (27%)]\tLoss: 1.007157\n",
      "Train Epoch: 14 [3200/9397 (34%)]\tLoss: 0.927385\n",
      "Train Epoch: 14 [3840/9397 (41%)]\tLoss: 0.918459\n",
      "Train Epoch: 14 [4480/9397 (48%)]\tLoss: 1.189105\n",
      "Train Epoch: 14 [5120/9397 (54%)]\tLoss: 1.205762\n",
      "Train Epoch: 14 [5760/9397 (61%)]\tLoss: 1.067963\n",
      "Train Epoch: 14 [6400/9397 (68%)]\tLoss: 0.949186\n",
      "Train Epoch: 14 [7040/9397 (75%)]\tLoss: 1.042077\n",
      "Train Epoch: 14 [7680/9397 (82%)]\tLoss: 1.064803\n",
      "Train Epoch: 14 [8320/9397 (88%)]\tLoss: 1.193063\n",
      "Train Epoch: 14 [8960/9397 (95%)]\tLoss: 1.118018\n",
      "Train Epoch: 15 [0/9397 (0%)]\tLoss: 1.014760\n",
      "Train Epoch: 15 [640/9397 (7%)]\tLoss: 1.215763\n",
      "Train Epoch: 15 [1280/9397 (14%)]\tLoss: 0.991080\n",
      "Train Epoch: 15 [1920/9397 (20%)]\tLoss: 1.170521\n",
      "Train Epoch: 15 [2560/9397 (27%)]\tLoss: 1.014642\n",
      "Train Epoch: 15 [3200/9397 (34%)]\tLoss: 1.163237\n",
      "Train Epoch: 15 [3840/9397 (41%)]\tLoss: 1.243590\n",
      "Train Epoch: 15 [4480/9397 (48%)]\tLoss: 1.074615\n",
      "Train Epoch: 15 [5120/9397 (54%)]\tLoss: 1.213639\n",
      "Train Epoch: 15 [5760/9397 (61%)]\tLoss: 1.177169\n",
      "Train Epoch: 15 [6400/9397 (68%)]\tLoss: 1.234916\n",
      "Train Epoch: 15 [7040/9397 (75%)]\tLoss: 1.131977\n",
      "Train Epoch: 15 [7680/9397 (82%)]\tLoss: 1.164405\n",
      "Train Epoch: 15 [8320/9397 (88%)]\tLoss: 1.050864\n",
      "Train Epoch: 15 [8960/9397 (95%)]\tLoss: 1.428324\n",
      "Train Epoch: 16 [0/9397 (0%)]\tLoss: 1.058280\n",
      "Train Epoch: 16 [640/9397 (7%)]\tLoss: 1.033412\n",
      "Train Epoch: 16 [1280/9397 (14%)]\tLoss: 1.001515\n",
      "Train Epoch: 16 [1920/9397 (20%)]\tLoss: 0.886451\n",
      "Train Epoch: 16 [2560/9397 (27%)]\tLoss: 0.939054\n",
      "Train Epoch: 16 [3200/9397 (34%)]\tLoss: 1.124177\n",
      "Train Epoch: 16 [3840/9397 (41%)]\tLoss: 1.070760\n",
      "Train Epoch: 16 [4480/9397 (48%)]\tLoss: 1.272175\n",
      "Train Epoch: 16 [5120/9397 (54%)]\tLoss: 1.151483\n",
      "Train Epoch: 16 [5760/9397 (61%)]\tLoss: 1.071914\n",
      "Train Epoch: 16 [6400/9397 (68%)]\tLoss: 0.935248\n",
      "Train Epoch: 16 [7040/9397 (75%)]\tLoss: 1.008753\n",
      "Train Epoch: 16 [7680/9397 (82%)]\tLoss: 0.899420\n",
      "Train Epoch: 16 [8320/9397 (88%)]\tLoss: 1.077034\n",
      "Train Epoch: 16 [8960/9397 (95%)]\tLoss: 1.304047\n",
      "Train Epoch: 17 [0/9397 (0%)]\tLoss: 1.319699\n",
      "Train Epoch: 17 [640/9397 (7%)]\tLoss: 1.162038\n",
      "Train Epoch: 17 [1280/9397 (14%)]\tLoss: 1.114857\n",
      "Train Epoch: 17 [1920/9397 (20%)]\tLoss: 1.009091\n",
      "Train Epoch: 17 [2560/9397 (27%)]\tLoss: 1.113317\n",
      "Train Epoch: 17 [3200/9397 (34%)]\tLoss: 1.143290\n",
      "Train Epoch: 17 [3840/9397 (41%)]\tLoss: 1.254614\n",
      "Train Epoch: 17 [4480/9397 (48%)]\tLoss: 1.064927\n",
      "Train Epoch: 17 [5120/9397 (54%)]\tLoss: 1.128171\n",
      "Train Epoch: 17 [5760/9397 (61%)]\tLoss: 1.139519\n",
      "Train Epoch: 17 [6400/9397 (68%)]\tLoss: 1.035538\n",
      "Train Epoch: 17 [7040/9397 (75%)]\tLoss: 0.869551\n",
      "Train Epoch: 17 [7680/9397 (82%)]\tLoss: 1.455762\n",
      "Train Epoch: 17 [8320/9397 (88%)]\tLoss: 1.133108\n",
      "Train Epoch: 17 [8960/9397 (95%)]\tLoss: 0.954250\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/1887 (0%)]\tLoss: 1.374629\n",
      "Train Epoch: 1 [640/1887 (33%)]\tLoss: 1.246838\n",
      "Train Epoch: 1 [1280/1887 (67%)]\tLoss: 1.355068\n",
      "Train Epoch: 2 [0/1887 (0%)]\tLoss: 1.323192\n",
      "Train Epoch: 2 [640/1887 (33%)]\tLoss: 1.314466\n",
      "Train Epoch: 2 [1280/1887 (67%)]\tLoss: 1.083916\n",
      "Train Epoch: 3 [0/1887 (0%)]\tLoss: 1.213651\n",
      "Train Epoch: 3 [640/1887 (33%)]\tLoss: 1.227559\n",
      "Train Epoch: 3 [1280/1887 (67%)]\tLoss: 1.206188\n",
      "Train Epoch: 4 [0/1887 (0%)]\tLoss: 1.151286\n",
      "Train Epoch: 4 [640/1887 (33%)]\tLoss: 1.287457\n",
      "Train Epoch: 4 [1280/1887 (67%)]\tLoss: 1.225840\n",
      "Train Epoch: 5 [0/1887 (0%)]\tLoss: 1.073219\n",
      "Train Epoch: 5 [640/1887 (33%)]\tLoss: 1.351359\n",
      "Train Epoch: 5 [1280/1887 (67%)]\tLoss: 1.106829\n",
      "Train Epoch: 6 [0/1887 (0%)]\tLoss: 1.304334\n",
      "Train Epoch: 6 [640/1887 (33%)]\tLoss: 1.306199\n",
      "Train Epoch: 6 [1280/1887 (67%)]\tLoss: 1.133485\n",
      "Train Epoch: 7 [0/1887 (0%)]\tLoss: 1.211851\n",
      "Train Epoch: 7 [640/1887 (33%)]\tLoss: 0.975085\n",
      "Train Epoch: 7 [1280/1887 (67%)]\tLoss: 1.197207\n",
      "Train Epoch: 8 [0/1887 (0%)]\tLoss: 1.146121\n",
      "Train Epoch: 8 [640/1887 (33%)]\tLoss: 1.253888\n",
      "Train Epoch: 8 [1280/1887 (67%)]\tLoss: 1.071814\n",
      "Train Epoch: 9 [0/1887 (0%)]\tLoss: 1.347027\n",
      "Train Epoch: 9 [640/1887 (33%)]\tLoss: 1.120045\n",
      "Train Epoch: 9 [1280/1887 (67%)]\tLoss: 1.217242\n",
      "Train Epoch: 10 [0/1887 (0%)]\tLoss: 1.186281\n",
      "Train Epoch: 10 [640/1887 (33%)]\tLoss: 1.149936\n",
      "Train Epoch: 10 [1280/1887 (67%)]\tLoss: 1.166611\n",
      "Train Epoch: 11 [0/1887 (0%)]\tLoss: 1.106657\n",
      "Train Epoch: 11 [640/1887 (33%)]\tLoss: 1.432699\n",
      "Train Epoch: 11 [1280/1887 (67%)]\tLoss: 1.249565\n",
      "Train Epoch: 12 [0/1887 (0%)]\tLoss: 1.147781\n",
      "Train Epoch: 12 [640/1887 (33%)]\tLoss: 0.883668\n",
      "Train Epoch: 12 [1280/1887 (67%)]\tLoss: 0.869313\n",
      "Train Epoch: 13 [0/1887 (0%)]\tLoss: 1.027221\n",
      "Train Epoch: 13 [640/1887 (33%)]\tLoss: 1.113936\n",
      "Train Epoch: 13 [1280/1887 (67%)]\tLoss: 1.135345\n",
      "Train Epoch: 14 [0/1887 (0%)]\tLoss: 1.002593\n",
      "Train Epoch: 14 [640/1887 (33%)]\tLoss: 1.026725\n",
      "Train Epoch: 14 [1280/1887 (67%)]\tLoss: 1.031658\n",
      "Train Epoch: 15 [0/1887 (0%)]\tLoss: 1.097010\n",
      "Train Epoch: 15 [640/1887 (33%)]\tLoss: 0.960049\n",
      "Train Epoch: 15 [1280/1887 (67%)]\tLoss: 1.101172\n",
      "Train Epoch: 16 [0/1887 (0%)]\tLoss: 1.009693\n",
      "Train Epoch: 16 [640/1887 (33%)]\tLoss: 1.301410\n",
      "Train Epoch: 16 [1280/1887 (67%)]\tLoss: 1.059645\n",
      "Train Epoch: 17 [0/1887 (0%)]\tLoss: 1.146422\n",
      "Train Epoch: 17 [640/1887 (33%)]\tLoss: 1.364328\n",
      "Train Epoch: 17 [1280/1887 (67%)]\tLoss: 1.291723\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/5490 (0%)]\tLoss: 1.553405\n",
      "Train Epoch: 1 [640/5490 (12%)]\tLoss: 0.960109\n",
      "Train Epoch: 1 [1280/5490 (23%)]\tLoss: 1.434313\n",
      "Train Epoch: 1 [1920/5490 (35%)]\tLoss: 1.488268\n",
      "Train Epoch: 1 [2560/5490 (47%)]\tLoss: 0.962912\n",
      "Train Epoch: 1 [3200/5490 (58%)]\tLoss: 1.105256\n",
      "Train Epoch: 1 [3840/5490 (70%)]\tLoss: 1.277120\n",
      "Train Epoch: 1 [4480/5490 (81%)]\tLoss: 1.132754\n",
      "Train Epoch: 1 [5120/5490 (93%)]\tLoss: 1.187582\n",
      "Train Epoch: 2 [0/5490 (0%)]\tLoss: 0.980781\n",
      "Train Epoch: 2 [640/5490 (12%)]\tLoss: 0.930808\n",
      "Train Epoch: 2 [1280/5490 (23%)]\tLoss: 1.084267\n",
      "Train Epoch: 2 [1920/5490 (35%)]\tLoss: 1.608740\n",
      "Train Epoch: 2 [2560/5490 (47%)]\tLoss: 1.306514\n",
      "Train Epoch: 2 [3200/5490 (58%)]\tLoss: 1.453531\n",
      "Train Epoch: 2 [3840/5490 (70%)]\tLoss: 0.869068\n",
      "Train Epoch: 2 [4480/5490 (81%)]\tLoss: 1.017427\n",
      "Train Epoch: 2 [5120/5490 (93%)]\tLoss: 1.241807\n",
      "Train Epoch: 3 [0/5490 (0%)]\tLoss: 1.122007\n",
      "Train Epoch: 3 [640/5490 (12%)]\tLoss: 1.116504\n",
      "Train Epoch: 3 [1280/5490 (23%)]\tLoss: 1.437568\n",
      "Train Epoch: 3 [1920/5490 (35%)]\tLoss: 1.017666\n",
      "Train Epoch: 3 [2560/5490 (47%)]\tLoss: 1.005772\n",
      "Train Epoch: 3 [3200/5490 (58%)]\tLoss: 0.987806\n",
      "Train Epoch: 3 [3840/5490 (70%)]\tLoss: 0.975829\n",
      "Train Epoch: 3 [4480/5490 (81%)]\tLoss: 1.228513\n",
      "Train Epoch: 3 [5120/5490 (93%)]\tLoss: 0.991185\n",
      "Train Epoch: 4 [0/5490 (0%)]\tLoss: 1.415832\n",
      "Train Epoch: 4 [640/5490 (12%)]\tLoss: 1.119595\n",
      "Train Epoch: 4 [1280/5490 (23%)]\tLoss: 1.019336\n",
      "Train Epoch: 4 [1920/5490 (35%)]\tLoss: 0.966583\n",
      "Train Epoch: 4 [2560/5490 (47%)]\tLoss: 1.052573\n",
      "Train Epoch: 4 [3200/5490 (58%)]\tLoss: 0.962831\n",
      "Train Epoch: 4 [3840/5490 (70%)]\tLoss: 1.133777\n",
      "Train Epoch: 4 [4480/5490 (81%)]\tLoss: 0.988381\n",
      "Train Epoch: 4 [5120/5490 (93%)]\tLoss: 1.343292\n",
      "Train Epoch: 5 [0/5490 (0%)]\tLoss: 0.971267\n",
      "Train Epoch: 5 [640/5490 (12%)]\tLoss: 1.201469\n",
      "Train Epoch: 5 [1280/5490 (23%)]\tLoss: 1.017432\n",
      "Train Epoch: 5 [1920/5490 (35%)]\tLoss: 1.050754\n",
      "Train Epoch: 5 [2560/5490 (47%)]\tLoss: 1.172964\n",
      "Train Epoch: 5 [3200/5490 (58%)]\tLoss: 0.972158\n",
      "Train Epoch: 5 [3840/5490 (70%)]\tLoss: 1.121465\n",
      "Train Epoch: 5 [4480/5490 (81%)]\tLoss: 0.903314\n",
      "Train Epoch: 5 [5120/5490 (93%)]\tLoss: 1.263743\n",
      "Train Epoch: 6 [0/5490 (0%)]\tLoss: 1.091219\n",
      "Train Epoch: 6 [640/5490 (12%)]\tLoss: 0.864673\n",
      "Train Epoch: 6 [1280/5490 (23%)]\tLoss: 0.956638\n",
      "Train Epoch: 6 [1920/5490 (35%)]\tLoss: 1.112113\n",
      "Train Epoch: 6 [2560/5490 (47%)]\tLoss: 1.031306\n",
      "Train Epoch: 6 [3200/5490 (58%)]\tLoss: 1.192247\n",
      "Train Epoch: 6 [3840/5490 (70%)]\tLoss: 1.125466\n",
      "Train Epoch: 6 [4480/5490 (81%)]\tLoss: 0.972107\n",
      "Train Epoch: 6 [5120/5490 (93%)]\tLoss: 0.987185\n",
      "Train Epoch: 7 [0/5490 (0%)]\tLoss: 1.131267\n",
      "Train Epoch: 7 [640/5490 (12%)]\tLoss: 1.210151\n",
      "Train Epoch: 7 [1280/5490 (23%)]\tLoss: 1.171860\n",
      "Train Epoch: 7 [1920/5490 (35%)]\tLoss: 0.848209\n",
      "Train Epoch: 7 [2560/5490 (47%)]\tLoss: 0.995048\n",
      "Train Epoch: 7 [3200/5490 (58%)]\tLoss: 0.894138\n",
      "Train Epoch: 7 [3840/5490 (70%)]\tLoss: 1.054935\n",
      "Train Epoch: 7 [4480/5490 (81%)]\tLoss: 1.120883\n",
      "Train Epoch: 7 [5120/5490 (93%)]\tLoss: 1.338841\n",
      "Train Epoch: 8 [0/5490 (0%)]\tLoss: 1.160868\n",
      "Train Epoch: 8 [640/5490 (12%)]\tLoss: 0.956874\n",
      "Train Epoch: 8 [1280/5490 (23%)]\tLoss: 0.959830\n",
      "Train Epoch: 8 [1920/5490 (35%)]\tLoss: 1.070549\n",
      "Train Epoch: 8 [2560/5490 (47%)]\tLoss: 1.072777\n",
      "Train Epoch: 8 [3200/5490 (58%)]\tLoss: 1.180182\n",
      "Train Epoch: 8 [3840/5490 (70%)]\tLoss: 1.112422\n",
      "Train Epoch: 8 [4480/5490 (81%)]\tLoss: 1.163631\n",
      "Train Epoch: 8 [5120/5490 (93%)]\tLoss: 1.021261\n",
      "Train Epoch: 9 [0/5490 (0%)]\tLoss: 1.050618\n",
      "Train Epoch: 9 [640/5490 (12%)]\tLoss: 1.032963\n",
      "Train Epoch: 9 [1280/5490 (23%)]\tLoss: 0.976554\n",
      "Train Epoch: 9 [1920/5490 (35%)]\tLoss: 0.847716\n",
      "Train Epoch: 9 [2560/5490 (47%)]\tLoss: 1.056066\n",
      "Train Epoch: 9 [3200/5490 (58%)]\tLoss: 1.099693\n",
      "Train Epoch: 9 [3840/5490 (70%)]\tLoss: 0.933515\n",
      "Train Epoch: 9 [4480/5490 (81%)]\tLoss: 1.049809\n",
      "Train Epoch: 9 [5120/5490 (93%)]\tLoss: 0.921276\n",
      "Train Epoch: 10 [0/5490 (0%)]\tLoss: 1.162779\n",
      "Train Epoch: 10 [640/5490 (12%)]\tLoss: 0.822093\n",
      "Train Epoch: 10 [1280/5490 (23%)]\tLoss: 1.138741\n",
      "Train Epoch: 10 [1920/5490 (35%)]\tLoss: 1.259806\n",
      "Train Epoch: 10 [2560/5490 (47%)]\tLoss: 0.812012\n",
      "Train Epoch: 10 [3200/5490 (58%)]\tLoss: 1.033451\n",
      "Train Epoch: 10 [3840/5490 (70%)]\tLoss: 1.104201\n",
      "Train Epoch: 10 [4480/5490 (81%)]\tLoss: 0.833018\n",
      "Train Epoch: 10 [5120/5490 (93%)]\tLoss: 1.176731\n",
      "Train Epoch: 11 [0/5490 (0%)]\tLoss: 0.718453\n",
      "Train Epoch: 11 [640/5490 (12%)]\tLoss: 1.093064\n",
      "Train Epoch: 11 [1280/5490 (23%)]\tLoss: 0.871358\n",
      "Train Epoch: 11 [1920/5490 (35%)]\tLoss: 1.002664\n",
      "Train Epoch: 11 [2560/5490 (47%)]\tLoss: 0.905751\n",
      "Train Epoch: 11 [3200/5490 (58%)]\tLoss: 0.865323\n",
      "Train Epoch: 11 [3840/5490 (70%)]\tLoss: 1.083015\n",
      "Train Epoch: 11 [4480/5490 (81%)]\tLoss: 1.057723\n",
      "Train Epoch: 11 [5120/5490 (93%)]\tLoss: 1.001066\n",
      "Train Epoch: 12 [0/5490 (0%)]\tLoss: 1.157034\n",
      "Train Epoch: 12 [640/5490 (12%)]\tLoss: 1.097814\n",
      "Train Epoch: 12 [1280/5490 (23%)]\tLoss: 1.205211\n",
      "Train Epoch: 12 [1920/5490 (35%)]\tLoss: 1.049624\n",
      "Train Epoch: 12 [2560/5490 (47%)]\tLoss: 0.992680\n",
      "Train Epoch: 12 [3200/5490 (58%)]\tLoss: 1.110721\n",
      "Train Epoch: 12 [3840/5490 (70%)]\tLoss: 1.131562\n",
      "Train Epoch: 12 [4480/5490 (81%)]\tLoss: 0.916238\n",
      "Train Epoch: 12 [5120/5490 (93%)]\tLoss: 0.831645\n",
      "Train Epoch: 13 [0/5490 (0%)]\tLoss: 1.016841\n",
      "Train Epoch: 13 [640/5490 (12%)]\tLoss: 0.948355\n",
      "Train Epoch: 13 [1280/5490 (23%)]\tLoss: 1.174914\n",
      "Train Epoch: 13 [1920/5490 (35%)]\tLoss: 0.918917\n",
      "Train Epoch: 13 [2560/5490 (47%)]\tLoss: 1.013657\n",
      "Train Epoch: 13 [3200/5490 (58%)]\tLoss: 1.001313\n",
      "Train Epoch: 13 [3840/5490 (70%)]\tLoss: 1.313770\n",
      "Train Epoch: 13 [4480/5490 (81%)]\tLoss: 0.996220\n",
      "Train Epoch: 13 [5120/5490 (93%)]\tLoss: 1.063495\n",
      "Train Epoch: 14 [0/5490 (0%)]\tLoss: 0.992169\n",
      "Train Epoch: 14 [640/5490 (12%)]\tLoss: 1.053381\n",
      "Train Epoch: 14 [1280/5490 (23%)]\tLoss: 1.017157\n",
      "Train Epoch: 14 [1920/5490 (35%)]\tLoss: 0.927153\n",
      "Train Epoch: 14 [2560/5490 (47%)]\tLoss: 0.999310\n",
      "Train Epoch: 14 [3200/5490 (58%)]\tLoss: 1.202924\n",
      "Train Epoch: 14 [3840/5490 (70%)]\tLoss: 1.078052\n",
      "Train Epoch: 14 [4480/5490 (81%)]\tLoss: 1.035957\n",
      "Train Epoch: 14 [5120/5490 (93%)]\tLoss: 0.979843\n",
      "Train Epoch: 15 [0/5490 (0%)]\tLoss: 0.865648\n",
      "Train Epoch: 15 [640/5490 (12%)]\tLoss: 1.106761\n",
      "Train Epoch: 15 [1280/5490 (23%)]\tLoss: 1.150026\n",
      "Train Epoch: 15 [1920/5490 (35%)]\tLoss: 0.948157\n",
      "Train Epoch: 15 [2560/5490 (47%)]\tLoss: 1.036406\n",
      "Train Epoch: 15 [3200/5490 (58%)]\tLoss: 1.175745\n",
      "Train Epoch: 15 [3840/5490 (70%)]\tLoss: 0.951744\n",
      "Train Epoch: 15 [4480/5490 (81%)]\tLoss: 1.076013\n",
      "Train Epoch: 15 [5120/5490 (93%)]\tLoss: 1.108285\n",
      "Train Epoch: 16 [0/5490 (0%)]\tLoss: 1.026730\n",
      "Train Epoch: 16 [640/5490 (12%)]\tLoss: 0.806053\n",
      "Train Epoch: 16 [1280/5490 (23%)]\tLoss: 0.961015\n",
      "Train Epoch: 16 [1920/5490 (35%)]\tLoss: 0.896578\n",
      "Train Epoch: 16 [2560/5490 (47%)]\tLoss: 0.984977\n",
      "Train Epoch: 16 [3200/5490 (58%)]\tLoss: 1.003180\n",
      "Train Epoch: 16 [3840/5490 (70%)]\tLoss: 0.926693\n",
      "Train Epoch: 16 [4480/5490 (81%)]\tLoss: 1.077205\n",
      "Train Epoch: 16 [5120/5490 (93%)]\tLoss: 1.237804\n",
      "Train Epoch: 17 [0/5490 (0%)]\tLoss: 1.053458\n",
      "Train Epoch: 17 [640/5490 (12%)]\tLoss: 0.762807\n",
      "Train Epoch: 17 [1280/5490 (23%)]\tLoss: 0.931376\n",
      "Train Epoch: 17 [1920/5490 (35%)]\tLoss: 0.927217\n",
      "Train Epoch: 17 [2560/5490 (47%)]\tLoss: 0.910498\n",
      "Train Epoch: 17 [3200/5490 (58%)]\tLoss: 0.950296\n",
      "Train Epoch: 17 [3840/5490 (70%)]\tLoss: 0.913195\n",
      "Train Epoch: 17 [4480/5490 (81%)]\tLoss: 1.299315\n",
      "Train Epoch: 17 [5120/5490 (93%)]\tLoss: 1.012802\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/2468 (0%)]\tLoss: 1.597325\n",
      "Train Epoch: 1 [640/2468 (26%)]\tLoss: 1.380728\n",
      "Train Epoch: 1 [1280/2468 (51%)]\tLoss: 1.385597\n",
      "Train Epoch: 1 [1920/2468 (77%)]\tLoss: 1.264528\n",
      "Train Epoch: 2 [0/2468 (0%)]\tLoss: 1.401597\n",
      "Train Epoch: 2 [640/2468 (26%)]\tLoss: 1.115333\n",
      "Train Epoch: 2 [1280/2468 (51%)]\tLoss: 1.106099\n",
      "Train Epoch: 2 [1920/2468 (77%)]\tLoss: 1.138988\n",
      "Train Epoch: 3 [0/2468 (0%)]\tLoss: 1.236514\n",
      "Train Epoch: 3 [640/2468 (26%)]\tLoss: 1.350346\n",
      "Train Epoch: 3 [1280/2468 (51%)]\tLoss: 1.259986\n",
      "Train Epoch: 3 [1920/2468 (77%)]\tLoss: 1.125655\n",
      "Train Epoch: 4 [0/2468 (0%)]\tLoss: 1.232038\n",
      "Train Epoch: 4 [640/2468 (26%)]\tLoss: 1.176275\n",
      "Train Epoch: 4 [1280/2468 (51%)]\tLoss: 1.169983\n",
      "Train Epoch: 4 [1920/2468 (77%)]\tLoss: 1.491304\n",
      "Train Epoch: 5 [0/2468 (0%)]\tLoss: 1.096778\n",
      "Train Epoch: 5 [640/2468 (26%)]\tLoss: 1.036010\n",
      "Train Epoch: 5 [1280/2468 (51%)]\tLoss: 1.048260\n",
      "Train Epoch: 5 [1920/2468 (77%)]\tLoss: 0.982887\n",
      "Train Epoch: 6 [0/2468 (0%)]\tLoss: 1.183932\n",
      "Train Epoch: 6 [640/2468 (26%)]\tLoss: 1.170085\n",
      "Train Epoch: 6 [1280/2468 (51%)]\tLoss: 1.173854\n",
      "Train Epoch: 6 [1920/2468 (77%)]\tLoss: 1.105581\n",
      "Train Epoch: 7 [0/2468 (0%)]\tLoss: 1.135336\n",
      "Train Epoch: 7 [640/2468 (26%)]\tLoss: 1.183332\n",
      "Train Epoch: 7 [1280/2468 (51%)]\tLoss: 1.522073\n",
      "Train Epoch: 7 [1920/2468 (77%)]\tLoss: 1.239190\n",
      "Train Epoch: 8 [0/2468 (0%)]\tLoss: 1.183801\n",
      "Train Epoch: 8 [640/2468 (26%)]\tLoss: 1.041268\n",
      "Train Epoch: 8 [1280/2468 (51%)]\tLoss: 0.955407\n",
      "Train Epoch: 8 [1920/2468 (77%)]\tLoss: 0.958614\n",
      "Train Epoch: 9 [0/2468 (0%)]\tLoss: 1.075577\n",
      "Train Epoch: 9 [640/2468 (26%)]\tLoss: 1.345663\n",
      "Train Epoch: 9 [1280/2468 (51%)]\tLoss: 1.232870\n",
      "Train Epoch: 9 [1920/2468 (77%)]\tLoss: 0.917812\n",
      "Train Epoch: 10 [0/2468 (0%)]\tLoss: 0.989845\n",
      "Train Epoch: 10 [640/2468 (26%)]\tLoss: 0.972233\n",
      "Train Epoch: 10 [1280/2468 (51%)]\tLoss: 1.065042\n",
      "Train Epoch: 10 [1920/2468 (77%)]\tLoss: 1.148983\n",
      "Train Epoch: 11 [0/2468 (0%)]\tLoss: 1.085235\n",
      "Train Epoch: 11 [640/2468 (26%)]\tLoss: 0.929814\n",
      "Train Epoch: 11 [1280/2468 (51%)]\tLoss: 1.557107\n",
      "Train Epoch: 11 [1920/2468 (77%)]\tLoss: 1.163991\n",
      "Train Epoch: 12 [0/2468 (0%)]\tLoss: 1.022083\n",
      "Train Epoch: 12 [640/2468 (26%)]\tLoss: 1.112770\n",
      "Train Epoch: 12 [1280/2468 (51%)]\tLoss: 1.204241\n",
      "Train Epoch: 12 [1920/2468 (77%)]\tLoss: 0.960675\n",
      "Train Epoch: 13 [0/2468 (0%)]\tLoss: 1.008293\n",
      "Train Epoch: 13 [640/2468 (26%)]\tLoss: 1.080069\n",
      "Train Epoch: 13 [1280/2468 (51%)]\tLoss: 1.208861\n",
      "Train Epoch: 13 [1920/2468 (77%)]\tLoss: 1.039792\n",
      "Train Epoch: 14 [0/2468 (0%)]\tLoss: 0.949965\n",
      "Train Epoch: 14 [640/2468 (26%)]\tLoss: 1.271517\n",
      "Train Epoch: 14 [1280/2468 (51%)]\tLoss: 1.073616\n",
      "Train Epoch: 14 [1920/2468 (77%)]\tLoss: 1.236639\n",
      "Train Epoch: 15 [0/2468 (0%)]\tLoss: 1.170433\n",
      "Train Epoch: 15 [640/2468 (26%)]\tLoss: 0.907167\n",
      "Train Epoch: 15 [1280/2468 (51%)]\tLoss: 1.213580\n",
      "Train Epoch: 15 [1920/2468 (77%)]\tLoss: 0.972881\n",
      "Train Epoch: 16 [0/2468 (0%)]\tLoss: 1.069885\n",
      "Train Epoch: 16 [640/2468 (26%)]\tLoss: 1.094462\n",
      "Train Epoch: 16 [1280/2468 (51%)]\tLoss: 1.045216\n",
      "Train Epoch: 16 [1920/2468 (77%)]\tLoss: 1.337545\n",
      "Train Epoch: 17 [0/2468 (0%)]\tLoss: 1.168955\n",
      "Train Epoch: 17 [640/2468 (26%)]\tLoss: 1.205575\n",
      "Train Epoch: 17 [1280/2468 (51%)]\tLoss: 0.877955\n",
      "Train Epoch: 17 [1920/2468 (77%)]\tLoss: 1.000201\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.3304, Accuracy: 5212/10000 (52%)\n",
      "\n",
      "Round 4/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.549946\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.307887\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.151141\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.226243\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.358703\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.248609\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.523517\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.299170\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.533558\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.289126\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.368422\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.429486\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.357832\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.459117\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.280863\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.278722\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.190593\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.168683\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.384924\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.256800\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.447143\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.585403\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.462878\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.366296\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.045271\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.264621\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.644046\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.406372\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.416244\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.207966\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.857143\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.524526\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 1.382236\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.296306\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.402041\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.348189\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.478175\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.338285\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.065390\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.123906\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.528558\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.074866\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.345348\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.341080\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.130860\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.389228\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.455384\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.319841\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.385268\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.290357\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.268658\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.346581\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.079858\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.289982\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.209494\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.295774\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.361128\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.374540\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.082655\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.130863\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.239395\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.089040\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.291550\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.109553\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.368531\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 0.980242\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.367468\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.177242\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.161540\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.052238\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.028922\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.188062\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.108743\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.487930\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.230245\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.115525\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.191244\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.185686\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 0.986688\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.220400\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.271305\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.430279\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.275571\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 0.936903\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.137865\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.339291\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.087528\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.019136\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 1.136819\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.202244\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.171442\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.087625\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.121681\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.217673\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.147809\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.140272\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.171205\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.073695\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.173068\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 1.211281\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.234579\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.189125\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.649866\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.267335\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.512187\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.345020\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.416949\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.425845\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.490943\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.285876\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.292297\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.270497\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.460668\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.081446\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.256524\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.314054\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.331998\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.215792\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.311899\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.385822\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.354810\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.233342\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.307926\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.202094\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.223067\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.177626\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.148798\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.304628\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.249862\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.448640\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.264536\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.180174\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.283646\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.138777\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.258746\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.250606\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.252981\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.435419\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.126415\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.413186\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.441650\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.130650\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.391614\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 1.454651\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.411258\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.398677\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.132121\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.165952\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.382911\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.198599\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.295012\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.358168\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.278287\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.214953\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.185322\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.412485\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.499772\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.359097\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.187730\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.210680\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.111582\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.234309\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 1.166994\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.193724\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.251525\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.038200\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.180142\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.124525\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.091058\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.173857\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.219332\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.065450\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.268225\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.156595\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.211641\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.076337\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.203327\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.144076\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.300678\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.012871\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 1.143101\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.094315\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.253006\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 0.893957\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.434147\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.152963\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.024330\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.271057\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 1.066423\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.305777\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.370716\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.436796\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.278722\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.276739\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.237021\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.124568\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.093813\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.223130\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.173785\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.318808\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.166448\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 0.967654\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.140075\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.059939\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.451002\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.160171\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.188461\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 0.998838\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.248181\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 1.098458\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 1.111983\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.180354\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.125665\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 0.933393\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 1.094636\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.130410\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 1.204251\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.199400\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 1.138621\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.183132\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 0.948122\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 1.145757\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.066218\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.253079\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 1.353935\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 1.224101\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.107465\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.122227\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.165791\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.146139\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 1.193686\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.172166\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 1.083614\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 1.205288\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 0.885207\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 1.024317\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 0.845290\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 1.172270\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 1.054447\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 0.822535\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.170974\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.057647\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 0.966891\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 0.908075\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 1.058822\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 1.388202\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 1.032555\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 0.939860\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 1.153798\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 0.792335\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 1.055072\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 1.044757\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 1.165753\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 0.979304\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 1.061241\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.135879\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 0.989364\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 0.957552\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 0.873622\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 0.819335\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 0.936858\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 1.342137\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 0.918930\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 0.960469\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 1.098683\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 0.923536\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 0.986247\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 1.082956\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 0.982462\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 1.083254\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.011832\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 1.084915\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 1.240107\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 0.930333\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 0.941863\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 1.186567\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 0.905621\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 1.041360\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.202213\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 0.959712\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.077905\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 1.040599\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 0.962089\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 0.838228\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 0.848293\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 1.011211\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 0.918878\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 1.208847\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 1.125383\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 1.089730\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 0.914834\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 0.961454\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 1.117224\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 1.106296\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 1.027971\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 1.277486\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 0.809090\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 0.872298\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 0.913434\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 0.908091\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 1.034551\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 0.995369\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 0.974416\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 1.009113\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 1.035579\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 1.082902\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 0.864540\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 0.822645\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 1.081013\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 1.053041\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 0.898127\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 1.208524\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 0.883015\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 0.930871\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 0.803740\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 1.036983\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.207568\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 1.080517\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 0.947128\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.895618\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 0.849838\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 0.844025\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 1.165535\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 0.786276\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 0.978728\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 0.818782\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 0.777391\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 1.248091\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 1.014806\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 1.099966\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 1.204122\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 0.955924\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 0.893071\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 0.820728\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 0.904741\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 1.256903\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 0.772008\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 1.011657\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 0.902425\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 0.957358\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 0.677074\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 0.847859\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 0.904379\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 1.095852\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 0.858173\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 0.946205\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.837227\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 1.116421\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 0.987602\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 0.940755\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 1.083905\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 0.882152\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 1.107838\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 0.883770\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 1.058868\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 0.866232\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 1.168692\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 0.980723\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 1.205952\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.799038\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 1.218932\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 0.825221\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 0.958981\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.085906\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 1.086073\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 0.762980\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 0.920850\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 0.780628\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 1.210214\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 1.030294\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.912693\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.851149\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.806848\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 1.180787\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.241519\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.106182\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 1.258864\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 0.889346\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.018006\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.157295\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 1.347708\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.037708\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 1.105405\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.001623\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 1.030936\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 1.078251\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 1.180926\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 0.967283\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.239193\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 0.955944\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 0.966280\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 1.048797\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 0.881659\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 0.898485\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 0.902290\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 0.960071\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 0.810753\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 0.981592\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 1.113359\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 0.884510\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 0.932984\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 1.093664\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 1.259553\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 1.260464\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 1.083294\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 1.042548\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 1.035212\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 1.110963\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 0.921611\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 1.204788\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 0.688330\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 0.877857\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 1.009583\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 1.162509\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 0.927723\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 0.920586\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 0.895951\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 1.088197\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 1.194710\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 0.962796\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 1.090648\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 0.604085\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 0.780347\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 0.880696\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 0.781049\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 1.061638\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 1.025260\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 0.996721\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 1.399181\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.803304\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 0.914707\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 1.232572\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 0.889331\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 0.980909\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 0.914505\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 1.044876\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 0.873072\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 0.960424\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.123405\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 0.894007\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 0.691452\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 0.883139\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 0.961766\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 1.058095\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 1.003727\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 0.891363\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 1.198423\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 1.192042\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 0.907421\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 0.872557\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 0.997113\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 1.014675\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 0.909001\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 0.918804\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 0.770803\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 1.184476\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 0.959810\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.890042\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 0.956240\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 0.970310\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 1.254094\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.886730\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 0.814085\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 1.003987\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 0.856146\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 1.146182\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 1.131566\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 0.816822\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 0.941784\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 0.958021\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 1.127955\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.882371\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 1.140223\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.862428\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 0.762348\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.974556\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 0.829694\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 0.897501\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 1.248496\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 0.843532\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 0.917815\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 0.870704\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 1.051515\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 1.155968\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 0.925038\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.967741\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 0.969413\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.934652\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 0.982421\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 0.933692\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.854863\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 0.901384\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 1.165561\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 0.991275\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 0.938855\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 1.149730\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 1.027644\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 0.930534\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 0.897025\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.841078\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.988342\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 1.059394\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 0.759218\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 1.057339\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.929845\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 1.023857\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 1.102808\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 0.709206\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.929533\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 0.827789\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 0.888147\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 0.988346\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 1.067167\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 1.032226\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 0.844143\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 0.741510\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 0.816346\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 0.910635\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.871408\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 1.035231\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 1.052465\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 1.018087\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 0.867621\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.004362\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.907615\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 0.987480\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 0.926175\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.911208\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 1.224770\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 0.803929\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 0.835253\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 1.113530\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 0.806972\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.785930\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.940067\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 1.071664\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 1.095753\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.918180\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 0.966512\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 0.706018\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 0.955216\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 1.034538\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.976028\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 1.050989\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 0.892681\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 0.903931\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.889364\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.909050\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 0.763421\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.827904\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 0.668755\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.710616\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 0.938683\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 1.051824\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.850738\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.868089\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 0.998299\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.854451\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 0.682776\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 0.637874\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 0.955081\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.648579\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.500812\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.441274\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.445233\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.246561\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.301365\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.291088\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.256114\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.263635\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.310948\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.266717\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.352047\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.162104\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.215019\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.195972\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.054089\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.220899\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.084081\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.451239\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.193282\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.550454\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.380371\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.695781\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.214950\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.263997\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.314656\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.293053\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.926844\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.269090\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.295651\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.229172\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.271235\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.341018\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.519794\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.334107\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 0.877278\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.405798\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.180451\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.305229\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.419940\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.116115\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 0.705035\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.183774\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.210737\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.077164\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.385278\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.268504\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.213137\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.232399\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.144607\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.278754\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.330907\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.198081\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.368501\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.283684\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 3.358673\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.333005\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.195774\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.551143\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.234613\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.388777\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.215763\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.349021\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.121986\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.386695\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.314907\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.325298\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.221943\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.381100\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 1.071404\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.315144\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.216164\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 1.294095\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.143332\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.153271\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.062008\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.081598\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.433791\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.321118\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.216811\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 1.168874\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.074844\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.075850\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 0.716175\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.358937\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.275871\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.066306\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 0.950524\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.333934\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.348514\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.222151\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.131339\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.160108\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.051715\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.423360\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.386637\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.208381\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 2.298885\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.537004\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.247992\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.207841\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.260997\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.423609\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.239764\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.016570\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.210853\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.510298\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.406701\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.352054\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.243198\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.215211\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 0.777385\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.227819\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.407284\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.226747\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.355597\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.206749\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.309150\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.300466\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.088125\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.166585\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.067064\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.317410\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.310313\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 0.945141\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 1.339075\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 1.211617\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.058479\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.199391\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.110991\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.140024\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.161074\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.109480\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.320130\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.249889\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.077453\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.351768\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.053301\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.239583\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 0.756256\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.318365\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.058308\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 0.928983\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.239440\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.311755\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.244324\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.074453\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.211911\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.002758\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.079238\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.146182\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.236768\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 1.145396\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 1.085359\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.420840\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.154835\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 1.140852\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 1.201242\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 0.960308\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 1.111920\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.108793\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.021954\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 1.320565\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.152443\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.226927\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 0.991983\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.208011\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 1.828607\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.085999\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.243916\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.214528\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.334419\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.124225\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.270502\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 1.077504\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.058292\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 0.963307\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.360729\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.405968\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.232948\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.202988\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 1.202646\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 1.198174\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.115923\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.115740\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 1.192548\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 1.083244\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 0.935008\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.050565\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.052537\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.212381\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.014738\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 1.092554\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.381890\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.148997\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 1.919505\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.206170\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.254463\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.063609\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.155806\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 1.439543\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 0.962510\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.223319\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 0.960520\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.138871\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.155731\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 1.203633\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 0.989847\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 1.009199\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 2.225293\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 1.263013\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.222703\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.335500\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.254167\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.074483\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.321963\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.121149\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.267388\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 1.302051\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 1.026836\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.283121\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.273821\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.285143\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 1.085289\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.154756\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.246152\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 1.061212\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.094078\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.154274\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.076008\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.147728\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.272193\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.299483\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.318520\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.097144\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.152183\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.265332\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 1.200633\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.561556\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.420065\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.411830\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.489044\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.428085\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.246434\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.372465\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.233336\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.309973\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.263883\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.285987\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.280418\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.244404\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.187318\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.328919\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.500262\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.309762\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.368068\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.378189\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.406965\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.629555\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.297693\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.146065\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.056535\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.299083\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.329380\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.252027\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.261684\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.197350\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.625441\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.479037\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.145563\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.165163\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.109829\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.272600\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.316192\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.088904\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.368944\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.327115\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.053870\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.182597\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.321446\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.233979\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.400968\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.250441\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.221836\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.179586\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.265296\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.138021\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.569349\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.334051\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.051690\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.341682\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.441275\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.202888\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.507503\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.094122\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 0.986687\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.287178\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.017752\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.319127\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.174773\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.422936\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.238982\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.255357\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 0.864308\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.429310\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.257114\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 0.849944\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.061322\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.109985\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.199003\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.278127\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.301873\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 1.050001\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 1.064682\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 1.290620\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 0.999630\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.373470\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.055301\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.234825\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.250178\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.093171\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.263720\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.240531\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/9397 (0%)]\tLoss: 1.387709\n",
      "Train Epoch: 1 [640/9397 (7%)]\tLoss: 1.421156\n",
      "Train Epoch: 1 [1280/9397 (14%)]\tLoss: 1.497573\n",
      "Train Epoch: 1 [1920/9397 (20%)]\tLoss: 1.482294\n",
      "Train Epoch: 1 [2560/9397 (27%)]\tLoss: 1.477150\n",
      "Train Epoch: 1 [3200/9397 (34%)]\tLoss: 1.176353\n",
      "Train Epoch: 1 [3840/9397 (41%)]\tLoss: 1.202880\n",
      "Train Epoch: 1 [4480/9397 (48%)]\tLoss: 1.401361\n",
      "Train Epoch: 1 [5120/9397 (54%)]\tLoss: 1.412032\n",
      "Train Epoch: 1 [5760/9397 (61%)]\tLoss: 1.261413\n",
      "Train Epoch: 1 [6400/9397 (68%)]\tLoss: 1.389821\n",
      "Train Epoch: 1 [7040/9397 (75%)]\tLoss: 1.211673\n",
      "Train Epoch: 1 [7680/9397 (82%)]\tLoss: 0.905703\n",
      "Train Epoch: 1 [8320/9397 (88%)]\tLoss: 1.357390\n",
      "Train Epoch: 1 [8960/9397 (95%)]\tLoss: 0.974732\n",
      "Train Epoch: 2 [0/9397 (0%)]\tLoss: 1.321327\n",
      "Train Epoch: 2 [640/9397 (7%)]\tLoss: 1.068265\n",
      "Train Epoch: 2 [1280/9397 (14%)]\tLoss: 1.301434\n",
      "Train Epoch: 2 [1920/9397 (20%)]\tLoss: 1.170985\n",
      "Train Epoch: 2 [2560/9397 (27%)]\tLoss: 1.217414\n",
      "Train Epoch: 2 [3200/9397 (34%)]\tLoss: 1.295088\n",
      "Train Epoch: 2 [3840/9397 (41%)]\tLoss: 1.009688\n",
      "Train Epoch: 2 [4480/9397 (48%)]\tLoss: 1.189182\n",
      "Train Epoch: 2 [5120/9397 (54%)]\tLoss: 1.278172\n",
      "Train Epoch: 2 [5760/9397 (61%)]\tLoss: 1.233864\n",
      "Train Epoch: 2 [6400/9397 (68%)]\tLoss: 1.188772\n",
      "Train Epoch: 2 [7040/9397 (75%)]\tLoss: 1.270414\n",
      "Train Epoch: 2 [7680/9397 (82%)]\tLoss: 1.437392\n",
      "Train Epoch: 2 [8320/9397 (88%)]\tLoss: 1.068744\n",
      "Train Epoch: 2 [8960/9397 (95%)]\tLoss: 1.114724\n",
      "Train Epoch: 3 [0/9397 (0%)]\tLoss: 1.217486\n",
      "Train Epoch: 3 [640/9397 (7%)]\tLoss: 1.027627\n",
      "Train Epoch: 3 [1280/9397 (14%)]\tLoss: 1.124581\n",
      "Train Epoch: 3 [1920/9397 (20%)]\tLoss: 1.062302\n",
      "Train Epoch: 3 [2560/9397 (27%)]\tLoss: 1.039477\n",
      "Train Epoch: 3 [3200/9397 (34%)]\tLoss: 1.337006\n",
      "Train Epoch: 3 [3840/9397 (41%)]\tLoss: 1.236330\n",
      "Train Epoch: 3 [4480/9397 (48%)]\tLoss: 1.214582\n",
      "Train Epoch: 3 [5120/9397 (54%)]\tLoss: 1.101203\n",
      "Train Epoch: 3 [5760/9397 (61%)]\tLoss: 0.806177\n",
      "Train Epoch: 3 [6400/9397 (68%)]\tLoss: 1.294800\n",
      "Train Epoch: 3 [7040/9397 (75%)]\tLoss: 1.238243\n",
      "Train Epoch: 3 [7680/9397 (82%)]\tLoss: 1.403899\n",
      "Train Epoch: 3 [8320/9397 (88%)]\tLoss: 1.373106\n",
      "Train Epoch: 3 [8960/9397 (95%)]\tLoss: 1.215036\n",
      "Train Epoch: 4 [0/9397 (0%)]\tLoss: 1.009503\n",
      "Train Epoch: 4 [640/9397 (7%)]\tLoss: 1.435236\n",
      "Train Epoch: 4 [1280/9397 (14%)]\tLoss: 1.125942\n",
      "Train Epoch: 4 [1920/9397 (20%)]\tLoss: 1.161528\n",
      "Train Epoch: 4 [2560/9397 (27%)]\tLoss: 1.145370\n",
      "Train Epoch: 4 [3200/9397 (34%)]\tLoss: 1.410396\n",
      "Train Epoch: 4 [3840/9397 (41%)]\tLoss: 1.096893\n",
      "Train Epoch: 4 [4480/9397 (48%)]\tLoss: 1.105665\n",
      "Train Epoch: 4 [5120/9397 (54%)]\tLoss: 1.390390\n",
      "Train Epoch: 4 [5760/9397 (61%)]\tLoss: 1.189381\n",
      "Train Epoch: 4 [6400/9397 (68%)]\tLoss: 0.985849\n",
      "Train Epoch: 4 [7040/9397 (75%)]\tLoss: 1.061239\n",
      "Train Epoch: 4 [7680/9397 (82%)]\tLoss: 1.259287\n",
      "Train Epoch: 4 [8320/9397 (88%)]\tLoss: 1.391536\n",
      "Train Epoch: 4 [8960/9397 (95%)]\tLoss: 1.632524\n",
      "Train Epoch: 5 [0/9397 (0%)]\tLoss: 1.011269\n",
      "Train Epoch: 5 [640/9397 (7%)]\tLoss: 1.213368\n",
      "Train Epoch: 5 [1280/9397 (14%)]\tLoss: 1.063426\n",
      "Train Epoch: 5 [1920/9397 (20%)]\tLoss: 1.246763\n",
      "Train Epoch: 5 [2560/9397 (27%)]\tLoss: 1.391691\n",
      "Train Epoch: 5 [3200/9397 (34%)]\tLoss: 1.243083\n",
      "Train Epoch: 5 [3840/9397 (41%)]\tLoss: 1.296050\n",
      "Train Epoch: 5 [4480/9397 (48%)]\tLoss: 1.150945\n",
      "Train Epoch: 5 [5120/9397 (54%)]\tLoss: 1.085278\n",
      "Train Epoch: 5 [5760/9397 (61%)]\tLoss: 1.176828\n",
      "Train Epoch: 5 [6400/9397 (68%)]\tLoss: 1.019367\n",
      "Train Epoch: 5 [7040/9397 (75%)]\tLoss: 1.490272\n",
      "Train Epoch: 5 [7680/9397 (82%)]\tLoss: 1.217244\n",
      "Train Epoch: 5 [8320/9397 (88%)]\tLoss: 1.347495\n",
      "Train Epoch: 5 [8960/9397 (95%)]\tLoss: 1.348272\n",
      "Train Epoch: 6 [0/9397 (0%)]\tLoss: 1.164298\n",
      "Train Epoch: 6 [640/9397 (7%)]\tLoss: 1.200534\n",
      "Train Epoch: 6 [1280/9397 (14%)]\tLoss: 1.216272\n",
      "Train Epoch: 6 [1920/9397 (20%)]\tLoss: 1.205906\n",
      "Train Epoch: 6 [2560/9397 (27%)]\tLoss: 1.252332\n",
      "Train Epoch: 6 [3200/9397 (34%)]\tLoss: 1.346212\n",
      "Train Epoch: 6 [3840/9397 (41%)]\tLoss: 1.002286\n",
      "Train Epoch: 6 [4480/9397 (48%)]\tLoss: 1.187439\n",
      "Train Epoch: 6 [5120/9397 (54%)]\tLoss: 0.972829\n",
      "Train Epoch: 6 [5760/9397 (61%)]\tLoss: 1.165127\n",
      "Train Epoch: 6 [6400/9397 (68%)]\tLoss: 1.214121\n",
      "Train Epoch: 6 [7040/9397 (75%)]\tLoss: 1.085494\n",
      "Train Epoch: 6 [7680/9397 (82%)]\tLoss: 1.292411\n",
      "Train Epoch: 6 [8320/9397 (88%)]\tLoss: 1.017972\n",
      "Train Epoch: 6 [8960/9397 (95%)]\tLoss: 1.006546\n",
      "Train Epoch: 7 [0/9397 (0%)]\tLoss: 1.262009\n",
      "Train Epoch: 7 [640/9397 (7%)]\tLoss: 1.333815\n",
      "Train Epoch: 7 [1280/9397 (14%)]\tLoss: 1.077643\n",
      "Train Epoch: 7 [1920/9397 (20%)]\tLoss: 1.224221\n",
      "Train Epoch: 7 [2560/9397 (27%)]\tLoss: 1.462783\n",
      "Train Epoch: 7 [3200/9397 (34%)]\tLoss: 1.090376\n",
      "Train Epoch: 7 [3840/9397 (41%)]\tLoss: 1.120165\n",
      "Train Epoch: 7 [4480/9397 (48%)]\tLoss: 1.193968\n",
      "Train Epoch: 7 [5120/9397 (54%)]\tLoss: 1.032370\n",
      "Train Epoch: 7 [5760/9397 (61%)]\tLoss: 1.232671\n",
      "Train Epoch: 7 [6400/9397 (68%)]\tLoss: 1.135624\n",
      "Train Epoch: 7 [7040/9397 (75%)]\tLoss: 1.199337\n",
      "Train Epoch: 7 [7680/9397 (82%)]\tLoss: 1.390176\n",
      "Train Epoch: 7 [8320/9397 (88%)]\tLoss: 1.083041\n",
      "Train Epoch: 7 [8960/9397 (95%)]\tLoss: 0.873978\n",
      "Train Epoch: 8 [0/9397 (0%)]\tLoss: 1.425355\n",
      "Train Epoch: 8 [640/9397 (7%)]\tLoss: 1.002414\n",
      "Train Epoch: 8 [1280/9397 (14%)]\tLoss: 1.094229\n",
      "Train Epoch: 8 [1920/9397 (20%)]\tLoss: 1.141713\n",
      "Train Epoch: 8 [2560/9397 (27%)]\tLoss: 1.198788\n",
      "Train Epoch: 8 [3200/9397 (34%)]\tLoss: 1.024445\n",
      "Train Epoch: 8 [3840/9397 (41%)]\tLoss: 1.209452\n",
      "Train Epoch: 8 [4480/9397 (48%)]\tLoss: 1.158177\n",
      "Train Epoch: 8 [5120/9397 (54%)]\tLoss: 1.354055\n",
      "Train Epoch: 8 [5760/9397 (61%)]\tLoss: 1.243422\n",
      "Train Epoch: 8 [6400/9397 (68%)]\tLoss: 1.305649\n",
      "Train Epoch: 8 [7040/9397 (75%)]\tLoss: 0.971909\n",
      "Train Epoch: 8 [7680/9397 (82%)]\tLoss: 1.299632\n",
      "Train Epoch: 8 [8320/9397 (88%)]\tLoss: 1.524117\n",
      "Train Epoch: 8 [8960/9397 (95%)]\tLoss: 1.171808\n",
      "Train Epoch: 9 [0/9397 (0%)]\tLoss: 1.125076\n",
      "Train Epoch: 9 [640/9397 (7%)]\tLoss: 1.139540\n",
      "Train Epoch: 9 [1280/9397 (14%)]\tLoss: 1.099371\n",
      "Train Epoch: 9 [1920/9397 (20%)]\tLoss: 1.015577\n",
      "Train Epoch: 9 [2560/9397 (27%)]\tLoss: 1.235078\n",
      "Train Epoch: 9 [3200/9397 (34%)]\tLoss: 1.000430\n",
      "Train Epoch: 9 [3840/9397 (41%)]\tLoss: 1.235465\n",
      "Train Epoch: 9 [4480/9397 (48%)]\tLoss: 1.387893\n",
      "Train Epoch: 9 [5120/9397 (54%)]\tLoss: 1.231778\n",
      "Train Epoch: 9 [5760/9397 (61%)]\tLoss: 1.162023\n",
      "Train Epoch: 9 [6400/9397 (68%)]\tLoss: 1.269674\n",
      "Train Epoch: 9 [7040/9397 (75%)]\tLoss: 0.976722\n",
      "Train Epoch: 9 [7680/9397 (82%)]\tLoss: 1.302651\n",
      "Train Epoch: 9 [8320/9397 (88%)]\tLoss: 0.958867\n",
      "Train Epoch: 9 [8960/9397 (95%)]\tLoss: 1.115639\n",
      "Train Epoch: 10 [0/9397 (0%)]\tLoss: 1.026417\n",
      "Train Epoch: 10 [640/9397 (7%)]\tLoss: 1.322743\n",
      "Train Epoch: 10 [1280/9397 (14%)]\tLoss: 1.094468\n",
      "Train Epoch: 10 [1920/9397 (20%)]\tLoss: 1.064516\n",
      "Train Epoch: 10 [2560/9397 (27%)]\tLoss: 0.918340\n",
      "Train Epoch: 10 [3200/9397 (34%)]\tLoss: 1.092737\n",
      "Train Epoch: 10 [3840/9397 (41%)]\tLoss: 0.750542\n",
      "Train Epoch: 10 [4480/9397 (48%)]\tLoss: 1.208636\n",
      "Train Epoch: 10 [5120/9397 (54%)]\tLoss: 1.002421\n",
      "Train Epoch: 10 [5760/9397 (61%)]\tLoss: 1.226940\n",
      "Train Epoch: 10 [6400/9397 (68%)]\tLoss: 1.251158\n",
      "Train Epoch: 10 [7040/9397 (75%)]\tLoss: 1.455644\n",
      "Train Epoch: 10 [7680/9397 (82%)]\tLoss: 1.096960\n",
      "Train Epoch: 10 [8320/9397 (88%)]\tLoss: 0.849884\n",
      "Train Epoch: 10 [8960/9397 (95%)]\tLoss: 1.356864\n",
      "Train Epoch: 11 [0/9397 (0%)]\tLoss: 1.381563\n",
      "Train Epoch: 11 [640/9397 (7%)]\tLoss: 0.892991\n",
      "Train Epoch: 11 [1280/9397 (14%)]\tLoss: 1.112475\n",
      "Train Epoch: 11 [1920/9397 (20%)]\tLoss: 1.310942\n",
      "Train Epoch: 11 [2560/9397 (27%)]\tLoss: 0.977663\n",
      "Train Epoch: 11 [3200/9397 (34%)]\tLoss: 1.102175\n",
      "Train Epoch: 11 [3840/9397 (41%)]\tLoss: 1.156404\n",
      "Train Epoch: 11 [4480/9397 (48%)]\tLoss: 1.058869\n",
      "Train Epoch: 11 [5120/9397 (54%)]\tLoss: 1.055703\n",
      "Train Epoch: 11 [5760/9397 (61%)]\tLoss: 1.290775\n",
      "Train Epoch: 11 [6400/9397 (68%)]\tLoss: 1.323975\n",
      "Train Epoch: 11 [7040/9397 (75%)]\tLoss: 1.213880\n",
      "Train Epoch: 11 [7680/9397 (82%)]\tLoss: 0.920436\n",
      "Train Epoch: 11 [8320/9397 (88%)]\tLoss: 0.974761\n",
      "Train Epoch: 11 [8960/9397 (95%)]\tLoss: 1.070789\n",
      "Train Epoch: 12 [0/9397 (0%)]\tLoss: 0.939165\n",
      "Train Epoch: 12 [640/9397 (7%)]\tLoss: 1.203708\n",
      "Train Epoch: 12 [1280/9397 (14%)]\tLoss: 1.169520\n",
      "Train Epoch: 12 [1920/9397 (20%)]\tLoss: 0.939655\n",
      "Train Epoch: 12 [2560/9397 (27%)]\tLoss: 0.990282\n",
      "Train Epoch: 12 [3200/9397 (34%)]\tLoss: 0.966484\n",
      "Train Epoch: 12 [3840/9397 (41%)]\tLoss: 1.395649\n",
      "Train Epoch: 12 [4480/9397 (48%)]\tLoss: 1.262138\n",
      "Train Epoch: 12 [5120/9397 (54%)]\tLoss: 1.053329\n",
      "Train Epoch: 12 [5760/9397 (61%)]\tLoss: 1.319875\n",
      "Train Epoch: 12 [6400/9397 (68%)]\tLoss: 1.090191\n",
      "Train Epoch: 12 [7040/9397 (75%)]\tLoss: 1.332713\n",
      "Train Epoch: 12 [7680/9397 (82%)]\tLoss: 1.022843\n",
      "Train Epoch: 12 [8320/9397 (88%)]\tLoss: 1.032271\n",
      "Train Epoch: 12 [8960/9397 (95%)]\tLoss: 1.068435\n",
      "Train Epoch: 13 [0/9397 (0%)]\tLoss: 1.063441\n",
      "Train Epoch: 13 [640/9397 (7%)]\tLoss: 1.015563\n",
      "Train Epoch: 13 [1280/9397 (14%)]\tLoss: 1.164479\n",
      "Train Epoch: 13 [1920/9397 (20%)]\tLoss: 1.162050\n",
      "Train Epoch: 13 [2560/9397 (27%)]\tLoss: 1.157607\n",
      "Train Epoch: 13 [3200/9397 (34%)]\tLoss: 1.135119\n",
      "Train Epoch: 13 [3840/9397 (41%)]\tLoss: 1.236885\n",
      "Train Epoch: 13 [4480/9397 (48%)]\tLoss: 1.161937\n",
      "Train Epoch: 13 [5120/9397 (54%)]\tLoss: 1.107160\n",
      "Train Epoch: 13 [5760/9397 (61%)]\tLoss: 1.078633\n",
      "Train Epoch: 13 [6400/9397 (68%)]\tLoss: 1.076803\n",
      "Train Epoch: 13 [7040/9397 (75%)]\tLoss: 1.227528\n",
      "Train Epoch: 13 [7680/9397 (82%)]\tLoss: 1.040670\n",
      "Train Epoch: 13 [8320/9397 (88%)]\tLoss: 1.138430\n",
      "Train Epoch: 13 [8960/9397 (95%)]\tLoss: 1.073610\n",
      "Train Epoch: 14 [0/9397 (0%)]\tLoss: 1.221450\n",
      "Train Epoch: 14 [640/9397 (7%)]\tLoss: 0.862287\n",
      "Train Epoch: 14 [1280/9397 (14%)]\tLoss: 1.023149\n",
      "Train Epoch: 14 [1920/9397 (20%)]\tLoss: 1.164515\n",
      "Train Epoch: 14 [2560/9397 (27%)]\tLoss: 0.923917\n",
      "Train Epoch: 14 [3200/9397 (34%)]\tLoss: 0.998953\n",
      "Train Epoch: 14 [3840/9397 (41%)]\tLoss: 1.072879\n",
      "Train Epoch: 14 [4480/9397 (48%)]\tLoss: 1.129674\n",
      "Train Epoch: 14 [5120/9397 (54%)]\tLoss: 1.254804\n",
      "Train Epoch: 14 [5760/9397 (61%)]\tLoss: 0.996932\n",
      "Train Epoch: 14 [6400/9397 (68%)]\tLoss: 1.155839\n",
      "Train Epoch: 14 [7040/9397 (75%)]\tLoss: 1.168596\n",
      "Train Epoch: 14 [7680/9397 (82%)]\tLoss: 1.208773\n",
      "Train Epoch: 14 [8320/9397 (88%)]\tLoss: 1.122876\n",
      "Train Epoch: 14 [8960/9397 (95%)]\tLoss: 1.048741\n",
      "Train Epoch: 15 [0/9397 (0%)]\tLoss: 1.037330\n",
      "Train Epoch: 15 [640/9397 (7%)]\tLoss: 1.169258\n",
      "Train Epoch: 15 [1280/9397 (14%)]\tLoss: 0.997352\n",
      "Train Epoch: 15 [1920/9397 (20%)]\tLoss: 1.198742\n",
      "Train Epoch: 15 [2560/9397 (27%)]\tLoss: 1.083635\n",
      "Train Epoch: 15 [3200/9397 (34%)]\tLoss: 1.345306\n",
      "Train Epoch: 15 [3840/9397 (41%)]\tLoss: 0.970152\n",
      "Train Epoch: 15 [4480/9397 (48%)]\tLoss: 1.198667\n",
      "Train Epoch: 15 [5120/9397 (54%)]\tLoss: 1.204961\n",
      "Train Epoch: 15 [5760/9397 (61%)]\tLoss: 1.006036\n",
      "Train Epoch: 15 [6400/9397 (68%)]\tLoss: 1.412127\n",
      "Train Epoch: 15 [7040/9397 (75%)]\tLoss: 1.331761\n",
      "Train Epoch: 15 [7680/9397 (82%)]\tLoss: 0.867771\n",
      "Train Epoch: 15 [8320/9397 (88%)]\tLoss: 1.140147\n",
      "Train Epoch: 15 [8960/9397 (95%)]\tLoss: 1.286839\n",
      "Train Epoch: 16 [0/9397 (0%)]\tLoss: 1.023129\n",
      "Train Epoch: 16 [640/9397 (7%)]\tLoss: 0.975978\n",
      "Train Epoch: 16 [1280/9397 (14%)]\tLoss: 1.001516\n",
      "Train Epoch: 16 [1920/9397 (20%)]\tLoss: 1.027350\n",
      "Train Epoch: 16 [2560/9397 (27%)]\tLoss: 1.164916\n",
      "Train Epoch: 16 [3200/9397 (34%)]\tLoss: 1.052151\n",
      "Train Epoch: 16 [3840/9397 (41%)]\tLoss: 1.093226\n",
      "Train Epoch: 16 [4480/9397 (48%)]\tLoss: 1.049747\n",
      "Train Epoch: 16 [5120/9397 (54%)]\tLoss: 1.255393\n",
      "Train Epoch: 16 [5760/9397 (61%)]\tLoss: 1.172159\n",
      "Train Epoch: 16 [6400/9397 (68%)]\tLoss: 0.965952\n",
      "Train Epoch: 16 [7040/9397 (75%)]\tLoss: 1.006300\n",
      "Train Epoch: 16 [7680/9397 (82%)]\tLoss: 1.084347\n",
      "Train Epoch: 16 [8320/9397 (88%)]\tLoss: 1.078947\n",
      "Train Epoch: 16 [8960/9397 (95%)]\tLoss: 1.183960\n",
      "Train Epoch: 17 [0/9397 (0%)]\tLoss: 0.995851\n",
      "Train Epoch: 17 [640/9397 (7%)]\tLoss: 1.385393\n",
      "Train Epoch: 17 [1280/9397 (14%)]\tLoss: 0.818957\n",
      "Train Epoch: 17 [1920/9397 (20%)]\tLoss: 0.986097\n",
      "Train Epoch: 17 [2560/9397 (27%)]\tLoss: 1.218083\n",
      "Train Epoch: 17 [3200/9397 (34%)]\tLoss: 0.981111\n",
      "Train Epoch: 17 [3840/9397 (41%)]\tLoss: 1.144212\n",
      "Train Epoch: 17 [4480/9397 (48%)]\tLoss: 1.109622\n",
      "Train Epoch: 17 [5120/9397 (54%)]\tLoss: 1.050536\n",
      "Train Epoch: 17 [5760/9397 (61%)]\tLoss: 1.157468\n",
      "Train Epoch: 17 [6400/9397 (68%)]\tLoss: 1.202196\n",
      "Train Epoch: 17 [7040/9397 (75%)]\tLoss: 1.361632\n",
      "Train Epoch: 17 [7680/9397 (82%)]\tLoss: 1.124939\n",
      "Train Epoch: 17 [8320/9397 (88%)]\tLoss: 1.331805\n",
      "Train Epoch: 17 [8960/9397 (95%)]\tLoss: 1.088282\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/1887 (0%)]\tLoss: 1.370152\n",
      "Train Epoch: 1 [640/1887 (33%)]\tLoss: 1.259651\n",
      "Train Epoch: 1 [1280/1887 (67%)]\tLoss: 1.235265\n",
      "Train Epoch: 2 [0/1887 (0%)]\tLoss: 1.269017\n",
      "Train Epoch: 2 [640/1887 (33%)]\tLoss: 1.186915\n",
      "Train Epoch: 2 [1280/1887 (67%)]\tLoss: 1.280295\n",
      "Train Epoch: 3 [0/1887 (0%)]\tLoss: 1.251217\n",
      "Train Epoch: 3 [640/1887 (33%)]\tLoss: 1.027357\n",
      "Train Epoch: 3 [1280/1887 (67%)]\tLoss: 0.978466\n",
      "Train Epoch: 4 [0/1887 (0%)]\tLoss: 1.019383\n",
      "Train Epoch: 4 [640/1887 (33%)]\tLoss: 1.256514\n",
      "Train Epoch: 4 [1280/1887 (67%)]\tLoss: 1.214327\n",
      "Train Epoch: 5 [0/1887 (0%)]\tLoss: 1.094801\n",
      "Train Epoch: 5 [640/1887 (33%)]\tLoss: 1.113201\n",
      "Train Epoch: 5 [1280/1887 (67%)]\tLoss: 1.085149\n",
      "Train Epoch: 6 [0/1887 (0%)]\tLoss: 1.143691\n",
      "Train Epoch: 6 [640/1887 (33%)]\tLoss: 1.122620\n",
      "Train Epoch: 6 [1280/1887 (67%)]\tLoss: 1.079674\n",
      "Train Epoch: 7 [0/1887 (0%)]\tLoss: 1.246875\n",
      "Train Epoch: 7 [640/1887 (33%)]\tLoss: 1.344291\n",
      "Train Epoch: 7 [1280/1887 (67%)]\tLoss: 0.990964\n",
      "Train Epoch: 8 [0/1887 (0%)]\tLoss: 1.193248\n",
      "Train Epoch: 8 [640/1887 (33%)]\tLoss: 1.268450\n",
      "Train Epoch: 8 [1280/1887 (67%)]\tLoss: 1.248400\n",
      "Train Epoch: 9 [0/1887 (0%)]\tLoss: 1.088289\n",
      "Train Epoch: 9 [640/1887 (33%)]\tLoss: 1.345305\n",
      "Train Epoch: 9 [1280/1887 (67%)]\tLoss: 1.191379\n",
      "Train Epoch: 10 [0/1887 (0%)]\tLoss: 1.280510\n",
      "Train Epoch: 10 [640/1887 (33%)]\tLoss: 1.230669\n",
      "Train Epoch: 10 [1280/1887 (67%)]\tLoss: 1.264181\n",
      "Train Epoch: 11 [0/1887 (0%)]\tLoss: 1.282990\n",
      "Train Epoch: 11 [640/1887 (33%)]\tLoss: 1.276453\n",
      "Train Epoch: 11 [1280/1887 (67%)]\tLoss: 1.123426\n",
      "Train Epoch: 12 [0/1887 (0%)]\tLoss: 1.081686\n",
      "Train Epoch: 12 [640/1887 (33%)]\tLoss: 1.137484\n",
      "Train Epoch: 12 [1280/1887 (67%)]\tLoss: 1.082382\n",
      "Train Epoch: 13 [0/1887 (0%)]\tLoss: 0.865396\n",
      "Train Epoch: 13 [640/1887 (33%)]\tLoss: 0.990488\n",
      "Train Epoch: 13 [1280/1887 (67%)]\tLoss: 0.995909\n",
      "Train Epoch: 14 [0/1887 (0%)]\tLoss: 1.127461\n",
      "Train Epoch: 14 [640/1887 (33%)]\tLoss: 1.088176\n",
      "Train Epoch: 14 [1280/1887 (67%)]\tLoss: 1.057664\n",
      "Train Epoch: 15 [0/1887 (0%)]\tLoss: 1.002565\n",
      "Train Epoch: 15 [640/1887 (33%)]\tLoss: 1.207249\n",
      "Train Epoch: 15 [1280/1887 (67%)]\tLoss: 1.173835\n",
      "Train Epoch: 16 [0/1887 (0%)]\tLoss: 0.921560\n",
      "Train Epoch: 16 [640/1887 (33%)]\tLoss: 0.976394\n",
      "Train Epoch: 16 [1280/1887 (67%)]\tLoss: 1.067577\n",
      "Train Epoch: 17 [0/1887 (0%)]\tLoss: 1.025688\n",
      "Train Epoch: 17 [640/1887 (33%)]\tLoss: 0.911321\n",
      "Train Epoch: 17 [1280/1887 (67%)]\tLoss: 1.106163\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/5490 (0%)]\tLoss: 1.469330\n",
      "Train Epoch: 1 [640/5490 (12%)]\tLoss: 1.261998\n",
      "Train Epoch: 1 [1280/5490 (23%)]\tLoss: 1.092888\n",
      "Train Epoch: 1 [1920/5490 (35%)]\tLoss: 1.162968\n",
      "Train Epoch: 1 [2560/5490 (47%)]\tLoss: 0.992821\n",
      "Train Epoch: 1 [3200/5490 (58%)]\tLoss: 1.014796\n",
      "Train Epoch: 1 [3840/5490 (70%)]\tLoss: 1.196234\n",
      "Train Epoch: 1 [4480/5490 (81%)]\tLoss: 0.943795\n",
      "Train Epoch: 1 [5120/5490 (93%)]\tLoss: 1.094689\n",
      "Train Epoch: 2 [0/5490 (0%)]\tLoss: 1.051225\n",
      "Train Epoch: 2 [640/5490 (12%)]\tLoss: 0.910594\n",
      "Train Epoch: 2 [1280/5490 (23%)]\tLoss: 1.118397\n",
      "Train Epoch: 2 [1920/5490 (35%)]\tLoss: 1.012820\n",
      "Train Epoch: 2 [2560/5490 (47%)]\tLoss: 1.229579\n",
      "Train Epoch: 2 [3200/5490 (58%)]\tLoss: 0.886004\n",
      "Train Epoch: 2 [3840/5490 (70%)]\tLoss: 0.945200\n",
      "Train Epoch: 2 [4480/5490 (81%)]\tLoss: 1.158605\n",
      "Train Epoch: 2 [5120/5490 (93%)]\tLoss: 1.008382\n",
      "Train Epoch: 3 [0/5490 (0%)]\tLoss: 0.839158\n",
      "Train Epoch: 3 [640/5490 (12%)]\tLoss: 1.031191\n",
      "Train Epoch: 3 [1280/5490 (23%)]\tLoss: 1.155214\n",
      "Train Epoch: 3 [1920/5490 (35%)]\tLoss: 1.220760\n",
      "Train Epoch: 3 [2560/5490 (47%)]\tLoss: 0.981808\n",
      "Train Epoch: 3 [3200/5490 (58%)]\tLoss: 0.964456\n",
      "Train Epoch: 3 [3840/5490 (70%)]\tLoss: 1.285522\n",
      "Train Epoch: 3 [4480/5490 (81%)]\tLoss: 1.195229\n",
      "Train Epoch: 3 [5120/5490 (93%)]\tLoss: 1.412990\n",
      "Train Epoch: 4 [0/5490 (0%)]\tLoss: 0.905053\n",
      "Train Epoch: 4 [640/5490 (12%)]\tLoss: 1.174548\n",
      "Train Epoch: 4 [1280/5490 (23%)]\tLoss: 1.208724\n",
      "Train Epoch: 4 [1920/5490 (35%)]\tLoss: 1.155078\n",
      "Train Epoch: 4 [2560/5490 (47%)]\tLoss: 0.921170\n",
      "Train Epoch: 4 [3200/5490 (58%)]\tLoss: 0.994903\n",
      "Train Epoch: 4 [3840/5490 (70%)]\tLoss: 1.231684\n",
      "Train Epoch: 4 [4480/5490 (81%)]\tLoss: 1.080571\n",
      "Train Epoch: 4 [5120/5490 (93%)]\tLoss: 1.046390\n",
      "Train Epoch: 5 [0/5490 (0%)]\tLoss: 0.932678\n",
      "Train Epoch: 5 [640/5490 (12%)]\tLoss: 1.192937\n",
      "Train Epoch: 5 [1280/5490 (23%)]\tLoss: 0.843962\n",
      "Train Epoch: 5 [1920/5490 (35%)]\tLoss: 1.105410\n",
      "Train Epoch: 5 [2560/5490 (47%)]\tLoss: 0.952808\n",
      "Train Epoch: 5 [3200/5490 (58%)]\tLoss: 1.141460\n",
      "Train Epoch: 5 [3840/5490 (70%)]\tLoss: 1.033691\n",
      "Train Epoch: 5 [4480/5490 (81%)]\tLoss: 1.231731\n",
      "Train Epoch: 5 [5120/5490 (93%)]\tLoss: 1.034790\n",
      "Train Epoch: 6 [0/5490 (0%)]\tLoss: 1.116520\n",
      "Train Epoch: 6 [640/5490 (12%)]\tLoss: 1.121515\n",
      "Train Epoch: 6 [1280/5490 (23%)]\tLoss: 0.936841\n",
      "Train Epoch: 6 [1920/5490 (35%)]\tLoss: 1.254925\n",
      "Train Epoch: 6 [2560/5490 (47%)]\tLoss: 0.943871\n",
      "Train Epoch: 6 [3200/5490 (58%)]\tLoss: 1.452277\n",
      "Train Epoch: 6 [3840/5490 (70%)]\tLoss: 1.317986\n",
      "Train Epoch: 6 [4480/5490 (81%)]\tLoss: 0.978538\n",
      "Train Epoch: 6 [5120/5490 (93%)]\tLoss: 1.173186\n",
      "Train Epoch: 7 [0/5490 (0%)]\tLoss: 0.922203\n",
      "Train Epoch: 7 [640/5490 (12%)]\tLoss: 1.162012\n",
      "Train Epoch: 7 [1280/5490 (23%)]\tLoss: 0.982785\n",
      "Train Epoch: 7 [1920/5490 (35%)]\tLoss: 1.377784\n",
      "Train Epoch: 7 [2560/5490 (47%)]\tLoss: 0.975887\n",
      "Train Epoch: 7 [3200/5490 (58%)]\tLoss: 1.292053\n",
      "Train Epoch: 7 [3840/5490 (70%)]\tLoss: 0.974882\n",
      "Train Epoch: 7 [4480/5490 (81%)]\tLoss: 0.944872\n",
      "Train Epoch: 7 [5120/5490 (93%)]\tLoss: 0.912339\n",
      "Train Epoch: 8 [0/5490 (0%)]\tLoss: 1.074078\n",
      "Train Epoch: 8 [640/5490 (12%)]\tLoss: 1.325122\n",
      "Train Epoch: 8 [1280/5490 (23%)]\tLoss: 0.895321\n",
      "Train Epoch: 8 [1920/5490 (35%)]\tLoss: 1.023689\n",
      "Train Epoch: 8 [2560/5490 (47%)]\tLoss: 0.988295\n",
      "Train Epoch: 8 [3200/5490 (58%)]\tLoss: 1.257927\n",
      "Train Epoch: 8 [3840/5490 (70%)]\tLoss: 0.930074\n",
      "Train Epoch: 8 [4480/5490 (81%)]\tLoss: 1.156426\n",
      "Train Epoch: 8 [5120/5490 (93%)]\tLoss: 1.394468\n",
      "Train Epoch: 9 [0/5490 (0%)]\tLoss: 1.018561\n",
      "Train Epoch: 9 [640/5490 (12%)]\tLoss: 1.013999\n",
      "Train Epoch: 9 [1280/5490 (23%)]\tLoss: 1.212042\n",
      "Train Epoch: 9 [1920/5490 (35%)]\tLoss: 0.974229\n",
      "Train Epoch: 9 [2560/5490 (47%)]\tLoss: 1.136312\n",
      "Train Epoch: 9 [3200/5490 (58%)]\tLoss: 0.935462\n",
      "Train Epoch: 9 [3840/5490 (70%)]\tLoss: 1.096160\n",
      "Train Epoch: 9 [4480/5490 (81%)]\tLoss: 1.078057\n",
      "Train Epoch: 9 [5120/5490 (93%)]\tLoss: 1.024359\n",
      "Train Epoch: 10 [0/5490 (0%)]\tLoss: 1.054623\n",
      "Train Epoch: 10 [640/5490 (12%)]\tLoss: 1.104535\n",
      "Train Epoch: 10 [1280/5490 (23%)]\tLoss: 1.325641\n",
      "Train Epoch: 10 [1920/5490 (35%)]\tLoss: 0.862441\n",
      "Train Epoch: 10 [2560/5490 (47%)]\tLoss: 1.171663\n",
      "Train Epoch: 10 [3200/5490 (58%)]\tLoss: 1.048975\n",
      "Train Epoch: 10 [3840/5490 (70%)]\tLoss: 1.117692\n",
      "Train Epoch: 10 [4480/5490 (81%)]\tLoss: 1.040679\n",
      "Train Epoch: 10 [5120/5490 (93%)]\tLoss: 0.831958\n",
      "Train Epoch: 11 [0/5490 (0%)]\tLoss: 0.933681\n",
      "Train Epoch: 11 [640/5490 (12%)]\tLoss: 1.036381\n",
      "Train Epoch: 11 [1280/5490 (23%)]\tLoss: 1.036843\n",
      "Train Epoch: 11 [1920/5490 (35%)]\tLoss: 0.859581\n",
      "Train Epoch: 11 [2560/5490 (47%)]\tLoss: 1.038832\n",
      "Train Epoch: 11 [3200/5490 (58%)]\tLoss: 1.182523\n",
      "Train Epoch: 11 [3840/5490 (70%)]\tLoss: 1.289163\n",
      "Train Epoch: 11 [4480/5490 (81%)]\tLoss: 1.018426\n",
      "Train Epoch: 11 [5120/5490 (93%)]\tLoss: 1.208801\n",
      "Train Epoch: 12 [0/5490 (0%)]\tLoss: 0.835400\n",
      "Train Epoch: 12 [640/5490 (12%)]\tLoss: 1.073816\n",
      "Train Epoch: 12 [1280/5490 (23%)]\tLoss: 0.828594\n",
      "Train Epoch: 12 [1920/5490 (35%)]\tLoss: 1.182998\n",
      "Train Epoch: 12 [2560/5490 (47%)]\tLoss: 1.088755\n",
      "Train Epoch: 12 [3200/5490 (58%)]\tLoss: 1.200171\n",
      "Train Epoch: 12 [3840/5490 (70%)]\tLoss: 1.066986\n",
      "Train Epoch: 12 [4480/5490 (81%)]\tLoss: 0.901244\n",
      "Train Epoch: 12 [5120/5490 (93%)]\tLoss: 1.050258\n",
      "Train Epoch: 13 [0/5490 (0%)]\tLoss: 0.988351\n",
      "Train Epoch: 13 [640/5490 (12%)]\tLoss: 1.048635\n",
      "Train Epoch: 13 [1280/5490 (23%)]\tLoss: 0.800857\n",
      "Train Epoch: 13 [1920/5490 (35%)]\tLoss: 1.250633\n",
      "Train Epoch: 13 [2560/5490 (47%)]\tLoss: 1.231130\n",
      "Train Epoch: 13 [3200/5490 (58%)]\tLoss: 0.868586\n",
      "Train Epoch: 13 [3840/5490 (70%)]\tLoss: 1.035030\n",
      "Train Epoch: 13 [4480/5490 (81%)]\tLoss: 0.904012\n",
      "Train Epoch: 13 [5120/5490 (93%)]\tLoss: 1.068292\n",
      "Train Epoch: 14 [0/5490 (0%)]\tLoss: 0.989276\n",
      "Train Epoch: 14 [640/5490 (12%)]\tLoss: 0.956935\n",
      "Train Epoch: 14 [1280/5490 (23%)]\tLoss: 1.041434\n",
      "Train Epoch: 14 [1920/5490 (35%)]\tLoss: 0.899016\n",
      "Train Epoch: 14 [2560/5490 (47%)]\tLoss: 1.375362\n",
      "Train Epoch: 14 [3200/5490 (58%)]\tLoss: 0.933635\n",
      "Train Epoch: 14 [3840/5490 (70%)]\tLoss: 1.140088\n",
      "Train Epoch: 14 [4480/5490 (81%)]\tLoss: 0.946789\n",
      "Train Epoch: 14 [5120/5490 (93%)]\tLoss: 0.889381\n",
      "Train Epoch: 15 [0/5490 (0%)]\tLoss: 1.153675\n",
      "Train Epoch: 15 [640/5490 (12%)]\tLoss: 0.863086\n",
      "Train Epoch: 15 [1280/5490 (23%)]\tLoss: 0.967142\n",
      "Train Epoch: 15 [1920/5490 (35%)]\tLoss: 0.991841\n",
      "Train Epoch: 15 [2560/5490 (47%)]\tLoss: 1.426701\n",
      "Train Epoch: 15 [3200/5490 (58%)]\tLoss: 1.063491\n",
      "Train Epoch: 15 [3840/5490 (70%)]\tLoss: 1.072352\n",
      "Train Epoch: 15 [4480/5490 (81%)]\tLoss: 0.895804\n",
      "Train Epoch: 15 [5120/5490 (93%)]\tLoss: 1.088598\n",
      "Train Epoch: 16 [0/5490 (0%)]\tLoss: 0.924808\n",
      "Train Epoch: 16 [640/5490 (12%)]\tLoss: 0.818184\n",
      "Train Epoch: 16 [1280/5490 (23%)]\tLoss: 1.236479\n",
      "Train Epoch: 16 [1920/5490 (35%)]\tLoss: 0.960895\n",
      "Train Epoch: 16 [2560/5490 (47%)]\tLoss: 1.026145\n",
      "Train Epoch: 16 [3200/5490 (58%)]\tLoss: 0.947704\n",
      "Train Epoch: 16 [3840/5490 (70%)]\tLoss: 0.628471\n",
      "Train Epoch: 16 [4480/5490 (81%)]\tLoss: 0.993985\n",
      "Train Epoch: 16 [5120/5490 (93%)]\tLoss: 0.849742\n",
      "Train Epoch: 17 [0/5490 (0%)]\tLoss: 1.088821\n",
      "Train Epoch: 17 [640/5490 (12%)]\tLoss: 0.805682\n",
      "Train Epoch: 17 [1280/5490 (23%)]\tLoss: 0.998708\n",
      "Train Epoch: 17 [1920/5490 (35%)]\tLoss: 0.986689\n",
      "Train Epoch: 17 [2560/5490 (47%)]\tLoss: 1.019372\n",
      "Train Epoch: 17 [3200/5490 (58%)]\tLoss: 0.958433\n",
      "Train Epoch: 17 [3840/5490 (70%)]\tLoss: 0.770727\n",
      "Train Epoch: 17 [4480/5490 (81%)]\tLoss: 1.008505\n",
      "Train Epoch: 17 [5120/5490 (93%)]\tLoss: 1.055711\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/2468 (0%)]\tLoss: 1.549185\n",
      "Train Epoch: 1 [640/2468 (26%)]\tLoss: 1.388521\n",
      "Train Epoch: 1 [1280/2468 (51%)]\tLoss: 1.107437\n",
      "Train Epoch: 1 [1920/2468 (77%)]\tLoss: 1.268308\n",
      "Train Epoch: 2 [0/2468 (0%)]\tLoss: 1.442713\n",
      "Train Epoch: 2 [640/2468 (26%)]\tLoss: 0.883441\n",
      "Train Epoch: 2 [1280/2468 (51%)]\tLoss: 1.174499\n",
      "Train Epoch: 2 [1920/2468 (77%)]\tLoss: 0.953916\n",
      "Train Epoch: 3 [0/2468 (0%)]\tLoss: 0.974512\n",
      "Train Epoch: 3 [640/2468 (26%)]\tLoss: 1.200254\n",
      "Train Epoch: 3 [1280/2468 (51%)]\tLoss: 1.446070\n",
      "Train Epoch: 3 [1920/2468 (77%)]\tLoss: 1.222042\n",
      "Train Epoch: 4 [0/2468 (0%)]\tLoss: 1.066700\n",
      "Train Epoch: 4 [640/2468 (26%)]\tLoss: 1.056376\n",
      "Train Epoch: 4 [1280/2468 (51%)]\tLoss: 1.218449\n",
      "Train Epoch: 4 [1920/2468 (77%)]\tLoss: 1.262055\n",
      "Train Epoch: 5 [0/2468 (0%)]\tLoss: 1.433602\n",
      "Train Epoch: 5 [640/2468 (26%)]\tLoss: 1.034032\n",
      "Train Epoch: 5 [1280/2468 (51%)]\tLoss: 0.766466\n",
      "Train Epoch: 5 [1920/2468 (77%)]\tLoss: 1.105711\n",
      "Train Epoch: 6 [0/2468 (0%)]\tLoss: 1.008207\n",
      "Train Epoch: 6 [640/2468 (26%)]\tLoss: 1.161456\n",
      "Train Epoch: 6 [1280/2468 (51%)]\tLoss: 1.229366\n",
      "Train Epoch: 6 [1920/2468 (77%)]\tLoss: 1.160898\n",
      "Train Epoch: 7 [0/2468 (0%)]\tLoss: 1.186127\n",
      "Train Epoch: 7 [640/2468 (26%)]\tLoss: 1.063362\n",
      "Train Epoch: 7 [1280/2468 (51%)]\tLoss: 1.104115\n",
      "Train Epoch: 7 [1920/2468 (77%)]\tLoss: 1.274417\n",
      "Train Epoch: 8 [0/2468 (0%)]\tLoss: 1.101582\n",
      "Train Epoch: 8 [640/2468 (26%)]\tLoss: 1.090953\n",
      "Train Epoch: 8 [1280/2468 (51%)]\tLoss: 1.074627\n",
      "Train Epoch: 8 [1920/2468 (77%)]\tLoss: 1.138645\n",
      "Train Epoch: 9 [0/2468 (0%)]\tLoss: 0.906457\n",
      "Train Epoch: 9 [640/2468 (26%)]\tLoss: 1.186251\n",
      "Train Epoch: 9 [1280/2468 (51%)]\tLoss: 0.932738\n",
      "Train Epoch: 9 [1920/2468 (77%)]\tLoss: 0.958620\n",
      "Train Epoch: 10 [0/2468 (0%)]\tLoss: 1.255358\n",
      "Train Epoch: 10 [640/2468 (26%)]\tLoss: 0.866995\n",
      "Train Epoch: 10 [1280/2468 (51%)]\tLoss: 1.493633\n",
      "Train Epoch: 10 [1920/2468 (77%)]\tLoss: 0.985202\n",
      "Train Epoch: 11 [0/2468 (0%)]\tLoss: 1.227515\n",
      "Train Epoch: 11 [640/2468 (26%)]\tLoss: 0.973413\n",
      "Train Epoch: 11 [1280/2468 (51%)]\tLoss: 1.096223\n",
      "Train Epoch: 11 [1920/2468 (77%)]\tLoss: 1.079662\n",
      "Train Epoch: 12 [0/2468 (0%)]\tLoss: 1.372176\n",
      "Train Epoch: 12 [640/2468 (26%)]\tLoss: 1.025624\n",
      "Train Epoch: 12 [1280/2468 (51%)]\tLoss: 1.105224\n",
      "Train Epoch: 12 [1920/2468 (77%)]\tLoss: 0.922453\n",
      "Train Epoch: 13 [0/2468 (0%)]\tLoss: 1.034961\n",
      "Train Epoch: 13 [640/2468 (26%)]\tLoss: 0.874141\n",
      "Train Epoch: 13 [1280/2468 (51%)]\tLoss: 0.994057\n",
      "Train Epoch: 13 [1920/2468 (77%)]\tLoss: 1.061155\n",
      "Train Epoch: 14 [0/2468 (0%)]\tLoss: 1.049412\n",
      "Train Epoch: 14 [640/2468 (26%)]\tLoss: 1.056031\n",
      "Train Epoch: 14 [1280/2468 (51%)]\tLoss: 1.139039\n",
      "Train Epoch: 14 [1920/2468 (77%)]\tLoss: 1.001854\n",
      "Train Epoch: 15 [0/2468 (0%)]\tLoss: 1.228786\n",
      "Train Epoch: 15 [640/2468 (26%)]\tLoss: 0.854218\n",
      "Train Epoch: 15 [1280/2468 (51%)]\tLoss: 1.159639\n",
      "Train Epoch: 15 [1920/2468 (77%)]\tLoss: 1.133987\n",
      "Train Epoch: 16 [0/2468 (0%)]\tLoss: 0.901661\n",
      "Train Epoch: 16 [640/2468 (26%)]\tLoss: 0.978882\n",
      "Train Epoch: 16 [1280/2468 (51%)]\tLoss: 1.029975\n",
      "Train Epoch: 16 [1920/2468 (77%)]\tLoss: 1.035554\n",
      "Train Epoch: 17 [0/2468 (0%)]\tLoss: 1.162082\n",
      "Train Epoch: 17 [640/2468 (26%)]\tLoss: 0.965035\n",
      "Train Epoch: 17 [1280/2468 (51%)]\tLoss: 0.807791\n",
      "Train Epoch: 17 [1920/2468 (77%)]\tLoss: 0.814170\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.3263, Accuracy: 5223/10000 (52%)\n",
      "\n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.547400\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.293258\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.301464\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.389700\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.251730\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.096483\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.301334\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.169067\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.460552\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.369474\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.261564\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.301437\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.490339\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.344750\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.388450\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.346178\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.244487\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.233132\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.243099\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.201956\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.361274\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.168805\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.267952\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.239343\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.540026\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.291454\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.380644\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.212902\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.066969\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.387712\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.311514\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.423519\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 1.301197\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.071458\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.075695\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.359493\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.456294\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.669875\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.518779\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.159309\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.129323\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.233191\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.229048\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.274359\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.033011\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.375720\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.273614\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.246333\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.276748\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.300210\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.179879\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.204684\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.264167\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.230091\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.308992\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.259015\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.101309\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.290495\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.042370\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.207665\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.113423\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.090227\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 0.893911\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.246173\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.080518\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.316066\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.240239\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.392068\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.074410\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.247513\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.183738\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.268760\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.122485\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.201091\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.334519\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.377475\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.276736\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.278852\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 1.359276\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.084131\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.178330\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.057632\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 0.959354\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.240894\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.094952\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.090359\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.217772\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 0.970555\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 1.258486\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.042146\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 0.992044\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.270350\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.148745\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.327653\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.161102\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.163309\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.124091\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.229091\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.082024\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 1.104137\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.279550\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.006362\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.450760\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.345899\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.341558\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.319366\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.325385\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.271856\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.159171\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.321912\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.375153\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.343260\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.358576\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.234474\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.242080\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.124074\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.130607\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.149902\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.306244\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.219014\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.218425\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.527412\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.261028\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.227313\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.295687\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.416439\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.307994\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.392732\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.167507\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.194746\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.251751\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.452921\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.382136\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.441582\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.192896\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.474006\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.131968\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.102072\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.119526\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.279623\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.089731\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.704272\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.070510\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 1.053410\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.154360\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.165461\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.404692\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.261590\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.286520\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.249704\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.051373\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.008250\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.111568\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.352012\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.171982\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.168910\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.059768\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.536956\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.190618\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.116333\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.195422\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.428173\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 0.975601\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.310555\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.015398\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.187936\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.288514\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.120564\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.342184\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.071454\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 0.967224\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.066745\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.202438\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.132430\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.003510\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.368729\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.508097\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 0.998912\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.205346\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.105913\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 1.172908\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.193895\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.175856\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.060661\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.131815\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 0.946418\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.178457\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.115330\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 1.071029\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.276849\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 0.992408\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.069940\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.155575\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.154874\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.300020\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.506052\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.125804\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.279496\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.051475\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.139358\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.372079\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.091758\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.250714\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.320419\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.749726\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.403060\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.265605\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.104350\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.356552\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 0.907408\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 1.151284\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.170294\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.083961\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 1.379873\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 1.061971\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.019853\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 1.147959\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.095315\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 1.165034\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.224571\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 1.123264\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 1.053104\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.164266\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.170871\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 1.023706\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 1.055508\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.071976\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.241843\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.136306\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.215166\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 0.950857\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.020235\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 1.032213\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 0.945375\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 0.779634\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 1.087010\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 1.114631\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 1.047163\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 0.990440\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 0.832104\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.080163\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.072778\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 1.052173\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 1.028192\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 1.064214\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 1.103861\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 0.948948\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 0.917862\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 0.939301\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 0.946995\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 0.978906\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 1.043296\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 0.894819\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 1.043668\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 0.896733\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.049109\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 1.211448\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 0.699310\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 0.953890\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 1.064302\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 0.969940\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 0.798227\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 1.172145\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 1.035904\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 1.006068\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 1.278883\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 1.001343\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 0.876482\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 0.977990\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 0.945602\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.109011\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 0.842353\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.970532\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 1.079969\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 1.145283\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 0.832978\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.005381\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 1.250769\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.059402\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 1.202277\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 0.971978\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 0.840422\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 0.776348\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 0.885224\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 1.094227\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 0.872169\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 1.201750\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 1.042826\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 0.969174\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 0.899777\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 1.079835\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 1.136642\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 1.086634\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 0.944361\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 0.941971\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 1.033182\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 1.048754\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 1.052301\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 1.123100\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 1.114669\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 0.992275\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.018089\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 1.036033\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 1.078301\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 0.845235\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 1.045963\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 0.811781\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 0.907055\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 1.093073\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 0.838991\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 0.929518\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 1.113709\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 1.002879\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 0.800583\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 0.991891\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 0.720871\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.112149\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 1.042430\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 0.979924\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.928569\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 1.039044\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 0.906538\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 0.989106\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 1.031074\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 0.897477\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 0.800020\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 0.913198\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 0.989109\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 0.864792\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 0.762166\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 1.148327\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 1.035084\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 1.033992\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 0.961783\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 0.894210\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 1.066127\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 0.894486\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 1.049041\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 0.804675\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 1.008393\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 0.805937\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 1.001245\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 0.866562\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 0.969345\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 0.841114\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 1.227030\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.788746\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 1.051702\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 1.052586\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 1.137900\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 0.983771\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 1.121447\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 0.802159\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 0.791660\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 1.110892\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 1.093186\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 0.743232\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 0.998634\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 0.955058\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.733814\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 1.001807\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 1.377824\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 0.995756\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.021745\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 1.211470\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 1.026549\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 1.135054\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 0.931196\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 0.952459\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 0.745971\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.814016\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.991599\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.782647\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 0.825396\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.264476\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.029790\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 1.045776\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 0.896971\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.270279\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.264148\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 0.830876\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.008372\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 1.112253\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.011540\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 1.149717\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.916959\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 1.103504\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 0.991724\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.006955\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.049544\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 1.111612\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 1.256482\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 1.373293\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 0.913022\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 1.194593\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 0.849850\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 1.277301\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 0.854473\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 1.362936\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 1.097332\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 1.152442\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 0.821612\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 0.912734\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 1.074661\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 1.041135\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 1.036988\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 1.067446\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 1.113737\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 0.951159\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 1.015923\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 0.957274\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 1.038784\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 1.040407\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 0.957371\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 1.148571\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 1.096415\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 1.062312\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 0.750647\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 1.054478\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 0.744719\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 0.883261\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 0.894153\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 1.172866\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 1.005704\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 1.168511\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 0.871895\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 1.034209\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 0.882206\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 1.430343\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.748537\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 0.951358\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 0.988105\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 1.017902\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 0.944856\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 1.102773\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 1.050302\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 1.408058\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 0.993427\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.085246\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 1.026843\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 0.814618\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 1.024722\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 0.670707\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 0.886748\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 1.008857\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 0.698104\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 0.885927\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 1.023033\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 0.971860\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 1.119335\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 0.946168\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 0.908894\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 0.866731\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 0.993827\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 0.859651\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.891191\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 0.839893\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.988442\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 0.861197\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 0.988458\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 1.051107\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.923572\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 0.942280\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 0.892000\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 1.080475\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 0.957316\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 0.996944\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 0.860761\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 1.143983\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 1.111727\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 1.024249\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.891987\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 0.998703\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.805441\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 1.221411\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.882421\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 1.092091\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 1.045528\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 1.242514\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 0.979325\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 0.924723\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 1.081167\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 0.982501\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 0.930678\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 0.977377\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.867041\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 0.943603\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.795455\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 0.786410\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 0.912731\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.852242\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 1.176340\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 0.737511\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 0.891640\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 1.005339\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 1.005580\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 1.148052\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 0.748983\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 1.006077\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.981327\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.896351\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 0.785070\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 1.045838\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 0.912219\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.912353\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 1.093172\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 0.931139\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 0.949869\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 1.055593\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 1.181921\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 0.851197\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 0.927146\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.826896\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 0.986313\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 0.821090\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 1.143609\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 0.786495\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 0.766613\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.804288\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 0.772383\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 0.898288\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 0.893408\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 0.887442\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 0.950037\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.876807\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 0.643416\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 1.071879\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.890951\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 1.138550\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 1.077552\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 1.075913\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 1.155770\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 1.132293\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.770763\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 1.251778\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 0.875734\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 1.054554\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.800055\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 0.955812\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 0.757746\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 0.881844\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 0.779013\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.718059\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 0.796571\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 0.772552\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 0.903520\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.916135\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.831024\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 1.056436\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.788611\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 0.988874\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.892415\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 0.902035\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 1.006654\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.803591\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.920721\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 0.727813\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.886067\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 1.145200\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 0.944851\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 0.880487\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.660451\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.543608\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.312119\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.166779\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.211472\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.315193\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.407796\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.321082\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.264120\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.148153\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.313316\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.631358\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.478317\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.486743\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.101783\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.040840\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.412837\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.479486\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.484510\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.167337\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.431497\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.250545\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.341420\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.293485\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.034911\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.346539\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.205800\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.128978\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.372852\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.376890\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.273856\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.173499\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.212213\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.478719\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.179503\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.156884\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.260195\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.142170\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.156921\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.048325\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.242821\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 1.020634\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.287773\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.378447\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.309858\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.040158\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.343399\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.142708\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.343619\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.240636\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.129837\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.284399\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.421739\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.042506\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.224777\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 0.995455\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.247791\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.249907\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.015571\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.168253\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.119272\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.391760\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.295192\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 0.998395\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 0.925151\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.305407\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.150899\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.294544\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.294882\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 1.673563\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.194442\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.419973\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 0.997997\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.244769\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.260997\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.105927\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.047004\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.222473\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.428326\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.048182\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 0.879940\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.359323\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.163940\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 1.085508\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.349646\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.328297\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.507074\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.113808\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 0.979410\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.615882\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.270381\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.215440\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.201815\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.363780\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.170825\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.243074\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.168013\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 1.135415\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.306103\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.279545\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.184955\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.276911\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 0.996061\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.213878\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.043777\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.048501\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.344274\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.088156\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.306271\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.306542\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.185242\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 1.328559\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.270178\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.069858\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.332745\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.226891\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.111088\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.198931\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.196686\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.046490\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.186163\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.265299\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.226329\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.356538\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.621390\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 0.559997\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 1.296266\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 0.962191\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.277865\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.091753\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.357207\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.284727\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.002828\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.041324\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.072880\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.328727\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.293526\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.006786\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.239549\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 2.233221\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.273492\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.362352\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.254538\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.192667\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.235813\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.024409\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 0.768613\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 0.948679\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.204379\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.315555\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.213299\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.117781\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 1.209617\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 0.356338\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.075057\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.197725\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 1.206205\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 1.002624\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 0.815370\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 0.980814\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.146962\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.124691\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 1.196876\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.228230\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 0.977416\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.318327\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.168180\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 1.428124\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.020473\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.233631\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.048213\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.129119\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.105940\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.035815\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 1.087543\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.187903\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 1.178011\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.352066\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 0.998209\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.218843\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.277789\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 2.120391\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 1.284079\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.413662\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.301024\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 1.274773\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 1.186916\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 1.286468\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.251208\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.274080\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.220947\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.265617\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 1.099156\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.095934\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.077373\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 2.572394\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.215316\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.245364\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.125622\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.028900\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 1.047711\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.361168\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.277716\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 0.953518\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.083159\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 0.911853\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 1.217801\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.116880\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 1.284780\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 0.739539\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 1.142916\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.100605\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.176665\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.068440\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.191387\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.179667\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.072823\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.339278\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 1.352527\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 1.257131\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.314130\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.071922\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.158708\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 2.232927\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.175059\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.068250\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 0.983823\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.369190\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.107523\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.247758\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.074194\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.526107\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.254047\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.237546\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 0.877293\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.136862\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.280960\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 1.528847\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.681311\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.559754\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.240760\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.264098\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.276836\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.058862\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.390285\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.178577\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.406525\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.181521\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.368373\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.311030\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.513893\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.321873\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.315748\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.323721\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.193976\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 0.983390\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.531082\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.458475\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.323287\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.275472\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.553383\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.166579\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.388465\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.234312\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.410924\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.358091\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.284877\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.340210\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.168757\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.125080\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.324023\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.274809\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.238818\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.213577\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.273691\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.353689\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.307936\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.241026\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.120382\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.254771\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.242745\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.465540\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.632840\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.257844\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.123179\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.343749\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.388148\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.085737\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.450505\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.296450\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.098813\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.223993\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.168332\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.198317\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.084681\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.345228\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.163922\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.045323\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.273429\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.111150\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.120688\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.152496\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.210092\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 1.260628\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.375575\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.063359\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 1.035341\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.165805\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.221258\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.068618\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.273156\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.517718\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 1.371617\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 1.040026\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 0.993703\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.336126\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.391910\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.167496\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.251804\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.119459\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.098632\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.167242\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.472730\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.2905, Accuracy: 5290/10000 (53%)\n",
      "\n",
      "Round 2/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.401444\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.352196\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.276804\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.111408\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.456612\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.486050\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.475406\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.266163\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.131086\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.019218\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.379104\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.254408\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.086762\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.016014\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.170093\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.298466\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.521896\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.201564\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.123852\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.342682\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.298572\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.255368\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.275083\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.063883\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.200882\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.176266\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.416346\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.384653\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.039388\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.304219\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.133631\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.266984\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 0.995220\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.203465\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.260405\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.256472\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.479212\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.198854\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.206125\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.603026\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.100425\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.025929\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.215612\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.319838\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.127357\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.259813\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.237413\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.052984\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.004929\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.397680\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.208298\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.194935\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.358054\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.283004\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.190317\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.096118\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.174238\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.100266\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 0.963575\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.036403\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 0.938883\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 0.952290\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.142716\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.213318\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.044993\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.187709\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.464697\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.489846\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.311995\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.105350\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.085031\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.024119\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.525520\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.293937\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.113210\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.214476\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.223201\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.200568\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 1.170561\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.129199\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.155129\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.269983\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.201026\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.046758\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.059064\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.076771\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.223670\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.066634\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 1.132291\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.063194\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.298066\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.212535\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 0.888064\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.279331\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.141359\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.248030\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.164192\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.011217\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.011422\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 0.986715\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.070492\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.207379\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.547458\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.308093\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.231013\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.209945\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.319821\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.333343\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.325228\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.514317\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.298007\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.057671\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.391456\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.192931\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.303644\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.001159\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.547642\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.173310\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.165814\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.247054\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.291820\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.207435\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.201053\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.159816\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.239392\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.204792\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.185601\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.246083\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.104204\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.133272\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.140169\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.307754\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.132981\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.133520\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.047066\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.416935\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.338204\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.097285\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.075922\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.355638\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.505051\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.085282\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.152624\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 0.950001\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.205789\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.223067\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.191919\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.232730\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.169437\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.117621\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.264149\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.217734\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.163213\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.212399\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.277984\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.201147\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.272545\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.258757\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.087895\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.216415\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.142876\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.250472\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 1.233152\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.153939\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.233380\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.570145\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.250651\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.275453\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 0.875687\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.185161\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.396086\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.123831\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.033785\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.357423\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 0.996720\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.349510\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.299193\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.028357\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.058332\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.029552\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 0.945466\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.282109\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.224764\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.013875\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.131500\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.214574\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.323316\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.145899\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 0.966912\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.132595\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.164607\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.048054\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.220363\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.123426\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.068157\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.055667\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.263710\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.238072\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.273475\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.353267\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.268984\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.230990\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.306031\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.087266\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.369101\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 0.796145\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.243733\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.134717\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.307544\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 1.314699\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 0.933082\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.133684\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 0.898405\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 1.101048\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 0.939939\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.085137\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 1.090665\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.080352\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 1.142612\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.073193\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 1.031484\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 1.113628\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.024206\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.193272\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 0.857365\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 1.131742\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.056490\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.138648\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.048655\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.249306\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 1.062468\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.082162\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 1.062400\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 0.893700\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 1.060240\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 0.978470\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 1.022744\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 0.962007\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 1.164640\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 1.108976\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.028836\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.024693\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 0.989208\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 1.058783\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 0.969621\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 0.944360\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 1.027656\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 1.220449\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 0.962313\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 0.906278\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 0.955094\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 0.918564\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 1.098712\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 0.853015\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 1.127815\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 0.861763\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 1.020592\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 1.268342\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 1.101718\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 0.735809\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 1.020048\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 1.002344\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 1.065865\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 0.957195\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 1.098206\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 1.121939\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 1.117541\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 0.973553\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 1.102118\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 0.966077\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.151533\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 0.922752\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.884957\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 0.935410\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 0.984337\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 1.030040\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.066731\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 1.223084\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 0.961010\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 1.148778\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.040805\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 0.957174\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 0.948027\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 1.258526\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 1.094211\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 0.955625\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 1.382344\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 0.885531\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 0.876928\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 0.956841\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 0.822383\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 0.809510\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 0.886822\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 1.046787\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 0.916722\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 0.907048\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 1.009768\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 0.830396\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 1.111971\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 1.036226\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 1.195336\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.211343\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 0.943774\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 0.860286\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 0.757507\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 1.189901\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 1.154200\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 0.940386\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 0.944556\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 1.003844\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 1.144942\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 0.818871\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 0.883268\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 1.089375\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 1.007717\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 0.937649\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.275964\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 1.101841\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 0.969959\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.764930\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 0.810747\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 0.787923\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 0.729381\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 0.877131\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 0.950093\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 1.193103\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 0.974209\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 1.191194\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 1.157339\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 0.884575\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 0.852007\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 1.050916\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 0.693666\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 1.164880\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 1.099405\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 0.980919\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 0.838894\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 0.848393\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 0.951816\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 0.856000\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 0.999275\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 0.862094\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 1.226701\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 0.948466\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 1.004763\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 0.953560\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.890169\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 0.874255\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 0.956982\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 0.934389\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 1.151805\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 0.883456\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 0.790930\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 0.666959\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 0.975745\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 0.821825\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 0.978636\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 1.112510\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 1.109676\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.793170\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 1.050729\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 1.122558\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 0.885794\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.139242\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 0.846169\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 0.728523\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 0.875247\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 0.971888\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 0.969510\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 0.972492\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 1.000913\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.854069\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.929677\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 1.082319\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.137478\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.152974\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 0.901713\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 1.324212\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.192643\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.008376\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 0.919229\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.081617\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 0.884863\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 0.953511\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 1.176826\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.958600\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 1.081963\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 1.200644\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.027520\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.088341\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 1.159545\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 0.901808\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 0.933150\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 0.968419\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 1.069860\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 0.778658\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 1.146882\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 1.279853\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 1.069951\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 0.978285\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 0.873956\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 0.898758\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 0.850252\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 0.869395\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 0.934772\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 0.803065\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 0.747762\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 0.899435\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 1.041443\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 1.050641\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 0.981156\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 0.932915\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 1.339672\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 1.297809\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 1.082194\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 0.895203\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 1.260281\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 1.215732\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 0.983531\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 0.908854\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 1.042057\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 1.007380\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 0.880259\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 1.060898\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 1.099133\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 1.003981\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 1.363405\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 1.075982\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 0.835986\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 1.044492\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 0.742811\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 0.979208\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 0.781530\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 0.901502\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 0.759288\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 0.863719\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 1.101173\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 0.994611\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.218294\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 0.949167\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 0.644668\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 0.939225\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 0.833790\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 0.829846\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 1.187773\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 1.068624\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 0.885202\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 1.110563\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 1.433339\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 1.140900\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 0.959040\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 0.872667\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 0.879992\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 1.084813\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 0.947794\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.854313\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 0.948854\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.929879\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 0.595313\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 1.057378\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 1.200240\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.913261\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 1.005105\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 0.764018\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 1.181211\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 0.808548\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 0.818193\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 1.048650\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 1.037983\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 0.961308\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 0.812130\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 1.139127\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 0.921424\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.990053\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 1.021141\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.923081\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 1.093130\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 0.965328\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 0.983552\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 0.697021\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 1.052013\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 0.851559\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 1.150753\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 0.853102\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 0.916350\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.997514\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 1.003621\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.998395\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 1.092911\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 1.276819\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.809641\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 1.035574\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 0.962459\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 0.918204\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 0.832170\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 1.129906\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 0.929305\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 0.974312\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 1.136903\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.995327\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.936251\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 1.159421\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 0.844611\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 0.812022\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 1.130327\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 0.809312\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 0.882142\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 1.204520\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.946026\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 1.068689\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 0.918819\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 1.049503\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.977545\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 0.828351\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 0.830719\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 0.844417\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 0.913282\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 1.081072\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.768479\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 1.076796\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 0.714144\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 0.782384\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 0.790528\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.216046\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.804484\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 1.120039\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 1.159644\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.703005\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 0.652516\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 0.980469\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 0.859072\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 1.103059\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 0.975450\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.823217\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.858712\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 0.998294\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 0.777242\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.933215\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 0.780526\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 0.780182\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 1.127322\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 0.843224\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.850526\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 0.791133\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 1.053028\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 0.799406\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.839826\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.825669\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 0.834065\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.884797\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 1.091569\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.904444\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 0.984543\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 0.896699\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.815109\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.862278\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 0.715546\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.812682\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 0.950411\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 1.042336\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 1.119847\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.354020\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.213932\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.411419\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.198834\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.080526\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.221258\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.175212\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.079581\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.326095\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.176562\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.305357\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.342914\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.315847\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 2.064827\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.496348\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.203845\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.330700\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.522048\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.191964\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.363756\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.061299\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.043355\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.301772\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.179901\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.306114\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.560305\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.154393\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.283699\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.067982\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.348344\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.309900\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.304957\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.398586\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.155684\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.143851\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.179924\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.479489\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.368569\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.101984\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.308423\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.120658\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 1.201383\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.213345\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.337276\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.128578\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 0.974187\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 0.906826\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.236654\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.083364\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.044393\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.410138\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.242946\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.335139\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.421443\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.140864\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 1.766709\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.168109\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.205786\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.216728\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.077857\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.393952\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.303242\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.014160\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.248904\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.447693\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 0.888317\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.280240\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.091196\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.321362\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 1.715683\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.105814\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 0.980784\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 0.987154\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.108072\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.348869\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.414830\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.327809\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.120654\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.421835\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.156766\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 0.861612\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.305696\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.093540\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 1.367966\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.272612\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.134619\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.045948\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.149631\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.226688\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.462704\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.107293\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.111203\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.070592\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.196216\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.069090\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.130649\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.221288\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 1.113733\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 0.863347\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.275671\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.129335\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.345833\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.140924\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.267720\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.069263\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.520399\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.045363\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.169666\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.004919\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.090204\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.198748\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 0.985642\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.342820\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.061688\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.087328\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.062126\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.237538\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.002589\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.256924\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.110854\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.025994\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.425653\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.378121\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 0.974413\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.277860\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 0.825645\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 1.165169\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.362256\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.239423\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.548288\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.170829\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.245092\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.185098\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.303795\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.354880\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.101422\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.252398\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.257966\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.154614\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 0.761204\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.482605\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.221919\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.094457\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.004659\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.067423\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.285431\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.302706\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.038865\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.179199\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.156742\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.220330\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.229321\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 1.138144\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 1.494977\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.179317\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.502977\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 1.134393\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 1.051483\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 1.230899\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 1.047693\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.320251\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.209231\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 1.088330\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.465840\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.206282\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.291165\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.003923\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 1.117920\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.281427\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.058243\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.281207\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.237332\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.078426\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.147646\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 1.161568\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.296586\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 0.947182\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 0.988008\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.079235\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.080705\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.037796\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 0.779282\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 1.240233\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.218528\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.253867\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 0.828849\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 1.256237\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 1.027036\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.302735\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.119470\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.096801\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 0.875335\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 1.120020\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.000311\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.081096\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 1.197936\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.244442\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.376829\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.325358\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.073244\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 1.138360\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.210243\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.102760\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 1.374221\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.134015\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.156013\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 1.236651\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.174912\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 0.954921\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 0.699733\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 0.989678\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.091272\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.076210\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.276934\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.012680\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.254619\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.223695\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.050230\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 1.081231\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 1.253303\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.082877\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.089240\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 0.943742\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 0.519229\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.071085\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.161450\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 0.882980\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.161702\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.278215\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.195356\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.291455\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.017981\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.094490\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.495852\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.275652\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.141407\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 0.998763\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 1.400303\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.608758\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.351256\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.416984\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.226254\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.193919\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.342555\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.319149\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.237658\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.152937\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.219818\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.384223\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.259906\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.218707\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.172386\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.228271\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.192311\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.131110\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.253375\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.285192\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.322830\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.068655\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.114599\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.130352\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.244033\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.079164\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.421331\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.323884\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.160834\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.114419\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 0.971304\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.161051\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 0.952456\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.077297\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.215848\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.058908\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.019996\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.342848\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.125774\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.373901\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.295775\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.408890\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.249421\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.303462\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.131767\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.383860\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.321042\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.412678\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.131899\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.290661\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.363286\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.216502\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.239168\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.428348\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.335165\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.207971\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.261212\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.264528\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.105497\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.243996\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.338619\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.217091\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.229854\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.200738\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.002429\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.293245\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 1.092746\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.234924\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.313820\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 1.326314\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 0.962335\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.156895\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.241900\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.136611\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.115412\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 1.357073\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 1.090012\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 1.240303\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.189493\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.038174\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.406620\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.096959\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.162367\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.112008\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.450250\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.138293\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.2809, Accuracy: 5314/10000 (53%)\n",
      "\n",
      "Round 3/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.618202\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 1.170845\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.229804\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.508232\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.433420\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.161813\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.301498\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.481828\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.270692\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.068924\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.125267\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.041623\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.482558\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.619083\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.069099\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.525625\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.100372\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.188354\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.164357\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.222814\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.135870\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.182348\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.170968\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.099002\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.648713\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.317922\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.111158\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.333360\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.124764\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 0.952433\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.203333\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 1.329448\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 1.362142\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.070647\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.059199\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.173262\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 0.898019\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.124123\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.090853\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.174577\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.146356\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.158923\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.154155\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.041757\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.291296\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.221983\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.077590\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 0.930129\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.124716\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.396408\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.211981\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.162774\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.274522\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 1.117135\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 0.933939\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.207434\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.511604\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 0.991670\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.213711\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 0.772877\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.259586\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.157913\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.088297\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.305064\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.032544\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.020242\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 1.119671\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.227856\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.032291\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.130036\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 0.974328\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.060168\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.173106\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 0.984994\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.274315\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 0.991743\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.039054\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.204498\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 1.353687\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.044980\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.138369\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 1.058897\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.349216\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.064700\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 2.082271\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.399748\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 0.991469\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.222176\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 1.179459\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.038522\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.173027\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.269637\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.182672\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 0.880953\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.107241\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.198650\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.323519\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.078755\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.227589\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 1.104869\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.019993\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.077756\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.625081\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.327719\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.401282\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.258254\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.346537\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.207768\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.374327\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.258531\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.246522\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.511521\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.167210\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.250214\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.354129\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.198730\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.114884\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.128469\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.099750\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.306093\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.288159\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.149127\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.290866\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.297598\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.161260\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.087264\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.088184\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.002807\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.178012\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.225122\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 0.945569\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.383320\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.380315\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.150618\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.411663\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.193083\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.064895\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.312934\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.262590\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.319965\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.000623\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.270920\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.272578\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 1.349409\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 0.901426\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.267756\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.247036\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.184885\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.113215\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.119742\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.241946\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.163417\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.106134\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.281377\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.321105\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.206010\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.068640\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.209137\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.332088\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.110831\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.181828\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.141159\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 0.924829\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.155340\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.137057\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.349074\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.307734\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.012657\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.202168\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.258302\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.194891\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.085530\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.295053\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.077068\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.295187\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.103407\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.158799\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.203571\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.247627\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.345329\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 1.361386\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 0.958817\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.018207\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.159385\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.118922\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.043439\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.121583\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.147972\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 0.973925\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 0.958428\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.089801\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.194415\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.327814\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.163846\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 1.404418\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 1.015643\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.127826\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.050054\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.028650\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.158345\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 1.215477\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.218826\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.186635\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.011159\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.352658\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.067461\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.233850\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.247655\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.097060\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 0.912749\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 1.086785\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.054845\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.045224\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 0.950913\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 0.968538\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.087079\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 0.944704\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.302739\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 0.998310\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.190265\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 1.064081\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 0.997967\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 0.945384\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 1.064861\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 0.899075\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 0.999815\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 1.133827\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.074880\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 1.050408\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.176533\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 0.964800\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.011603\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 1.104209\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 1.038197\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 1.085996\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 0.977538\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 1.007046\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 1.095816\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 1.001086\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 1.363349\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 0.877599\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.137778\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 1.060618\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 1.075101\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 1.027180\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 0.813632\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 1.167292\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 0.892393\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 0.884080\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 1.059322\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 1.236248\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 0.929817\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 1.159192\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 0.884072\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 0.980149\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.039848\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 1.044920\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 0.916101\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 1.046150\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 0.892994\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 0.894131\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 0.921278\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 1.063962\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 1.085699\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 0.874540\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 0.943563\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 0.981739\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 1.051114\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 0.864770\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 0.999033\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.121247\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 0.989500\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.927833\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 1.177468\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 1.011410\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 0.919517\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.115779\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 1.109887\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.060694\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 0.773019\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.252394\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 1.084895\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 1.100653\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 0.997463\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 1.082659\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 0.848832\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 0.935341\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 0.748213\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 0.968188\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 0.965331\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 1.116089\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 0.892486\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 1.404417\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 0.969785\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 0.945891\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 1.117765\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 1.074292\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 1.089626\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 1.125604\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 1.084920\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 1.221076\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.087952\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 1.038901\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 1.052221\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 0.814517\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 0.945231\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 0.866875\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 1.001256\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 1.100586\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 0.891504\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 0.909009\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 0.946436\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 0.933114\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 0.954154\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 1.053958\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 0.836471\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 1.038220\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 0.862363\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 1.131452\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.751853\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 0.858161\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 1.113160\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 1.086123\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 1.084529\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 0.862445\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 1.018974\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 0.956126\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 1.185454\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 1.010381\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 0.807656\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 0.845621\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 0.923011\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 0.720002\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 0.821818\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 0.784388\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 0.857865\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 1.056343\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 1.055831\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 0.951596\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 0.883273\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 1.121044\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 1.030292\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 1.067617\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 0.945992\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 1.137760\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 0.866786\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.766106\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 1.085984\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 0.852495\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 0.967125\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 0.994189\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 1.098444\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 1.135076\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 1.087692\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 0.874488\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 0.910106\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 1.072991\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 0.687727\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 0.974826\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.869907\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 0.744076\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 0.936636\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 0.905075\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 0.859300\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 1.034590\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 0.794825\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 0.927152\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 0.863116\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 1.029602\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 0.747359\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.931377\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.878668\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.885576\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 1.096910\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.325663\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.213235\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 1.097406\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 1.099109\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 0.971024\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.010804\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 1.351615\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 0.971316\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 0.935032\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.225892\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 0.950689\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.828197\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 0.758435\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 0.909008\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 0.960038\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.115166\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 0.812257\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 0.979962\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 0.689205\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 1.360394\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 1.019589\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 0.985811\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 0.789499\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 0.790648\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 0.953193\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 0.996117\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 0.814708\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 0.856535\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 1.037643\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 1.233068\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 0.746861\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 0.997149\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 0.860946\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 0.836969\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 1.009796\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 0.799954\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 0.871655\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 1.007056\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 0.999612\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 1.095214\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 0.961842\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 1.032948\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 0.966267\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 0.887513\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 0.971216\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 0.936480\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 1.028225\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 0.990249\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 0.742287\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 0.927615\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 1.140078\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 0.831690\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 0.965970\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 0.965895\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 1.170297\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.924715\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 1.115593\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 1.209725\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 1.011615\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 1.011302\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 0.817611\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 1.104947\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 1.051234\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 0.849001\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.097687\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 0.916637\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 0.956481\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 1.070580\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 1.048338\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 1.059373\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 0.943855\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 1.032017\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 1.092327\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 0.763399\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 0.813281\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 0.942797\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 0.782129\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 1.029143\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 0.859606\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 0.912071\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 0.965077\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.940550\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 0.698271\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.905859\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 1.104970\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 0.946218\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 0.982085\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 0.873930\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 1.022769\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 1.199919\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 1.093569\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 0.830773\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 0.709420\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 0.985529\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 0.541150\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 0.723646\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 0.944605\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.768535\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 1.004587\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.885508\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 0.860079\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.712370\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 0.716630\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 0.741498\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 1.136676\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 1.155410\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 1.122249\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 0.802466\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 0.690315\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 1.027907\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 1.190177\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 1.002099\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 0.945886\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.809897\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 1.017420\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 0.941406\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.802823\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 0.914940\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 0.993776\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 0.570820\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 0.916824\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 0.975156\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 0.787325\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 1.019564\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 0.926238\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.816826\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.864109\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 0.982510\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 0.996153\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 0.913762\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.873729\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 1.003499\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 0.656440\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 0.711158\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.867149\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 0.827046\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 0.739364\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 0.810767\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.736777\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 1.008211\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 1.172637\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 0.873955\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 1.185363\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 0.837265\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.818076\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 0.805266\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 0.845594\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 0.758891\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 1.073163\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.011337\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.771489\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 0.916504\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 1.222193\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.983639\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 0.952867\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 0.874370\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 0.750710\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 0.688697\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 0.719378\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.995583\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.719654\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 0.738843\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 1.144611\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.655629\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 0.902386\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 0.631810\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 0.851476\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 0.978076\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.707139\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 0.988241\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 0.849819\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 0.990767\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.953665\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.989479\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 0.766462\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.846442\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 0.954159\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.950648\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 0.966699\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 0.803155\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.591718\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.751221\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 1.090127\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.778689\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 0.938933\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 0.894562\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 0.730928\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.674339\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.299486\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.375635\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.289030\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.297272\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.448162\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.114414\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.235573\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.228712\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.402708\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.355325\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.540873\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.319324\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.594956\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.306518\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 1.190187\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.125754\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.494286\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.352535\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 1.151339\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 0.935193\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.326090\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.190796\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 1.157845\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.083567\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.359031\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.212744\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 0.835312\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 1.435590\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.386813\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.359544\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 0.952452\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.292246\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.276417\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.205246\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.168655\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.089546\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.011323\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.027046\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.060807\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.144018\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 1.636200\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 0.912375\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.186472\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 0.918615\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.153091\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.453897\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.091633\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.164107\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.248841\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.291581\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.244194\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.134936\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.121492\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.281504\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 1.328637\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.158918\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.055778\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.233338\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.053413\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.260373\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.094100\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.109589\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.132314\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.236300\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.312681\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.187218\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.043997\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.167613\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 0.757714\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.496329\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.255239\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 1.421181\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.048762\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.150650\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 1.341011\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 0.891814\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.094181\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.004912\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.230429\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 1.119632\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.080939\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.304573\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 1.315807\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 0.975162\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.051620\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.012721\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.183732\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.171812\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.167118\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.116002\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 1.419242\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.359423\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.326605\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.311446\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.134795\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.155718\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 1.726343\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.067077\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.356155\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.047736\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.274719\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.091666\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.227475\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.100132\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.125806\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.409068\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 0.993473\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.305373\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.103784\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.244165\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 1.110720\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.146068\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.228745\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.398597\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 0.865147\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.184566\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.461167\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 1.407429\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.182369\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.033896\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.245247\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.090363\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.014746\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.401639\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 1.791753\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 0.981097\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.155187\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.038355\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 1.114823\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.134503\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.105862\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.101649\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 1.002298\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 1.363167\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.275530\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 1.376812\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.094102\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.076276\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 1.202658\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.186139\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 0.994442\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.051009\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.163719\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.085073\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.037252\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.311324\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.370009\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.251510\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.119156\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.407852\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.118899\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 0.987568\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 0.926566\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.186059\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.138637\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 0.901412\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 1.039632\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 1.212993\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 0.973965\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.151651\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.059656\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 1.062520\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.252028\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.194082\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.117988\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.132947\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 0.314873\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 1.136240\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.182694\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.087380\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.019223\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.169134\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.101096\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 0.973728\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.236778\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 1.201566\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.064756\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.274192\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.278780\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.172120\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 0.865735\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 1.052417\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.181281\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.090775\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 0.822741\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 1.104841\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 1.149458\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 0.994912\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 0.851170\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 0.829665\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.109856\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 0.970204\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.051216\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.028206\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 1.482477\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.309235\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.192980\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.250850\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.211645\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 0.831761\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.186347\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 0.955256\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 1.049338\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.400146\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.380432\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 0.873543\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.025170\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 1.046051\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 0.951138\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 0.945446\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.114819\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 0.970663\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.153206\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.110642\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.014155\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 1.182647\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.347248\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 0.978841\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 0.996958\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.190278\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.090294\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.161268\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 0.741645\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.303284\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 0.958455\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 0.898788\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.223174\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.096354\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.207312\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 0.838616\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.286959\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.221628\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.297498\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.023701\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.321337\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.045735\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 1.050885\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.472522\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.525343\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.502914\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.417548\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.207229\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.371676\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.025223\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.266137\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.137758\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.183277\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.563928\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.044587\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.437188\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.274988\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.348752\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.180874\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.137328\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.112334\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.261490\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.262453\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 1.391939\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.127004\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.351111\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.206766\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.343200\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.147442\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.015389\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.353648\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.141094\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.370450\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.062737\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.325812\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.327094\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.049689\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.190168\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.112982\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 1.138185\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.042183\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.048972\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.105653\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.215250\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.128177\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 1.178490\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.365872\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.458452\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.056759\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.052889\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.204911\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.130144\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.307975\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.031486\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.370072\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.256100\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.215031\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.279422\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.213451\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.043550\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.237142\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.206656\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 0.958436\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.288426\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 1.205759\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.268743\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.287252\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.149002\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 1.276931\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.049412\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.157943\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 1.207505\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.294128\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 0.970581\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.422471\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.177761\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.369603\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 0.995468\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 1.234384\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 0.858338\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.336220\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 0.961608\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 1.325098\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 0.974137\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 1.151391\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.211479\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.183643\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.098253\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.2713, Accuracy: 5326/10000 (53%)\n",
      "\n",
      "Round 4/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/3586 (0%)]\tLoss: 1.570618\n",
      "Train Epoch: 1 [640/3586 (18%)]\tLoss: 0.994702\n",
      "Train Epoch: 1 [1280/3586 (35%)]\tLoss: 1.136458\n",
      "Train Epoch: 1 [1920/3586 (53%)]\tLoss: 1.249283\n",
      "Train Epoch: 1 [2560/3586 (70%)]\tLoss: 1.256580\n",
      "Train Epoch: 1 [3200/3586 (88%)]\tLoss: 1.307312\n",
      "Train Epoch: 2 [0/3586 (0%)]\tLoss: 1.463435\n",
      "Train Epoch: 2 [640/3586 (18%)]\tLoss: 1.135018\n",
      "Train Epoch: 2 [1280/3586 (35%)]\tLoss: 1.038937\n",
      "Train Epoch: 2 [1920/3586 (53%)]\tLoss: 1.484289\n",
      "Train Epoch: 2 [2560/3586 (70%)]\tLoss: 1.178799\n",
      "Train Epoch: 2 [3200/3586 (88%)]\tLoss: 1.114118\n",
      "Train Epoch: 3 [0/3586 (0%)]\tLoss: 1.169354\n",
      "Train Epoch: 3 [640/3586 (18%)]\tLoss: 1.217652\n",
      "Train Epoch: 3 [1280/3586 (35%)]\tLoss: 1.045561\n",
      "Train Epoch: 3 [1920/3586 (53%)]\tLoss: 1.231896\n",
      "Train Epoch: 3 [2560/3586 (70%)]\tLoss: 1.117139\n",
      "Train Epoch: 3 [3200/3586 (88%)]\tLoss: 1.319924\n",
      "Train Epoch: 4 [0/3586 (0%)]\tLoss: 1.469322\n",
      "Train Epoch: 4 [640/3586 (18%)]\tLoss: 1.454512\n",
      "Train Epoch: 4 [1280/3586 (35%)]\tLoss: 1.327825\n",
      "Train Epoch: 4 [1920/3586 (53%)]\tLoss: 1.360642\n",
      "Train Epoch: 4 [2560/3586 (70%)]\tLoss: 1.220729\n",
      "Train Epoch: 4 [3200/3586 (88%)]\tLoss: 1.085387\n",
      "Train Epoch: 5 [0/3586 (0%)]\tLoss: 1.250324\n",
      "Train Epoch: 5 [640/3586 (18%)]\tLoss: 1.319979\n",
      "Train Epoch: 5 [1280/3586 (35%)]\tLoss: 1.189986\n",
      "Train Epoch: 5 [1920/3586 (53%)]\tLoss: 1.284295\n",
      "Train Epoch: 5 [2560/3586 (70%)]\tLoss: 1.240526\n",
      "Train Epoch: 5 [3200/3586 (88%)]\tLoss: 1.236785\n",
      "Train Epoch: 6 [0/3586 (0%)]\tLoss: 1.079656\n",
      "Train Epoch: 6 [640/3586 (18%)]\tLoss: 0.934817\n",
      "Train Epoch: 6 [1280/3586 (35%)]\tLoss: 1.204173\n",
      "Train Epoch: 6 [1920/3586 (53%)]\tLoss: 1.277597\n",
      "Train Epoch: 6 [2560/3586 (70%)]\tLoss: 1.338710\n",
      "Train Epoch: 6 [3200/3586 (88%)]\tLoss: 1.315233\n",
      "Train Epoch: 7 [0/3586 (0%)]\tLoss: 1.069867\n",
      "Train Epoch: 7 [640/3586 (18%)]\tLoss: 1.264675\n",
      "Train Epoch: 7 [1280/3586 (35%)]\tLoss: 1.335255\n",
      "Train Epoch: 7 [1920/3586 (53%)]\tLoss: 1.155361\n",
      "Train Epoch: 7 [2560/3586 (70%)]\tLoss: 1.054764\n",
      "Train Epoch: 7 [3200/3586 (88%)]\tLoss: 1.085709\n",
      "Train Epoch: 8 [0/3586 (0%)]\tLoss: 1.152003\n",
      "Train Epoch: 8 [640/3586 (18%)]\tLoss: 1.235006\n",
      "Train Epoch: 8 [1280/3586 (35%)]\tLoss: 1.272487\n",
      "Train Epoch: 8 [1920/3586 (53%)]\tLoss: 1.287750\n",
      "Train Epoch: 8 [2560/3586 (70%)]\tLoss: 1.374144\n",
      "Train Epoch: 8 [3200/3586 (88%)]\tLoss: 1.147645\n",
      "Train Epoch: 9 [0/3586 (0%)]\tLoss: 1.045445\n",
      "Train Epoch: 9 [640/3586 (18%)]\tLoss: 1.224656\n",
      "Train Epoch: 9 [1280/3586 (35%)]\tLoss: 1.092754\n",
      "Train Epoch: 9 [1920/3586 (53%)]\tLoss: 1.161946\n",
      "Train Epoch: 9 [2560/3586 (70%)]\tLoss: 1.556636\n",
      "Train Epoch: 9 [3200/3586 (88%)]\tLoss: 0.934345\n",
      "Train Epoch: 10 [0/3586 (0%)]\tLoss: 1.202793\n",
      "Train Epoch: 10 [640/3586 (18%)]\tLoss: 1.137091\n",
      "Train Epoch: 10 [1280/3586 (35%)]\tLoss: 1.367375\n",
      "Train Epoch: 10 [1920/3586 (53%)]\tLoss: 1.169445\n",
      "Train Epoch: 10 [2560/3586 (70%)]\tLoss: 1.254678\n",
      "Train Epoch: 10 [3200/3586 (88%)]\tLoss: 1.398072\n",
      "Train Epoch: 11 [0/3586 (0%)]\tLoss: 1.143204\n",
      "Train Epoch: 11 [640/3586 (18%)]\tLoss: 1.316806\n",
      "Train Epoch: 11 [1280/3586 (35%)]\tLoss: 1.079377\n",
      "Train Epoch: 11 [1920/3586 (53%)]\tLoss: 1.224050\n",
      "Train Epoch: 11 [2560/3586 (70%)]\tLoss: 1.216282\n",
      "Train Epoch: 11 [3200/3586 (88%)]\tLoss: 1.298147\n",
      "Train Epoch: 12 [0/3586 (0%)]\tLoss: 0.913033\n",
      "Train Epoch: 12 [640/3586 (18%)]\tLoss: 1.274912\n",
      "Train Epoch: 12 [1280/3586 (35%)]\tLoss: 1.080804\n",
      "Train Epoch: 12 [1920/3586 (53%)]\tLoss: 1.317467\n",
      "Train Epoch: 12 [2560/3586 (70%)]\tLoss: 1.157069\n",
      "Train Epoch: 12 [3200/3586 (88%)]\tLoss: 1.197868\n",
      "Train Epoch: 13 [0/3586 (0%)]\tLoss: 1.381582\n",
      "Train Epoch: 13 [640/3586 (18%)]\tLoss: 1.310711\n",
      "Train Epoch: 13 [1280/3586 (35%)]\tLoss: 1.253951\n",
      "Train Epoch: 13 [1920/3586 (53%)]\tLoss: 1.254361\n",
      "Train Epoch: 13 [2560/3586 (70%)]\tLoss: 1.332379\n",
      "Train Epoch: 13 [3200/3586 (88%)]\tLoss: 1.327390\n",
      "Train Epoch: 14 [0/3586 (0%)]\tLoss: 0.999079\n",
      "Train Epoch: 14 [640/3586 (18%)]\tLoss: 1.147921\n",
      "Train Epoch: 14 [1280/3586 (35%)]\tLoss: 1.059972\n",
      "Train Epoch: 14 [1920/3586 (53%)]\tLoss: 0.941001\n",
      "Train Epoch: 14 [2560/3586 (70%)]\tLoss: 1.318829\n",
      "Train Epoch: 14 [3200/3586 (88%)]\tLoss: 1.045931\n",
      "Train Epoch: 15 [0/3586 (0%)]\tLoss: 1.404989\n",
      "Train Epoch: 15 [640/3586 (18%)]\tLoss: 1.181519\n",
      "Train Epoch: 15 [1280/3586 (35%)]\tLoss: 1.011831\n",
      "Train Epoch: 15 [1920/3586 (53%)]\tLoss: 1.113531\n",
      "Train Epoch: 15 [2560/3586 (70%)]\tLoss: 0.892646\n",
      "Train Epoch: 15 [3200/3586 (88%)]\tLoss: 1.044018\n",
      "Train Epoch: 16 [0/3586 (0%)]\tLoss: 1.228777\n",
      "Train Epoch: 16 [640/3586 (18%)]\tLoss: 1.458442\n",
      "Train Epoch: 16 [1280/3586 (35%)]\tLoss: 1.262798\n",
      "Train Epoch: 16 [1920/3586 (53%)]\tLoss: 1.125184\n",
      "Train Epoch: 16 [2560/3586 (70%)]\tLoss: 1.193319\n",
      "Train Epoch: 16 [3200/3586 (88%)]\tLoss: 1.123423\n",
      "Train Epoch: 17 [0/3586 (0%)]\tLoss: 1.269892\n",
      "Train Epoch: 17 [640/3586 (18%)]\tLoss: 1.123922\n",
      "Train Epoch: 17 [1280/3586 (35%)]\tLoss: 1.221647\n",
      "Train Epoch: 17 [1920/3586 (53%)]\tLoss: 0.966022\n",
      "Train Epoch: 17 [2560/3586 (70%)]\tLoss: 1.072073\n",
      "Train Epoch: 17 [3200/3586 (88%)]\tLoss: 1.071581\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/3262 (0%)]\tLoss: 1.311762\n",
      "Train Epoch: 1 [640/3262 (20%)]\tLoss: 1.430913\n",
      "Train Epoch: 1 [1280/3262 (39%)]\tLoss: 1.353476\n",
      "Train Epoch: 1 [1920/3262 (59%)]\tLoss: 1.469859\n",
      "Train Epoch: 1 [2560/3262 (78%)]\tLoss: 1.107985\n",
      "Train Epoch: 1 [3100/3262 (98%)]\tLoss: 1.016538\n",
      "Train Epoch: 2 [0/3262 (0%)]\tLoss: 1.286242\n",
      "Train Epoch: 2 [640/3262 (20%)]\tLoss: 1.236308\n",
      "Train Epoch: 2 [1280/3262 (39%)]\tLoss: 1.095415\n",
      "Train Epoch: 2 [1920/3262 (59%)]\tLoss: 1.378971\n",
      "Train Epoch: 2 [2560/3262 (78%)]\tLoss: 1.286720\n",
      "Train Epoch: 2 [3100/3262 (98%)]\tLoss: 1.557054\n",
      "Train Epoch: 3 [0/3262 (0%)]\tLoss: 1.182345\n",
      "Train Epoch: 3 [640/3262 (20%)]\tLoss: 1.340127\n",
      "Train Epoch: 3 [1280/3262 (39%)]\tLoss: 1.147582\n",
      "Train Epoch: 3 [1920/3262 (59%)]\tLoss: 1.072231\n",
      "Train Epoch: 3 [2560/3262 (78%)]\tLoss: 1.500836\n",
      "Train Epoch: 3 [3100/3262 (98%)]\tLoss: 1.021966\n",
      "Train Epoch: 4 [0/3262 (0%)]\tLoss: 1.052261\n",
      "Train Epoch: 4 [640/3262 (20%)]\tLoss: 1.119744\n",
      "Train Epoch: 4 [1280/3262 (39%)]\tLoss: 1.231259\n",
      "Train Epoch: 4 [1920/3262 (59%)]\tLoss: 1.188759\n",
      "Train Epoch: 4 [2560/3262 (78%)]\tLoss: 1.098341\n",
      "Train Epoch: 4 [3100/3262 (98%)]\tLoss: 1.077618\n",
      "Train Epoch: 5 [0/3262 (0%)]\tLoss: 1.060927\n",
      "Train Epoch: 5 [640/3262 (20%)]\tLoss: 1.369633\n",
      "Train Epoch: 5 [1280/3262 (39%)]\tLoss: 1.185667\n",
      "Train Epoch: 5 [1920/3262 (59%)]\tLoss: 1.113505\n",
      "Train Epoch: 5 [2560/3262 (78%)]\tLoss: 1.104914\n",
      "Train Epoch: 5 [3100/3262 (98%)]\tLoss: 1.227368\n",
      "Train Epoch: 6 [0/3262 (0%)]\tLoss: 1.255516\n",
      "Train Epoch: 6 [640/3262 (20%)]\tLoss: 1.323971\n",
      "Train Epoch: 6 [1280/3262 (39%)]\tLoss: 1.065829\n",
      "Train Epoch: 6 [1920/3262 (59%)]\tLoss: 1.265377\n",
      "Train Epoch: 6 [2560/3262 (78%)]\tLoss: 1.179636\n",
      "Train Epoch: 6 [3100/3262 (98%)]\tLoss: 1.077944\n",
      "Train Epoch: 7 [0/3262 (0%)]\tLoss: 1.102471\n",
      "Train Epoch: 7 [640/3262 (20%)]\tLoss: 1.241866\n",
      "Train Epoch: 7 [1280/3262 (39%)]\tLoss: 1.210328\n",
      "Train Epoch: 7 [1920/3262 (59%)]\tLoss: 1.221950\n",
      "Train Epoch: 7 [2560/3262 (78%)]\tLoss: 1.037529\n",
      "Train Epoch: 7 [3100/3262 (98%)]\tLoss: 0.930092\n",
      "Train Epoch: 8 [0/3262 (0%)]\tLoss: 1.153913\n",
      "Train Epoch: 8 [640/3262 (20%)]\tLoss: 1.262821\n",
      "Train Epoch: 8 [1280/3262 (39%)]\tLoss: 1.006810\n",
      "Train Epoch: 8 [1920/3262 (59%)]\tLoss: 1.152357\n",
      "Train Epoch: 8 [2560/3262 (78%)]\tLoss: 1.096462\n",
      "Train Epoch: 8 [3100/3262 (98%)]\tLoss: 1.170069\n",
      "Train Epoch: 9 [0/3262 (0%)]\tLoss: 1.187566\n",
      "Train Epoch: 9 [640/3262 (20%)]\tLoss: 1.262133\n",
      "Train Epoch: 9 [1280/3262 (39%)]\tLoss: 1.367988\n",
      "Train Epoch: 9 [1920/3262 (59%)]\tLoss: 1.210633\n",
      "Train Epoch: 9 [2560/3262 (78%)]\tLoss: 1.236958\n",
      "Train Epoch: 9 [3100/3262 (98%)]\tLoss: 1.424621\n",
      "Train Epoch: 10 [0/3262 (0%)]\tLoss: 1.105078\n",
      "Train Epoch: 10 [640/3262 (20%)]\tLoss: 1.099557\n",
      "Train Epoch: 10 [1280/3262 (39%)]\tLoss: 1.303078\n",
      "Train Epoch: 10 [1920/3262 (59%)]\tLoss: 1.182740\n",
      "Train Epoch: 10 [2560/3262 (78%)]\tLoss: 1.048950\n",
      "Train Epoch: 10 [3100/3262 (98%)]\tLoss: 1.002168\n",
      "Train Epoch: 11 [0/3262 (0%)]\tLoss: 0.777957\n",
      "Train Epoch: 11 [640/3262 (20%)]\tLoss: 1.174486\n",
      "Train Epoch: 11 [1280/3262 (39%)]\tLoss: 1.119818\n",
      "Train Epoch: 11 [1920/3262 (59%)]\tLoss: 1.351385\n",
      "Train Epoch: 11 [2560/3262 (78%)]\tLoss: 1.234771\n",
      "Train Epoch: 11 [3100/3262 (98%)]\tLoss: 1.165045\n",
      "Train Epoch: 12 [0/3262 (0%)]\tLoss: 1.107859\n",
      "Train Epoch: 12 [640/3262 (20%)]\tLoss: 1.330287\n",
      "Train Epoch: 12 [1280/3262 (39%)]\tLoss: 1.308877\n",
      "Train Epoch: 12 [1920/3262 (59%)]\tLoss: 1.220404\n",
      "Train Epoch: 12 [2560/3262 (78%)]\tLoss: 1.133640\n",
      "Train Epoch: 12 [3100/3262 (98%)]\tLoss: 1.101827\n",
      "Train Epoch: 13 [0/3262 (0%)]\tLoss: 1.202466\n",
      "Train Epoch: 13 [640/3262 (20%)]\tLoss: 1.020693\n",
      "Train Epoch: 13 [1280/3262 (39%)]\tLoss: 1.114379\n",
      "Train Epoch: 13 [1920/3262 (59%)]\tLoss: 1.176917\n",
      "Train Epoch: 13 [2560/3262 (78%)]\tLoss: 1.020136\n",
      "Train Epoch: 13 [3100/3262 (98%)]\tLoss: 1.006164\n",
      "Train Epoch: 14 [0/3262 (0%)]\tLoss: 0.975201\n",
      "Train Epoch: 14 [640/3262 (20%)]\tLoss: 1.239487\n",
      "Train Epoch: 14 [1280/3262 (39%)]\tLoss: 1.044168\n",
      "Train Epoch: 14 [1920/3262 (59%)]\tLoss: 1.186498\n",
      "Train Epoch: 14 [2560/3262 (78%)]\tLoss: 1.207667\n",
      "Train Epoch: 14 [3100/3262 (98%)]\tLoss: 1.283776\n",
      "Train Epoch: 15 [0/3262 (0%)]\tLoss: 1.286711\n",
      "Train Epoch: 15 [640/3262 (20%)]\tLoss: 1.148679\n",
      "Train Epoch: 15 [1280/3262 (39%)]\tLoss: 1.261449\n",
      "Train Epoch: 15 [1920/3262 (59%)]\tLoss: 1.080779\n",
      "Train Epoch: 15 [2560/3262 (78%)]\tLoss: 1.107819\n",
      "Train Epoch: 15 [3100/3262 (98%)]\tLoss: 1.138828\n",
      "Train Epoch: 16 [0/3262 (0%)]\tLoss: 1.160328\n",
      "Train Epoch: 16 [640/3262 (20%)]\tLoss: 1.118759\n",
      "Train Epoch: 16 [1280/3262 (39%)]\tLoss: 0.956578\n",
      "Train Epoch: 16 [1920/3262 (59%)]\tLoss: 0.965757\n",
      "Train Epoch: 16 [2560/3262 (78%)]\tLoss: 1.258656\n",
      "Train Epoch: 16 [3100/3262 (98%)]\tLoss: 1.065372\n",
      "Train Epoch: 17 [0/3262 (0%)]\tLoss: 1.168300\n",
      "Train Epoch: 17 [640/3262 (20%)]\tLoss: 1.042402\n",
      "Train Epoch: 17 [1280/3262 (39%)]\tLoss: 0.908183\n",
      "Train Epoch: 17 [1920/3262 (59%)]\tLoss: 1.112288\n",
      "Train Epoch: 17 [2560/3262 (78%)]\tLoss: 1.065122\n",
      "Train Epoch: 17 [3100/3262 (98%)]\tLoss: 1.009380\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5871 (0%)]\tLoss: 1.492680\n",
      "Train Epoch: 1 [640/5871 (11%)]\tLoss: 1.321643\n",
      "Train Epoch: 1 [1280/5871 (22%)]\tLoss: 1.185038\n",
      "Train Epoch: 1 [1920/5871 (33%)]\tLoss: 1.141328\n",
      "Train Epoch: 1 [2560/5871 (43%)]\tLoss: 1.167551\n",
      "Train Epoch: 1 [3200/5871 (54%)]\tLoss: 0.950442\n",
      "Train Epoch: 1 [3840/5871 (65%)]\tLoss: 0.919023\n",
      "Train Epoch: 1 [4480/5871 (76%)]\tLoss: 1.422435\n",
      "Train Epoch: 1 [5120/5871 (87%)]\tLoss: 1.305363\n",
      "Train Epoch: 1 [5760/5871 (98%)]\tLoss: 0.937942\n",
      "Train Epoch: 2 [0/5871 (0%)]\tLoss: 1.188923\n",
      "Train Epoch: 2 [640/5871 (11%)]\tLoss: 1.031421\n",
      "Train Epoch: 2 [1280/5871 (22%)]\tLoss: 0.961791\n",
      "Train Epoch: 2 [1920/5871 (33%)]\tLoss: 1.138598\n",
      "Train Epoch: 2 [2560/5871 (43%)]\tLoss: 0.881339\n",
      "Train Epoch: 2 [3200/5871 (54%)]\tLoss: 1.034475\n",
      "Train Epoch: 2 [3840/5871 (65%)]\tLoss: 0.925885\n",
      "Train Epoch: 2 [4480/5871 (76%)]\tLoss: 0.934584\n",
      "Train Epoch: 2 [5120/5871 (87%)]\tLoss: 1.064150\n",
      "Train Epoch: 2 [5760/5871 (98%)]\tLoss: 0.928984\n",
      "Train Epoch: 3 [0/5871 (0%)]\tLoss: 1.153835\n",
      "Train Epoch: 3 [640/5871 (11%)]\tLoss: 0.930121\n",
      "Train Epoch: 3 [1280/5871 (22%)]\tLoss: 0.971631\n",
      "Train Epoch: 3 [1920/5871 (33%)]\tLoss: 1.129374\n",
      "Train Epoch: 3 [2560/5871 (43%)]\tLoss: 0.876096\n",
      "Train Epoch: 3 [3200/5871 (54%)]\tLoss: 1.265299\n",
      "Train Epoch: 3 [3840/5871 (65%)]\tLoss: 1.058590\n",
      "Train Epoch: 3 [4480/5871 (76%)]\tLoss: 1.043588\n",
      "Train Epoch: 3 [5120/5871 (87%)]\tLoss: 0.968355\n",
      "Train Epoch: 3 [5760/5871 (98%)]\tLoss: 1.290239\n",
      "Train Epoch: 4 [0/5871 (0%)]\tLoss: 0.977656\n",
      "Train Epoch: 4 [640/5871 (11%)]\tLoss: 0.880363\n",
      "Train Epoch: 4 [1280/5871 (22%)]\tLoss: 0.963550\n",
      "Train Epoch: 4 [1920/5871 (33%)]\tLoss: 0.980705\n",
      "Train Epoch: 4 [2560/5871 (43%)]\tLoss: 1.050163\n",
      "Train Epoch: 4 [3200/5871 (54%)]\tLoss: 0.980650\n",
      "Train Epoch: 4 [3840/5871 (65%)]\tLoss: 1.166217\n",
      "Train Epoch: 4 [4480/5871 (76%)]\tLoss: 1.183035\n",
      "Train Epoch: 4 [5120/5871 (87%)]\tLoss: 0.853163\n",
      "Train Epoch: 4 [5760/5871 (98%)]\tLoss: 0.942395\n",
      "Train Epoch: 5 [0/5871 (0%)]\tLoss: 0.954020\n",
      "Train Epoch: 5 [640/5871 (11%)]\tLoss: 1.216797\n",
      "Train Epoch: 5 [1280/5871 (22%)]\tLoss: 1.121968\n",
      "Train Epoch: 5 [1920/5871 (33%)]\tLoss: 0.973624\n",
      "Train Epoch: 5 [2560/5871 (43%)]\tLoss: 0.984642\n",
      "Train Epoch: 5 [3200/5871 (54%)]\tLoss: 1.176215\n",
      "Train Epoch: 5 [3840/5871 (65%)]\tLoss: 0.843494\n",
      "Train Epoch: 5 [4480/5871 (76%)]\tLoss: 1.155043\n",
      "Train Epoch: 5 [5120/5871 (87%)]\tLoss: 0.984832\n",
      "Train Epoch: 5 [5760/5871 (98%)]\tLoss: 0.839138\n",
      "Train Epoch: 6 [0/5871 (0%)]\tLoss: 1.108449\n",
      "Train Epoch: 6 [640/5871 (11%)]\tLoss: 1.282561\n",
      "Train Epoch: 6 [1280/5871 (22%)]\tLoss: 0.707762\n",
      "Train Epoch: 6 [1920/5871 (33%)]\tLoss: 0.797818\n",
      "Train Epoch: 6 [2560/5871 (43%)]\tLoss: 0.928042\n",
      "Train Epoch: 6 [3200/5871 (54%)]\tLoss: 0.893718\n",
      "Train Epoch: 6 [3840/5871 (65%)]\tLoss: 0.960024\n",
      "Train Epoch: 6 [4480/5871 (76%)]\tLoss: 1.090208\n",
      "Train Epoch: 6 [5120/5871 (87%)]\tLoss: 1.112468\n",
      "Train Epoch: 6 [5760/5871 (98%)]\tLoss: 0.850668\n",
      "Train Epoch: 7 [0/5871 (0%)]\tLoss: 1.016792\n",
      "Train Epoch: 7 [640/5871 (11%)]\tLoss: 1.179829\n",
      "Train Epoch: 7 [1280/5871 (22%)]\tLoss: 1.109587\n",
      "Train Epoch: 7 [1920/5871 (33%)]\tLoss: 0.939846\n",
      "Train Epoch: 7 [2560/5871 (43%)]\tLoss: 1.116817\n",
      "Train Epoch: 7 [3200/5871 (54%)]\tLoss: 0.922629\n",
      "Train Epoch: 7 [3840/5871 (65%)]\tLoss: 1.082943\n",
      "Train Epoch: 7 [4480/5871 (76%)]\tLoss: 1.004413\n",
      "Train Epoch: 7 [5120/5871 (87%)]\tLoss: 0.877938\n",
      "Train Epoch: 7 [5760/5871 (98%)]\tLoss: 1.010344\n",
      "Train Epoch: 8 [0/5871 (0%)]\tLoss: 0.827868\n",
      "Train Epoch: 8 [640/5871 (11%)]\tLoss: 0.798541\n",
      "Train Epoch: 8 [1280/5871 (22%)]\tLoss: 1.078529\n",
      "Train Epoch: 8 [1920/5871 (33%)]\tLoss: 0.919951\n",
      "Train Epoch: 8 [2560/5871 (43%)]\tLoss: 1.032049\n",
      "Train Epoch: 8 [3200/5871 (54%)]\tLoss: 1.094294\n",
      "Train Epoch: 8 [3840/5871 (65%)]\tLoss: 1.055689\n",
      "Train Epoch: 8 [4480/5871 (76%)]\tLoss: 0.771461\n",
      "Train Epoch: 8 [5120/5871 (87%)]\tLoss: 1.088371\n",
      "Train Epoch: 8 [5760/5871 (98%)]\tLoss: 0.895625\n",
      "Train Epoch: 9 [0/5871 (0%)]\tLoss: 0.874133\n",
      "Train Epoch: 9 [640/5871 (11%)]\tLoss: 0.800228\n",
      "Train Epoch: 9 [1280/5871 (22%)]\tLoss: 1.284216\n",
      "Train Epoch: 9 [1920/5871 (33%)]\tLoss: 0.755917\n",
      "Train Epoch: 9 [2560/5871 (43%)]\tLoss: 0.937542\n",
      "Train Epoch: 9 [3200/5871 (54%)]\tLoss: 1.067272\n",
      "Train Epoch: 9 [3840/5871 (65%)]\tLoss: 0.887185\n",
      "Train Epoch: 9 [4480/5871 (76%)]\tLoss: 1.151179\n",
      "Train Epoch: 9 [5120/5871 (87%)]\tLoss: 0.977817\n",
      "Train Epoch: 9 [5760/5871 (98%)]\tLoss: 0.990816\n",
      "Train Epoch: 10 [0/5871 (0%)]\tLoss: 0.764408\n",
      "Train Epoch: 10 [640/5871 (11%)]\tLoss: 0.887946\n",
      "Train Epoch: 10 [1280/5871 (22%)]\tLoss: 0.967505\n",
      "Train Epoch: 10 [1920/5871 (33%)]\tLoss: 1.109579\n",
      "Train Epoch: 10 [2560/5871 (43%)]\tLoss: 1.114177\n",
      "Train Epoch: 10 [3200/5871 (54%)]\tLoss: 0.926215\n",
      "Train Epoch: 10 [3840/5871 (65%)]\tLoss: 0.843423\n",
      "Train Epoch: 10 [4480/5871 (76%)]\tLoss: 1.151973\n",
      "Train Epoch: 10 [5120/5871 (87%)]\tLoss: 1.026085\n",
      "Train Epoch: 10 [5760/5871 (98%)]\tLoss: 1.125632\n",
      "Train Epoch: 11 [0/5871 (0%)]\tLoss: 1.052133\n",
      "Train Epoch: 11 [640/5871 (11%)]\tLoss: 1.064211\n",
      "Train Epoch: 11 [1280/5871 (22%)]\tLoss: 1.065855\n",
      "Train Epoch: 11 [1920/5871 (33%)]\tLoss: 0.937501\n",
      "Train Epoch: 11 [2560/5871 (43%)]\tLoss: 0.911870\n",
      "Train Epoch: 11 [3200/5871 (54%)]\tLoss: 0.808480\n",
      "Train Epoch: 11 [3840/5871 (65%)]\tLoss: 0.833983\n",
      "Train Epoch: 11 [4480/5871 (76%)]\tLoss: 1.115567\n",
      "Train Epoch: 11 [5120/5871 (87%)]\tLoss: 1.232186\n",
      "Train Epoch: 11 [5760/5871 (98%)]\tLoss: 0.714087\n",
      "Train Epoch: 12 [0/5871 (0%)]\tLoss: 0.909817\n",
      "Train Epoch: 12 [640/5871 (11%)]\tLoss: 1.018008\n",
      "Train Epoch: 12 [1280/5871 (22%)]\tLoss: 0.852454\n",
      "Train Epoch: 12 [1920/5871 (33%)]\tLoss: 1.229583\n",
      "Train Epoch: 12 [2560/5871 (43%)]\tLoss: 1.007668\n",
      "Train Epoch: 12 [3200/5871 (54%)]\tLoss: 0.915624\n",
      "Train Epoch: 12 [3840/5871 (65%)]\tLoss: 0.794231\n",
      "Train Epoch: 12 [4480/5871 (76%)]\tLoss: 0.970504\n",
      "Train Epoch: 12 [5120/5871 (87%)]\tLoss: 1.221386\n",
      "Train Epoch: 12 [5760/5871 (98%)]\tLoss: 0.959511\n",
      "Train Epoch: 13 [0/5871 (0%)]\tLoss: 0.905775\n",
      "Train Epoch: 13 [640/5871 (11%)]\tLoss: 1.027674\n",
      "Train Epoch: 13 [1280/5871 (22%)]\tLoss: 0.739148\n",
      "Train Epoch: 13 [1920/5871 (33%)]\tLoss: 1.032857\n",
      "Train Epoch: 13 [2560/5871 (43%)]\tLoss: 0.925732\n",
      "Train Epoch: 13 [3200/5871 (54%)]\tLoss: 1.009255\n",
      "Train Epoch: 13 [3840/5871 (65%)]\tLoss: 1.176237\n",
      "Train Epoch: 13 [4480/5871 (76%)]\tLoss: 1.022649\n",
      "Train Epoch: 13 [5120/5871 (87%)]\tLoss: 1.030387\n",
      "Train Epoch: 13 [5760/5871 (98%)]\tLoss: 0.876423\n",
      "Train Epoch: 14 [0/5871 (0%)]\tLoss: 1.028851\n",
      "Train Epoch: 14 [640/5871 (11%)]\tLoss: 0.872043\n",
      "Train Epoch: 14 [1280/5871 (22%)]\tLoss: 0.958651\n",
      "Train Epoch: 14 [1920/5871 (33%)]\tLoss: 0.798883\n",
      "Train Epoch: 14 [2560/5871 (43%)]\tLoss: 1.107388\n",
      "Train Epoch: 14 [3200/5871 (54%)]\tLoss: 0.833342\n",
      "Train Epoch: 14 [3840/5871 (65%)]\tLoss: 0.991246\n",
      "Train Epoch: 14 [4480/5871 (76%)]\tLoss: 0.796756\n",
      "Train Epoch: 14 [5120/5871 (87%)]\tLoss: 0.853364\n",
      "Train Epoch: 14 [5760/5871 (98%)]\tLoss: 0.804570\n",
      "Train Epoch: 15 [0/5871 (0%)]\tLoss: 1.075599\n",
      "Train Epoch: 15 [640/5871 (11%)]\tLoss: 0.949257\n",
      "Train Epoch: 15 [1280/5871 (22%)]\tLoss: 0.737988\n",
      "Train Epoch: 15 [1920/5871 (33%)]\tLoss: 0.672480\n",
      "Train Epoch: 15 [2560/5871 (43%)]\tLoss: 0.778449\n",
      "Train Epoch: 15 [3200/5871 (54%)]\tLoss: 1.110568\n",
      "Train Epoch: 15 [3840/5871 (65%)]\tLoss: 0.884301\n",
      "Train Epoch: 15 [4480/5871 (76%)]\tLoss: 1.101113\n",
      "Train Epoch: 15 [5120/5871 (87%)]\tLoss: 0.932595\n",
      "Train Epoch: 15 [5760/5871 (98%)]\tLoss: 1.007861\n",
      "Train Epoch: 16 [0/5871 (0%)]\tLoss: 0.916708\n",
      "Train Epoch: 16 [640/5871 (11%)]\tLoss: 0.829993\n",
      "Train Epoch: 16 [1280/5871 (22%)]\tLoss: 0.978282\n",
      "Train Epoch: 16 [1920/5871 (33%)]\tLoss: 0.777292\n",
      "Train Epoch: 16 [2560/5871 (43%)]\tLoss: 0.849505\n",
      "Train Epoch: 16 [3200/5871 (54%)]\tLoss: 0.757698\n",
      "Train Epoch: 16 [3840/5871 (65%)]\tLoss: 1.116503\n",
      "Train Epoch: 16 [4480/5871 (76%)]\tLoss: 1.161259\n",
      "Train Epoch: 16 [5120/5871 (87%)]\tLoss: 1.002207\n",
      "Train Epoch: 16 [5760/5871 (98%)]\tLoss: 1.058716\n",
      "Train Epoch: 17 [0/5871 (0%)]\tLoss: 0.864140\n",
      "Train Epoch: 17 [640/5871 (11%)]\tLoss: 0.927500\n",
      "Train Epoch: 17 [1280/5871 (22%)]\tLoss: 0.770175\n",
      "Train Epoch: 17 [1920/5871 (33%)]\tLoss: 0.764982\n",
      "Train Epoch: 17 [2560/5871 (43%)]\tLoss: 0.830968\n",
      "Train Epoch: 17 [3200/5871 (54%)]\tLoss: 0.750846\n",
      "Train Epoch: 17 [3840/5871 (65%)]\tLoss: 0.939147\n",
      "Train Epoch: 17 [4480/5871 (76%)]\tLoss: 0.875473\n",
      "Train Epoch: 17 [5120/5871 (87%)]\tLoss: 0.880271\n",
      "Train Epoch: 17 [5760/5871 (98%)]\tLoss: 0.931301\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/6581 (0%)]\tLoss: 1.254130\n",
      "Train Epoch: 1 [640/6581 (10%)]\tLoss: 1.043675\n",
      "Train Epoch: 1 [1280/6581 (19%)]\tLoss: 0.785942\n",
      "Train Epoch: 1 [1920/6581 (29%)]\tLoss: 1.369924\n",
      "Train Epoch: 1 [2560/6581 (39%)]\tLoss: 1.128155\n",
      "Train Epoch: 1 [3200/6581 (49%)]\tLoss: 1.260167\n",
      "Train Epoch: 1 [3840/6581 (58%)]\tLoss: 0.843786\n",
      "Train Epoch: 1 [4480/6581 (68%)]\tLoss: 1.137578\n",
      "Train Epoch: 1 [5120/6581 (78%)]\tLoss: 1.120095\n",
      "Train Epoch: 1 [5760/6581 (87%)]\tLoss: 1.014306\n",
      "Train Epoch: 1 [6400/6581 (97%)]\tLoss: 0.891118\n",
      "Train Epoch: 2 [0/6581 (0%)]\tLoss: 0.824842\n",
      "Train Epoch: 2 [640/6581 (10%)]\tLoss: 1.108885\n",
      "Train Epoch: 2 [1280/6581 (19%)]\tLoss: 0.952086\n",
      "Train Epoch: 2 [1920/6581 (29%)]\tLoss: 1.036361\n",
      "Train Epoch: 2 [2560/6581 (39%)]\tLoss: 1.044230\n",
      "Train Epoch: 2 [3200/6581 (49%)]\tLoss: 0.952106\n",
      "Train Epoch: 2 [3840/6581 (58%)]\tLoss: 0.960100\n",
      "Train Epoch: 2 [4480/6581 (68%)]\tLoss: 0.882719\n",
      "Train Epoch: 2 [5120/6581 (78%)]\tLoss: 1.234535\n",
      "Train Epoch: 2 [5760/6581 (87%)]\tLoss: 1.119641\n",
      "Train Epoch: 2 [6400/6581 (97%)]\tLoss: 1.006650\n",
      "Train Epoch: 3 [0/6581 (0%)]\tLoss: 0.907306\n",
      "Train Epoch: 3 [640/6581 (10%)]\tLoss: 0.941373\n",
      "Train Epoch: 3 [1280/6581 (19%)]\tLoss: 1.225865\n",
      "Train Epoch: 3 [1920/6581 (29%)]\tLoss: 0.867503\n",
      "Train Epoch: 3 [2560/6581 (39%)]\tLoss: 0.978172\n",
      "Train Epoch: 3 [3200/6581 (49%)]\tLoss: 0.738722\n",
      "Train Epoch: 3 [3840/6581 (58%)]\tLoss: 0.788771\n",
      "Train Epoch: 3 [4480/6581 (68%)]\tLoss: 0.935060\n",
      "Train Epoch: 3 [5120/6581 (78%)]\tLoss: 1.160570\n",
      "Train Epoch: 3 [5760/6581 (87%)]\tLoss: 0.983342\n",
      "Train Epoch: 3 [6400/6581 (97%)]\tLoss: 0.851381\n",
      "Train Epoch: 4 [0/6581 (0%)]\tLoss: 1.035058\n",
      "Train Epoch: 4 [640/6581 (10%)]\tLoss: 0.940855\n",
      "Train Epoch: 4 [1280/6581 (19%)]\tLoss: 1.001678\n",
      "Train Epoch: 4 [1920/6581 (29%)]\tLoss: 0.701298\n",
      "Train Epoch: 4 [2560/6581 (39%)]\tLoss: 0.761064\n",
      "Train Epoch: 4 [3200/6581 (49%)]\tLoss: 1.104795\n",
      "Train Epoch: 4 [3840/6581 (58%)]\tLoss: 0.778675\n",
      "Train Epoch: 4 [4480/6581 (68%)]\tLoss: 0.820734\n",
      "Train Epoch: 4 [5120/6581 (78%)]\tLoss: 0.841022\n",
      "Train Epoch: 4 [5760/6581 (87%)]\tLoss: 0.613530\n",
      "Train Epoch: 4 [6400/6581 (97%)]\tLoss: 1.008987\n",
      "Train Epoch: 5 [0/6581 (0%)]\tLoss: 1.003015\n",
      "Train Epoch: 5 [640/6581 (10%)]\tLoss: 0.956774\n",
      "Train Epoch: 5 [1280/6581 (19%)]\tLoss: 0.887799\n",
      "Train Epoch: 5 [1920/6581 (29%)]\tLoss: 1.053314\n",
      "Train Epoch: 5 [2560/6581 (39%)]\tLoss: 0.793006\n",
      "Train Epoch: 5 [3200/6581 (49%)]\tLoss: 1.016098\n",
      "Train Epoch: 5 [3840/6581 (58%)]\tLoss: 0.964736\n",
      "Train Epoch: 5 [4480/6581 (68%)]\tLoss: 0.973176\n",
      "Train Epoch: 5 [5120/6581 (78%)]\tLoss: 0.964709\n",
      "Train Epoch: 5 [5760/6581 (87%)]\tLoss: 1.034630\n",
      "Train Epoch: 5 [6400/6581 (97%)]\tLoss: 1.025414\n",
      "Train Epoch: 6 [0/6581 (0%)]\tLoss: 0.752216\n",
      "Train Epoch: 6 [640/6581 (10%)]\tLoss: 0.838653\n",
      "Train Epoch: 6 [1280/6581 (19%)]\tLoss: 0.967843\n",
      "Train Epoch: 6 [1920/6581 (29%)]\tLoss: 0.993835\n",
      "Train Epoch: 6 [2560/6581 (39%)]\tLoss: 1.044611\n",
      "Train Epoch: 6 [3200/6581 (49%)]\tLoss: 0.787773\n",
      "Train Epoch: 6 [3840/6581 (58%)]\tLoss: 1.050521\n",
      "Train Epoch: 6 [4480/6581 (68%)]\tLoss: 0.762400\n",
      "Train Epoch: 6 [5120/6581 (78%)]\tLoss: 0.730197\n",
      "Train Epoch: 6 [5760/6581 (87%)]\tLoss: 1.005984\n",
      "Train Epoch: 6 [6400/6581 (97%)]\tLoss: 0.881058\n",
      "Train Epoch: 7 [0/6581 (0%)]\tLoss: 0.824582\n",
      "Train Epoch: 7 [640/6581 (10%)]\tLoss: 0.857341\n",
      "Train Epoch: 7 [1280/6581 (19%)]\tLoss: 0.823692\n",
      "Train Epoch: 7 [1920/6581 (29%)]\tLoss: 0.782559\n",
      "Train Epoch: 7 [2560/6581 (39%)]\tLoss: 0.826161\n",
      "Train Epoch: 7 [3200/6581 (49%)]\tLoss: 0.967527\n",
      "Train Epoch: 7 [3840/6581 (58%)]\tLoss: 1.089790\n",
      "Train Epoch: 7 [4480/6581 (68%)]\tLoss: 0.991282\n",
      "Train Epoch: 7 [5120/6581 (78%)]\tLoss: 1.153852\n",
      "Train Epoch: 7 [5760/6581 (87%)]\tLoss: 0.852957\n",
      "Train Epoch: 7 [6400/6581 (97%)]\tLoss: 1.108941\n",
      "Train Epoch: 8 [0/6581 (0%)]\tLoss: 0.862248\n",
      "Train Epoch: 8 [640/6581 (10%)]\tLoss: 0.973196\n",
      "Train Epoch: 8 [1280/6581 (19%)]\tLoss: 0.874725\n",
      "Train Epoch: 8 [1920/6581 (29%)]\tLoss: 0.827252\n",
      "Train Epoch: 8 [2560/6581 (39%)]\tLoss: 0.872060\n",
      "Train Epoch: 8 [3200/6581 (49%)]\tLoss: 0.636193\n",
      "Train Epoch: 8 [3840/6581 (58%)]\tLoss: 0.765767\n",
      "Train Epoch: 8 [4480/6581 (68%)]\tLoss: 0.732132\n",
      "Train Epoch: 8 [5120/6581 (78%)]\tLoss: 0.875146\n",
      "Train Epoch: 8 [5760/6581 (87%)]\tLoss: 0.758584\n",
      "Train Epoch: 8 [6400/6581 (97%)]\tLoss: 1.032372\n",
      "Train Epoch: 9 [0/6581 (0%)]\tLoss: 0.815165\n",
      "Train Epoch: 9 [640/6581 (10%)]\tLoss: 0.739448\n",
      "Train Epoch: 9 [1280/6581 (19%)]\tLoss: 0.708694\n",
      "Train Epoch: 9 [1920/6581 (29%)]\tLoss: 0.945997\n",
      "Train Epoch: 9 [2560/6581 (39%)]\tLoss: 0.657537\n",
      "Train Epoch: 9 [3200/6581 (49%)]\tLoss: 1.115653\n",
      "Train Epoch: 9 [3840/6581 (58%)]\tLoss: 0.825819\n",
      "Train Epoch: 9 [4480/6581 (68%)]\tLoss: 0.931552\n",
      "Train Epoch: 9 [5120/6581 (78%)]\tLoss: 0.989175\n",
      "Train Epoch: 9 [5760/6581 (87%)]\tLoss: 0.870470\n",
      "Train Epoch: 9 [6400/6581 (97%)]\tLoss: 0.831635\n",
      "Train Epoch: 10 [0/6581 (0%)]\tLoss: 0.939905\n",
      "Train Epoch: 10 [640/6581 (10%)]\tLoss: 0.839686\n",
      "Train Epoch: 10 [1280/6581 (19%)]\tLoss: 0.918051\n",
      "Train Epoch: 10 [1920/6581 (29%)]\tLoss: 0.731682\n",
      "Train Epoch: 10 [2560/6581 (39%)]\tLoss: 1.030359\n",
      "Train Epoch: 10 [3200/6581 (49%)]\tLoss: 0.834975\n",
      "Train Epoch: 10 [3840/6581 (58%)]\tLoss: 0.721993\n",
      "Train Epoch: 10 [4480/6581 (68%)]\tLoss: 0.864722\n",
      "Train Epoch: 10 [5120/6581 (78%)]\tLoss: 0.734385\n",
      "Train Epoch: 10 [5760/6581 (87%)]\tLoss: 0.881829\n",
      "Train Epoch: 10 [6400/6581 (97%)]\tLoss: 0.835979\n",
      "Train Epoch: 11 [0/6581 (0%)]\tLoss: 0.863431\n",
      "Train Epoch: 11 [640/6581 (10%)]\tLoss: 0.937693\n",
      "Train Epoch: 11 [1280/6581 (19%)]\tLoss: 0.782263\n",
      "Train Epoch: 11 [1920/6581 (29%)]\tLoss: 0.741312\n",
      "Train Epoch: 11 [2560/6581 (39%)]\tLoss: 0.905343\n",
      "Train Epoch: 11 [3200/6581 (49%)]\tLoss: 0.765938\n",
      "Train Epoch: 11 [3840/6581 (58%)]\tLoss: 0.891164\n",
      "Train Epoch: 11 [4480/6581 (68%)]\tLoss: 0.860776\n",
      "Train Epoch: 11 [5120/6581 (78%)]\tLoss: 1.083342\n",
      "Train Epoch: 11 [5760/6581 (87%)]\tLoss: 0.704624\n",
      "Train Epoch: 11 [6400/6581 (97%)]\tLoss: 1.063553\n",
      "Train Epoch: 12 [0/6581 (0%)]\tLoss: 0.665146\n",
      "Train Epoch: 12 [640/6581 (10%)]\tLoss: 0.848384\n",
      "Train Epoch: 12 [1280/6581 (19%)]\tLoss: 0.882405\n",
      "Train Epoch: 12 [1920/6581 (29%)]\tLoss: 0.630251\n",
      "Train Epoch: 12 [2560/6581 (39%)]\tLoss: 0.859958\n",
      "Train Epoch: 12 [3200/6581 (49%)]\tLoss: 0.858637\n",
      "Train Epoch: 12 [3840/6581 (58%)]\tLoss: 0.999687\n",
      "Train Epoch: 12 [4480/6581 (68%)]\tLoss: 0.990816\n",
      "Train Epoch: 12 [5120/6581 (78%)]\tLoss: 0.887826\n",
      "Train Epoch: 12 [5760/6581 (87%)]\tLoss: 0.757273\n",
      "Train Epoch: 12 [6400/6581 (97%)]\tLoss: 0.904528\n",
      "Train Epoch: 13 [0/6581 (0%)]\tLoss: 1.042475\n",
      "Train Epoch: 13 [640/6581 (10%)]\tLoss: 0.923869\n",
      "Train Epoch: 13 [1280/6581 (19%)]\tLoss: 0.992197\n",
      "Train Epoch: 13 [1920/6581 (29%)]\tLoss: 0.904719\n",
      "Train Epoch: 13 [2560/6581 (39%)]\tLoss: 0.806189\n",
      "Train Epoch: 13 [3200/6581 (49%)]\tLoss: 1.082721\n",
      "Train Epoch: 13 [3840/6581 (58%)]\tLoss: 0.983134\n",
      "Train Epoch: 13 [4480/6581 (68%)]\tLoss: 1.111613\n",
      "Train Epoch: 13 [5120/6581 (78%)]\tLoss: 0.701301\n",
      "Train Epoch: 13 [5760/6581 (87%)]\tLoss: 0.898039\n",
      "Train Epoch: 13 [6400/6581 (97%)]\tLoss: 0.859047\n",
      "Train Epoch: 14 [0/6581 (0%)]\tLoss: 1.045114\n",
      "Train Epoch: 14 [640/6581 (10%)]\tLoss: 0.745267\n",
      "Train Epoch: 14 [1280/6581 (19%)]\tLoss: 1.012900\n",
      "Train Epoch: 14 [1920/6581 (29%)]\tLoss: 0.876316\n",
      "Train Epoch: 14 [2560/6581 (39%)]\tLoss: 0.725737\n",
      "Train Epoch: 14 [3200/6581 (49%)]\tLoss: 0.840486\n",
      "Train Epoch: 14 [3840/6581 (58%)]\tLoss: 1.035135\n",
      "Train Epoch: 14 [4480/6581 (68%)]\tLoss: 0.995907\n",
      "Train Epoch: 14 [5120/6581 (78%)]\tLoss: 0.791291\n",
      "Train Epoch: 14 [5760/6581 (87%)]\tLoss: 1.092136\n",
      "Train Epoch: 14 [6400/6581 (97%)]\tLoss: 0.915969\n",
      "Train Epoch: 15 [0/6581 (0%)]\tLoss: 1.009392\n",
      "Train Epoch: 15 [640/6581 (10%)]\tLoss: 0.841172\n",
      "Train Epoch: 15 [1280/6581 (19%)]\tLoss: 0.832194\n",
      "Train Epoch: 15 [1920/6581 (29%)]\tLoss: 0.853429\n",
      "Train Epoch: 15 [2560/6581 (39%)]\tLoss: 0.817027\n",
      "Train Epoch: 15 [3200/6581 (49%)]\tLoss: 0.764478\n",
      "Train Epoch: 15 [3840/6581 (58%)]\tLoss: 0.743764\n",
      "Train Epoch: 15 [4480/6581 (68%)]\tLoss: 0.773100\n",
      "Train Epoch: 15 [5120/6581 (78%)]\tLoss: 0.807684\n",
      "Train Epoch: 15 [5760/6581 (87%)]\tLoss: 0.882187\n",
      "Train Epoch: 15 [6400/6581 (97%)]\tLoss: 0.970838\n",
      "Train Epoch: 16 [0/6581 (0%)]\tLoss: 1.050640\n",
      "Train Epoch: 16 [640/6581 (10%)]\tLoss: 0.875378\n",
      "Train Epoch: 16 [1280/6581 (19%)]\tLoss: 0.815813\n",
      "Train Epoch: 16 [1920/6581 (29%)]\tLoss: 0.969031\n",
      "Train Epoch: 16 [2560/6581 (39%)]\tLoss: 1.113755\n",
      "Train Epoch: 16 [3200/6581 (49%)]\tLoss: 0.789439\n",
      "Train Epoch: 16 [3840/6581 (58%)]\tLoss: 0.785998\n",
      "Train Epoch: 16 [4480/6581 (68%)]\tLoss: 0.954334\n",
      "Train Epoch: 16 [5120/6581 (78%)]\tLoss: 0.772185\n",
      "Train Epoch: 16 [5760/6581 (87%)]\tLoss: 1.025537\n",
      "Train Epoch: 16 [6400/6581 (97%)]\tLoss: 0.676894\n",
      "Train Epoch: 17 [0/6581 (0%)]\tLoss: 0.673974\n",
      "Train Epoch: 17 [640/6581 (10%)]\tLoss: 0.788443\n",
      "Train Epoch: 17 [1280/6581 (19%)]\tLoss: 0.831458\n",
      "Train Epoch: 17 [1920/6581 (29%)]\tLoss: 0.968459\n",
      "Train Epoch: 17 [2560/6581 (39%)]\tLoss: 0.828613\n",
      "Train Epoch: 17 [3200/6581 (49%)]\tLoss: 0.913835\n",
      "Train Epoch: 17 [3840/6581 (58%)]\tLoss: 0.847938\n",
      "Train Epoch: 17 [4480/6581 (68%)]\tLoss: 0.939875\n",
      "Train Epoch: 17 [5120/6581 (78%)]\tLoss: 0.765079\n",
      "Train Epoch: 17 [5760/6581 (87%)]\tLoss: 0.835527\n",
      "Train Epoch: 17 [6400/6581 (97%)]\tLoss: 0.839622\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/8325 (0%)]\tLoss: 1.474105\n",
      "Train Epoch: 1 [640/8325 (8%)]\tLoss: 1.211869\n",
      "Train Epoch: 1 [1280/8325 (15%)]\tLoss: 1.392681\n",
      "Train Epoch: 1 [1920/8325 (23%)]\tLoss: 1.117989\n",
      "Train Epoch: 1 [2560/8325 (31%)]\tLoss: 1.226980\n",
      "Train Epoch: 1 [3200/8325 (38%)]\tLoss: 1.095836\n",
      "Train Epoch: 1 [3840/8325 (46%)]\tLoss: 1.075530\n",
      "Train Epoch: 1 [4480/8325 (53%)]\tLoss: 1.176528\n",
      "Train Epoch: 1 [5120/8325 (61%)]\tLoss: 1.297720\n",
      "Train Epoch: 1 [5760/8325 (69%)]\tLoss: 1.086481\n",
      "Train Epoch: 1 [6400/8325 (76%)]\tLoss: 1.311636\n",
      "Train Epoch: 1 [7040/8325 (84%)]\tLoss: 1.251899\n",
      "Train Epoch: 1 [7680/8325 (92%)]\tLoss: 1.226334\n",
      "Train Epoch: 1 [650/8325 (99%)]\tLoss: 1.228968\n",
      "Train Epoch: 2 [0/8325 (0%)]\tLoss: 1.316419\n",
      "Train Epoch: 2 [640/8325 (8%)]\tLoss: 0.935975\n",
      "Train Epoch: 2 [1280/8325 (15%)]\tLoss: 1.209671\n",
      "Train Epoch: 2 [1920/8325 (23%)]\tLoss: 1.331090\n",
      "Train Epoch: 2 [2560/8325 (31%)]\tLoss: 1.044908\n",
      "Train Epoch: 2 [3200/8325 (38%)]\tLoss: 0.982553\n",
      "Train Epoch: 2 [3840/8325 (46%)]\tLoss: 1.391190\n",
      "Train Epoch: 2 [4480/8325 (53%)]\tLoss: 1.226060\n",
      "Train Epoch: 2 [5120/8325 (61%)]\tLoss: 1.264298\n",
      "Train Epoch: 2 [5760/8325 (69%)]\tLoss: 0.986668\n",
      "Train Epoch: 2 [6400/8325 (76%)]\tLoss: 1.497079\n",
      "Train Epoch: 2 [7040/8325 (84%)]\tLoss: 1.197015\n",
      "Train Epoch: 2 [7680/8325 (92%)]\tLoss: 1.241559\n",
      "Train Epoch: 2 [650/8325 (99%)]\tLoss: 1.392528\n",
      "Train Epoch: 3 [0/8325 (0%)]\tLoss: 0.989395\n",
      "Train Epoch: 3 [640/8325 (8%)]\tLoss: 1.005458\n",
      "Train Epoch: 3 [1280/8325 (15%)]\tLoss: 1.056399\n",
      "Train Epoch: 3 [1920/8325 (23%)]\tLoss: 1.241422\n",
      "Train Epoch: 3 [2560/8325 (31%)]\tLoss: 1.312942\n",
      "Train Epoch: 3 [3200/8325 (38%)]\tLoss: 1.063912\n",
      "Train Epoch: 3 [3840/8325 (46%)]\tLoss: 1.124662\n",
      "Train Epoch: 3 [4480/8325 (53%)]\tLoss: 1.198561\n",
      "Train Epoch: 3 [5120/8325 (61%)]\tLoss: 1.346069\n",
      "Train Epoch: 3 [5760/8325 (69%)]\tLoss: 1.134958\n",
      "Train Epoch: 3 [6400/8325 (76%)]\tLoss: 1.344801\n",
      "Train Epoch: 3 [7040/8325 (84%)]\tLoss: 1.127919\n",
      "Train Epoch: 3 [7680/8325 (92%)]\tLoss: 1.191532\n",
      "Train Epoch: 3 [650/8325 (99%)]\tLoss: 0.566892\n",
      "Train Epoch: 4 [0/8325 (0%)]\tLoss: 1.166773\n",
      "Train Epoch: 4 [640/8325 (8%)]\tLoss: 1.116996\n",
      "Train Epoch: 4 [1280/8325 (15%)]\tLoss: 1.336959\n",
      "Train Epoch: 4 [1920/8325 (23%)]\tLoss: 1.016283\n",
      "Train Epoch: 4 [2560/8325 (31%)]\tLoss: 1.242921\n",
      "Train Epoch: 4 [3200/8325 (38%)]\tLoss: 1.326332\n",
      "Train Epoch: 4 [3840/8325 (46%)]\tLoss: 1.281067\n",
      "Train Epoch: 4 [4480/8325 (53%)]\tLoss: 1.010330\n",
      "Train Epoch: 4 [5120/8325 (61%)]\tLoss: 1.060552\n",
      "Train Epoch: 4 [5760/8325 (69%)]\tLoss: 1.333358\n",
      "Train Epoch: 4 [6400/8325 (76%)]\tLoss: 1.027295\n",
      "Train Epoch: 4 [7040/8325 (84%)]\tLoss: 1.175531\n",
      "Train Epoch: 4 [7680/8325 (92%)]\tLoss: 1.239357\n",
      "Train Epoch: 4 [650/8325 (99%)]\tLoss: 2.036327\n",
      "Train Epoch: 5 [0/8325 (0%)]\tLoss: 1.127917\n",
      "Train Epoch: 5 [640/8325 (8%)]\tLoss: 1.122680\n",
      "Train Epoch: 5 [1280/8325 (15%)]\tLoss: 1.272117\n",
      "Train Epoch: 5 [1920/8325 (23%)]\tLoss: 1.125376\n",
      "Train Epoch: 5 [2560/8325 (31%)]\tLoss: 1.271694\n",
      "Train Epoch: 5 [3200/8325 (38%)]\tLoss: 1.248008\n",
      "Train Epoch: 5 [3840/8325 (46%)]\tLoss: 1.088119\n",
      "Train Epoch: 5 [4480/8325 (53%)]\tLoss: 1.177319\n",
      "Train Epoch: 5 [5120/8325 (61%)]\tLoss: 1.132065\n",
      "Train Epoch: 5 [5760/8325 (69%)]\tLoss: 1.299612\n",
      "Train Epoch: 5 [6400/8325 (76%)]\tLoss: 1.235207\n",
      "Train Epoch: 5 [7040/8325 (84%)]\tLoss: 1.223881\n",
      "Train Epoch: 5 [7680/8325 (92%)]\tLoss: 1.100357\n",
      "Train Epoch: 5 [650/8325 (99%)]\tLoss: 1.884835\n",
      "Train Epoch: 6 [0/8325 (0%)]\tLoss: 1.353237\n",
      "Train Epoch: 6 [640/8325 (8%)]\tLoss: 1.194151\n",
      "Train Epoch: 6 [1280/8325 (15%)]\tLoss: 1.244564\n",
      "Train Epoch: 6 [1920/8325 (23%)]\tLoss: 1.083670\n",
      "Train Epoch: 6 [2560/8325 (31%)]\tLoss: 1.060380\n",
      "Train Epoch: 6 [3200/8325 (38%)]\tLoss: 0.984604\n",
      "Train Epoch: 6 [3840/8325 (46%)]\tLoss: 1.218254\n",
      "Train Epoch: 6 [4480/8325 (53%)]\tLoss: 1.179556\n",
      "Train Epoch: 6 [5120/8325 (61%)]\tLoss: 1.190591\n",
      "Train Epoch: 6 [5760/8325 (69%)]\tLoss: 1.178047\n",
      "Train Epoch: 6 [6400/8325 (76%)]\tLoss: 1.251026\n",
      "Train Epoch: 6 [7040/8325 (84%)]\tLoss: 1.182955\n",
      "Train Epoch: 6 [7680/8325 (92%)]\tLoss: 1.115569\n",
      "Train Epoch: 6 [650/8325 (99%)]\tLoss: 1.887744\n",
      "Train Epoch: 7 [0/8325 (0%)]\tLoss: 1.098172\n",
      "Train Epoch: 7 [640/8325 (8%)]\tLoss: 1.279375\n",
      "Train Epoch: 7 [1280/8325 (15%)]\tLoss: 1.308652\n",
      "Train Epoch: 7 [1920/8325 (23%)]\tLoss: 1.204758\n",
      "Train Epoch: 7 [2560/8325 (31%)]\tLoss: 1.020559\n",
      "Train Epoch: 7 [3200/8325 (38%)]\tLoss: 1.121230\n",
      "Train Epoch: 7 [3840/8325 (46%)]\tLoss: 1.297733\n",
      "Train Epoch: 7 [4480/8325 (53%)]\tLoss: 0.957413\n",
      "Train Epoch: 7 [5120/8325 (61%)]\tLoss: 1.227917\n",
      "Train Epoch: 7 [5760/8325 (69%)]\tLoss: 1.397602\n",
      "Train Epoch: 7 [6400/8325 (76%)]\tLoss: 1.093600\n",
      "Train Epoch: 7 [7040/8325 (84%)]\tLoss: 1.292670\n",
      "Train Epoch: 7 [7680/8325 (92%)]\tLoss: 1.098335\n",
      "Train Epoch: 7 [650/8325 (99%)]\tLoss: 0.960483\n",
      "Train Epoch: 8 [0/8325 (0%)]\tLoss: 1.277712\n",
      "Train Epoch: 8 [640/8325 (8%)]\tLoss: 1.161841\n",
      "Train Epoch: 8 [1280/8325 (15%)]\tLoss: 1.144106\n",
      "Train Epoch: 8 [1920/8325 (23%)]\tLoss: 1.093152\n",
      "Train Epoch: 8 [2560/8325 (31%)]\tLoss: 1.052517\n",
      "Train Epoch: 8 [3200/8325 (38%)]\tLoss: 1.282309\n",
      "Train Epoch: 8 [3840/8325 (46%)]\tLoss: 1.118287\n",
      "Train Epoch: 8 [4480/8325 (53%)]\tLoss: 1.015109\n",
      "Train Epoch: 8 [5120/8325 (61%)]\tLoss: 1.015622\n",
      "Train Epoch: 8 [5760/8325 (69%)]\tLoss: 1.179178\n",
      "Train Epoch: 8 [6400/8325 (76%)]\tLoss: 1.115495\n",
      "Train Epoch: 8 [7040/8325 (84%)]\tLoss: 1.038607\n",
      "Train Epoch: 8 [7680/8325 (92%)]\tLoss: 1.260070\n",
      "Train Epoch: 8 [650/8325 (99%)]\tLoss: 1.047806\n",
      "Train Epoch: 9 [0/8325 (0%)]\tLoss: 1.023702\n",
      "Train Epoch: 9 [640/8325 (8%)]\tLoss: 1.226302\n",
      "Train Epoch: 9 [1280/8325 (15%)]\tLoss: 1.208429\n",
      "Train Epoch: 9 [1920/8325 (23%)]\tLoss: 1.014820\n",
      "Train Epoch: 9 [2560/8325 (31%)]\tLoss: 1.355716\n",
      "Train Epoch: 9 [3200/8325 (38%)]\tLoss: 1.066428\n",
      "Train Epoch: 9 [3840/8325 (46%)]\tLoss: 0.977551\n",
      "Train Epoch: 9 [4480/8325 (53%)]\tLoss: 1.321072\n",
      "Train Epoch: 9 [5120/8325 (61%)]\tLoss: 1.281224\n",
      "Train Epoch: 9 [5760/8325 (69%)]\tLoss: 1.215899\n",
      "Train Epoch: 9 [6400/8325 (76%)]\tLoss: 1.200207\n",
      "Train Epoch: 9 [7040/8325 (84%)]\tLoss: 1.292225\n",
      "Train Epoch: 9 [7680/8325 (92%)]\tLoss: 1.218132\n",
      "Train Epoch: 9 [650/8325 (99%)]\tLoss: 2.010625\n",
      "Train Epoch: 10 [0/8325 (0%)]\tLoss: 0.942645\n",
      "Train Epoch: 10 [640/8325 (8%)]\tLoss: 1.101091\n",
      "Train Epoch: 10 [1280/8325 (15%)]\tLoss: 1.290326\n",
      "Train Epoch: 10 [1920/8325 (23%)]\tLoss: 0.974671\n",
      "Train Epoch: 10 [2560/8325 (31%)]\tLoss: 1.083880\n",
      "Train Epoch: 10 [3200/8325 (38%)]\tLoss: 1.006566\n",
      "Train Epoch: 10 [3840/8325 (46%)]\tLoss: 1.258076\n",
      "Train Epoch: 10 [4480/8325 (53%)]\tLoss: 0.977796\n",
      "Train Epoch: 10 [5120/8325 (61%)]\tLoss: 0.901323\n",
      "Train Epoch: 10 [5760/8325 (69%)]\tLoss: 1.127696\n",
      "Train Epoch: 10 [6400/8325 (76%)]\tLoss: 0.984341\n",
      "Train Epoch: 10 [7040/8325 (84%)]\tLoss: 1.138392\n",
      "Train Epoch: 10 [7680/8325 (92%)]\tLoss: 1.164093\n",
      "Train Epoch: 10 [650/8325 (99%)]\tLoss: 0.669527\n",
      "Train Epoch: 11 [0/8325 (0%)]\tLoss: 1.122292\n",
      "Train Epoch: 11 [640/8325 (8%)]\tLoss: 1.152017\n",
      "Train Epoch: 11 [1280/8325 (15%)]\tLoss: 1.303401\n",
      "Train Epoch: 11 [1920/8325 (23%)]\tLoss: 1.054776\n",
      "Train Epoch: 11 [2560/8325 (31%)]\tLoss: 1.148076\n",
      "Train Epoch: 11 [3200/8325 (38%)]\tLoss: 1.076692\n",
      "Train Epoch: 11 [3840/8325 (46%)]\tLoss: 1.199704\n",
      "Train Epoch: 11 [4480/8325 (53%)]\tLoss: 1.269812\n",
      "Train Epoch: 11 [5120/8325 (61%)]\tLoss: 1.047044\n",
      "Train Epoch: 11 [5760/8325 (69%)]\tLoss: 1.317346\n",
      "Train Epoch: 11 [6400/8325 (76%)]\tLoss: 1.051730\n",
      "Train Epoch: 11 [7040/8325 (84%)]\tLoss: 1.281272\n",
      "Train Epoch: 11 [7680/8325 (92%)]\tLoss: 0.906392\n",
      "Train Epoch: 11 [650/8325 (99%)]\tLoss: 0.702594\n",
      "Train Epoch: 12 [0/8325 (0%)]\tLoss: 1.266738\n",
      "Train Epoch: 12 [640/8325 (8%)]\tLoss: 1.048003\n",
      "Train Epoch: 12 [1280/8325 (15%)]\tLoss: 0.985306\n",
      "Train Epoch: 12 [1920/8325 (23%)]\tLoss: 0.964071\n",
      "Train Epoch: 12 [2560/8325 (31%)]\tLoss: 1.336747\n",
      "Train Epoch: 12 [3200/8325 (38%)]\tLoss: 0.991671\n",
      "Train Epoch: 12 [3840/8325 (46%)]\tLoss: 1.181152\n",
      "Train Epoch: 12 [4480/8325 (53%)]\tLoss: 1.422477\n",
      "Train Epoch: 12 [5120/8325 (61%)]\tLoss: 0.827374\n",
      "Train Epoch: 12 [5760/8325 (69%)]\tLoss: 1.310714\n",
      "Train Epoch: 12 [6400/8325 (76%)]\tLoss: 1.082013\n",
      "Train Epoch: 12 [7040/8325 (84%)]\tLoss: 1.087483\n",
      "Train Epoch: 12 [7680/8325 (92%)]\tLoss: 1.222417\n",
      "Train Epoch: 12 [650/8325 (99%)]\tLoss: 1.037482\n",
      "Train Epoch: 13 [0/8325 (0%)]\tLoss: 0.962951\n",
      "Train Epoch: 13 [640/8325 (8%)]\tLoss: 1.142210\n",
      "Train Epoch: 13 [1280/8325 (15%)]\tLoss: 1.212948\n",
      "Train Epoch: 13 [1920/8325 (23%)]\tLoss: 1.062242\n",
      "Train Epoch: 13 [2560/8325 (31%)]\tLoss: 1.017170\n",
      "Train Epoch: 13 [3200/8325 (38%)]\tLoss: 1.413350\n",
      "Train Epoch: 13 [3840/8325 (46%)]\tLoss: 0.917340\n",
      "Train Epoch: 13 [4480/8325 (53%)]\tLoss: 1.235999\n",
      "Train Epoch: 13 [5120/8325 (61%)]\tLoss: 1.129985\n",
      "Train Epoch: 13 [5760/8325 (69%)]\tLoss: 1.225780\n",
      "Train Epoch: 13 [6400/8325 (76%)]\tLoss: 1.285093\n",
      "Train Epoch: 13 [7040/8325 (84%)]\tLoss: 1.169711\n",
      "Train Epoch: 13 [7680/8325 (92%)]\tLoss: 1.458664\n",
      "Train Epoch: 13 [650/8325 (99%)]\tLoss: 0.732559\n",
      "Train Epoch: 14 [0/8325 (0%)]\tLoss: 0.929787\n",
      "Train Epoch: 14 [640/8325 (8%)]\tLoss: 1.063928\n",
      "Train Epoch: 14 [1280/8325 (15%)]\tLoss: 1.127313\n",
      "Train Epoch: 14 [1920/8325 (23%)]\tLoss: 1.061292\n",
      "Train Epoch: 14 [2560/8325 (31%)]\tLoss: 0.952152\n",
      "Train Epoch: 14 [3200/8325 (38%)]\tLoss: 0.993475\n",
      "Train Epoch: 14 [3840/8325 (46%)]\tLoss: 1.314633\n",
      "Train Epoch: 14 [4480/8325 (53%)]\tLoss: 1.196931\n",
      "Train Epoch: 14 [5120/8325 (61%)]\tLoss: 1.253170\n",
      "Train Epoch: 14 [5760/8325 (69%)]\tLoss: 1.161247\n",
      "Train Epoch: 14 [6400/8325 (76%)]\tLoss: 0.991107\n",
      "Train Epoch: 14 [7040/8325 (84%)]\tLoss: 1.156975\n",
      "Train Epoch: 14 [7680/8325 (92%)]\tLoss: 1.309045\n",
      "Train Epoch: 14 [650/8325 (99%)]\tLoss: 0.828724\n",
      "Train Epoch: 15 [0/8325 (0%)]\tLoss: 1.053183\n",
      "Train Epoch: 15 [640/8325 (8%)]\tLoss: 1.156523\n",
      "Train Epoch: 15 [1280/8325 (15%)]\tLoss: 1.179342\n",
      "Train Epoch: 15 [1920/8325 (23%)]\tLoss: 1.207343\n",
      "Train Epoch: 15 [2560/8325 (31%)]\tLoss: 1.011467\n",
      "Train Epoch: 15 [3200/8325 (38%)]\tLoss: 1.032215\n",
      "Train Epoch: 15 [3840/8325 (46%)]\tLoss: 1.057088\n",
      "Train Epoch: 15 [4480/8325 (53%)]\tLoss: 1.104898\n",
      "Train Epoch: 15 [5120/8325 (61%)]\tLoss: 1.061334\n",
      "Train Epoch: 15 [5760/8325 (69%)]\tLoss: 1.224380\n",
      "Train Epoch: 15 [6400/8325 (76%)]\tLoss: 1.211996\n",
      "Train Epoch: 15 [7040/8325 (84%)]\tLoss: 1.030161\n",
      "Train Epoch: 15 [7680/8325 (92%)]\tLoss: 0.990075\n",
      "Train Epoch: 15 [650/8325 (99%)]\tLoss: 1.715038\n",
      "Train Epoch: 16 [0/8325 (0%)]\tLoss: 1.019769\n",
      "Train Epoch: 16 [640/8325 (8%)]\tLoss: 1.173074\n",
      "Train Epoch: 16 [1280/8325 (15%)]\tLoss: 1.114437\n",
      "Train Epoch: 16 [1920/8325 (23%)]\tLoss: 1.182257\n",
      "Train Epoch: 16 [2560/8325 (31%)]\tLoss: 1.237295\n",
      "Train Epoch: 16 [3200/8325 (38%)]\tLoss: 1.216267\n",
      "Train Epoch: 16 [3840/8325 (46%)]\tLoss: 0.992715\n",
      "Train Epoch: 16 [4480/8325 (53%)]\tLoss: 1.241981\n",
      "Train Epoch: 16 [5120/8325 (61%)]\tLoss: 0.984457\n",
      "Train Epoch: 16 [5760/8325 (69%)]\tLoss: 0.981662\n",
      "Train Epoch: 16 [6400/8325 (76%)]\tLoss: 1.090716\n",
      "Train Epoch: 16 [7040/8325 (84%)]\tLoss: 1.269060\n",
      "Train Epoch: 16 [7680/8325 (92%)]\tLoss: 1.396311\n",
      "Train Epoch: 16 [650/8325 (99%)]\tLoss: 1.332402\n",
      "Train Epoch: 17 [0/8325 (0%)]\tLoss: 1.125242\n",
      "Train Epoch: 17 [640/8325 (8%)]\tLoss: 1.284239\n",
      "Train Epoch: 17 [1280/8325 (15%)]\tLoss: 1.162231\n",
      "Train Epoch: 17 [1920/8325 (23%)]\tLoss: 1.035299\n",
      "Train Epoch: 17 [2560/8325 (31%)]\tLoss: 1.305410\n",
      "Train Epoch: 17 [3200/8325 (38%)]\tLoss: 1.118254\n",
      "Train Epoch: 17 [3840/8325 (46%)]\tLoss: 1.292903\n",
      "Train Epoch: 17 [4480/8325 (53%)]\tLoss: 1.181948\n",
      "Train Epoch: 17 [5120/8325 (61%)]\tLoss: 1.014728\n",
      "Train Epoch: 17 [5760/8325 (69%)]\tLoss: 1.087404\n",
      "Train Epoch: 17 [6400/8325 (76%)]\tLoss: 1.214973\n",
      "Train Epoch: 17 [7040/8325 (84%)]\tLoss: 1.133067\n",
      "Train Epoch: 17 [7680/8325 (92%)]\tLoss: 1.126198\n",
      "Train Epoch: 17 [650/8325 (99%)]\tLoss: 0.906613\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3133 (0%)]\tLoss: 1.748240\n",
      "Train Epoch: 1 [640/3133 (20%)]\tLoss: 1.794369\n",
      "Train Epoch: 1 [1280/3133 (41%)]\tLoss: 1.177882\n",
      "Train Epoch: 1 [1920/3133 (61%)]\tLoss: 1.554397\n",
      "Train Epoch: 1 [2560/3133 (82%)]\tLoss: 1.166253\n",
      "Train Epoch: 2 [0/3133 (0%)]\tLoss: 1.547675\n",
      "Train Epoch: 2 [640/3133 (20%)]\tLoss: 1.230518\n",
      "Train Epoch: 2 [1280/3133 (41%)]\tLoss: 1.518506\n",
      "Train Epoch: 2 [1920/3133 (61%)]\tLoss: 1.364772\n",
      "Train Epoch: 2 [2560/3133 (82%)]\tLoss: 1.244712\n",
      "Train Epoch: 3 [0/3133 (0%)]\tLoss: 1.177245\n",
      "Train Epoch: 3 [640/3133 (20%)]\tLoss: 1.345752\n",
      "Train Epoch: 3 [1280/3133 (41%)]\tLoss: 1.235482\n",
      "Train Epoch: 3 [1920/3133 (61%)]\tLoss: 1.246171\n",
      "Train Epoch: 3 [2560/3133 (82%)]\tLoss: 1.430696\n",
      "Train Epoch: 4 [0/3133 (0%)]\tLoss: 1.499501\n",
      "Train Epoch: 4 [640/3133 (20%)]\tLoss: 1.297571\n",
      "Train Epoch: 4 [1280/3133 (41%)]\tLoss: 1.047520\n",
      "Train Epoch: 4 [1920/3133 (61%)]\tLoss: 1.303576\n",
      "Train Epoch: 4 [2560/3133 (82%)]\tLoss: 1.242900\n",
      "Train Epoch: 5 [0/3133 (0%)]\tLoss: 0.994403\n",
      "Train Epoch: 5 [640/3133 (20%)]\tLoss: 1.162165\n",
      "Train Epoch: 5 [1280/3133 (41%)]\tLoss: 1.282667\n",
      "Train Epoch: 5 [1920/3133 (61%)]\tLoss: 1.282305\n",
      "Train Epoch: 5 [2560/3133 (82%)]\tLoss: 1.250119\n",
      "Train Epoch: 6 [0/3133 (0%)]\tLoss: 1.284128\n",
      "Train Epoch: 6 [640/3133 (20%)]\tLoss: 1.013755\n",
      "Train Epoch: 6 [1280/3133 (41%)]\tLoss: 1.363775\n",
      "Train Epoch: 6 [1920/3133 (61%)]\tLoss: 1.071048\n",
      "Train Epoch: 6 [2560/3133 (82%)]\tLoss: 1.257739\n",
      "Train Epoch: 7 [0/3133 (0%)]\tLoss: 1.178546\n",
      "Train Epoch: 7 [640/3133 (20%)]\tLoss: 1.296818\n",
      "Train Epoch: 7 [1280/3133 (41%)]\tLoss: 1.256006\n",
      "Train Epoch: 7 [1920/3133 (61%)]\tLoss: 1.240425\n",
      "Train Epoch: 7 [2560/3133 (82%)]\tLoss: 1.101621\n",
      "Train Epoch: 8 [0/3133 (0%)]\tLoss: 1.201723\n",
      "Train Epoch: 8 [640/3133 (20%)]\tLoss: 0.987397\n",
      "Train Epoch: 8 [1280/3133 (41%)]\tLoss: 1.344541\n",
      "Train Epoch: 8 [1920/3133 (61%)]\tLoss: 1.471231\n",
      "Train Epoch: 8 [2560/3133 (82%)]\tLoss: 1.286252\n",
      "Train Epoch: 9 [0/3133 (0%)]\tLoss: 1.404972\n",
      "Train Epoch: 9 [640/3133 (20%)]\tLoss: 1.258104\n",
      "Train Epoch: 9 [1280/3133 (41%)]\tLoss: 0.958115\n",
      "Train Epoch: 9 [1920/3133 (61%)]\tLoss: 1.222426\n",
      "Train Epoch: 9 [2560/3133 (82%)]\tLoss: 1.448853\n",
      "Train Epoch: 10 [0/3133 (0%)]\tLoss: 1.201960\n",
      "Train Epoch: 10 [640/3133 (20%)]\tLoss: 1.586525\n",
      "Train Epoch: 10 [1280/3133 (41%)]\tLoss: 1.278853\n",
      "Train Epoch: 10 [1920/3133 (61%)]\tLoss: 1.235343\n",
      "Train Epoch: 10 [2560/3133 (82%)]\tLoss: 1.342380\n",
      "Train Epoch: 11 [0/3133 (0%)]\tLoss: 1.267735\n",
      "Train Epoch: 11 [640/3133 (20%)]\tLoss: 1.133116\n",
      "Train Epoch: 11 [1280/3133 (41%)]\tLoss: 1.187803\n",
      "Train Epoch: 11 [1920/3133 (61%)]\tLoss: 1.228107\n",
      "Train Epoch: 11 [2560/3133 (82%)]\tLoss: 1.150713\n",
      "Train Epoch: 12 [0/3133 (0%)]\tLoss: 1.017593\n",
      "Train Epoch: 12 [640/3133 (20%)]\tLoss: 1.367199\n",
      "Train Epoch: 12 [1280/3133 (41%)]\tLoss: 1.078944\n",
      "Train Epoch: 12 [1920/3133 (61%)]\tLoss: 1.260893\n",
      "Train Epoch: 12 [2560/3133 (82%)]\tLoss: 1.024574\n",
      "Train Epoch: 13 [0/3133 (0%)]\tLoss: 1.269339\n",
      "Train Epoch: 13 [640/3133 (20%)]\tLoss: 0.940280\n",
      "Train Epoch: 13 [1280/3133 (41%)]\tLoss: 1.019339\n",
      "Train Epoch: 13 [1920/3133 (61%)]\tLoss: 1.027259\n",
      "Train Epoch: 13 [2560/3133 (82%)]\tLoss: 1.130649\n",
      "Train Epoch: 14 [0/3133 (0%)]\tLoss: 0.998456\n",
      "Train Epoch: 14 [640/3133 (20%)]\tLoss: 1.051394\n",
      "Train Epoch: 14 [1280/3133 (41%)]\tLoss: 1.100551\n",
      "Train Epoch: 14 [1920/3133 (61%)]\tLoss: 0.993989\n",
      "Train Epoch: 14 [2560/3133 (82%)]\tLoss: 1.119825\n",
      "Train Epoch: 15 [0/3133 (0%)]\tLoss: 1.203958\n",
      "Train Epoch: 15 [640/3133 (20%)]\tLoss: 1.075411\n",
      "Train Epoch: 15 [1280/3133 (41%)]\tLoss: 1.120734\n",
      "Train Epoch: 15 [1920/3133 (61%)]\tLoss: 1.355533\n",
      "Train Epoch: 15 [2560/3133 (82%)]\tLoss: 0.998358\n",
      "Train Epoch: 16 [0/3133 (0%)]\tLoss: 0.970522\n",
      "Train Epoch: 16 [640/3133 (20%)]\tLoss: 1.257588\n",
      "Train Epoch: 16 [1280/3133 (41%)]\tLoss: 1.131063\n",
      "Train Epoch: 16 [1920/3133 (61%)]\tLoss: 1.276763\n",
      "Train Epoch: 16 [2560/3133 (82%)]\tLoss: 0.770199\n",
      "Train Epoch: 17 [0/3133 (0%)]\tLoss: 1.144694\n",
      "Train Epoch: 17 [640/3133 (20%)]\tLoss: 0.860076\n",
      "Train Epoch: 17 [1280/3133 (41%)]\tLoss: 1.232756\n",
      "Train Epoch: 17 [1920/3133 (61%)]\tLoss: 1.171864\n",
      "Train Epoch: 17 [2560/3133 (82%)]\tLoss: 1.185710\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.2602, Accuracy: 5399/10000 (54%)\n",
      "\n",
      "Running experiment with alpha: 1 \n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.607347\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.424621\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.424219\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.236083\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.327251\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.319801\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.296839\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 0.920149\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.205254\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 1.248414\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.201531\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.131467\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.060412\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.228545\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.188902\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.458117\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.228080\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.338717\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.283659\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.467824\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.264552\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.066061\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.507013\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.189227\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.301739\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 1.395541\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.139141\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.427326\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.036281\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.093318\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.288104\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.342671\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.200609\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.233856\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.204988\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.125873\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.095841\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.435125\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.181531\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.122042\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 1.229851\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 1.048055\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 1.351733\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 1.248205\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.074896\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.113217\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 0.993065\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 1.265410\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.606981\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.090437\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.469508\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 0.937931\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 1.219839\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 1.042563\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 1.396848\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.177224\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 1.093788\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 1.101730\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.305048\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 0.889619\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.377669\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 1.303385\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 1.052941\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.062793\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.163124\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 1.322962\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 0.939552\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.003500\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 1.018304\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 1.196018\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.039271\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 1.118155\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 1.257823\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.250122\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.118294\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 1.284507\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.455167\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.228738\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 1.250495\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 1.162687\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.053119\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 0.980435\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.329900\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.000807\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 1.054639\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 1.129918\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 0.968841\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 1.188791\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 0.998179\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.181862\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.095312\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.061705\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 1.291095\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 1.103723\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 1.071899\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 1.199824\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.093977\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 0.980264\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 1.032505\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 1.366722\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.111081\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 0.930081\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 0.894368\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 1.075091\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 1.060584\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 1.079925\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 0.985319\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.058254\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 1.071149\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 0.980649\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.099144\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 1.195802\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 1.140141\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 1.078833\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.049525\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 0.772412\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 0.876509\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.216309\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 1.185383\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.543240\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.065103\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.607840\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.366399\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.275111\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.310469\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.688687\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.191157\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.232966\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.234568\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.542011\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.176538\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.138695\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.039289\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.072800\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.320565\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.337491\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.013221\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.234480\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 0.986852\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.074965\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.191996\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.463793\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.330240\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.173633\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.312809\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.383029\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.224423\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.364434\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.255007\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.407529\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.221772\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.089948\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.118631\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.326766\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.268709\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.643817\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.206492\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.431805\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.289148\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.173428\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.388475\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.121301\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.176164\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 0.949507\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.137673\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.288370\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.185399\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.183134\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.282395\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.306017\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.296455\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.260423\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.325116\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 0.969593\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.050228\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.168338\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.173463\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.217018\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 1.154630\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.250932\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 0.901094\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.299090\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.000058\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.071049\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.085955\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.369067\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.252053\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.472079\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.173915\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.076149\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.072596\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.229570\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.241164\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.108813\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.108119\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.114854\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.312791\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 1.260736\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.146232\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.305108\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.296932\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.379494\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.407683\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.102244\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.081846\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 1.345360\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.165600\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.212167\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.328900\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.119066\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.189677\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.167997\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.244237\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 1.154749\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 1.320943\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.273336\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.105997\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.088364\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.217050\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.206290\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.210047\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.212245\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.263095\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 1.063780\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.187235\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 0.918195\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.041442\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.347906\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.128964\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.164965\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.315692\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 1.380704\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.188504\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.202247\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.273613\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.197826\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.088493\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 1.155039\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.090425\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.133802\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.208838\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 0.946841\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.275988\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.147704\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.146437\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 0.985546\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.348774\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.105674\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.051808\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.080594\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.029342\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.591045\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 1.302136\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 1.123246\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 1.129729\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.858494\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.184358\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.240876\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.598790\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.315262\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.166941\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.277997\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.086658\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.350479\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.568450\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.265289\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.179728\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.485473\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.294465\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.198514\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.146627\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.154390\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.358955\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.260880\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.177959\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.218390\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.167358\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.492705\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.296299\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.132825\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.211674\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.480065\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.266052\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.361312\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.255281\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.155949\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.403112\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.178619\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.360814\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.346091\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.387851\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.257931\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.286696\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.327383\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.297949\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.449488\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.333962\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.190234\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.047617\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.265782\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.165541\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.401916\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.393303\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.231790\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.290835\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.141442\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.268924\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.281746\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.273439\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.301305\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.186843\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.171033\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.034473\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.459778\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.210552\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.177053\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.283407\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.345704\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.339093\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.178957\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.350719\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.213788\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.387971\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.395463\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.647735\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 0.996690\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.363500\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.083925\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.176799\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.196364\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.383751\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.331647\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.057373\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.373668\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 1.410137\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.193715\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.020754\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.178110\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.504488\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.387113\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.430890\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.162849\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.086614\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.107452\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.400102\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.278711\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.302203\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.275041\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.291891\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.261683\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.309185\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.451357\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.267240\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.006182\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 1.193856\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.284975\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.232570\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.214159\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 0.990970\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 0.908840\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.198804\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.329567\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.212723\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.352252\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.389955\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 1.308731\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.415573\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.234951\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.093846\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.287020\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.443216\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.221122\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.163991\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.386344\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.152341\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.189620\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.522233\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.508448\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.293503\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.076490\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.102775\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.415833\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.240902\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.120363\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.096991\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 1.288804\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.201054\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.272843\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.201052\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.360526\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.188290\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.272658\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.220421\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.111274\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.320302\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 1.121619\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 1.212875\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.170546\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 1.127016\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.363374\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.195608\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.359751\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.257903\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.165697\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 0.953522\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 0.859380\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.264711\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.257743\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 0.995103\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.092072\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.147517\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.206137\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.201389\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.095478\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.157224\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.036689\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.327505\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 1.306515\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.060930\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.176711\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.449724\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.391195\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.320973\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.291406\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.073567\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.567167\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 0.970638\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 1.323645\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 1.041044\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 0.933853\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 1.015925\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 0.992028\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 1.105780\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 0.953892\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 0.838868\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 1.226304\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 1.065149\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 1.061996\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 1.064789\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 1.105780\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 1.089373\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 0.967361\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 1.323899\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.954065\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.933845\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 1.047959\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 0.755469\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 1.001241\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 1.058897\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 1.076259\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 1.003323\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 0.826842\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 0.622207\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 1.215997\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 1.322508\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 1.148658\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 0.638796\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.765408\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 1.046077\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 0.900918\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 0.635940\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 1.073416\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 1.239630\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.779081\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.823705\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 0.773850\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.868777\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.655805\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.998822\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 0.952566\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 0.940729\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.755434\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.888187\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 1.090688\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 0.856965\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.945953\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 1.001389\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.804123\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 1.022636\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 0.897049\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.797875\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 0.929317\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.895737\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.848146\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.813978\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.998333\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 1.152525\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 1.002470\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.897652\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.872330\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 1.105097\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.768637\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.971959\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.740304\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 1.215901\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 0.816787\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.794058\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.736573\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.947539\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.919639\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.737833\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.998003\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.905599\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.691140\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.722222\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.794868\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.891646\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.744503\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.805015\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.968619\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 1.095951\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.963745\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.847475\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 0.905012\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.678409\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.948518\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.757123\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 1.094398\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.951455\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.826316\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.776721\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.936205\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 1.070329\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.779294\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 0.720643\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.911261\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.822201\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.369484\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.668423\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.058859\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 1.069223\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 1.217711\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.096046\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.220012\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 1.112867\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 1.023245\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 1.156740\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 1.497358\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 1.140050\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.070165\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.356737\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.263861\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 1.286566\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 1.091789\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 1.169264\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 1.169298\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 1.002841\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.594185\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 1.243491\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.405568\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 0.980148\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 1.028556\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.024171\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 1.409736\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 1.083489\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 1.071734\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 1.268674\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 1.100241\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 1.095700\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 1.274441\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 0.910094\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.325662\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 1.051124\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 1.187661\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.847158\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 0.943977\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 0.886511\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 1.087053\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 1.026948\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 1.130492\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 1.171066\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 1.057183\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 1.325687\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 1.107750\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 1.188656\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 1.133461\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 1.070517\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 1.101780\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 1.083711\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 0.896682\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 1.195542\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 1.157763\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 1.146281\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 1.192064\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 0.779845\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 1.242473\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 0.998615\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 1.108771\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 0.966553\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 1.271914\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 1.250179\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 0.989099\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 1.218889\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 0.908986\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 1.076172\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 1.043458\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 0.904291\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 1.021296\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 1.500001\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 1.077818\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 1.059525\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 0.786170\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 0.771368\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 1.140301\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 1.049098\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 1.029354\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 1.311891\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 1.157972\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 0.872964\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 1.189809\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 0.949872\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 1.035907\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 1.025068\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 0.867823\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 0.897932\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 0.923508\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 0.987567\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 1.215276\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 0.855111\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 0.843384\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 1.155885\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 1.260614\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 0.968529\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 1.076210\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 1.041287\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 0.975585\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 0.973918\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 1.156389\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 1.000753\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.360897\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.311314\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.381384\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.294140\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.447176\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.419177\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.285334\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.730669\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.337647\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.157964\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.135549\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.174699\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.570338\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.285565\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.187733\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.372570\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.426468\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.381439\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.183802\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.529078\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.294927\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.170968\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.162332\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 1.499147\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.246021\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.348774\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.248716\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.309494\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.407897\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.440709\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.307173\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.273353\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.290673\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.314304\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.100603\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.086673\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.269531\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.007793\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.174560\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.272888\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.302535\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.240509\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.274828\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.197511\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 1.456529\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.250679\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.020572\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.086717\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.234494\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.434000\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 0.934081\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 0.946206\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.115033\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.487377\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.315470\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.286036\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.124847\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.037124\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.452240\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.190299\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.321276\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.333151\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.103738\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.222011\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.220463\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.040045\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.316199\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.009877\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 0.968490\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.249177\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.153689\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.031652\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.139564\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 1.362974\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.457956\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.330759\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 1.062673\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 0.892374\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.239261\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.240527\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.016745\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.101119\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.375151\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.243583\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.045738\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.417217\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.116814\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.207801\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.199490\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.335866\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.347608\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.479730\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 0.926663\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 0.926712\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 1.282276\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.152211\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.164262\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.231247\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.252316\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.301785\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.310883\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.233635\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.179269\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.165304\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.345496\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.117516\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 0.821100\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 0.985339\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.226683\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 1.089786\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.412940\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.124640\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 1.110703\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 1.178667\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.153498\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 1.076162\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.216366\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.352966\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 1.348669\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.379101\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.328405\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 1.185642\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.014596\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.283069\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.128229\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.306205\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.286749\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.479724\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.257311\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.415411\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.192533\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.112002\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 1.270177\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.199247\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 0.897913\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 1.199021\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/5548 (0%)]\tLoss: 1.490518\n",
      "Train Epoch: 1 [640/5548 (11%)]\tLoss: 1.256704\n",
      "Train Epoch: 1 [1280/5548 (23%)]\tLoss: 1.403521\n",
      "Train Epoch: 1 [1920/5548 (34%)]\tLoss: 1.422290\n",
      "Train Epoch: 1 [2560/5548 (46%)]\tLoss: 1.085574\n",
      "Train Epoch: 1 [3200/5548 (57%)]\tLoss: 1.138579\n",
      "Train Epoch: 1 [3840/5548 (69%)]\tLoss: 1.338098\n",
      "Train Epoch: 1 [4480/5548 (80%)]\tLoss: 1.090227\n",
      "Train Epoch: 1 [5120/5548 (92%)]\tLoss: 1.198430\n",
      "Train Epoch: 2 [0/5548 (0%)]\tLoss: 1.341300\n",
      "Train Epoch: 2 [640/5548 (11%)]\tLoss: 1.153587\n",
      "Train Epoch: 2 [1280/5548 (23%)]\tLoss: 1.181905\n",
      "Train Epoch: 2 [1920/5548 (34%)]\tLoss: 1.195512\n",
      "Train Epoch: 2 [2560/5548 (46%)]\tLoss: 1.285712\n",
      "Train Epoch: 2 [3200/5548 (57%)]\tLoss: 1.156469\n",
      "Train Epoch: 2 [3840/5548 (69%)]\tLoss: 0.970181\n",
      "Train Epoch: 2 [4480/5548 (80%)]\tLoss: 0.830150\n",
      "Train Epoch: 2 [5120/5548 (92%)]\tLoss: 1.036342\n",
      "Train Epoch: 3 [0/5548 (0%)]\tLoss: 1.068330\n",
      "Train Epoch: 3 [640/5548 (11%)]\tLoss: 1.126069\n",
      "Train Epoch: 3 [1280/5548 (23%)]\tLoss: 1.254268\n",
      "Train Epoch: 3 [1920/5548 (34%)]\tLoss: 1.204115\n",
      "Train Epoch: 3 [2560/5548 (46%)]\tLoss: 1.154467\n",
      "Train Epoch: 3 [3200/5548 (57%)]\tLoss: 1.037144\n",
      "Train Epoch: 3 [3840/5548 (69%)]\tLoss: 1.143757\n",
      "Train Epoch: 3 [4480/5548 (80%)]\tLoss: 1.150118\n",
      "Train Epoch: 3 [5120/5548 (92%)]\tLoss: 1.262529\n",
      "Train Epoch: 4 [0/5548 (0%)]\tLoss: 1.087531\n",
      "Train Epoch: 4 [640/5548 (11%)]\tLoss: 1.220024\n",
      "Train Epoch: 4 [1280/5548 (23%)]\tLoss: 1.217163\n",
      "Train Epoch: 4 [1920/5548 (34%)]\tLoss: 0.949046\n",
      "Train Epoch: 4 [2560/5548 (46%)]\tLoss: 0.869051\n",
      "Train Epoch: 4 [3200/5548 (57%)]\tLoss: 1.371750\n",
      "Train Epoch: 4 [3840/5548 (69%)]\tLoss: 1.273715\n",
      "Train Epoch: 4 [4480/5548 (80%)]\tLoss: 1.036862\n",
      "Train Epoch: 4 [5120/5548 (92%)]\tLoss: 1.230497\n",
      "Train Epoch: 5 [0/5548 (0%)]\tLoss: 1.108365\n",
      "Train Epoch: 5 [640/5548 (11%)]\tLoss: 1.156646\n",
      "Train Epoch: 5 [1280/5548 (23%)]\tLoss: 1.161126\n",
      "Train Epoch: 5 [1920/5548 (34%)]\tLoss: 1.054123\n",
      "Train Epoch: 5 [2560/5548 (46%)]\tLoss: 1.152836\n",
      "Train Epoch: 5 [3200/5548 (57%)]\tLoss: 0.991081\n",
      "Train Epoch: 5 [3840/5548 (69%)]\tLoss: 1.148969\n",
      "Train Epoch: 5 [4480/5548 (80%)]\tLoss: 1.193223\n",
      "Train Epoch: 5 [5120/5548 (92%)]\tLoss: 1.299911\n",
      "Train Epoch: 6 [0/5548 (0%)]\tLoss: 1.208003\n",
      "Train Epoch: 6 [640/5548 (11%)]\tLoss: 1.099147\n",
      "Train Epoch: 6 [1280/5548 (23%)]\tLoss: 0.958325\n",
      "Train Epoch: 6 [1920/5548 (34%)]\tLoss: 1.254534\n",
      "Train Epoch: 6 [2560/5548 (46%)]\tLoss: 1.146329\n",
      "Train Epoch: 6 [3200/5548 (57%)]\tLoss: 1.181327\n",
      "Train Epoch: 6 [3840/5548 (69%)]\tLoss: 0.928515\n",
      "Train Epoch: 6 [4480/5548 (80%)]\tLoss: 1.147864\n",
      "Train Epoch: 6 [5120/5548 (92%)]\tLoss: 1.128549\n",
      "Train Epoch: 7 [0/5548 (0%)]\tLoss: 1.172467\n",
      "Train Epoch: 7 [640/5548 (11%)]\tLoss: 1.141239\n",
      "Train Epoch: 7 [1280/5548 (23%)]\tLoss: 0.753209\n",
      "Train Epoch: 7 [1920/5548 (34%)]\tLoss: 1.188485\n",
      "Train Epoch: 7 [2560/5548 (46%)]\tLoss: 0.804986\n",
      "Train Epoch: 7 [3200/5548 (57%)]\tLoss: 1.184136\n",
      "Train Epoch: 7 [3840/5548 (69%)]\tLoss: 1.116231\n",
      "Train Epoch: 7 [4480/5548 (80%)]\tLoss: 1.048341\n",
      "Train Epoch: 7 [5120/5548 (92%)]\tLoss: 1.174343\n",
      "Train Epoch: 8 [0/5548 (0%)]\tLoss: 0.993360\n",
      "Train Epoch: 8 [640/5548 (11%)]\tLoss: 1.257108\n",
      "Train Epoch: 8 [1280/5548 (23%)]\tLoss: 1.014358\n",
      "Train Epoch: 8 [1920/5548 (34%)]\tLoss: 1.240564\n",
      "Train Epoch: 8 [2560/5548 (46%)]\tLoss: 1.157158\n",
      "Train Epoch: 8 [3200/5548 (57%)]\tLoss: 1.024880\n",
      "Train Epoch: 8 [3840/5548 (69%)]\tLoss: 1.156923\n",
      "Train Epoch: 8 [4480/5548 (80%)]\tLoss: 1.205651\n",
      "Train Epoch: 8 [5120/5548 (92%)]\tLoss: 1.065863\n",
      "Train Epoch: 9 [0/5548 (0%)]\tLoss: 1.056815\n",
      "Train Epoch: 9 [640/5548 (11%)]\tLoss: 1.130798\n",
      "Train Epoch: 9 [1280/5548 (23%)]\tLoss: 1.099495\n",
      "Train Epoch: 9 [1920/5548 (34%)]\tLoss: 1.015056\n",
      "Train Epoch: 9 [2560/5548 (46%)]\tLoss: 1.121340\n",
      "Train Epoch: 9 [3200/5548 (57%)]\tLoss: 0.920345\n",
      "Train Epoch: 9 [3840/5548 (69%)]\tLoss: 1.137437\n",
      "Train Epoch: 9 [4480/5548 (80%)]\tLoss: 0.874853\n",
      "Train Epoch: 9 [5120/5548 (92%)]\tLoss: 0.999200\n",
      "Train Epoch: 10 [0/5548 (0%)]\tLoss: 1.075805\n",
      "Train Epoch: 10 [640/5548 (11%)]\tLoss: 1.104288\n",
      "Train Epoch: 10 [1280/5548 (23%)]\tLoss: 1.439145\n",
      "Train Epoch: 10 [1920/5548 (34%)]\tLoss: 1.027951\n",
      "Train Epoch: 10 [2560/5548 (46%)]\tLoss: 1.154541\n",
      "Train Epoch: 10 [3200/5548 (57%)]\tLoss: 1.160918\n",
      "Train Epoch: 10 [3840/5548 (69%)]\tLoss: 1.023008\n",
      "Train Epoch: 10 [4480/5548 (80%)]\tLoss: 1.189478\n",
      "Train Epoch: 10 [5120/5548 (92%)]\tLoss: 1.087810\n",
      "Train Epoch: 11 [0/5548 (0%)]\tLoss: 1.041873\n",
      "Train Epoch: 11 [640/5548 (11%)]\tLoss: 1.137087\n",
      "Train Epoch: 11 [1280/5548 (23%)]\tLoss: 0.960002\n",
      "Train Epoch: 11 [1920/5548 (34%)]\tLoss: 1.029851\n",
      "Train Epoch: 11 [2560/5548 (46%)]\tLoss: 1.161232\n",
      "Train Epoch: 11 [3200/5548 (57%)]\tLoss: 1.084643\n",
      "Train Epoch: 11 [3840/5548 (69%)]\tLoss: 0.952978\n",
      "Train Epoch: 11 [4480/5548 (80%)]\tLoss: 1.284117\n",
      "Train Epoch: 11 [5120/5548 (92%)]\tLoss: 1.399625\n",
      "Train Epoch: 12 [0/5548 (0%)]\tLoss: 1.395482\n",
      "Train Epoch: 12 [640/5548 (11%)]\tLoss: 1.010161\n",
      "Train Epoch: 12 [1280/5548 (23%)]\tLoss: 0.943598\n",
      "Train Epoch: 12 [1920/5548 (34%)]\tLoss: 1.276727\n",
      "Train Epoch: 12 [2560/5548 (46%)]\tLoss: 1.051469\n",
      "Train Epoch: 12 [3200/5548 (57%)]\tLoss: 1.108623\n",
      "Train Epoch: 12 [3840/5548 (69%)]\tLoss: 1.036749\n",
      "Train Epoch: 12 [4480/5548 (80%)]\tLoss: 1.056960\n",
      "Train Epoch: 12 [5120/5548 (92%)]\tLoss: 0.996426\n",
      "Train Epoch: 13 [0/5548 (0%)]\tLoss: 0.816917\n",
      "Train Epoch: 13 [640/5548 (11%)]\tLoss: 0.928098\n",
      "Train Epoch: 13 [1280/5548 (23%)]\tLoss: 0.983616\n",
      "Train Epoch: 13 [1920/5548 (34%)]\tLoss: 1.278241\n",
      "Train Epoch: 13 [2560/5548 (46%)]\tLoss: 0.918152\n",
      "Train Epoch: 13 [3200/5548 (57%)]\tLoss: 0.906678\n",
      "Train Epoch: 13 [3840/5548 (69%)]\tLoss: 0.985053\n",
      "Train Epoch: 13 [4480/5548 (80%)]\tLoss: 1.090100\n",
      "Train Epoch: 13 [5120/5548 (92%)]\tLoss: 1.133417\n",
      "Train Epoch: 14 [0/5548 (0%)]\tLoss: 1.121580\n",
      "Train Epoch: 14 [640/5548 (11%)]\tLoss: 1.188732\n",
      "Train Epoch: 14 [1280/5548 (23%)]\tLoss: 1.122056\n",
      "Train Epoch: 14 [1920/5548 (34%)]\tLoss: 0.954788\n",
      "Train Epoch: 14 [2560/5548 (46%)]\tLoss: 0.903560\n",
      "Train Epoch: 14 [3200/5548 (57%)]\tLoss: 1.244823\n",
      "Train Epoch: 14 [3840/5548 (69%)]\tLoss: 1.269850\n",
      "Train Epoch: 14 [4480/5548 (80%)]\tLoss: 1.035546\n",
      "Train Epoch: 14 [5120/5548 (92%)]\tLoss: 1.226563\n",
      "Train Epoch: 15 [0/5548 (0%)]\tLoss: 0.916644\n",
      "Train Epoch: 15 [640/5548 (11%)]\tLoss: 0.808359\n",
      "Train Epoch: 15 [1280/5548 (23%)]\tLoss: 1.175951\n",
      "Train Epoch: 15 [1920/5548 (34%)]\tLoss: 1.343775\n",
      "Train Epoch: 15 [2560/5548 (46%)]\tLoss: 0.989774\n",
      "Train Epoch: 15 [3200/5548 (57%)]\tLoss: 1.081567\n",
      "Train Epoch: 15 [3840/5548 (69%)]\tLoss: 1.090746\n",
      "Train Epoch: 15 [4480/5548 (80%)]\tLoss: 1.170979\n",
      "Train Epoch: 15 [5120/5548 (92%)]\tLoss: 1.064501\n",
      "Train Epoch: 16 [0/5548 (0%)]\tLoss: 1.193857\n",
      "Train Epoch: 16 [640/5548 (11%)]\tLoss: 1.066014\n",
      "Train Epoch: 16 [1280/5548 (23%)]\tLoss: 1.166911\n",
      "Train Epoch: 16 [1920/5548 (34%)]\tLoss: 1.033383\n",
      "Train Epoch: 16 [2560/5548 (46%)]\tLoss: 0.964203\n",
      "Train Epoch: 16 [3200/5548 (57%)]\tLoss: 1.018715\n",
      "Train Epoch: 16 [3840/5548 (69%)]\tLoss: 0.981410\n",
      "Train Epoch: 16 [4480/5548 (80%)]\tLoss: 0.950142\n",
      "Train Epoch: 16 [5120/5548 (92%)]\tLoss: 1.119482\n",
      "Train Epoch: 17 [0/5548 (0%)]\tLoss: 1.264469\n",
      "Train Epoch: 17 [640/5548 (11%)]\tLoss: 0.933593\n",
      "Train Epoch: 17 [1280/5548 (23%)]\tLoss: 1.114271\n",
      "Train Epoch: 17 [1920/5548 (34%)]\tLoss: 1.143596\n",
      "Train Epoch: 17 [2560/5548 (46%)]\tLoss: 1.168560\n",
      "Train Epoch: 17 [3200/5548 (57%)]\tLoss: 1.045515\n",
      "Train Epoch: 17 [3840/5548 (69%)]\tLoss: 0.926298\n",
      "Train Epoch: 17 [4480/5548 (80%)]\tLoss: 1.163996\n",
      "Train Epoch: 17 [5120/5548 (92%)]\tLoss: 1.060622\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/4097 (0%)]\tLoss: 1.414646\n",
      "Train Epoch: 1 [640/4097 (15%)]\tLoss: 1.262729\n",
      "Train Epoch: 1 [1280/4097 (31%)]\tLoss: 1.759959\n",
      "Train Epoch: 1 [1920/4097 (46%)]\tLoss: 1.296360\n",
      "Train Epoch: 1 [2560/4097 (62%)]\tLoss: 1.143714\n",
      "Train Epoch: 1 [3200/4097 (77%)]\tLoss: 1.541149\n",
      "Train Epoch: 1 [3840/4097 (92%)]\tLoss: 1.441968\n",
      "Train Epoch: 2 [0/4097 (0%)]\tLoss: 1.545929\n",
      "Train Epoch: 2 [640/4097 (15%)]\tLoss: 1.280799\n",
      "Train Epoch: 2 [1280/4097 (31%)]\tLoss: 1.442442\n",
      "Train Epoch: 2 [1920/4097 (46%)]\tLoss: 1.619123\n",
      "Train Epoch: 2 [2560/4097 (62%)]\tLoss: 1.520423\n",
      "Train Epoch: 2 [3200/4097 (77%)]\tLoss: 1.358835\n",
      "Train Epoch: 2 [3840/4097 (92%)]\tLoss: 1.264820\n",
      "Train Epoch: 3 [0/4097 (0%)]\tLoss: 1.521480\n",
      "Train Epoch: 3 [640/4097 (15%)]\tLoss: 1.366470\n",
      "Train Epoch: 3 [1280/4097 (31%)]\tLoss: 1.266021\n",
      "Train Epoch: 3 [1920/4097 (46%)]\tLoss: 1.471996\n",
      "Train Epoch: 3 [2560/4097 (62%)]\tLoss: 1.471937\n",
      "Train Epoch: 3 [3200/4097 (77%)]\tLoss: 1.205274\n",
      "Train Epoch: 3 [3840/4097 (92%)]\tLoss: 1.199005\n",
      "Train Epoch: 4 [0/4097 (0%)]\tLoss: 1.453470\n",
      "Train Epoch: 4 [640/4097 (15%)]\tLoss: 1.113608\n",
      "Train Epoch: 4 [1280/4097 (31%)]\tLoss: 1.314905\n",
      "Train Epoch: 4 [1920/4097 (46%)]\tLoss: 1.239282\n",
      "Train Epoch: 4 [2560/4097 (62%)]\tLoss: 1.269133\n",
      "Train Epoch: 4 [3200/4097 (77%)]\tLoss: 1.233655\n",
      "Train Epoch: 4 [3840/4097 (92%)]\tLoss: 1.442345\n",
      "Train Epoch: 5 [0/4097 (0%)]\tLoss: 1.329288\n",
      "Train Epoch: 5 [640/4097 (15%)]\tLoss: 1.115600\n",
      "Train Epoch: 5 [1280/4097 (31%)]\tLoss: 1.193874\n",
      "Train Epoch: 5 [1920/4097 (46%)]\tLoss: 1.489191\n",
      "Train Epoch: 5 [2560/4097 (62%)]\tLoss: 1.173541\n",
      "Train Epoch: 5 [3200/4097 (77%)]\tLoss: 1.435804\n",
      "Train Epoch: 5 [3840/4097 (92%)]\tLoss: 1.303082\n",
      "Train Epoch: 6 [0/4097 (0%)]\tLoss: 1.554620\n",
      "Train Epoch: 6 [640/4097 (15%)]\tLoss: 1.400324\n",
      "Train Epoch: 6 [1280/4097 (31%)]\tLoss: 1.230977\n",
      "Train Epoch: 6 [1920/4097 (46%)]\tLoss: 1.008092\n",
      "Train Epoch: 6 [2560/4097 (62%)]\tLoss: 1.453853\n",
      "Train Epoch: 6 [3200/4097 (77%)]\tLoss: 1.480646\n",
      "Train Epoch: 6 [3840/4097 (92%)]\tLoss: 1.255249\n",
      "Train Epoch: 7 [0/4097 (0%)]\tLoss: 1.651051\n",
      "Train Epoch: 7 [640/4097 (15%)]\tLoss: 1.389767\n",
      "Train Epoch: 7 [1280/4097 (31%)]\tLoss: 1.252918\n",
      "Train Epoch: 7 [1920/4097 (46%)]\tLoss: 1.187029\n",
      "Train Epoch: 7 [2560/4097 (62%)]\tLoss: 1.631979\n",
      "Train Epoch: 7 [3200/4097 (77%)]\tLoss: 1.295895\n",
      "Train Epoch: 7 [3840/4097 (92%)]\tLoss: 1.545179\n",
      "Train Epoch: 8 [0/4097 (0%)]\tLoss: 1.337283\n",
      "Train Epoch: 8 [640/4097 (15%)]\tLoss: 1.338683\n",
      "Train Epoch: 8 [1280/4097 (31%)]\tLoss: 1.382296\n",
      "Train Epoch: 8 [1920/4097 (46%)]\tLoss: 1.280597\n",
      "Train Epoch: 8 [2560/4097 (62%)]\tLoss: 1.235649\n",
      "Train Epoch: 8 [3200/4097 (77%)]\tLoss: 1.209556\n",
      "Train Epoch: 8 [3840/4097 (92%)]\tLoss: 1.081459\n",
      "Train Epoch: 9 [0/4097 (0%)]\tLoss: 1.295009\n",
      "Train Epoch: 9 [640/4097 (15%)]\tLoss: 1.463553\n",
      "Train Epoch: 9 [1280/4097 (31%)]\tLoss: 1.205709\n",
      "Train Epoch: 9 [1920/4097 (46%)]\tLoss: 1.189686\n",
      "Train Epoch: 9 [2560/4097 (62%)]\tLoss: 1.306913\n",
      "Train Epoch: 9 [3200/4097 (77%)]\tLoss: 0.896477\n",
      "Train Epoch: 9 [3840/4097 (92%)]\tLoss: 1.220627\n",
      "Train Epoch: 10 [0/4097 (0%)]\tLoss: 1.119953\n",
      "Train Epoch: 10 [640/4097 (15%)]\tLoss: 1.149899\n",
      "Train Epoch: 10 [1280/4097 (31%)]\tLoss: 1.263910\n",
      "Train Epoch: 10 [1920/4097 (46%)]\tLoss: 1.267726\n",
      "Train Epoch: 10 [2560/4097 (62%)]\tLoss: 1.171593\n",
      "Train Epoch: 10 [3200/4097 (77%)]\tLoss: 1.037615\n",
      "Train Epoch: 10 [3840/4097 (92%)]\tLoss: 1.067115\n",
      "Train Epoch: 11 [0/4097 (0%)]\tLoss: 1.218663\n",
      "Train Epoch: 11 [640/4097 (15%)]\tLoss: 1.303332\n",
      "Train Epoch: 11 [1280/4097 (31%)]\tLoss: 1.196407\n",
      "Train Epoch: 11 [1920/4097 (46%)]\tLoss: 1.228317\n",
      "Train Epoch: 11 [2560/4097 (62%)]\tLoss: 1.522860\n",
      "Train Epoch: 11 [3200/4097 (77%)]\tLoss: 1.256724\n",
      "Train Epoch: 11 [3840/4097 (92%)]\tLoss: 1.128329\n",
      "Train Epoch: 12 [0/4097 (0%)]\tLoss: 1.186848\n",
      "Train Epoch: 12 [640/4097 (15%)]\tLoss: 1.251173\n",
      "Train Epoch: 12 [1280/4097 (31%)]\tLoss: 1.307164\n",
      "Train Epoch: 12 [1920/4097 (46%)]\tLoss: 1.005121\n",
      "Train Epoch: 12 [2560/4097 (62%)]\tLoss: 1.292662\n",
      "Train Epoch: 12 [3200/4097 (77%)]\tLoss: 1.484281\n",
      "Train Epoch: 12 [3840/4097 (92%)]\tLoss: 1.272052\n",
      "Train Epoch: 13 [0/4097 (0%)]\tLoss: 1.500344\n",
      "Train Epoch: 13 [640/4097 (15%)]\tLoss: 1.217215\n",
      "Train Epoch: 13 [1280/4097 (31%)]\tLoss: 1.382974\n",
      "Train Epoch: 13 [1920/4097 (46%)]\tLoss: 1.079717\n",
      "Train Epoch: 13 [2560/4097 (62%)]\tLoss: 1.195848\n",
      "Train Epoch: 13 [3200/4097 (77%)]\tLoss: 1.578290\n",
      "Train Epoch: 13 [3840/4097 (92%)]\tLoss: 1.224669\n",
      "Train Epoch: 14 [0/4097 (0%)]\tLoss: 1.305099\n",
      "Train Epoch: 14 [640/4097 (15%)]\tLoss: 1.327432\n",
      "Train Epoch: 14 [1280/4097 (31%)]\tLoss: 1.169219\n",
      "Train Epoch: 14 [1920/4097 (46%)]\tLoss: 1.199254\n",
      "Train Epoch: 14 [2560/4097 (62%)]\tLoss: 1.206128\n",
      "Train Epoch: 14 [3200/4097 (77%)]\tLoss: 1.072314\n",
      "Train Epoch: 14 [3840/4097 (92%)]\tLoss: 1.250709\n",
      "Train Epoch: 15 [0/4097 (0%)]\tLoss: 1.440342\n",
      "Train Epoch: 15 [640/4097 (15%)]\tLoss: 1.164924\n",
      "Train Epoch: 15 [1280/4097 (31%)]\tLoss: 1.123003\n",
      "Train Epoch: 15 [1920/4097 (46%)]\tLoss: 1.190906\n",
      "Train Epoch: 15 [2560/4097 (62%)]\tLoss: 1.514426\n",
      "Train Epoch: 15 [3200/4097 (77%)]\tLoss: 1.300326\n",
      "Train Epoch: 15 [3840/4097 (92%)]\tLoss: 1.297495\n",
      "Train Epoch: 16 [0/4097 (0%)]\tLoss: 1.271014\n",
      "Train Epoch: 16 [640/4097 (15%)]\tLoss: 1.356660\n",
      "Train Epoch: 16 [1280/4097 (31%)]\tLoss: 1.282579\n",
      "Train Epoch: 16 [1920/4097 (46%)]\tLoss: 1.230239\n",
      "Train Epoch: 16 [2560/4097 (62%)]\tLoss: 1.207704\n",
      "Train Epoch: 16 [3200/4097 (77%)]\tLoss: 0.987470\n",
      "Train Epoch: 16 [3840/4097 (92%)]\tLoss: 1.082725\n",
      "Train Epoch: 17 [0/4097 (0%)]\tLoss: 1.273980\n",
      "Train Epoch: 17 [640/4097 (15%)]\tLoss: 1.356504\n",
      "Train Epoch: 17 [1280/4097 (31%)]\tLoss: 1.419178\n",
      "Train Epoch: 17 [1920/4097 (46%)]\tLoss: 1.023643\n",
      "Train Epoch: 17 [2560/4097 (62%)]\tLoss: 1.423987\n",
      "Train Epoch: 17 [3200/4097 (77%)]\tLoss: 1.232505\n",
      "Train Epoch: 17 [3840/4097 (92%)]\tLoss: 1.295834\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/8272 (0%)]\tLoss: 1.646791\n",
      "Train Epoch: 1 [640/8272 (8%)]\tLoss: 1.292565\n",
      "Train Epoch: 1 [1280/8272 (15%)]\tLoss: 1.234273\n",
      "Train Epoch: 1 [1920/8272 (23%)]\tLoss: 1.280503\n",
      "Train Epoch: 1 [2560/8272 (31%)]\tLoss: 1.466814\n",
      "Train Epoch: 1 [3200/8272 (38%)]\tLoss: 1.485713\n",
      "Train Epoch: 1 [3840/8272 (46%)]\tLoss: 1.343931\n",
      "Train Epoch: 1 [4480/8272 (54%)]\tLoss: 1.675975\n",
      "Train Epoch: 1 [5120/8272 (62%)]\tLoss: 1.569547\n",
      "Train Epoch: 1 [5760/8272 (69%)]\tLoss: 1.438154\n",
      "Train Epoch: 1 [6400/8272 (77%)]\tLoss: 1.564811\n",
      "Train Epoch: 1 [7040/8272 (85%)]\tLoss: 1.483362\n",
      "Train Epoch: 1 [7680/8272 (92%)]\tLoss: 1.686848\n",
      "Train Epoch: 2 [0/8272 (0%)]\tLoss: 1.589498\n",
      "Train Epoch: 2 [640/8272 (8%)]\tLoss: 1.253537\n",
      "Train Epoch: 2 [1280/8272 (15%)]\tLoss: 1.604419\n",
      "Train Epoch: 2 [1920/8272 (23%)]\tLoss: 1.652569\n",
      "Train Epoch: 2 [2560/8272 (31%)]\tLoss: 1.422320\n",
      "Train Epoch: 2 [3200/8272 (38%)]\tLoss: 1.216501\n",
      "Train Epoch: 2 [3840/8272 (46%)]\tLoss: 1.326480\n",
      "Train Epoch: 2 [4480/8272 (54%)]\tLoss: 1.348745\n",
      "Train Epoch: 2 [5120/8272 (62%)]\tLoss: 1.353723\n",
      "Train Epoch: 2 [5760/8272 (69%)]\tLoss: 1.511617\n",
      "Train Epoch: 2 [6400/8272 (77%)]\tLoss: 1.457109\n",
      "Train Epoch: 2 [7040/8272 (85%)]\tLoss: 1.231273\n",
      "Train Epoch: 2 [7680/8272 (92%)]\tLoss: 1.370197\n",
      "Train Epoch: 3 [0/8272 (0%)]\tLoss: 1.290093\n",
      "Train Epoch: 3 [640/8272 (8%)]\tLoss: 1.303404\n",
      "Train Epoch: 3 [1280/8272 (15%)]\tLoss: 1.355462\n",
      "Train Epoch: 3 [1920/8272 (23%)]\tLoss: 1.408928\n",
      "Train Epoch: 3 [2560/8272 (31%)]\tLoss: 1.251383\n",
      "Train Epoch: 3 [3200/8272 (38%)]\tLoss: 1.872732\n",
      "Train Epoch: 3 [3840/8272 (46%)]\tLoss: 1.372228\n",
      "Train Epoch: 3 [4480/8272 (54%)]\tLoss: 1.135638\n",
      "Train Epoch: 3 [5120/8272 (62%)]\tLoss: 1.176594\n",
      "Train Epoch: 3 [5760/8272 (69%)]\tLoss: 1.425515\n",
      "Train Epoch: 3 [6400/8272 (77%)]\tLoss: 1.302330\n",
      "Train Epoch: 3 [7040/8272 (85%)]\tLoss: 1.303969\n",
      "Train Epoch: 3 [7680/8272 (92%)]\tLoss: 1.428835\n",
      "Train Epoch: 4 [0/8272 (0%)]\tLoss: 1.474787\n",
      "Train Epoch: 4 [640/8272 (8%)]\tLoss: 1.417291\n",
      "Train Epoch: 4 [1280/8272 (15%)]\tLoss: 1.535372\n",
      "Train Epoch: 4 [1920/8272 (23%)]\tLoss: 1.600533\n",
      "Train Epoch: 4 [2560/8272 (31%)]\tLoss: 1.536039\n",
      "Train Epoch: 4 [3200/8272 (38%)]\tLoss: 1.277975\n",
      "Train Epoch: 4 [3840/8272 (46%)]\tLoss: 1.299822\n",
      "Train Epoch: 4 [4480/8272 (54%)]\tLoss: 1.250371\n",
      "Train Epoch: 4 [5120/8272 (62%)]\tLoss: 1.332813\n",
      "Train Epoch: 4 [5760/8272 (69%)]\tLoss: 1.725757\n",
      "Train Epoch: 4 [6400/8272 (77%)]\tLoss: 1.433168\n",
      "Train Epoch: 4 [7040/8272 (85%)]\tLoss: 1.337474\n",
      "Train Epoch: 4 [7680/8272 (92%)]\tLoss: 1.647609\n",
      "Train Epoch: 5 [0/8272 (0%)]\tLoss: 1.447507\n",
      "Train Epoch: 5 [640/8272 (8%)]\tLoss: 1.499396\n",
      "Train Epoch: 5 [1280/8272 (15%)]\tLoss: 1.480256\n",
      "Train Epoch: 5 [1920/8272 (23%)]\tLoss: 1.348337\n",
      "Train Epoch: 5 [2560/8272 (31%)]\tLoss: 1.278539\n",
      "Train Epoch: 5 [3200/8272 (38%)]\tLoss: 1.329416\n",
      "Train Epoch: 5 [3840/8272 (46%)]\tLoss: 1.278681\n",
      "Train Epoch: 5 [4480/8272 (54%)]\tLoss: 1.377041\n",
      "Train Epoch: 5 [5120/8272 (62%)]\tLoss: 1.537695\n",
      "Train Epoch: 5 [5760/8272 (69%)]\tLoss: 1.414723\n",
      "Train Epoch: 5 [6400/8272 (77%)]\tLoss: 1.219907\n",
      "Train Epoch: 5 [7040/8272 (85%)]\tLoss: 1.363069\n",
      "Train Epoch: 5 [7680/8272 (92%)]\tLoss: 1.390063\n",
      "Train Epoch: 6 [0/8272 (0%)]\tLoss: 1.254993\n",
      "Train Epoch: 6 [640/8272 (8%)]\tLoss: 1.308331\n",
      "Train Epoch: 6 [1280/8272 (15%)]\tLoss: 1.383758\n",
      "Train Epoch: 6 [1920/8272 (23%)]\tLoss: 1.562846\n",
      "Train Epoch: 6 [2560/8272 (31%)]\tLoss: 1.354002\n",
      "Train Epoch: 6 [3200/8272 (38%)]\tLoss: 1.339508\n",
      "Train Epoch: 6 [3840/8272 (46%)]\tLoss: 1.448412\n",
      "Train Epoch: 6 [4480/8272 (54%)]\tLoss: 1.273124\n",
      "Train Epoch: 6 [5120/8272 (62%)]\tLoss: 1.139270\n",
      "Train Epoch: 6 [5760/8272 (69%)]\tLoss: 1.323600\n",
      "Train Epoch: 6 [6400/8272 (77%)]\tLoss: 1.283905\n",
      "Train Epoch: 6 [7040/8272 (85%)]\tLoss: 1.420615\n",
      "Train Epoch: 6 [7680/8272 (92%)]\tLoss: 1.362102\n",
      "Train Epoch: 7 [0/8272 (0%)]\tLoss: 1.522668\n",
      "Train Epoch: 7 [640/8272 (8%)]\tLoss: 1.331403\n",
      "Train Epoch: 7 [1280/8272 (15%)]\tLoss: 1.085936\n",
      "Train Epoch: 7 [1920/8272 (23%)]\tLoss: 1.346366\n",
      "Train Epoch: 7 [2560/8272 (31%)]\tLoss: 1.444715\n",
      "Train Epoch: 7 [3200/8272 (38%)]\tLoss: 1.561332\n",
      "Train Epoch: 7 [3840/8272 (46%)]\tLoss: 1.284558\n",
      "Train Epoch: 7 [4480/8272 (54%)]\tLoss: 1.366144\n",
      "Train Epoch: 7 [5120/8272 (62%)]\tLoss: 1.324489\n",
      "Train Epoch: 7 [5760/8272 (69%)]\tLoss: 1.220664\n",
      "Train Epoch: 7 [6400/8272 (77%)]\tLoss: 1.214033\n",
      "Train Epoch: 7 [7040/8272 (85%)]\tLoss: 1.614599\n",
      "Train Epoch: 7 [7680/8272 (92%)]\tLoss: 1.457189\n",
      "Train Epoch: 8 [0/8272 (0%)]\tLoss: 1.375865\n",
      "Train Epoch: 8 [640/8272 (8%)]\tLoss: 1.151134\n",
      "Train Epoch: 8 [1280/8272 (15%)]\tLoss: 1.430880\n",
      "Train Epoch: 8 [1920/8272 (23%)]\tLoss: 1.438585\n",
      "Train Epoch: 8 [2560/8272 (31%)]\tLoss: 1.414650\n",
      "Train Epoch: 8 [3200/8272 (38%)]\tLoss: 1.220810\n",
      "Train Epoch: 8 [3840/8272 (46%)]\tLoss: 1.387759\n",
      "Train Epoch: 8 [4480/8272 (54%)]\tLoss: 1.445698\n",
      "Train Epoch: 8 [5120/8272 (62%)]\tLoss: 1.393102\n",
      "Train Epoch: 8 [5760/8272 (69%)]\tLoss: 1.213492\n",
      "Train Epoch: 8 [6400/8272 (77%)]\tLoss: 1.335572\n",
      "Train Epoch: 8 [7040/8272 (85%)]\tLoss: 1.471616\n",
      "Train Epoch: 8 [7680/8272 (92%)]\tLoss: 1.465849\n",
      "Train Epoch: 9 [0/8272 (0%)]\tLoss: 1.208313\n",
      "Train Epoch: 9 [640/8272 (8%)]\tLoss: 1.318609\n",
      "Train Epoch: 9 [1280/8272 (15%)]\tLoss: 1.169168\n",
      "Train Epoch: 9 [1920/8272 (23%)]\tLoss: 1.241362\n",
      "Train Epoch: 9 [2560/8272 (31%)]\tLoss: 1.131322\n",
      "Train Epoch: 9 [3200/8272 (38%)]\tLoss: 1.513235\n",
      "Train Epoch: 9 [3840/8272 (46%)]\tLoss: 1.331346\n",
      "Train Epoch: 9 [4480/8272 (54%)]\tLoss: 1.228508\n",
      "Train Epoch: 9 [5120/8272 (62%)]\tLoss: 1.153373\n",
      "Train Epoch: 9 [5760/8272 (69%)]\tLoss: 1.155030\n",
      "Train Epoch: 9 [6400/8272 (77%)]\tLoss: 1.232831\n",
      "Train Epoch: 9 [7040/8272 (85%)]\tLoss: 1.338582\n",
      "Train Epoch: 9 [7680/8272 (92%)]\tLoss: 1.547151\n",
      "Train Epoch: 10 [0/8272 (0%)]\tLoss: 1.384010\n",
      "Train Epoch: 10 [640/8272 (8%)]\tLoss: 1.283309\n",
      "Train Epoch: 10 [1280/8272 (15%)]\tLoss: 1.457942\n",
      "Train Epoch: 10 [1920/8272 (23%)]\tLoss: 1.577367\n",
      "Train Epoch: 10 [2560/8272 (31%)]\tLoss: 1.204579\n",
      "Train Epoch: 10 [3200/8272 (38%)]\tLoss: 1.389774\n",
      "Train Epoch: 10 [3840/8272 (46%)]\tLoss: 1.187095\n",
      "Train Epoch: 10 [4480/8272 (54%)]\tLoss: 1.428065\n",
      "Train Epoch: 10 [5120/8272 (62%)]\tLoss: 1.422618\n",
      "Train Epoch: 10 [5760/8272 (69%)]\tLoss: 1.387459\n",
      "Train Epoch: 10 [6400/8272 (77%)]\tLoss: 1.364489\n",
      "Train Epoch: 10 [7040/8272 (85%)]\tLoss: 1.403438\n",
      "Train Epoch: 10 [7680/8272 (92%)]\tLoss: 1.422880\n",
      "Train Epoch: 11 [0/8272 (0%)]\tLoss: 1.285244\n",
      "Train Epoch: 11 [640/8272 (8%)]\tLoss: 1.730361\n",
      "Train Epoch: 11 [1280/8272 (15%)]\tLoss: 1.194137\n",
      "Train Epoch: 11 [1920/8272 (23%)]\tLoss: 1.271357\n",
      "Train Epoch: 11 [2560/8272 (31%)]\tLoss: 1.360284\n",
      "Train Epoch: 11 [3200/8272 (38%)]\tLoss: 1.611784\n",
      "Train Epoch: 11 [3840/8272 (46%)]\tLoss: 1.467640\n",
      "Train Epoch: 11 [4480/8272 (54%)]\tLoss: 1.331504\n",
      "Train Epoch: 11 [5120/8272 (62%)]\tLoss: 1.248885\n",
      "Train Epoch: 11 [5760/8272 (69%)]\tLoss: 1.314778\n",
      "Train Epoch: 11 [6400/8272 (77%)]\tLoss: 1.243485\n",
      "Train Epoch: 11 [7040/8272 (85%)]\tLoss: 1.665996\n",
      "Train Epoch: 11 [7680/8272 (92%)]\tLoss: 1.408104\n",
      "Train Epoch: 12 [0/8272 (0%)]\tLoss: 1.258934\n",
      "Train Epoch: 12 [640/8272 (8%)]\tLoss: 1.345648\n",
      "Train Epoch: 12 [1280/8272 (15%)]\tLoss: 1.491472\n",
      "Train Epoch: 12 [1920/8272 (23%)]\tLoss: 1.087748\n",
      "Train Epoch: 12 [2560/8272 (31%)]\tLoss: 1.371087\n",
      "Train Epoch: 12 [3200/8272 (38%)]\tLoss: 1.383947\n",
      "Train Epoch: 12 [3840/8272 (46%)]\tLoss: 1.146886\n",
      "Train Epoch: 12 [4480/8272 (54%)]\tLoss: 1.420738\n",
      "Train Epoch: 12 [5120/8272 (62%)]\tLoss: 1.287424\n",
      "Train Epoch: 12 [5760/8272 (69%)]\tLoss: 1.197740\n",
      "Train Epoch: 12 [6400/8272 (77%)]\tLoss: 1.342110\n",
      "Train Epoch: 12 [7040/8272 (85%)]\tLoss: 1.148636\n",
      "Train Epoch: 12 [7680/8272 (92%)]\tLoss: 1.302793\n",
      "Train Epoch: 13 [0/8272 (0%)]\tLoss: 1.356600\n",
      "Train Epoch: 13 [640/8272 (8%)]\tLoss: 1.137786\n",
      "Train Epoch: 13 [1280/8272 (15%)]\tLoss: 1.297616\n",
      "Train Epoch: 13 [1920/8272 (23%)]\tLoss: 1.458147\n",
      "Train Epoch: 13 [2560/8272 (31%)]\tLoss: 1.348262\n",
      "Train Epoch: 13 [3200/8272 (38%)]\tLoss: 1.172570\n",
      "Train Epoch: 13 [3840/8272 (46%)]\tLoss: 1.183129\n",
      "Train Epoch: 13 [4480/8272 (54%)]\tLoss: 1.047571\n",
      "Train Epoch: 13 [5120/8272 (62%)]\tLoss: 1.390744\n",
      "Train Epoch: 13 [5760/8272 (69%)]\tLoss: 1.312152\n",
      "Train Epoch: 13 [6400/8272 (77%)]\tLoss: 1.365162\n",
      "Train Epoch: 13 [7040/8272 (85%)]\tLoss: 1.475454\n",
      "Train Epoch: 13 [7680/8272 (92%)]\tLoss: 1.256569\n",
      "Train Epoch: 14 [0/8272 (0%)]\tLoss: 1.014677\n",
      "Train Epoch: 14 [640/8272 (8%)]\tLoss: 1.230152\n",
      "Train Epoch: 14 [1280/8272 (15%)]\tLoss: 1.669781\n",
      "Train Epoch: 14 [1920/8272 (23%)]\tLoss: 1.054024\n",
      "Train Epoch: 14 [2560/8272 (31%)]\tLoss: 1.281278\n",
      "Train Epoch: 14 [3200/8272 (38%)]\tLoss: 1.230579\n",
      "Train Epoch: 14 [3840/8272 (46%)]\tLoss: 1.491650\n",
      "Train Epoch: 14 [4480/8272 (54%)]\tLoss: 1.166223\n",
      "Train Epoch: 14 [5120/8272 (62%)]\tLoss: 1.224664\n",
      "Train Epoch: 14 [5760/8272 (69%)]\tLoss: 1.526622\n",
      "Train Epoch: 14 [6400/8272 (77%)]\tLoss: 1.479133\n",
      "Train Epoch: 14 [7040/8272 (85%)]\tLoss: 1.153605\n",
      "Train Epoch: 14 [7680/8272 (92%)]\tLoss: 1.296957\n",
      "Train Epoch: 15 [0/8272 (0%)]\tLoss: 1.457162\n",
      "Train Epoch: 15 [640/8272 (8%)]\tLoss: 1.306130\n",
      "Train Epoch: 15 [1280/8272 (15%)]\tLoss: 1.226693\n",
      "Train Epoch: 15 [1920/8272 (23%)]\tLoss: 1.462555\n",
      "Train Epoch: 15 [2560/8272 (31%)]\tLoss: 1.021550\n",
      "Train Epoch: 15 [3200/8272 (38%)]\tLoss: 1.282848\n",
      "Train Epoch: 15 [3840/8272 (46%)]\tLoss: 1.143044\n",
      "Train Epoch: 15 [4480/8272 (54%)]\tLoss: 1.286161\n",
      "Train Epoch: 15 [5120/8272 (62%)]\tLoss: 1.020975\n",
      "Train Epoch: 15 [5760/8272 (69%)]\tLoss: 1.053418\n",
      "Train Epoch: 15 [6400/8272 (77%)]\tLoss: 1.227777\n",
      "Train Epoch: 15 [7040/8272 (85%)]\tLoss: 1.231629\n",
      "Train Epoch: 15 [7680/8272 (92%)]\tLoss: 1.278811\n",
      "Train Epoch: 16 [0/8272 (0%)]\tLoss: 1.326059\n",
      "Train Epoch: 16 [640/8272 (8%)]\tLoss: 1.304561\n",
      "Train Epoch: 16 [1280/8272 (15%)]\tLoss: 1.188901\n",
      "Train Epoch: 16 [1920/8272 (23%)]\tLoss: 1.309489\n",
      "Train Epoch: 16 [2560/8272 (31%)]\tLoss: 1.256554\n",
      "Train Epoch: 16 [3200/8272 (38%)]\tLoss: 1.431668\n",
      "Train Epoch: 16 [3840/8272 (46%)]\tLoss: 1.150634\n",
      "Train Epoch: 16 [4480/8272 (54%)]\tLoss: 1.143454\n",
      "Train Epoch: 16 [5120/8272 (62%)]\tLoss: 1.423190\n",
      "Train Epoch: 16 [5760/8272 (69%)]\tLoss: 1.249544\n",
      "Train Epoch: 16 [6400/8272 (77%)]\tLoss: 1.401095\n",
      "Train Epoch: 16 [7040/8272 (85%)]\tLoss: 1.275046\n",
      "Train Epoch: 16 [7680/8272 (92%)]\tLoss: 1.074456\n",
      "Train Epoch: 17 [0/8272 (0%)]\tLoss: 1.348072\n",
      "Train Epoch: 17 [640/8272 (8%)]\tLoss: 1.381350\n",
      "Train Epoch: 17 [1280/8272 (15%)]\tLoss: 1.271674\n",
      "Train Epoch: 17 [1920/8272 (23%)]\tLoss: 1.230525\n",
      "Train Epoch: 17 [2560/8272 (31%)]\tLoss: 1.501140\n",
      "Train Epoch: 17 [3200/8272 (38%)]\tLoss: 1.287888\n",
      "Train Epoch: 17 [3840/8272 (46%)]\tLoss: 1.104465\n",
      "Train Epoch: 17 [4480/8272 (54%)]\tLoss: 1.177357\n",
      "Train Epoch: 17 [5120/8272 (62%)]\tLoss: 1.294451\n",
      "Train Epoch: 17 [5760/8272 (69%)]\tLoss: 1.290187\n",
      "Train Epoch: 17 [6400/8272 (77%)]\tLoss: 1.475747\n",
      "Train Epoch: 17 [7040/8272 (85%)]\tLoss: 1.067944\n",
      "Train Epoch: 17 [7680/8272 (92%)]\tLoss: 1.127403\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/5100 (0%)]\tLoss: 1.219971\n",
      "Train Epoch: 1 [640/5100 (12%)]\tLoss: 1.382355\n",
      "Train Epoch: 1 [1280/5100 (25%)]\tLoss: 1.278157\n",
      "Train Epoch: 1 [1920/5100 (38%)]\tLoss: 1.346469\n",
      "Train Epoch: 1 [2560/5100 (50%)]\tLoss: 1.469033\n",
      "Train Epoch: 1 [3200/5100 (62%)]\tLoss: 1.630901\n",
      "Train Epoch: 1 [3840/5100 (75%)]\tLoss: 1.375161\n",
      "Train Epoch: 1 [4480/5100 (88%)]\tLoss: 1.274988\n",
      "Train Epoch: 2 [0/5100 (0%)]\tLoss: 1.204827\n",
      "Train Epoch: 2 [640/5100 (12%)]\tLoss: 1.349898\n",
      "Train Epoch: 2 [1280/5100 (25%)]\tLoss: 1.224779\n",
      "Train Epoch: 2 [1920/5100 (38%)]\tLoss: 0.924550\n",
      "Train Epoch: 2 [2560/5100 (50%)]\tLoss: 1.279243\n",
      "Train Epoch: 2 [3200/5100 (62%)]\tLoss: 1.154808\n",
      "Train Epoch: 2 [3840/5100 (75%)]\tLoss: 1.252377\n",
      "Train Epoch: 2 [4480/5100 (88%)]\tLoss: 1.245025\n",
      "Train Epoch: 3 [0/5100 (0%)]\tLoss: 1.355167\n",
      "Train Epoch: 3 [640/5100 (12%)]\tLoss: 1.353947\n",
      "Train Epoch: 3 [1280/5100 (25%)]\tLoss: 1.294188\n",
      "Train Epoch: 3 [1920/5100 (38%)]\tLoss: 1.193151\n",
      "Train Epoch: 3 [2560/5100 (50%)]\tLoss: 1.310486\n",
      "Train Epoch: 3 [3200/5100 (62%)]\tLoss: 1.275816\n",
      "Train Epoch: 3 [3840/5100 (75%)]\tLoss: 1.173441\n",
      "Train Epoch: 3 [4480/5100 (88%)]\tLoss: 1.409868\n",
      "Train Epoch: 4 [0/5100 (0%)]\tLoss: 1.462069\n",
      "Train Epoch: 4 [640/5100 (12%)]\tLoss: 1.342259\n",
      "Train Epoch: 4 [1280/5100 (25%)]\tLoss: 1.268555\n",
      "Train Epoch: 4 [1920/5100 (38%)]\tLoss: 1.284206\n",
      "Train Epoch: 4 [2560/5100 (50%)]\tLoss: 0.996927\n",
      "Train Epoch: 4 [3200/5100 (62%)]\tLoss: 1.305590\n",
      "Train Epoch: 4 [3840/5100 (75%)]\tLoss: 1.514255\n",
      "Train Epoch: 4 [4480/5100 (88%)]\tLoss: 1.317716\n",
      "Train Epoch: 5 [0/5100 (0%)]\tLoss: 1.131555\n",
      "Train Epoch: 5 [640/5100 (12%)]\tLoss: 1.142225\n",
      "Train Epoch: 5 [1280/5100 (25%)]\tLoss: 1.284548\n",
      "Train Epoch: 5 [1920/5100 (38%)]\tLoss: 1.265409\n",
      "Train Epoch: 5 [2560/5100 (50%)]\tLoss: 1.048206\n",
      "Train Epoch: 5 [3200/5100 (62%)]\tLoss: 1.040587\n",
      "Train Epoch: 5 [3840/5100 (75%)]\tLoss: 1.105280\n",
      "Train Epoch: 5 [4480/5100 (88%)]\tLoss: 1.157036\n",
      "Train Epoch: 6 [0/5100 (0%)]\tLoss: 1.429021\n",
      "Train Epoch: 6 [640/5100 (12%)]\tLoss: 1.336606\n",
      "Train Epoch: 6 [1280/5100 (25%)]\tLoss: 1.030241\n",
      "Train Epoch: 6 [1920/5100 (38%)]\tLoss: 1.353819\n",
      "Train Epoch: 6 [2560/5100 (50%)]\tLoss: 1.381936\n",
      "Train Epoch: 6 [3200/5100 (62%)]\tLoss: 1.253040\n",
      "Train Epoch: 6 [3840/5100 (75%)]\tLoss: 1.436108\n",
      "Train Epoch: 6 [4480/5100 (88%)]\tLoss: 1.155463\n",
      "Train Epoch: 7 [0/5100 (0%)]\tLoss: 1.235046\n",
      "Train Epoch: 7 [640/5100 (12%)]\tLoss: 1.260859\n",
      "Train Epoch: 7 [1280/5100 (25%)]\tLoss: 1.212100\n",
      "Train Epoch: 7 [1920/5100 (38%)]\tLoss: 1.271824\n",
      "Train Epoch: 7 [2560/5100 (50%)]\tLoss: 1.125493\n",
      "Train Epoch: 7 [3200/5100 (62%)]\tLoss: 1.178774\n",
      "Train Epoch: 7 [3840/5100 (75%)]\tLoss: 1.069052\n",
      "Train Epoch: 7 [4480/5100 (88%)]\tLoss: 1.297392\n",
      "Train Epoch: 8 [0/5100 (0%)]\tLoss: 1.068798\n",
      "Train Epoch: 8 [640/5100 (12%)]\tLoss: 1.307688\n",
      "Train Epoch: 8 [1280/5100 (25%)]\tLoss: 1.202660\n",
      "Train Epoch: 8 [1920/5100 (38%)]\tLoss: 1.120528\n",
      "Train Epoch: 8 [2560/5100 (50%)]\tLoss: 1.103789\n",
      "Train Epoch: 8 [3200/5100 (62%)]\tLoss: 1.099898\n",
      "Train Epoch: 8 [3840/5100 (75%)]\tLoss: 1.143518\n",
      "Train Epoch: 8 [4480/5100 (88%)]\tLoss: 1.547633\n",
      "Train Epoch: 9 [0/5100 (0%)]\tLoss: 1.516839\n",
      "Train Epoch: 9 [640/5100 (12%)]\tLoss: 1.393450\n",
      "Train Epoch: 9 [1280/5100 (25%)]\tLoss: 1.300743\n",
      "Train Epoch: 9 [1920/5100 (38%)]\tLoss: 1.255403\n",
      "Train Epoch: 9 [2560/5100 (50%)]\tLoss: 0.990228\n",
      "Train Epoch: 9 [3200/5100 (62%)]\tLoss: 1.205034\n",
      "Train Epoch: 9 [3840/5100 (75%)]\tLoss: 1.107943\n",
      "Train Epoch: 9 [4480/5100 (88%)]\tLoss: 1.213249\n",
      "Train Epoch: 10 [0/5100 (0%)]\tLoss: 1.265744\n",
      "Train Epoch: 10 [640/5100 (12%)]\tLoss: 1.137285\n",
      "Train Epoch: 10 [1280/5100 (25%)]\tLoss: 1.315846\n",
      "Train Epoch: 10 [1920/5100 (38%)]\tLoss: 1.399864\n",
      "Train Epoch: 10 [2560/5100 (50%)]\tLoss: 1.135855\n",
      "Train Epoch: 10 [3200/5100 (62%)]\tLoss: 1.202507\n",
      "Train Epoch: 10 [3840/5100 (75%)]\tLoss: 1.320366\n",
      "Train Epoch: 10 [4480/5100 (88%)]\tLoss: 1.132462\n",
      "Train Epoch: 11 [0/5100 (0%)]\tLoss: 0.977904\n",
      "Train Epoch: 11 [640/5100 (12%)]\tLoss: 0.955825\n",
      "Train Epoch: 11 [1280/5100 (25%)]\tLoss: 1.207004\n",
      "Train Epoch: 11 [1920/5100 (38%)]\tLoss: 1.071375\n",
      "Train Epoch: 11 [2560/5100 (50%)]\tLoss: 1.117075\n",
      "Train Epoch: 11 [3200/5100 (62%)]\tLoss: 1.198690\n",
      "Train Epoch: 11 [3840/5100 (75%)]\tLoss: 1.382940\n",
      "Train Epoch: 11 [4480/5100 (88%)]\tLoss: 1.164290\n",
      "Train Epoch: 12 [0/5100 (0%)]\tLoss: 1.108941\n",
      "Train Epoch: 12 [640/5100 (12%)]\tLoss: 0.948216\n",
      "Train Epoch: 12 [1280/5100 (25%)]\tLoss: 1.110642\n",
      "Train Epoch: 12 [1920/5100 (38%)]\tLoss: 1.312823\n",
      "Train Epoch: 12 [2560/5100 (50%)]\tLoss: 1.367325\n",
      "Train Epoch: 12 [3200/5100 (62%)]\tLoss: 0.985100\n",
      "Train Epoch: 12 [3840/5100 (75%)]\tLoss: 1.428560\n",
      "Train Epoch: 12 [4480/5100 (88%)]\tLoss: 1.334196\n",
      "Train Epoch: 13 [0/5100 (0%)]\tLoss: 1.130374\n",
      "Train Epoch: 13 [640/5100 (12%)]\tLoss: 1.518575\n",
      "Train Epoch: 13 [1280/5100 (25%)]\tLoss: 1.238302\n",
      "Train Epoch: 13 [1920/5100 (38%)]\tLoss: 1.183832\n",
      "Train Epoch: 13 [2560/5100 (50%)]\tLoss: 1.477236\n",
      "Train Epoch: 13 [3200/5100 (62%)]\tLoss: 1.020613\n",
      "Train Epoch: 13 [3840/5100 (75%)]\tLoss: 1.325715\n",
      "Train Epoch: 13 [4480/5100 (88%)]\tLoss: 1.219382\n",
      "Train Epoch: 14 [0/5100 (0%)]\tLoss: 1.137177\n",
      "Train Epoch: 14 [640/5100 (12%)]\tLoss: 1.047470\n",
      "Train Epoch: 14 [1280/5100 (25%)]\tLoss: 1.051411\n",
      "Train Epoch: 14 [1920/5100 (38%)]\tLoss: 1.197210\n",
      "Train Epoch: 14 [2560/5100 (50%)]\tLoss: 1.144016\n",
      "Train Epoch: 14 [3200/5100 (62%)]\tLoss: 1.236542\n",
      "Train Epoch: 14 [3840/5100 (75%)]\tLoss: 1.250657\n",
      "Train Epoch: 14 [4480/5100 (88%)]\tLoss: 1.045881\n",
      "Train Epoch: 15 [0/5100 (0%)]\tLoss: 0.964168\n",
      "Train Epoch: 15 [640/5100 (12%)]\tLoss: 1.184868\n",
      "Train Epoch: 15 [1280/5100 (25%)]\tLoss: 1.125062\n",
      "Train Epoch: 15 [1920/5100 (38%)]\tLoss: 1.197624\n",
      "Train Epoch: 15 [2560/5100 (50%)]\tLoss: 1.371472\n",
      "Train Epoch: 15 [3200/5100 (62%)]\tLoss: 1.084865\n",
      "Train Epoch: 15 [3840/5100 (75%)]\tLoss: 1.317018\n",
      "Train Epoch: 15 [4480/5100 (88%)]\tLoss: 1.269336\n",
      "Train Epoch: 16 [0/5100 (0%)]\tLoss: 1.220371\n",
      "Train Epoch: 16 [640/5100 (12%)]\tLoss: 1.082573\n",
      "Train Epoch: 16 [1280/5100 (25%)]\tLoss: 1.136167\n",
      "Train Epoch: 16 [1920/5100 (38%)]\tLoss: 1.154052\n",
      "Train Epoch: 16 [2560/5100 (50%)]\tLoss: 1.089257\n",
      "Train Epoch: 16 [3200/5100 (62%)]\tLoss: 1.164649\n",
      "Train Epoch: 16 [3840/5100 (75%)]\tLoss: 1.423416\n",
      "Train Epoch: 16 [4480/5100 (88%)]\tLoss: 1.043019\n",
      "Train Epoch: 17 [0/5100 (0%)]\tLoss: 1.476600\n",
      "Train Epoch: 17 [640/5100 (12%)]\tLoss: 1.125233\n",
      "Train Epoch: 17 [1280/5100 (25%)]\tLoss: 1.279093\n",
      "Train Epoch: 17 [1920/5100 (38%)]\tLoss: 1.032714\n",
      "Train Epoch: 17 [2560/5100 (50%)]\tLoss: 1.059346\n",
      "Train Epoch: 17 [3200/5100 (62%)]\tLoss: 1.133395\n",
      "Train Epoch: 17 [3840/5100 (75%)]\tLoss: 1.011083\n",
      "Train Epoch: 17 [4480/5100 (88%)]\tLoss: 1.036179\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.2165, Accuracy: 5673/10000 (57%)\n",
      "\n",
      "Round 2/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.384448\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.168415\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.290686\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.458629\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.772223\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.076682\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.654416\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.658936\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.061262\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 1.140750\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 0.951732\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.328388\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.032109\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.191723\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.221232\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.033248\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.305616\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.315804\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.127429\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.045359\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.322967\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.352449\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.122948\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.159083\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.348430\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 0.853722\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.143269\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.065277\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.308598\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.058280\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.067451\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.161281\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.352368\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.612768\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.304061\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.242756\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.001739\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.223818\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.330287\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.276258\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 1.240018\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 1.246676\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 1.244209\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 0.956831\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.266703\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.406702\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 1.428531\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 0.991419\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.299999\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.133649\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.381820\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 1.090138\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 1.006181\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 1.243605\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 1.467926\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.159134\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 1.185220\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 1.230297\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.144421\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.367984\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 0.962946\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 1.055616\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 0.937625\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.258383\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.037660\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 0.887621\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 0.975675\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.083461\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 1.066403\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 1.515168\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.141803\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 1.174836\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 1.035975\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.249035\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.374302\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 1.256588\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.046470\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.291503\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 0.966775\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 0.987528\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.193784\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 1.329594\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.251009\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.177117\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 1.028124\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 1.102726\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.141609\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 1.061085\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.178590\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.069109\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.409078\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.117162\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 1.073143\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 1.388009\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 1.249089\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 1.237270\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.124515\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 1.274469\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 1.022056\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 1.138094\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.156324\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 1.138614\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 0.973745\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 1.200206\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 1.105464\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 1.184336\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 1.363578\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.345048\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 1.309956\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 1.019317\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.228039\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 1.067164\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 1.131799\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 0.921855\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.209597\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 1.108909\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 1.229960\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.072240\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 1.280493\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.390143\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.361398\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.276736\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.240284\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.242813\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.490299\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.184805\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.606378\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.538267\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.425390\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.096585\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.157051\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.445758\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.276075\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.352676\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.252464\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.277438\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.130937\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.202468\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 1.064011\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.390228\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.466916\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.222067\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.222572\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.346470\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.085672\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.277719\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.262447\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.126924\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.152803\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.125242\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.280769\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.293657\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.146020\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.250413\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.361816\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.085615\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.416786\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.334608\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.108698\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.168615\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.213541\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.103963\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.286721\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.320198\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.192329\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.226090\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.112982\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.168586\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.059417\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.247945\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.146059\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.288222\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.559145\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 0.996495\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.085759\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.158147\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.194460\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.329777\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 1.287428\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.195026\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.168415\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.171771\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.051025\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.383693\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.195569\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.232143\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.115185\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.350724\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.071538\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 0.939330\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.232529\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.152865\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.061207\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.116750\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.218378\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.065569\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.173248\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 0.900397\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.123322\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.073234\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.276224\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.168401\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.240964\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.101937\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.153606\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 1.208034\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.073480\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.322164\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.197274\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.185674\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.150206\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.136466\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.182404\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 1.087767\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 1.157323\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.122296\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.108447\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.284506\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.347858\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.239872\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.010400\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.273054\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.166565\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 1.249239\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.264021\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 1.083498\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.091211\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.076834\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.193317\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.216706\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.082902\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 1.220728\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.234855\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.207968\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.042161\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.067181\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.044740\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 1.241285\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.111107\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.139127\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.192191\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 1.247200\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.512980\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.072692\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.223569\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 1.061808\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.143183\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.146038\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.165966\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.038670\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.079601\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.204418\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 1.369597\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 1.322423\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 1.266262\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.656680\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.260100\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.419398\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.469413\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.376526\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.004516\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.194284\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 0.957513\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.231341\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.419690\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.130735\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.232456\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.255518\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.244093\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.223404\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.229118\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.383589\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.362240\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.203926\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.141173\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.425676\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.312631\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.267272\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.185203\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.195672\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.196844\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.324319\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.263113\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.089766\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.207841\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.192230\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.170356\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.335892\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.178038\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.186616\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.389498\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.297519\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.350652\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.293602\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.317438\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.376287\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.183511\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.381000\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.203869\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.089012\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.198311\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.298399\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.327188\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.254208\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.337224\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.290665\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.440797\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.330335\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.304006\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.257779\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.505572\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 0.951488\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.156110\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.219548\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.183970\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.117884\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.136432\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.294333\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.131800\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.157510\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.156428\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.250619\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.271586\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.119750\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.193232\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.305318\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.320395\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.303526\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.318564\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.388844\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.149393\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.204105\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.241668\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.278578\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 1.147997\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.166474\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.138278\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.121938\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.372097\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.294058\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.470106\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.052399\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.210268\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.241789\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.368293\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.216992\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.154052\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.184690\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.451508\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.405058\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.291845\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.318344\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.006648\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.239998\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 1.339435\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.344754\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.125217\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.294874\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.175142\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 1.110526\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.184643\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.161053\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.117930\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.192047\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.469115\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 1.173453\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.075967\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.060371\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.157637\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.024274\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.262475\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.098860\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.305149\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.305030\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.285088\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.200352\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.232872\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.309515\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.253027\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.500700\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.177576\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.248389\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.460271\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.076674\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 0.975302\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 1.168379\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.274833\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.200490\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.191008\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.303975\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.157863\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.299877\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.161482\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.374356\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.171406\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 0.969493\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 1.164706\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.233005\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 1.247936\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.064003\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.512222\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.029679\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.233268\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.215513\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.229535\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 1.124128\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.105755\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.295209\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 1.008288\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.299965\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.176442\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.230454\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.007265\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.004494\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.322135\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.170772\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.031872\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 1.186044\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.070967\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.034926\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.144062\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.172750\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.371957\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.135879\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.183575\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 0.992835\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 1.244314\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 0.912281\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 1.091449\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 1.007894\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 0.978294\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 0.941230\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 1.009222\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 0.904553\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 0.991750\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 1.168097\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 0.943653\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 0.697979\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 1.040396\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 0.948984\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 1.079457\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 0.977219\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.841781\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.782125\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.872183\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 0.845119\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 1.047400\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 1.070170\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 1.041010\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 0.752386\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 0.723493\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 1.239480\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 1.095350\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 1.143769\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 0.746139\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 0.945359\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 1.346995\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.911299\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 0.734321\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 0.657128\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 0.780813\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.879832\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 0.967252\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.813153\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.714263\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 1.007511\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.895106\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.865232\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.894095\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 1.057158\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 1.150366\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.989994\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.894131\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 1.053228\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 0.784803\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.915289\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 1.164791\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.922916\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 1.104915\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 0.925985\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.921871\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 1.111557\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.701935\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.739052\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.825266\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.994396\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 0.958650\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 0.946455\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.883513\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.984920\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.717883\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.994745\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.885028\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.836240\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.850822\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 1.026345\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.983172\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.830096\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.904995\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.748619\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.675457\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.988544\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.922368\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.731022\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.743276\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.869425\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.837458\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.743378\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 1.053416\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.820115\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 0.859964\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.772030\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.870699\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 1.023313\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.763837\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.905858\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.878773\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.770929\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.927782\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.902295\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.759978\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.764889\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 1.067760\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.970921\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 0.840224\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.908933\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.829894\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.422315\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.301123\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.040920\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 1.192619\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 1.188232\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.332518\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.241960\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 1.043394\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 0.964675\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 1.355304\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 1.091380\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 0.934124\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.030939\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.002948\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.118928\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 1.123413\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 1.033648\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 1.104968\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 1.026456\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 0.986668\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.094743\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 0.877675\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.214840\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 1.378404\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 0.927124\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.246178\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 0.932893\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 0.992216\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 1.009208\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 1.170811\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 1.065740\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 1.041762\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 1.080476\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 1.180116\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.065671\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 0.963485\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 1.226277\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.873258\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.048652\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 1.134611\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 1.233240\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 1.048362\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 0.776674\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 1.097271\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 1.350922\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 1.285273\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 1.184587\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 1.352891\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 1.155632\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 0.995265\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 1.229745\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 0.942749\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 1.084449\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 0.990094\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 0.984765\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 0.964031\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 1.003181\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 1.016981\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 1.007048\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 1.027958\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 1.064171\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 1.083605\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 0.836681\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 0.938746\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 1.372176\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 1.100100\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 0.991165\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 1.076856\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.972813\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 0.957825\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 0.851251\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 1.111666\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 1.023070\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 0.892741\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 1.122685\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 0.927365\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 1.134290\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 0.805481\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 0.941794\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 0.882300\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 0.932274\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 0.971265\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 0.894676\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 0.990590\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 0.941986\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 0.728614\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 1.036116\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 0.831109\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 1.120316\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 1.088740\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 0.953787\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 1.038954\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 0.988922\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 0.922171\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 0.897145\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 1.065303\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 0.758834\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 1.153408\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 1.036316\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 1.031226\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 1.125100\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 1.093745\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.388873\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.260559\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.129271\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.428964\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.207174\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.476601\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.278157\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.448743\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.176984\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.361070\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.241261\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.237498\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.079432\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.069043\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.161590\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.525843\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.192857\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.264915\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.295229\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.137361\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.143319\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.327680\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.248109\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 1.104770\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.189904\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 0.894751\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.251593\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.219358\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.534941\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.270295\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.184790\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.201524\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.193619\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.045448\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.601443\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.231341\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.245410\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.159120\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.201010\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.295239\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.240283\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.349635\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.169721\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.075748\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 1.353336\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.155266\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.174489\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.229474\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.062560\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.222949\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.358881\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.205262\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.199694\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.184330\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.223107\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.368031\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.087848\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.189053\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.313681\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.209315\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.413954\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.566953\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.340438\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.174270\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.121538\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.061497\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.091290\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.090162\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.541922\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.429162\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.157928\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.185827\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.106109\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 1.281893\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.265145\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.243933\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 1.179649\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.071846\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.193780\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.170158\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.099622\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.250022\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.193275\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.586382\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.412125\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.220775\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.221249\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.226008\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.042495\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.096897\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.266619\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.055847\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.207448\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 1.313094\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 1.135079\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.294781\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.142759\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.446324\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.209093\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.001205\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.097296\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.332952\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.123448\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.166893\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.126149\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.183898\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 1.039891\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 1.348964\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.083868\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 1.167657\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.322745\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.171791\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 0.902234\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 1.005223\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.304285\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 1.147148\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.305977\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.261961\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 0.934596\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.127656\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.074612\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 1.116936\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.070986\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.022762\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.263406\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.100206\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.217064\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.293280\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.183272\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.184162\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.115914\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.295132\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 0.965105\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.157311\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.399925\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 1.225552\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/5548 (0%)]\tLoss: 1.337200\n",
      "Train Epoch: 1 [640/5548 (11%)]\tLoss: 1.431170\n",
      "Train Epoch: 1 [1280/5548 (23%)]\tLoss: 1.265070\n",
      "Train Epoch: 1 [1920/5548 (34%)]\tLoss: 1.098349\n",
      "Train Epoch: 1 [2560/5548 (46%)]\tLoss: 1.210041\n",
      "Train Epoch: 1 [3200/5548 (57%)]\tLoss: 1.372822\n",
      "Train Epoch: 1 [3840/5548 (69%)]\tLoss: 1.559658\n",
      "Train Epoch: 1 [4480/5548 (80%)]\tLoss: 1.274717\n",
      "Train Epoch: 1 [5120/5548 (92%)]\tLoss: 1.151358\n",
      "Train Epoch: 2 [0/5548 (0%)]\tLoss: 1.337763\n",
      "Train Epoch: 2 [640/5548 (11%)]\tLoss: 1.372425\n",
      "Train Epoch: 2 [1280/5548 (23%)]\tLoss: 1.035993\n",
      "Train Epoch: 2 [1920/5548 (34%)]\tLoss: 1.144754\n",
      "Train Epoch: 2 [2560/5548 (46%)]\tLoss: 0.884915\n",
      "Train Epoch: 2 [3200/5548 (57%)]\tLoss: 1.173226\n",
      "Train Epoch: 2 [3840/5548 (69%)]\tLoss: 1.070890\n",
      "Train Epoch: 2 [4480/5548 (80%)]\tLoss: 1.283780\n",
      "Train Epoch: 2 [5120/5548 (92%)]\tLoss: 1.249488\n",
      "Train Epoch: 3 [0/5548 (0%)]\tLoss: 0.971114\n",
      "Train Epoch: 3 [640/5548 (11%)]\tLoss: 1.211180\n",
      "Train Epoch: 3 [1280/5548 (23%)]\tLoss: 1.327388\n",
      "Train Epoch: 3 [1920/5548 (34%)]\tLoss: 1.127477\n",
      "Train Epoch: 3 [2560/5548 (46%)]\tLoss: 1.120249\n",
      "Train Epoch: 3 [3200/5548 (57%)]\tLoss: 1.117038\n",
      "Train Epoch: 3 [3840/5548 (69%)]\tLoss: 1.186258\n",
      "Train Epoch: 3 [4480/5548 (80%)]\tLoss: 0.923586\n",
      "Train Epoch: 3 [5120/5548 (92%)]\tLoss: 1.069312\n",
      "Train Epoch: 4 [0/5548 (0%)]\tLoss: 1.117083\n",
      "Train Epoch: 4 [640/5548 (11%)]\tLoss: 1.081700\n",
      "Train Epoch: 4 [1280/5548 (23%)]\tLoss: 1.025560\n",
      "Train Epoch: 4 [1920/5548 (34%)]\tLoss: 1.062377\n",
      "Train Epoch: 4 [2560/5548 (46%)]\tLoss: 1.328806\n",
      "Train Epoch: 4 [3200/5548 (57%)]\tLoss: 1.221642\n",
      "Train Epoch: 4 [3840/5548 (69%)]\tLoss: 1.032352\n",
      "Train Epoch: 4 [4480/5548 (80%)]\tLoss: 1.096085\n",
      "Train Epoch: 4 [5120/5548 (92%)]\tLoss: 1.398990\n",
      "Train Epoch: 5 [0/5548 (0%)]\tLoss: 1.034136\n",
      "Train Epoch: 5 [640/5548 (11%)]\tLoss: 1.408282\n",
      "Train Epoch: 5 [1280/5548 (23%)]\tLoss: 1.128669\n",
      "Train Epoch: 5 [1920/5548 (34%)]\tLoss: 0.977442\n",
      "Train Epoch: 5 [2560/5548 (46%)]\tLoss: 1.304431\n",
      "Train Epoch: 5 [3200/5548 (57%)]\tLoss: 0.995611\n",
      "Train Epoch: 5 [3840/5548 (69%)]\tLoss: 1.072870\n",
      "Train Epoch: 5 [4480/5548 (80%)]\tLoss: 1.228194\n",
      "Train Epoch: 5 [5120/5548 (92%)]\tLoss: 1.143404\n",
      "Train Epoch: 6 [0/5548 (0%)]\tLoss: 1.135826\n",
      "Train Epoch: 6 [640/5548 (11%)]\tLoss: 0.839812\n",
      "Train Epoch: 6 [1280/5548 (23%)]\tLoss: 0.882094\n",
      "Train Epoch: 6 [1920/5548 (34%)]\tLoss: 1.189178\n",
      "Train Epoch: 6 [2560/5548 (46%)]\tLoss: 1.035514\n",
      "Train Epoch: 6 [3200/5548 (57%)]\tLoss: 1.271996\n",
      "Train Epoch: 6 [3840/5548 (69%)]\tLoss: 1.014006\n",
      "Train Epoch: 6 [4480/5548 (80%)]\tLoss: 1.193532\n",
      "Train Epoch: 6 [5120/5548 (92%)]\tLoss: 1.271695\n",
      "Train Epoch: 7 [0/5548 (0%)]\tLoss: 0.950722\n",
      "Train Epoch: 7 [640/5548 (11%)]\tLoss: 1.105876\n",
      "Train Epoch: 7 [1280/5548 (23%)]\tLoss: 0.959359\n",
      "Train Epoch: 7 [1920/5548 (34%)]\tLoss: 1.110109\n",
      "Train Epoch: 7 [2560/5548 (46%)]\tLoss: 1.034500\n",
      "Train Epoch: 7 [3200/5548 (57%)]\tLoss: 1.071432\n",
      "Train Epoch: 7 [3840/5548 (69%)]\tLoss: 0.923586\n",
      "Train Epoch: 7 [4480/5548 (80%)]\tLoss: 1.020190\n",
      "Train Epoch: 7 [5120/5548 (92%)]\tLoss: 0.977173\n",
      "Train Epoch: 8 [0/5548 (0%)]\tLoss: 0.988214\n",
      "Train Epoch: 8 [640/5548 (11%)]\tLoss: 1.083323\n",
      "Train Epoch: 8 [1280/5548 (23%)]\tLoss: 1.118262\n",
      "Train Epoch: 8 [1920/5548 (34%)]\tLoss: 1.085334\n",
      "Train Epoch: 8 [2560/5548 (46%)]\tLoss: 1.194369\n",
      "Train Epoch: 8 [3200/5548 (57%)]\tLoss: 1.210192\n",
      "Train Epoch: 8 [3840/5548 (69%)]\tLoss: 1.110340\n",
      "Train Epoch: 8 [4480/5548 (80%)]\tLoss: 1.122571\n",
      "Train Epoch: 8 [5120/5548 (92%)]\tLoss: 1.283484\n",
      "Train Epoch: 9 [0/5548 (0%)]\tLoss: 1.292895\n",
      "Train Epoch: 9 [640/5548 (11%)]\tLoss: 0.867722\n",
      "Train Epoch: 9 [1280/5548 (23%)]\tLoss: 1.104580\n",
      "Train Epoch: 9 [1920/5548 (34%)]\tLoss: 1.072027\n",
      "Train Epoch: 9 [2560/5548 (46%)]\tLoss: 0.977937\n",
      "Train Epoch: 9 [3200/5548 (57%)]\tLoss: 1.157884\n",
      "Train Epoch: 9 [3840/5548 (69%)]\tLoss: 1.119775\n",
      "Train Epoch: 9 [4480/5548 (80%)]\tLoss: 0.979894\n",
      "Train Epoch: 9 [5120/5548 (92%)]\tLoss: 1.033612\n",
      "Train Epoch: 10 [0/5548 (0%)]\tLoss: 1.109182\n",
      "Train Epoch: 10 [640/5548 (11%)]\tLoss: 0.935739\n",
      "Train Epoch: 10 [1280/5548 (23%)]\tLoss: 1.224490\n",
      "Train Epoch: 10 [1920/5548 (34%)]\tLoss: 0.936406\n",
      "Train Epoch: 10 [2560/5548 (46%)]\tLoss: 1.039852\n",
      "Train Epoch: 10 [3200/5548 (57%)]\tLoss: 1.443701\n",
      "Train Epoch: 10 [3840/5548 (69%)]\tLoss: 1.215930\n",
      "Train Epoch: 10 [4480/5548 (80%)]\tLoss: 1.130503\n",
      "Train Epoch: 10 [5120/5548 (92%)]\tLoss: 1.104873\n",
      "Train Epoch: 11 [0/5548 (0%)]\tLoss: 1.415635\n",
      "Train Epoch: 11 [640/5548 (11%)]\tLoss: 1.096117\n",
      "Train Epoch: 11 [1280/5548 (23%)]\tLoss: 1.043095\n",
      "Train Epoch: 11 [1920/5548 (34%)]\tLoss: 1.209179\n",
      "Train Epoch: 11 [2560/5548 (46%)]\tLoss: 1.218168\n",
      "Train Epoch: 11 [3200/5548 (57%)]\tLoss: 0.979840\n",
      "Train Epoch: 11 [3840/5548 (69%)]\tLoss: 0.891660\n",
      "Train Epoch: 11 [4480/5548 (80%)]\tLoss: 1.042559\n",
      "Train Epoch: 11 [5120/5548 (92%)]\tLoss: 1.172472\n",
      "Train Epoch: 12 [0/5548 (0%)]\tLoss: 1.088881\n",
      "Train Epoch: 12 [640/5548 (11%)]\tLoss: 1.185587\n",
      "Train Epoch: 12 [1280/5548 (23%)]\tLoss: 0.842798\n",
      "Train Epoch: 12 [1920/5548 (34%)]\tLoss: 0.914592\n",
      "Train Epoch: 12 [2560/5548 (46%)]\tLoss: 1.057287\n",
      "Train Epoch: 12 [3200/5548 (57%)]\tLoss: 0.869094\n",
      "Train Epoch: 12 [3840/5548 (69%)]\tLoss: 1.232124\n",
      "Train Epoch: 12 [4480/5548 (80%)]\tLoss: 0.926155\n",
      "Train Epoch: 12 [5120/5548 (92%)]\tLoss: 1.052476\n",
      "Train Epoch: 13 [0/5548 (0%)]\tLoss: 0.961910\n",
      "Train Epoch: 13 [640/5548 (11%)]\tLoss: 1.051623\n",
      "Train Epoch: 13 [1280/5548 (23%)]\tLoss: 1.159966\n",
      "Train Epoch: 13 [1920/5548 (34%)]\tLoss: 1.137014\n",
      "Train Epoch: 13 [2560/5548 (46%)]\tLoss: 1.097631\n",
      "Train Epoch: 13 [3200/5548 (57%)]\tLoss: 1.011352\n",
      "Train Epoch: 13 [3840/5548 (69%)]\tLoss: 1.366076\n",
      "Train Epoch: 13 [4480/5548 (80%)]\tLoss: 1.160208\n",
      "Train Epoch: 13 [5120/5548 (92%)]\tLoss: 1.035256\n",
      "Train Epoch: 14 [0/5548 (0%)]\tLoss: 0.721022\n",
      "Train Epoch: 14 [640/5548 (11%)]\tLoss: 1.083243\n",
      "Train Epoch: 14 [1280/5548 (23%)]\tLoss: 1.171456\n",
      "Train Epoch: 14 [1920/5548 (34%)]\tLoss: 1.178697\n",
      "Train Epoch: 14 [2560/5548 (46%)]\tLoss: 1.258514\n",
      "Train Epoch: 14 [3200/5548 (57%)]\tLoss: 1.179405\n",
      "Train Epoch: 14 [3840/5548 (69%)]\tLoss: 1.032220\n",
      "Train Epoch: 14 [4480/5548 (80%)]\tLoss: 1.427981\n",
      "Train Epoch: 14 [5120/5548 (92%)]\tLoss: 1.170477\n",
      "Train Epoch: 15 [0/5548 (0%)]\tLoss: 1.309651\n",
      "Train Epoch: 15 [640/5548 (11%)]\tLoss: 0.894273\n",
      "Train Epoch: 15 [1280/5548 (23%)]\tLoss: 1.066259\n",
      "Train Epoch: 15 [1920/5548 (34%)]\tLoss: 1.154962\n",
      "Train Epoch: 15 [2560/5548 (46%)]\tLoss: 0.977744\n",
      "Train Epoch: 15 [3200/5548 (57%)]\tLoss: 1.004031\n",
      "Train Epoch: 15 [3840/5548 (69%)]\tLoss: 1.014636\n",
      "Train Epoch: 15 [4480/5548 (80%)]\tLoss: 1.037448\n",
      "Train Epoch: 15 [5120/5548 (92%)]\tLoss: 0.997386\n",
      "Train Epoch: 16 [0/5548 (0%)]\tLoss: 0.983522\n",
      "Train Epoch: 16 [640/5548 (11%)]\tLoss: 1.232590\n",
      "Train Epoch: 16 [1280/5548 (23%)]\tLoss: 0.955875\n",
      "Train Epoch: 16 [1920/5548 (34%)]\tLoss: 1.210235\n",
      "Train Epoch: 16 [2560/5548 (46%)]\tLoss: 0.899644\n",
      "Train Epoch: 16 [3200/5548 (57%)]\tLoss: 1.009600\n",
      "Train Epoch: 16 [3840/5548 (69%)]\tLoss: 1.420091\n",
      "Train Epoch: 16 [4480/5548 (80%)]\tLoss: 1.036061\n",
      "Train Epoch: 16 [5120/5548 (92%)]\tLoss: 1.157036\n",
      "Train Epoch: 17 [0/5548 (0%)]\tLoss: 0.971900\n",
      "Train Epoch: 17 [640/5548 (11%)]\tLoss: 0.876278\n",
      "Train Epoch: 17 [1280/5548 (23%)]\tLoss: 1.111960\n",
      "Train Epoch: 17 [1920/5548 (34%)]\tLoss: 1.082443\n",
      "Train Epoch: 17 [2560/5548 (46%)]\tLoss: 0.918874\n",
      "Train Epoch: 17 [3200/5548 (57%)]\tLoss: 0.962503\n",
      "Train Epoch: 17 [3840/5548 (69%)]\tLoss: 1.091235\n",
      "Train Epoch: 17 [4480/5548 (80%)]\tLoss: 1.104839\n",
      "Train Epoch: 17 [5120/5548 (92%)]\tLoss: 0.958211\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/4097 (0%)]\tLoss: 1.458784\n",
      "Train Epoch: 1 [640/4097 (15%)]\tLoss: 1.782417\n",
      "Train Epoch: 1 [1280/4097 (31%)]\tLoss: 1.220797\n",
      "Train Epoch: 1 [1920/4097 (46%)]\tLoss: 1.171436\n",
      "Train Epoch: 1 [2560/4097 (62%)]\tLoss: 1.381365\n",
      "Train Epoch: 1 [3200/4097 (77%)]\tLoss: 1.553928\n",
      "Train Epoch: 1 [3840/4097 (92%)]\tLoss: 1.112156\n",
      "Train Epoch: 2 [0/4097 (0%)]\tLoss: 1.495454\n",
      "Train Epoch: 2 [640/4097 (15%)]\tLoss: 1.222745\n",
      "Train Epoch: 2 [1280/4097 (31%)]\tLoss: 1.114112\n",
      "Train Epoch: 2 [1920/4097 (46%)]\tLoss: 1.648625\n",
      "Train Epoch: 2 [2560/4097 (62%)]\tLoss: 1.493148\n",
      "Train Epoch: 2 [3200/4097 (77%)]\tLoss: 1.183656\n",
      "Train Epoch: 2 [3840/4097 (92%)]\tLoss: 1.347064\n",
      "Train Epoch: 3 [0/4097 (0%)]\tLoss: 1.529562\n",
      "Train Epoch: 3 [640/4097 (15%)]\tLoss: 1.233408\n",
      "Train Epoch: 3 [1280/4097 (31%)]\tLoss: 1.293993\n",
      "Train Epoch: 3 [1920/4097 (46%)]\tLoss: 1.271876\n",
      "Train Epoch: 3 [2560/4097 (62%)]\tLoss: 1.243985\n",
      "Train Epoch: 3 [3200/4097 (77%)]\tLoss: 1.490872\n",
      "Train Epoch: 3 [3840/4097 (92%)]\tLoss: 1.530805\n",
      "Train Epoch: 4 [0/4097 (0%)]\tLoss: 1.299369\n",
      "Train Epoch: 4 [640/4097 (15%)]\tLoss: 1.394094\n",
      "Train Epoch: 4 [1280/4097 (31%)]\tLoss: 1.352857\n",
      "Train Epoch: 4 [1920/4097 (46%)]\tLoss: 1.506903\n",
      "Train Epoch: 4 [2560/4097 (62%)]\tLoss: 1.082303\n",
      "Train Epoch: 4 [3200/4097 (77%)]\tLoss: 1.248153\n",
      "Train Epoch: 4 [3840/4097 (92%)]\tLoss: 1.500378\n",
      "Train Epoch: 5 [0/4097 (0%)]\tLoss: 1.360584\n",
      "Train Epoch: 5 [640/4097 (15%)]\tLoss: 1.302110\n",
      "Train Epoch: 5 [1280/4097 (31%)]\tLoss: 1.283995\n",
      "Train Epoch: 5 [1920/4097 (46%)]\tLoss: 1.149286\n",
      "Train Epoch: 5 [2560/4097 (62%)]\tLoss: 1.530085\n",
      "Train Epoch: 5 [3200/4097 (77%)]\tLoss: 1.366879\n",
      "Train Epoch: 5 [3840/4097 (92%)]\tLoss: 1.246996\n",
      "Train Epoch: 6 [0/4097 (0%)]\tLoss: 1.389626\n",
      "Train Epoch: 6 [640/4097 (15%)]\tLoss: 1.118894\n",
      "Train Epoch: 6 [1280/4097 (31%)]\tLoss: 1.352976\n",
      "Train Epoch: 6 [1920/4097 (46%)]\tLoss: 1.467052\n",
      "Train Epoch: 6 [2560/4097 (62%)]\tLoss: 1.179382\n",
      "Train Epoch: 6 [3200/4097 (77%)]\tLoss: 1.323989\n",
      "Train Epoch: 6 [3840/4097 (92%)]\tLoss: 1.034749\n",
      "Train Epoch: 7 [0/4097 (0%)]\tLoss: 1.366298\n",
      "Train Epoch: 7 [640/4097 (15%)]\tLoss: 1.305635\n",
      "Train Epoch: 7 [1280/4097 (31%)]\tLoss: 1.203045\n",
      "Train Epoch: 7 [1920/4097 (46%)]\tLoss: 1.166239\n",
      "Train Epoch: 7 [2560/4097 (62%)]\tLoss: 1.398766\n",
      "Train Epoch: 7 [3200/4097 (77%)]\tLoss: 1.357560\n",
      "Train Epoch: 7 [3840/4097 (92%)]\tLoss: 1.297229\n",
      "Train Epoch: 8 [0/4097 (0%)]\tLoss: 1.367919\n",
      "Train Epoch: 8 [640/4097 (15%)]\tLoss: 1.318147\n",
      "Train Epoch: 8 [1280/4097 (31%)]\tLoss: 1.205760\n",
      "Train Epoch: 8 [1920/4097 (46%)]\tLoss: 1.383482\n",
      "Train Epoch: 8 [2560/4097 (62%)]\tLoss: 1.397747\n",
      "Train Epoch: 8 [3200/4097 (77%)]\tLoss: 0.968184\n",
      "Train Epoch: 8 [3840/4097 (92%)]\tLoss: 1.428818\n",
      "Train Epoch: 9 [0/4097 (0%)]\tLoss: 1.852494\n",
      "Train Epoch: 9 [640/4097 (15%)]\tLoss: 1.450366\n",
      "Train Epoch: 9 [1280/4097 (31%)]\tLoss: 1.481998\n",
      "Train Epoch: 9 [1920/4097 (46%)]\tLoss: 1.443537\n",
      "Train Epoch: 9 [2560/4097 (62%)]\tLoss: 1.359461\n",
      "Train Epoch: 9 [3200/4097 (77%)]\tLoss: 1.304017\n",
      "Train Epoch: 9 [3840/4097 (92%)]\tLoss: 1.518754\n",
      "Train Epoch: 10 [0/4097 (0%)]\tLoss: 1.145040\n",
      "Train Epoch: 10 [640/4097 (15%)]\tLoss: 1.112508\n",
      "Train Epoch: 10 [1280/4097 (31%)]\tLoss: 1.366419\n",
      "Train Epoch: 10 [1920/4097 (46%)]\tLoss: 1.454179\n",
      "Train Epoch: 10 [2560/4097 (62%)]\tLoss: 1.113858\n",
      "Train Epoch: 10 [3200/4097 (77%)]\tLoss: 1.309488\n",
      "Train Epoch: 10 [3840/4097 (92%)]\tLoss: 1.114314\n",
      "Train Epoch: 11 [0/4097 (0%)]\tLoss: 1.382122\n",
      "Train Epoch: 11 [640/4097 (15%)]\tLoss: 1.382823\n",
      "Train Epoch: 11 [1280/4097 (31%)]\tLoss: 1.263783\n",
      "Train Epoch: 11 [1920/4097 (46%)]\tLoss: 1.426996\n",
      "Train Epoch: 11 [2560/4097 (62%)]\tLoss: 1.470233\n",
      "Train Epoch: 11 [3200/4097 (77%)]\tLoss: 1.279647\n",
      "Train Epoch: 11 [3840/4097 (92%)]\tLoss: 1.292477\n",
      "Train Epoch: 12 [0/4097 (0%)]\tLoss: 1.720833\n",
      "Train Epoch: 12 [640/4097 (15%)]\tLoss: 1.350062\n",
      "Train Epoch: 12 [1280/4097 (31%)]\tLoss: 1.412674\n",
      "Train Epoch: 12 [1920/4097 (46%)]\tLoss: 1.585250\n",
      "Train Epoch: 12 [2560/4097 (62%)]\tLoss: 1.243862\n",
      "Train Epoch: 12 [3200/4097 (77%)]\tLoss: 1.239849\n",
      "Train Epoch: 12 [3840/4097 (92%)]\tLoss: 1.115836\n",
      "Train Epoch: 13 [0/4097 (0%)]\tLoss: 1.371722\n",
      "Train Epoch: 13 [640/4097 (15%)]\tLoss: 1.326146\n",
      "Train Epoch: 13 [1280/4097 (31%)]\tLoss: 1.357345\n",
      "Train Epoch: 13 [1920/4097 (46%)]\tLoss: 1.169171\n",
      "Train Epoch: 13 [2560/4097 (62%)]\tLoss: 1.125121\n",
      "Train Epoch: 13 [3200/4097 (77%)]\tLoss: 1.314972\n",
      "Train Epoch: 13 [3840/4097 (92%)]\tLoss: 1.203616\n",
      "Train Epoch: 14 [0/4097 (0%)]\tLoss: 1.364562\n",
      "Train Epoch: 14 [640/4097 (15%)]\tLoss: 1.203032\n",
      "Train Epoch: 14 [1280/4097 (31%)]\tLoss: 1.351860\n",
      "Train Epoch: 14 [1920/4097 (46%)]\tLoss: 1.412502\n",
      "Train Epoch: 14 [2560/4097 (62%)]\tLoss: 1.169284\n",
      "Train Epoch: 14 [3200/4097 (77%)]\tLoss: 0.978658\n",
      "Train Epoch: 14 [3840/4097 (92%)]\tLoss: 1.212572\n",
      "Train Epoch: 15 [0/4097 (0%)]\tLoss: 1.157797\n",
      "Train Epoch: 15 [640/4097 (15%)]\tLoss: 1.146746\n",
      "Train Epoch: 15 [1280/4097 (31%)]\tLoss: 1.487156\n",
      "Train Epoch: 15 [1920/4097 (46%)]\tLoss: 1.132265\n",
      "Train Epoch: 15 [2560/4097 (62%)]\tLoss: 1.120005\n",
      "Train Epoch: 15 [3200/4097 (77%)]\tLoss: 1.061753\n",
      "Train Epoch: 15 [3840/4097 (92%)]\tLoss: 1.190998\n",
      "Train Epoch: 16 [0/4097 (0%)]\tLoss: 2.469023\n",
      "Train Epoch: 16 [640/4097 (15%)]\tLoss: 1.495769\n",
      "Train Epoch: 16 [1280/4097 (31%)]\tLoss: 1.458727\n",
      "Train Epoch: 16 [1920/4097 (46%)]\tLoss: 1.537794\n",
      "Train Epoch: 16 [2560/4097 (62%)]\tLoss: 1.359225\n",
      "Train Epoch: 16 [3200/4097 (77%)]\tLoss: 1.386564\n",
      "Train Epoch: 16 [3840/4097 (92%)]\tLoss: 1.279174\n",
      "Train Epoch: 17 [0/4097 (0%)]\tLoss: 1.098682\n",
      "Train Epoch: 17 [640/4097 (15%)]\tLoss: 1.388785\n",
      "Train Epoch: 17 [1280/4097 (31%)]\tLoss: 1.201149\n",
      "Train Epoch: 17 [1920/4097 (46%)]\tLoss: 1.162345\n",
      "Train Epoch: 17 [2560/4097 (62%)]\tLoss: 1.470102\n",
      "Train Epoch: 17 [3200/4097 (77%)]\tLoss: 1.281369\n",
      "Train Epoch: 17 [3840/4097 (92%)]\tLoss: 1.280905\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/8272 (0%)]\tLoss: 1.484427\n",
      "Train Epoch: 1 [640/8272 (8%)]\tLoss: 1.210223\n",
      "Train Epoch: 1 [1280/8272 (15%)]\tLoss: 1.369924\n",
      "Train Epoch: 1 [1920/8272 (23%)]\tLoss: 1.564418\n",
      "Train Epoch: 1 [2560/8272 (31%)]\tLoss: 1.632305\n",
      "Train Epoch: 1 [3200/8272 (38%)]\tLoss: 1.080769\n",
      "Train Epoch: 1 [3840/8272 (46%)]\tLoss: 1.474076\n",
      "Train Epoch: 1 [4480/8272 (54%)]\tLoss: 1.265065\n",
      "Train Epoch: 1 [5120/8272 (62%)]\tLoss: 1.536073\n",
      "Train Epoch: 1 [5760/8272 (69%)]\tLoss: 1.602925\n",
      "Train Epoch: 1 [6400/8272 (77%)]\tLoss: 1.435825\n",
      "Train Epoch: 1 [7040/8272 (85%)]\tLoss: 1.438432\n",
      "Train Epoch: 1 [7680/8272 (92%)]\tLoss: 1.254057\n",
      "Train Epoch: 2 [0/8272 (0%)]\tLoss: 1.248921\n",
      "Train Epoch: 2 [640/8272 (8%)]\tLoss: 1.514587\n",
      "Train Epoch: 2 [1280/8272 (15%)]\tLoss: 1.379215\n",
      "Train Epoch: 2 [1920/8272 (23%)]\tLoss: 1.320346\n",
      "Train Epoch: 2 [2560/8272 (31%)]\tLoss: 1.399890\n",
      "Train Epoch: 2 [3200/8272 (38%)]\tLoss: 1.286924\n",
      "Train Epoch: 2 [3840/8272 (46%)]\tLoss: 1.493221\n",
      "Train Epoch: 2 [4480/8272 (54%)]\tLoss: 1.197108\n",
      "Train Epoch: 2 [5120/8272 (62%)]\tLoss: 1.260491\n",
      "Train Epoch: 2 [5760/8272 (69%)]\tLoss: 1.476399\n",
      "Train Epoch: 2 [6400/8272 (77%)]\tLoss: 1.302105\n",
      "Train Epoch: 2 [7040/8272 (85%)]\tLoss: 1.294018\n",
      "Train Epoch: 2 [7680/8272 (92%)]\tLoss: 1.477856\n",
      "Train Epoch: 3 [0/8272 (0%)]\tLoss: 1.442271\n",
      "Train Epoch: 3 [640/8272 (8%)]\tLoss: 1.608492\n",
      "Train Epoch: 3 [1280/8272 (15%)]\tLoss: 1.066002\n",
      "Train Epoch: 3 [1920/8272 (23%)]\tLoss: 1.631214\n",
      "Train Epoch: 3 [2560/8272 (31%)]\tLoss: 1.247697\n",
      "Train Epoch: 3 [3200/8272 (38%)]\tLoss: 1.343381\n",
      "Train Epoch: 3 [3840/8272 (46%)]\tLoss: 1.525524\n",
      "Train Epoch: 3 [4480/8272 (54%)]\tLoss: 1.391335\n",
      "Train Epoch: 3 [5120/8272 (62%)]\tLoss: 1.311641\n",
      "Train Epoch: 3 [5760/8272 (69%)]\tLoss: 1.262117\n",
      "Train Epoch: 3 [6400/8272 (77%)]\tLoss: 1.546905\n",
      "Train Epoch: 3 [7040/8272 (85%)]\tLoss: 1.571633\n",
      "Train Epoch: 3 [7680/8272 (92%)]\tLoss: 1.526657\n",
      "Train Epoch: 4 [0/8272 (0%)]\tLoss: 1.362070\n",
      "Train Epoch: 4 [640/8272 (8%)]\tLoss: 1.371760\n",
      "Train Epoch: 4 [1280/8272 (15%)]\tLoss: 1.388385\n",
      "Train Epoch: 4 [1920/8272 (23%)]\tLoss: 1.161403\n",
      "Train Epoch: 4 [2560/8272 (31%)]\tLoss: 1.214318\n",
      "Train Epoch: 4 [3200/8272 (38%)]\tLoss: 1.361930\n",
      "Train Epoch: 4 [3840/8272 (46%)]\tLoss: 1.272027\n",
      "Train Epoch: 4 [4480/8272 (54%)]\tLoss: 1.708277\n",
      "Train Epoch: 4 [5120/8272 (62%)]\tLoss: 1.351442\n",
      "Train Epoch: 4 [5760/8272 (69%)]\tLoss: 1.296193\n",
      "Train Epoch: 4 [6400/8272 (77%)]\tLoss: 1.418507\n",
      "Train Epoch: 4 [7040/8272 (85%)]\tLoss: 1.520003\n",
      "Train Epoch: 4 [7680/8272 (92%)]\tLoss: 1.448295\n",
      "Train Epoch: 5 [0/8272 (0%)]\tLoss: 1.025648\n",
      "Train Epoch: 5 [640/8272 (8%)]\tLoss: 1.286776\n",
      "Train Epoch: 5 [1280/8272 (15%)]\tLoss: 1.281799\n",
      "Train Epoch: 5 [1920/8272 (23%)]\tLoss: 1.091000\n",
      "Train Epoch: 5 [2560/8272 (31%)]\tLoss: 1.359016\n",
      "Train Epoch: 5 [3200/8272 (38%)]\tLoss: 1.428009\n",
      "Train Epoch: 5 [3840/8272 (46%)]\tLoss: 1.231880\n",
      "Train Epoch: 5 [4480/8272 (54%)]\tLoss: 1.405656\n",
      "Train Epoch: 5 [5120/8272 (62%)]\tLoss: 1.215951\n",
      "Train Epoch: 5 [5760/8272 (69%)]\tLoss: 1.245366\n",
      "Train Epoch: 5 [6400/8272 (77%)]\tLoss: 1.190822\n",
      "Train Epoch: 5 [7040/8272 (85%)]\tLoss: 1.212066\n",
      "Train Epoch: 5 [7680/8272 (92%)]\tLoss: 1.624119\n",
      "Train Epoch: 6 [0/8272 (0%)]\tLoss: 1.280681\n",
      "Train Epoch: 6 [640/8272 (8%)]\tLoss: 1.301064\n",
      "Train Epoch: 6 [1280/8272 (15%)]\tLoss: 1.274081\n",
      "Train Epoch: 6 [1920/8272 (23%)]\tLoss: 1.308757\n",
      "Train Epoch: 6 [2560/8272 (31%)]\tLoss: 1.128002\n",
      "Train Epoch: 6 [3200/8272 (38%)]\tLoss: 1.554702\n",
      "Train Epoch: 6 [3840/8272 (46%)]\tLoss: 1.421009\n",
      "Train Epoch: 6 [4480/8272 (54%)]\tLoss: 1.328045\n",
      "Train Epoch: 6 [5120/8272 (62%)]\tLoss: 1.130079\n",
      "Train Epoch: 6 [5760/8272 (69%)]\tLoss: 1.301455\n",
      "Train Epoch: 6 [6400/8272 (77%)]\tLoss: 1.351259\n",
      "Train Epoch: 6 [7040/8272 (85%)]\tLoss: 1.210659\n",
      "Train Epoch: 6 [7680/8272 (92%)]\tLoss: 1.353383\n",
      "Train Epoch: 7 [0/8272 (0%)]\tLoss: 1.616601\n",
      "Train Epoch: 7 [640/8272 (8%)]\tLoss: 1.547903\n",
      "Train Epoch: 7 [1280/8272 (15%)]\tLoss: 1.593150\n",
      "Train Epoch: 7 [1920/8272 (23%)]\tLoss: 1.273168\n",
      "Train Epoch: 7 [2560/8272 (31%)]\tLoss: 1.262190\n",
      "Train Epoch: 7 [3200/8272 (38%)]\tLoss: 1.254002\n",
      "Train Epoch: 7 [3840/8272 (46%)]\tLoss: 1.294181\n",
      "Train Epoch: 7 [4480/8272 (54%)]\tLoss: 1.276278\n",
      "Train Epoch: 7 [5120/8272 (62%)]\tLoss: 1.265728\n",
      "Train Epoch: 7 [5760/8272 (69%)]\tLoss: 1.614605\n",
      "Train Epoch: 7 [6400/8272 (77%)]\tLoss: 1.072404\n",
      "Train Epoch: 7 [7040/8272 (85%)]\tLoss: 1.458575\n",
      "Train Epoch: 7 [7680/8272 (92%)]\tLoss: 1.242718\n",
      "Train Epoch: 8 [0/8272 (0%)]\tLoss: 1.142761\n",
      "Train Epoch: 8 [640/8272 (8%)]\tLoss: 1.145611\n",
      "Train Epoch: 8 [1280/8272 (15%)]\tLoss: 1.481092\n",
      "Train Epoch: 8 [1920/8272 (23%)]\tLoss: 1.240940\n",
      "Train Epoch: 8 [2560/8272 (31%)]\tLoss: 1.300929\n",
      "Train Epoch: 8 [3200/8272 (38%)]\tLoss: 1.130676\n",
      "Train Epoch: 8 [3840/8272 (46%)]\tLoss: 1.488715\n",
      "Train Epoch: 8 [4480/8272 (54%)]\tLoss: 1.483902\n",
      "Train Epoch: 8 [5120/8272 (62%)]\tLoss: 1.451935\n",
      "Train Epoch: 8 [5760/8272 (69%)]\tLoss: 1.245790\n",
      "Train Epoch: 8 [6400/8272 (77%)]\tLoss: 1.039254\n",
      "Train Epoch: 8 [7040/8272 (85%)]\tLoss: 1.222801\n",
      "Train Epoch: 8 [7680/8272 (92%)]\tLoss: 1.352681\n",
      "Train Epoch: 9 [0/8272 (0%)]\tLoss: 1.068966\n",
      "Train Epoch: 9 [640/8272 (8%)]\tLoss: 1.159559\n",
      "Train Epoch: 9 [1280/8272 (15%)]\tLoss: 1.299956\n",
      "Train Epoch: 9 [1920/8272 (23%)]\tLoss: 1.186191\n",
      "Train Epoch: 9 [2560/8272 (31%)]\tLoss: 1.179091\n",
      "Train Epoch: 9 [3200/8272 (38%)]\tLoss: 1.277425\n",
      "Train Epoch: 9 [3840/8272 (46%)]\tLoss: 1.490453\n",
      "Train Epoch: 9 [4480/8272 (54%)]\tLoss: 1.014074\n",
      "Train Epoch: 9 [5120/8272 (62%)]\tLoss: 1.356782\n",
      "Train Epoch: 9 [5760/8272 (69%)]\tLoss: 1.150980\n",
      "Train Epoch: 9 [6400/8272 (77%)]\tLoss: 1.295430\n",
      "Train Epoch: 9 [7040/8272 (85%)]\tLoss: 1.734604\n",
      "Train Epoch: 9 [7680/8272 (92%)]\tLoss: 1.160829\n",
      "Train Epoch: 10 [0/8272 (0%)]\tLoss: 1.209034\n",
      "Train Epoch: 10 [640/8272 (8%)]\tLoss: 1.479610\n",
      "Train Epoch: 10 [1280/8272 (15%)]\tLoss: 1.231551\n",
      "Train Epoch: 10 [1920/8272 (23%)]\tLoss: 1.436539\n",
      "Train Epoch: 10 [2560/8272 (31%)]\tLoss: 1.111406\n",
      "Train Epoch: 10 [3200/8272 (38%)]\tLoss: 1.355188\n",
      "Train Epoch: 10 [3840/8272 (46%)]\tLoss: 1.481065\n",
      "Train Epoch: 10 [4480/8272 (54%)]\tLoss: 1.071049\n",
      "Train Epoch: 10 [5120/8272 (62%)]\tLoss: 1.209803\n",
      "Train Epoch: 10 [5760/8272 (69%)]\tLoss: 1.456148\n",
      "Train Epoch: 10 [6400/8272 (77%)]\tLoss: 1.446503\n",
      "Train Epoch: 10 [7040/8272 (85%)]\tLoss: 1.139368\n",
      "Train Epoch: 10 [7680/8272 (92%)]\tLoss: 1.360593\n",
      "Train Epoch: 11 [0/8272 (0%)]\tLoss: 1.387655\n",
      "Train Epoch: 11 [640/8272 (8%)]\tLoss: 1.182557\n",
      "Train Epoch: 11 [1280/8272 (15%)]\tLoss: 1.370671\n",
      "Train Epoch: 11 [1920/8272 (23%)]\tLoss: 1.495309\n",
      "Train Epoch: 11 [2560/8272 (31%)]\tLoss: 1.130212\n",
      "Train Epoch: 11 [3200/8272 (38%)]\tLoss: 1.442299\n",
      "Train Epoch: 11 [3840/8272 (46%)]\tLoss: 1.337954\n",
      "Train Epoch: 11 [4480/8272 (54%)]\tLoss: 1.112207\n",
      "Train Epoch: 11 [5120/8272 (62%)]\tLoss: 1.259351\n",
      "Train Epoch: 11 [5760/8272 (69%)]\tLoss: 1.706023\n",
      "Train Epoch: 11 [6400/8272 (77%)]\tLoss: 1.310719\n",
      "Train Epoch: 11 [7040/8272 (85%)]\tLoss: 1.275616\n",
      "Train Epoch: 11 [7680/8272 (92%)]\tLoss: 1.664680\n",
      "Train Epoch: 12 [0/8272 (0%)]\tLoss: 1.366787\n",
      "Train Epoch: 12 [640/8272 (8%)]\tLoss: 1.357853\n",
      "Train Epoch: 12 [1280/8272 (15%)]\tLoss: 1.252559\n",
      "Train Epoch: 12 [1920/8272 (23%)]\tLoss: 1.210021\n",
      "Train Epoch: 12 [2560/8272 (31%)]\tLoss: 1.344710\n",
      "Train Epoch: 12 [3200/8272 (38%)]\tLoss: 1.195373\n",
      "Train Epoch: 12 [3840/8272 (46%)]\tLoss: 1.181232\n",
      "Train Epoch: 12 [4480/8272 (54%)]\tLoss: 1.312605\n",
      "Train Epoch: 12 [5120/8272 (62%)]\tLoss: 1.358429\n",
      "Train Epoch: 12 [5760/8272 (69%)]\tLoss: 1.325074\n",
      "Train Epoch: 12 [6400/8272 (77%)]\tLoss: 1.368889\n",
      "Train Epoch: 12 [7040/8272 (85%)]\tLoss: 1.204322\n",
      "Train Epoch: 12 [7680/8272 (92%)]\tLoss: 1.551514\n",
      "Train Epoch: 13 [0/8272 (0%)]\tLoss: 1.307399\n",
      "Train Epoch: 13 [640/8272 (8%)]\tLoss: 1.355897\n",
      "Train Epoch: 13 [1280/8272 (15%)]\tLoss: 1.020865\n",
      "Train Epoch: 13 [1920/8272 (23%)]\tLoss: 1.441558\n",
      "Train Epoch: 13 [2560/8272 (31%)]\tLoss: 1.327930\n",
      "Train Epoch: 13 [3200/8272 (38%)]\tLoss: 1.460327\n",
      "Train Epoch: 13 [3840/8272 (46%)]\tLoss: 1.357278\n",
      "Train Epoch: 13 [4480/8272 (54%)]\tLoss: 1.255372\n",
      "Train Epoch: 13 [5120/8272 (62%)]\tLoss: 1.172259\n",
      "Train Epoch: 13 [5760/8272 (69%)]\tLoss: 1.136249\n",
      "Train Epoch: 13 [6400/8272 (77%)]\tLoss: 1.403735\n",
      "Train Epoch: 13 [7040/8272 (85%)]\tLoss: 1.391405\n",
      "Train Epoch: 13 [7680/8272 (92%)]\tLoss: 1.368724\n",
      "Train Epoch: 14 [0/8272 (0%)]\tLoss: 1.223792\n",
      "Train Epoch: 14 [640/8272 (8%)]\tLoss: 1.183890\n",
      "Train Epoch: 14 [1280/8272 (15%)]\tLoss: 1.279887\n",
      "Train Epoch: 14 [1920/8272 (23%)]\tLoss: 1.300073\n",
      "Train Epoch: 14 [2560/8272 (31%)]\tLoss: 1.275118\n",
      "Train Epoch: 14 [3200/8272 (38%)]\tLoss: 1.232547\n",
      "Train Epoch: 14 [3840/8272 (46%)]\tLoss: 1.211339\n",
      "Train Epoch: 14 [4480/8272 (54%)]\tLoss: 1.102401\n",
      "Train Epoch: 14 [5120/8272 (62%)]\tLoss: 1.089954\n",
      "Train Epoch: 14 [5760/8272 (69%)]\tLoss: 1.312920\n",
      "Train Epoch: 14 [6400/8272 (77%)]\tLoss: 1.204547\n",
      "Train Epoch: 14 [7040/8272 (85%)]\tLoss: 1.299811\n",
      "Train Epoch: 14 [7680/8272 (92%)]\tLoss: 1.360276\n",
      "Train Epoch: 15 [0/8272 (0%)]\tLoss: 1.323646\n",
      "Train Epoch: 15 [640/8272 (8%)]\tLoss: 1.316253\n",
      "Train Epoch: 15 [1280/8272 (15%)]\tLoss: 1.354292\n",
      "Train Epoch: 15 [1920/8272 (23%)]\tLoss: 1.346097\n",
      "Train Epoch: 15 [2560/8272 (31%)]\tLoss: 1.164200\n",
      "Train Epoch: 15 [3200/8272 (38%)]\tLoss: 1.291122\n",
      "Train Epoch: 15 [3840/8272 (46%)]\tLoss: 1.155362\n",
      "Train Epoch: 15 [4480/8272 (54%)]\tLoss: 1.267150\n",
      "Train Epoch: 15 [5120/8272 (62%)]\tLoss: 1.415581\n",
      "Train Epoch: 15 [5760/8272 (69%)]\tLoss: 1.300277\n",
      "Train Epoch: 15 [6400/8272 (77%)]\tLoss: 1.132186\n",
      "Train Epoch: 15 [7040/8272 (85%)]\tLoss: 1.146491\n",
      "Train Epoch: 15 [7680/8272 (92%)]\tLoss: 1.387942\n",
      "Train Epoch: 16 [0/8272 (0%)]\tLoss: 1.219541\n",
      "Train Epoch: 16 [640/8272 (8%)]\tLoss: 1.271960\n",
      "Train Epoch: 16 [1280/8272 (15%)]\tLoss: 1.224833\n",
      "Train Epoch: 16 [1920/8272 (23%)]\tLoss: 1.266084\n",
      "Train Epoch: 16 [2560/8272 (31%)]\tLoss: 1.056465\n",
      "Train Epoch: 16 [3200/8272 (38%)]\tLoss: 0.988320\n",
      "Train Epoch: 16 [3840/8272 (46%)]\tLoss: 1.455251\n",
      "Train Epoch: 16 [4480/8272 (54%)]\tLoss: 1.519220\n",
      "Train Epoch: 16 [5120/8272 (62%)]\tLoss: 1.219536\n",
      "Train Epoch: 16 [5760/8272 (69%)]\tLoss: 1.128147\n",
      "Train Epoch: 16 [6400/8272 (77%)]\tLoss: 1.284281\n",
      "Train Epoch: 16 [7040/8272 (85%)]\tLoss: 1.147409\n",
      "Train Epoch: 16 [7680/8272 (92%)]\tLoss: 1.229815\n",
      "Train Epoch: 17 [0/8272 (0%)]\tLoss: 1.346871\n",
      "Train Epoch: 17 [640/8272 (8%)]\tLoss: 1.307075\n",
      "Train Epoch: 17 [1280/8272 (15%)]\tLoss: 1.210683\n",
      "Train Epoch: 17 [1920/8272 (23%)]\tLoss: 1.517679\n",
      "Train Epoch: 17 [2560/8272 (31%)]\tLoss: 1.161787\n",
      "Train Epoch: 17 [3200/8272 (38%)]\tLoss: 1.034132\n",
      "Train Epoch: 17 [3840/8272 (46%)]\tLoss: 1.436151\n",
      "Train Epoch: 17 [4480/8272 (54%)]\tLoss: 1.279173\n",
      "Train Epoch: 17 [5120/8272 (62%)]\tLoss: 1.088325\n",
      "Train Epoch: 17 [5760/8272 (69%)]\tLoss: 1.352841\n",
      "Train Epoch: 17 [6400/8272 (77%)]\tLoss: 1.380907\n",
      "Train Epoch: 17 [7040/8272 (85%)]\tLoss: 1.275099\n",
      "Train Epoch: 17 [7680/8272 (92%)]\tLoss: 1.407807\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/5100 (0%)]\tLoss: 1.259044\n",
      "Train Epoch: 1 [640/5100 (12%)]\tLoss: 1.420162\n",
      "Train Epoch: 1 [1280/5100 (25%)]\tLoss: 1.336977\n",
      "Train Epoch: 1 [1920/5100 (38%)]\tLoss: 1.231169\n",
      "Train Epoch: 1 [2560/5100 (50%)]\tLoss: 1.357084\n",
      "Train Epoch: 1 [3200/5100 (62%)]\tLoss: 1.150057\n",
      "Train Epoch: 1 [3840/5100 (75%)]\tLoss: 1.305364\n",
      "Train Epoch: 1 [4480/5100 (88%)]\tLoss: 1.277818\n",
      "Train Epoch: 2 [0/5100 (0%)]\tLoss: 1.373630\n",
      "Train Epoch: 2 [640/5100 (12%)]\tLoss: 1.392366\n",
      "Train Epoch: 2 [1280/5100 (25%)]\tLoss: 1.275296\n",
      "Train Epoch: 2 [1920/5100 (38%)]\tLoss: 1.120541\n",
      "Train Epoch: 2 [2560/5100 (50%)]\tLoss: 1.354968\n",
      "Train Epoch: 2 [3200/5100 (62%)]\tLoss: 1.089723\n",
      "Train Epoch: 2 [3840/5100 (75%)]\tLoss: 1.310414\n",
      "Train Epoch: 2 [4480/5100 (88%)]\tLoss: 1.093616\n",
      "Train Epoch: 3 [0/5100 (0%)]\tLoss: 1.282104\n",
      "Train Epoch: 3 [640/5100 (12%)]\tLoss: 1.169805\n",
      "Train Epoch: 3 [1280/5100 (25%)]\tLoss: 1.289943\n",
      "Train Epoch: 3 [1920/5100 (38%)]\tLoss: 1.284600\n",
      "Train Epoch: 3 [2560/5100 (50%)]\tLoss: 1.204488\n",
      "Train Epoch: 3 [3200/5100 (62%)]\tLoss: 0.974338\n",
      "Train Epoch: 3 [3840/5100 (75%)]\tLoss: 1.345636\n",
      "Train Epoch: 3 [4480/5100 (88%)]\tLoss: 1.274521\n",
      "Train Epoch: 4 [0/5100 (0%)]\tLoss: 1.025542\n",
      "Train Epoch: 4 [640/5100 (12%)]\tLoss: 1.202895\n",
      "Train Epoch: 4 [1280/5100 (25%)]\tLoss: 1.309760\n",
      "Train Epoch: 4 [1920/5100 (38%)]\tLoss: 1.129507\n",
      "Train Epoch: 4 [2560/5100 (50%)]\tLoss: 1.135420\n",
      "Train Epoch: 4 [3200/5100 (62%)]\tLoss: 1.159033\n",
      "Train Epoch: 4 [3840/5100 (75%)]\tLoss: 1.301665\n",
      "Train Epoch: 4 [4480/5100 (88%)]\tLoss: 1.308267\n",
      "Train Epoch: 5 [0/5100 (0%)]\tLoss: 1.314904\n",
      "Train Epoch: 5 [640/5100 (12%)]\tLoss: 1.057555\n",
      "Train Epoch: 5 [1280/5100 (25%)]\tLoss: 1.099855\n",
      "Train Epoch: 5 [1920/5100 (38%)]\tLoss: 1.395169\n",
      "Train Epoch: 5 [2560/5100 (50%)]\tLoss: 1.471571\n",
      "Train Epoch: 5 [3200/5100 (62%)]\tLoss: 1.457688\n",
      "Train Epoch: 5 [3840/5100 (75%)]\tLoss: 1.069316\n",
      "Train Epoch: 5 [4480/5100 (88%)]\tLoss: 1.271354\n",
      "Train Epoch: 6 [0/5100 (0%)]\tLoss: 1.191544\n",
      "Train Epoch: 6 [640/5100 (12%)]\tLoss: 1.213087\n",
      "Train Epoch: 6 [1280/5100 (25%)]\tLoss: 1.186919\n",
      "Train Epoch: 6 [1920/5100 (38%)]\tLoss: 1.106371\n",
      "Train Epoch: 6 [2560/5100 (50%)]\tLoss: 1.232174\n",
      "Train Epoch: 6 [3200/5100 (62%)]\tLoss: 1.176248\n",
      "Train Epoch: 6 [3840/5100 (75%)]\tLoss: 1.023441\n",
      "Train Epoch: 6 [4480/5100 (88%)]\tLoss: 1.395070\n",
      "Train Epoch: 7 [0/5100 (0%)]\tLoss: 1.041866\n",
      "Train Epoch: 7 [640/5100 (12%)]\tLoss: 1.181179\n",
      "Train Epoch: 7 [1280/5100 (25%)]\tLoss: 1.254394\n",
      "Train Epoch: 7 [1920/5100 (38%)]\tLoss: 1.271821\n",
      "Train Epoch: 7 [2560/5100 (50%)]\tLoss: 1.329354\n",
      "Train Epoch: 7 [3200/5100 (62%)]\tLoss: 1.324111\n",
      "Train Epoch: 7 [3840/5100 (75%)]\tLoss: 1.299672\n",
      "Train Epoch: 7 [4480/5100 (88%)]\tLoss: 1.241823\n",
      "Train Epoch: 8 [0/5100 (0%)]\tLoss: 1.098326\n",
      "Train Epoch: 8 [640/5100 (12%)]\tLoss: 1.087285\n",
      "Train Epoch: 8 [1280/5100 (25%)]\tLoss: 1.159559\n",
      "Train Epoch: 8 [1920/5100 (38%)]\tLoss: 1.188428\n",
      "Train Epoch: 8 [2560/5100 (50%)]\tLoss: 1.003572\n",
      "Train Epoch: 8 [3200/5100 (62%)]\tLoss: 1.189294\n",
      "Train Epoch: 8 [3840/5100 (75%)]\tLoss: 1.240755\n",
      "Train Epoch: 8 [4480/5100 (88%)]\tLoss: 1.034931\n",
      "Train Epoch: 9 [0/5100 (0%)]\tLoss: 1.382765\n",
      "Train Epoch: 9 [640/5100 (12%)]\tLoss: 1.169153\n",
      "Train Epoch: 9 [1280/5100 (25%)]\tLoss: 1.259295\n",
      "Train Epoch: 9 [1920/5100 (38%)]\tLoss: 1.036294\n",
      "Train Epoch: 9 [2560/5100 (50%)]\tLoss: 1.188280\n",
      "Train Epoch: 9 [3200/5100 (62%)]\tLoss: 1.164229\n",
      "Train Epoch: 9 [3840/5100 (75%)]\tLoss: 1.197601\n",
      "Train Epoch: 9 [4480/5100 (88%)]\tLoss: 1.196729\n",
      "Train Epoch: 10 [0/5100 (0%)]\tLoss: 1.265357\n",
      "Train Epoch: 10 [640/5100 (12%)]\tLoss: 1.127158\n",
      "Train Epoch: 10 [1280/5100 (25%)]\tLoss: 1.208053\n",
      "Train Epoch: 10 [1920/5100 (38%)]\tLoss: 0.912686\n",
      "Train Epoch: 10 [2560/5100 (50%)]\tLoss: 1.192376\n",
      "Train Epoch: 10 [3200/5100 (62%)]\tLoss: 1.220288\n",
      "Train Epoch: 10 [3840/5100 (75%)]\tLoss: 1.397312\n",
      "Train Epoch: 10 [4480/5100 (88%)]\tLoss: 1.490227\n",
      "Train Epoch: 11 [0/5100 (0%)]\tLoss: 1.067800\n",
      "Train Epoch: 11 [640/5100 (12%)]\tLoss: 1.215770\n",
      "Train Epoch: 11 [1280/5100 (25%)]\tLoss: 1.076236\n",
      "Train Epoch: 11 [1920/5100 (38%)]\tLoss: 1.327761\n",
      "Train Epoch: 11 [2560/5100 (50%)]\tLoss: 1.239864\n",
      "Train Epoch: 11 [3200/5100 (62%)]\tLoss: 1.242933\n",
      "Train Epoch: 11 [3840/5100 (75%)]\tLoss: 1.148951\n",
      "Train Epoch: 11 [4480/5100 (88%)]\tLoss: 1.293499\n",
      "Train Epoch: 12 [0/5100 (0%)]\tLoss: 1.280851\n",
      "Train Epoch: 12 [640/5100 (12%)]\tLoss: 1.283520\n",
      "Train Epoch: 12 [1280/5100 (25%)]\tLoss: 1.165708\n",
      "Train Epoch: 12 [1920/5100 (38%)]\tLoss: 1.347499\n",
      "Train Epoch: 12 [2560/5100 (50%)]\tLoss: 1.281663\n",
      "Train Epoch: 12 [3200/5100 (62%)]\tLoss: 1.117028\n",
      "Train Epoch: 12 [3840/5100 (75%)]\tLoss: 1.083108\n",
      "Train Epoch: 12 [4480/5100 (88%)]\tLoss: 1.112376\n",
      "Train Epoch: 13 [0/5100 (0%)]\tLoss: 1.077690\n",
      "Train Epoch: 13 [640/5100 (12%)]\tLoss: 1.153133\n",
      "Train Epoch: 13 [1280/5100 (25%)]\tLoss: 1.214102\n",
      "Train Epoch: 13 [1920/5100 (38%)]\tLoss: 1.038440\n",
      "Train Epoch: 13 [2560/5100 (50%)]\tLoss: 1.441754\n",
      "Train Epoch: 13 [3200/5100 (62%)]\tLoss: 1.035998\n",
      "Train Epoch: 13 [3840/5100 (75%)]\tLoss: 1.305921\n",
      "Train Epoch: 13 [4480/5100 (88%)]\tLoss: 1.283513\n",
      "Train Epoch: 14 [0/5100 (0%)]\tLoss: 1.144471\n",
      "Train Epoch: 14 [640/5100 (12%)]\tLoss: 1.066760\n",
      "Train Epoch: 14 [1280/5100 (25%)]\tLoss: 1.093015\n",
      "Train Epoch: 14 [1920/5100 (38%)]\tLoss: 1.094431\n",
      "Train Epoch: 14 [2560/5100 (50%)]\tLoss: 1.122210\n",
      "Train Epoch: 14 [3200/5100 (62%)]\tLoss: 1.045987\n",
      "Train Epoch: 14 [3840/5100 (75%)]\tLoss: 1.513255\n",
      "Train Epoch: 14 [4480/5100 (88%)]\tLoss: 1.249792\n",
      "Train Epoch: 15 [0/5100 (0%)]\tLoss: 1.160470\n",
      "Train Epoch: 15 [640/5100 (12%)]\tLoss: 1.471723\n",
      "Train Epoch: 15 [1280/5100 (25%)]\tLoss: 1.280871\n",
      "Train Epoch: 15 [1920/5100 (38%)]\tLoss: 1.136387\n",
      "Train Epoch: 15 [2560/5100 (50%)]\tLoss: 1.125814\n",
      "Train Epoch: 15 [3200/5100 (62%)]\tLoss: 1.306999\n",
      "Train Epoch: 15 [3840/5100 (75%)]\tLoss: 1.188036\n",
      "Train Epoch: 15 [4480/5100 (88%)]\tLoss: 1.071298\n",
      "Train Epoch: 16 [0/5100 (0%)]\tLoss: 1.047704\n",
      "Train Epoch: 16 [640/5100 (12%)]\tLoss: 1.118034\n",
      "Train Epoch: 16 [1280/5100 (25%)]\tLoss: 1.154907\n",
      "Train Epoch: 16 [1920/5100 (38%)]\tLoss: 1.011230\n",
      "Train Epoch: 16 [2560/5100 (50%)]\tLoss: 1.316924\n",
      "Train Epoch: 16 [3200/5100 (62%)]\tLoss: 0.859663\n",
      "Train Epoch: 16 [3840/5100 (75%)]\tLoss: 1.060679\n",
      "Train Epoch: 16 [4480/5100 (88%)]\tLoss: 1.214191\n",
      "Train Epoch: 17 [0/5100 (0%)]\tLoss: 1.224476\n",
      "Train Epoch: 17 [640/5100 (12%)]\tLoss: 1.224571\n",
      "Train Epoch: 17 [1280/5100 (25%)]\tLoss: 1.101791\n",
      "Train Epoch: 17 [1920/5100 (38%)]\tLoss: 1.057919\n",
      "Train Epoch: 17 [2560/5100 (50%)]\tLoss: 1.204252\n",
      "Train Epoch: 17 [3200/5100 (62%)]\tLoss: 1.324838\n",
      "Train Epoch: 17 [3840/5100 (75%)]\tLoss: 1.168607\n",
      "Train Epoch: 17 [4480/5100 (88%)]\tLoss: 1.099174\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.2012, Accuracy: 5723/10000 (57%)\n",
      "\n",
      "Round 3/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.526382\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.383284\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.091146\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.565242\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.198783\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.115974\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.520906\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.369639\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.049851\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 1.189729\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.337179\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.048994\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.057343\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.346034\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.205004\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.086844\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.682500\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 0.991145\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.076409\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.287633\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.245304\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.130094\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.201437\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.231568\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.092145\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 1.290147\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.103188\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.197012\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.337326\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 0.952440\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.395117\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.128501\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.183979\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.164070\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.081564\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.192235\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.044640\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.214216\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.047649\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.186872\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 1.211380\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 1.197849\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 0.998793\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 0.908821\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.207484\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 0.956004\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 1.354985\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 0.999021\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.320387\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.090729\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 0.996835\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 1.122746\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 0.940542\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 1.276415\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 1.178314\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 0.844121\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 0.968830\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 1.195530\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.267398\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.126907\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.349907\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 1.173553\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 1.233205\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.089212\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.052922\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 1.077012\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 1.240907\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.256366\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 1.280209\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 1.099459\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.237096\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 1.273614\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 0.886393\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 0.980569\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.297088\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 1.122898\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.234537\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.155438\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 1.264676\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 0.976499\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.130728\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 1.251926\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.242079\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.051618\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 0.899164\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 1.008080\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.381744\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 1.326706\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.040989\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.065711\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.388034\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.350325\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 0.941903\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 0.980997\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 0.909507\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 1.223019\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.349294\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 1.011919\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 1.003944\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 1.044480\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 0.913770\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 1.084584\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 1.098604\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 1.248859\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 1.184001\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 1.170112\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 1.036819\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.054823\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 1.160696\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 1.142179\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.077569\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 0.996513\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 1.175583\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 1.159154\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 0.985536\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 0.947924\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 1.128998\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.023293\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 0.929531\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.294138\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.363186\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.398872\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.472777\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.181847\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 0.860362\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.254781\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.347277\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.407803\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.211289\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.289026\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.225114\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.014567\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.416880\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.345147\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.314783\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.130005\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.434468\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.458910\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 1.019445\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.242719\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.203343\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.048183\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.207780\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.383419\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.263161\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.179669\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 0.825486\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.164435\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.242403\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.017511\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.362261\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.247449\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.106988\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.129698\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.143552\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.283587\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 0.954759\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.186895\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.204948\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.358481\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.202287\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.159041\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.597329\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.036676\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.233292\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.222922\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.318660\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.015761\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.137742\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 0.940510\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.178439\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.055259\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.278904\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 1.098659\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.098804\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.177710\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.081017\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.243813\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 0.939893\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.360530\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.054958\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.140223\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.035316\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.014581\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.368556\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.209875\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.338609\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.245053\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.256107\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.129553\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 0.949524\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.358338\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.066910\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.093060\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.139297\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.077085\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 0.892966\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 1.130037\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.112845\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.092073\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.136519\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.380288\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.184077\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.008209\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.152498\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 1.347177\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.127481\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.186851\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.229754\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.336312\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.359995\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.323731\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.133023\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 1.078723\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 1.214437\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.142653\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.328669\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.230329\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.181411\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.052643\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.233150\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 0.998102\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.114965\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 1.397947\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.386924\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 0.938906\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.034412\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.099674\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.119295\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.152734\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 0.936995\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 0.970490\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.122089\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.162284\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.144307\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 0.950376\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.157357\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 1.164138\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.028109\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.089358\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.240214\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 1.324025\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 0.829468\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.259127\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.336233\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 1.216394\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.269119\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 0.895308\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.163604\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.073745\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.198289\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 0.990520\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 1.106267\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 0.995480\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 0.991363\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.571664\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.397110\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.376496\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.440882\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.277899\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.218169\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.247380\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.324657\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.007224\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.485906\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.384485\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.376702\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.202744\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.305578\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.321599\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.240422\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.312279\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.093736\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.216188\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.348526\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.271523\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.324049\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.209607\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.210597\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.353276\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.235602\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.308950\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.236662\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.283768\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.337801\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.164237\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.195434\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.620374\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.511580\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.175644\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.256481\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.231162\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.146744\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.216741\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.229333\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.083638\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.407323\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.394128\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.048967\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.441672\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.182434\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.234806\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.225050\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.196591\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.029635\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.072306\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.326504\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.458823\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.301986\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.404711\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.424730\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.256568\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.294180\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.168786\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 0.904858\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.144848\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.241846\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.117319\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.146426\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.234685\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.245061\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.334908\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.148716\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.004991\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.165435\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.266573\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.573910\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.158048\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.044974\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.352372\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.343406\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.068236\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.098662\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.706497\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 1.331214\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.104182\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.267692\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.455019\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.030076\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.037487\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.349946\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.166599\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.349763\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.230729\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.212755\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.189003\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.213981\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.278071\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 0.937975\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.084410\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.178517\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.232581\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.113398\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.273446\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 1.340437\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.089501\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.290562\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.128507\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.148466\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 1.207639\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.199995\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.255975\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.216853\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.314120\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.442523\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 1.294679\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.138300\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.303755\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.164095\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.288070\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.144293\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 0.945175\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.235497\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.386195\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.367314\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.204458\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.299351\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.308085\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.184083\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.333047\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.246227\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.494143\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.278451\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.391984\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.169613\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 0.978236\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.331391\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.464810\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.212862\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.127993\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.130465\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.235561\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.109089\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.140234\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.173630\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 1.240510\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 1.311001\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.331085\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 1.090614\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.132013\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.206462\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.236509\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.304106\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.346988\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.457141\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 1.159369\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.257315\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.096540\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 1.132395\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.198997\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.182032\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.231742\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.273341\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.321212\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.169581\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.178487\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.266134\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 1.026709\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.248495\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.420276\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.245185\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.178473\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 0.983354\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.003424\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.021637\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.017450\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 1.035232\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 1.355738\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 1.052498\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 1.142762\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 0.956701\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 0.978050\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 0.862329\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 0.874808\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 0.729596\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 1.120885\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 0.853193\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 0.785545\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 0.985363\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 0.761500\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 1.193554\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 0.984876\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.822494\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.943620\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.806399\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 1.063662\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 0.817430\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 1.133038\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 1.140884\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 1.249550\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 1.025563\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 1.111327\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 0.948624\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 0.891751\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 0.971144\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 0.823839\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 0.768249\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.832129\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 0.914610\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 0.800212\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 0.995757\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.996200\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 0.785934\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.956034\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 1.042256\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 0.910843\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.945192\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.959374\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.903793\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 0.807446\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 0.922434\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.847294\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.892416\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 0.647034\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 0.776578\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.806304\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 0.969035\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 1.347986\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 0.738485\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 0.737167\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.898690\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 0.743893\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 1.069650\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.912018\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.801599\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.814316\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 0.627989\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 0.940664\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.885178\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.876351\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.752167\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.697953\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.553349\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.888389\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.654700\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 1.083830\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.884536\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 1.085613\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.708466\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.603804\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.766571\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.884376\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 1.017857\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 1.157944\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.820320\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.951150\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.993702\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.835144\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.802627\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.796388\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 0.773921\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.725797\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.899054\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 1.023767\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.837291\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.855132\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.746863\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.784739\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.928385\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.916128\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.875521\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.795492\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 0.905975\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.844625\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 1.259469\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.674518\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.821057\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.404458\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.185855\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.329459\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 0.883089\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 1.294010\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.058645\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.139385\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 1.150327\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 1.082167\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 1.423644\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 0.989866\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 0.931177\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.057899\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.271602\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.342885\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 0.945102\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 1.032288\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 0.989767\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 1.384308\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 1.133377\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.357193\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 1.166557\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.266880\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 0.782475\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 1.033321\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.102649\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 0.918121\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 1.106678\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 1.203952\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 0.980620\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 1.279814\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 1.142047\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 1.096056\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 0.892617\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.197636\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 0.970124\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 1.274850\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.809090\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.143365\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 1.040908\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 0.763362\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 0.877972\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 1.135975\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 0.927817\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 0.907047\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 1.027015\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 0.895620\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 0.960800\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 1.053241\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 1.142942\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 1.155175\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 1.146772\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 1.122392\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 1.018641\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 1.228907\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 1.101204\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 0.996905\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 0.963500\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 1.004653\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 1.104074\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 1.041397\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 1.209329\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 1.225162\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 1.186397\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 1.215443\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 0.900058\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 0.996566\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 1.003619\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.949398\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 1.213798\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 0.764511\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 1.060326\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 1.402506\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 1.085546\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 0.956616\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 1.245142\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 0.988811\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 1.205207\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 0.973617\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 1.155225\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 0.963111\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 1.117547\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 0.985046\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 1.103490\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 1.112973\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 0.951531\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 0.959218\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 1.040242\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 1.093352\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 0.991352\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 0.982188\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 1.078404\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 1.089384\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 0.881725\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 0.996776\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 0.768243\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 0.966983\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 0.911600\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 0.911921\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 0.980934\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 0.994032\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 1.090197\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.286805\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.335197\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.409652\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.113190\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.375317\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.299078\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.118566\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.358562\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.037014\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.340466\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.214294\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.223929\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.296329\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.235918\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.181771\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.280177\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.155285\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.062976\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.094862\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.255706\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.107227\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.205640\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.435927\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 1.165194\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.261295\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.272348\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.265998\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.147875\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.276255\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.034272\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.405016\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.052415\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.302447\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.211890\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.199624\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.200476\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.234199\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.156429\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.210101\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.331555\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.112285\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.089718\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.131660\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.428277\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 1.248068\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.512232\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.337242\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.187166\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.030572\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.305477\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.346991\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.199093\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.488894\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.178926\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.055679\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.277126\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.286195\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.287817\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.394979\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.141353\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.159747\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.089171\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.192303\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.246740\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 0.994848\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.154701\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.511065\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.348269\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.403750\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 0.998765\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.054084\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.405428\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.092949\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 0.893042\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.101147\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 0.968717\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 0.999956\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.146275\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.232243\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.468172\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.152487\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.088711\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.093541\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.052034\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.084511\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.263201\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.011284\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.232238\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.104483\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 0.969281\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.273894\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.062660\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.035347\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 0.977650\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 1.182656\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 0.975704\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.131709\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.261821\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.056606\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.418163\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.195215\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.099383\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.183939\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.089875\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.060836\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.083402\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 1.094496\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 1.261993\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.122725\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 1.224703\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 0.900047\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.158572\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 1.100694\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 1.101928\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.172810\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 1.313792\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.141362\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.255107\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 1.068544\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.144234\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.123988\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 0.999905\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.148383\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.224342\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.142427\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.262567\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.030403\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.157521\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.214055\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.082285\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.445366\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.107212\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 0.982137\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.114039\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.120178\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 0.956423\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/5548 (0%)]\tLoss: 1.454707\n",
      "Train Epoch: 1 [640/5548 (11%)]\tLoss: 1.193035\n",
      "Train Epoch: 1 [1280/5548 (23%)]\tLoss: 1.355348\n",
      "Train Epoch: 1 [1920/5548 (34%)]\tLoss: 0.859076\n",
      "Train Epoch: 1 [2560/5548 (46%)]\tLoss: 1.146500\n",
      "Train Epoch: 1 [3200/5548 (57%)]\tLoss: 1.281133\n",
      "Train Epoch: 1 [3840/5548 (69%)]\tLoss: 1.146152\n",
      "Train Epoch: 1 [4480/5548 (80%)]\tLoss: 1.060226\n",
      "Train Epoch: 1 [5120/5548 (92%)]\tLoss: 1.135871\n",
      "Train Epoch: 2 [0/5548 (0%)]\tLoss: 1.235145\n",
      "Train Epoch: 2 [640/5548 (11%)]\tLoss: 1.151754\n",
      "Train Epoch: 2 [1280/5548 (23%)]\tLoss: 1.409023\n",
      "Train Epoch: 2 [1920/5548 (34%)]\tLoss: 1.132388\n",
      "Train Epoch: 2 [2560/5548 (46%)]\tLoss: 1.305993\n",
      "Train Epoch: 2 [3200/5548 (57%)]\tLoss: 1.042740\n",
      "Train Epoch: 2 [3840/5548 (69%)]\tLoss: 0.970822\n",
      "Train Epoch: 2 [4480/5548 (80%)]\tLoss: 1.206644\n",
      "Train Epoch: 2 [5120/5548 (92%)]\tLoss: 1.290009\n",
      "Train Epoch: 3 [0/5548 (0%)]\tLoss: 0.994168\n",
      "Train Epoch: 3 [640/5548 (11%)]\tLoss: 1.155487\n",
      "Train Epoch: 3 [1280/5548 (23%)]\tLoss: 1.179095\n",
      "Train Epoch: 3 [1920/5548 (34%)]\tLoss: 1.193565\n",
      "Train Epoch: 3 [2560/5548 (46%)]\tLoss: 1.064953\n",
      "Train Epoch: 3 [3200/5548 (57%)]\tLoss: 1.064130\n",
      "Train Epoch: 3 [3840/5548 (69%)]\tLoss: 1.209280\n",
      "Train Epoch: 3 [4480/5548 (80%)]\tLoss: 0.937140\n",
      "Train Epoch: 3 [5120/5548 (92%)]\tLoss: 1.310983\n",
      "Train Epoch: 4 [0/5548 (0%)]\tLoss: 1.401632\n",
      "Train Epoch: 4 [640/5548 (11%)]\tLoss: 1.303442\n",
      "Train Epoch: 4 [1280/5548 (23%)]\tLoss: 0.990042\n",
      "Train Epoch: 4 [1920/5548 (34%)]\tLoss: 0.945559\n",
      "Train Epoch: 4 [2560/5548 (46%)]\tLoss: 1.113386\n",
      "Train Epoch: 4 [3200/5548 (57%)]\tLoss: 1.177518\n",
      "Train Epoch: 4 [3840/5548 (69%)]\tLoss: 1.079938\n",
      "Train Epoch: 4 [4480/5548 (80%)]\tLoss: 1.238292\n",
      "Train Epoch: 4 [5120/5548 (92%)]\tLoss: 0.885779\n",
      "Train Epoch: 5 [0/5548 (0%)]\tLoss: 1.035974\n",
      "Train Epoch: 5 [640/5548 (11%)]\tLoss: 1.313629\n",
      "Train Epoch: 5 [1280/5548 (23%)]\tLoss: 1.088723\n",
      "Train Epoch: 5 [1920/5548 (34%)]\tLoss: 1.440057\n",
      "Train Epoch: 5 [2560/5548 (46%)]\tLoss: 0.947432\n",
      "Train Epoch: 5 [3200/5548 (57%)]\tLoss: 1.049525\n",
      "Train Epoch: 5 [3840/5548 (69%)]\tLoss: 0.980860\n",
      "Train Epoch: 5 [4480/5548 (80%)]\tLoss: 1.333208\n",
      "Train Epoch: 5 [5120/5548 (92%)]\tLoss: 0.850343\n",
      "Train Epoch: 6 [0/5548 (0%)]\tLoss: 1.220406\n",
      "Train Epoch: 6 [640/5548 (11%)]\tLoss: 1.265311\n",
      "Train Epoch: 6 [1280/5548 (23%)]\tLoss: 1.213459\n",
      "Train Epoch: 6 [1920/5548 (34%)]\tLoss: 1.117626\n",
      "Train Epoch: 6 [2560/5548 (46%)]\tLoss: 1.201603\n",
      "Train Epoch: 6 [3200/5548 (57%)]\tLoss: 1.300456\n",
      "Train Epoch: 6 [3840/5548 (69%)]\tLoss: 1.180946\n",
      "Train Epoch: 6 [4480/5548 (80%)]\tLoss: 1.006556\n",
      "Train Epoch: 6 [5120/5548 (92%)]\tLoss: 1.246832\n",
      "Train Epoch: 7 [0/5548 (0%)]\tLoss: 1.054460\n",
      "Train Epoch: 7 [640/5548 (11%)]\tLoss: 0.896098\n",
      "Train Epoch: 7 [1280/5548 (23%)]\tLoss: 1.003368\n",
      "Train Epoch: 7 [1920/5548 (34%)]\tLoss: 1.204396\n",
      "Train Epoch: 7 [2560/5548 (46%)]\tLoss: 1.397069\n",
      "Train Epoch: 7 [3200/5548 (57%)]\tLoss: 0.718732\n",
      "Train Epoch: 7 [3840/5548 (69%)]\tLoss: 1.055419\n",
      "Train Epoch: 7 [4480/5548 (80%)]\tLoss: 1.238386\n",
      "Train Epoch: 7 [5120/5548 (92%)]\tLoss: 1.104413\n",
      "Train Epoch: 8 [0/5548 (0%)]\tLoss: 1.431814\n",
      "Train Epoch: 8 [640/5548 (11%)]\tLoss: 1.212321\n",
      "Train Epoch: 8 [1280/5548 (23%)]\tLoss: 0.992091\n",
      "Train Epoch: 8 [1920/5548 (34%)]\tLoss: 0.761980\n",
      "Train Epoch: 8 [2560/5548 (46%)]\tLoss: 1.068381\n",
      "Train Epoch: 8 [3200/5548 (57%)]\tLoss: 1.073288\n",
      "Train Epoch: 8 [3840/5548 (69%)]\tLoss: 0.967670\n",
      "Train Epoch: 8 [4480/5548 (80%)]\tLoss: 1.021323\n",
      "Train Epoch: 8 [5120/5548 (92%)]\tLoss: 0.985450\n",
      "Train Epoch: 9 [0/5548 (0%)]\tLoss: 1.138592\n",
      "Train Epoch: 9 [640/5548 (11%)]\tLoss: 1.164765\n",
      "Train Epoch: 9 [1280/5548 (23%)]\tLoss: 0.979178\n",
      "Train Epoch: 9 [1920/5548 (34%)]\tLoss: 1.009025\n",
      "Train Epoch: 9 [2560/5548 (46%)]\tLoss: 0.818428\n",
      "Train Epoch: 9 [3200/5548 (57%)]\tLoss: 0.972910\n",
      "Train Epoch: 9 [3840/5548 (69%)]\tLoss: 1.296498\n",
      "Train Epoch: 9 [4480/5548 (80%)]\tLoss: 1.026759\n",
      "Train Epoch: 9 [5120/5548 (92%)]\tLoss: 1.059130\n",
      "Train Epoch: 10 [0/5548 (0%)]\tLoss: 1.112039\n",
      "Train Epoch: 10 [640/5548 (11%)]\tLoss: 1.126541\n",
      "Train Epoch: 10 [1280/5548 (23%)]\tLoss: 1.164322\n",
      "Train Epoch: 10 [1920/5548 (34%)]\tLoss: 1.373037\n",
      "Train Epoch: 10 [2560/5548 (46%)]\tLoss: 1.071584\n",
      "Train Epoch: 10 [3200/5548 (57%)]\tLoss: 1.142833\n",
      "Train Epoch: 10 [3840/5548 (69%)]\tLoss: 0.954424\n",
      "Train Epoch: 10 [4480/5548 (80%)]\tLoss: 1.057378\n",
      "Train Epoch: 10 [5120/5548 (92%)]\tLoss: 0.851804\n",
      "Train Epoch: 11 [0/5548 (0%)]\tLoss: 1.058058\n",
      "Train Epoch: 11 [640/5548 (11%)]\tLoss: 1.009110\n",
      "Train Epoch: 11 [1280/5548 (23%)]\tLoss: 1.130810\n",
      "Train Epoch: 11 [1920/5548 (34%)]\tLoss: 0.910827\n",
      "Train Epoch: 11 [2560/5548 (46%)]\tLoss: 0.904830\n",
      "Train Epoch: 11 [3200/5548 (57%)]\tLoss: 1.127666\n",
      "Train Epoch: 11 [3840/5548 (69%)]\tLoss: 0.878037\n",
      "Train Epoch: 11 [4480/5548 (80%)]\tLoss: 0.946720\n",
      "Train Epoch: 11 [5120/5548 (92%)]\tLoss: 1.152888\n",
      "Train Epoch: 12 [0/5548 (0%)]\tLoss: 1.031146\n",
      "Train Epoch: 12 [640/5548 (11%)]\tLoss: 1.171468\n",
      "Train Epoch: 12 [1280/5548 (23%)]\tLoss: 0.928110\n",
      "Train Epoch: 12 [1920/5548 (34%)]\tLoss: 1.162441\n",
      "Train Epoch: 12 [2560/5548 (46%)]\tLoss: 0.972365\n",
      "Train Epoch: 12 [3200/5548 (57%)]\tLoss: 1.098012\n",
      "Train Epoch: 12 [3840/5548 (69%)]\tLoss: 0.864284\n",
      "Train Epoch: 12 [4480/5548 (80%)]\tLoss: 1.108242\n",
      "Train Epoch: 12 [5120/5548 (92%)]\tLoss: 1.018010\n",
      "Train Epoch: 13 [0/5548 (0%)]\tLoss: 1.083270\n",
      "Train Epoch: 13 [640/5548 (11%)]\tLoss: 1.161289\n",
      "Train Epoch: 13 [1280/5548 (23%)]\tLoss: 1.108279\n",
      "Train Epoch: 13 [1920/5548 (34%)]\tLoss: 1.286064\n",
      "Train Epoch: 13 [2560/5548 (46%)]\tLoss: 1.272512\n",
      "Train Epoch: 13 [3200/5548 (57%)]\tLoss: 1.162019\n",
      "Train Epoch: 13 [3840/5548 (69%)]\tLoss: 1.392482\n",
      "Train Epoch: 13 [4480/5548 (80%)]\tLoss: 1.195670\n",
      "Train Epoch: 13 [5120/5548 (92%)]\tLoss: 1.161278\n",
      "Train Epoch: 14 [0/5548 (0%)]\tLoss: 0.937734\n",
      "Train Epoch: 14 [640/5548 (11%)]\tLoss: 1.031983\n",
      "Train Epoch: 14 [1280/5548 (23%)]\tLoss: 1.119961\n",
      "Train Epoch: 14 [1920/5548 (34%)]\tLoss: 0.965963\n",
      "Train Epoch: 14 [2560/5548 (46%)]\tLoss: 0.977879\n",
      "Train Epoch: 14 [3200/5548 (57%)]\tLoss: 0.975025\n",
      "Train Epoch: 14 [3840/5548 (69%)]\tLoss: 1.063722\n",
      "Train Epoch: 14 [4480/5548 (80%)]\tLoss: 1.107594\n",
      "Train Epoch: 14 [5120/5548 (92%)]\tLoss: 0.777039\n",
      "Train Epoch: 15 [0/5548 (0%)]\tLoss: 0.913977\n",
      "Train Epoch: 15 [640/5548 (11%)]\tLoss: 0.999558\n",
      "Train Epoch: 15 [1280/5548 (23%)]\tLoss: 1.067773\n",
      "Train Epoch: 15 [1920/5548 (34%)]\tLoss: 0.825499\n",
      "Train Epoch: 15 [2560/5548 (46%)]\tLoss: 1.152863\n",
      "Train Epoch: 15 [3200/5548 (57%)]\tLoss: 0.882131\n",
      "Train Epoch: 15 [3840/5548 (69%)]\tLoss: 1.211509\n",
      "Train Epoch: 15 [4480/5548 (80%)]\tLoss: 1.189359\n",
      "Train Epoch: 15 [5120/5548 (92%)]\tLoss: 0.991226\n",
      "Train Epoch: 16 [0/5548 (0%)]\tLoss: 0.969899\n",
      "Train Epoch: 16 [640/5548 (11%)]\tLoss: 0.776706\n",
      "Train Epoch: 16 [1280/5548 (23%)]\tLoss: 0.853776\n",
      "Train Epoch: 16 [1920/5548 (34%)]\tLoss: 0.960672\n",
      "Train Epoch: 16 [2560/5548 (46%)]\tLoss: 1.211731\n",
      "Train Epoch: 16 [3200/5548 (57%)]\tLoss: 1.160319\n",
      "Train Epoch: 16 [3840/5548 (69%)]\tLoss: 0.914102\n",
      "Train Epoch: 16 [4480/5548 (80%)]\tLoss: 0.945963\n",
      "Train Epoch: 16 [5120/5548 (92%)]\tLoss: 1.236933\n",
      "Train Epoch: 17 [0/5548 (0%)]\tLoss: 1.178396\n",
      "Train Epoch: 17 [640/5548 (11%)]\tLoss: 1.064268\n",
      "Train Epoch: 17 [1280/5548 (23%)]\tLoss: 1.043445\n",
      "Train Epoch: 17 [1920/5548 (34%)]\tLoss: 1.079490\n",
      "Train Epoch: 17 [2560/5548 (46%)]\tLoss: 0.840016\n",
      "Train Epoch: 17 [3200/5548 (57%)]\tLoss: 1.124844\n",
      "Train Epoch: 17 [3840/5548 (69%)]\tLoss: 1.175429\n",
      "Train Epoch: 17 [4480/5548 (80%)]\tLoss: 1.030660\n",
      "Train Epoch: 17 [5120/5548 (92%)]\tLoss: 1.278845\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/4097 (0%)]\tLoss: 1.576700\n",
      "Train Epoch: 1 [640/4097 (15%)]\tLoss: 1.310975\n",
      "Train Epoch: 1 [1280/4097 (31%)]\tLoss: 1.358994\n",
      "Train Epoch: 1 [1920/4097 (46%)]\tLoss: 1.287898\n",
      "Train Epoch: 1 [2560/4097 (62%)]\tLoss: 1.171556\n",
      "Train Epoch: 1 [3200/4097 (77%)]\tLoss: 1.069921\n",
      "Train Epoch: 1 [3840/4097 (92%)]\tLoss: 1.291643\n",
      "Train Epoch: 2 [0/4097 (0%)]\tLoss: 1.486835\n",
      "Train Epoch: 2 [640/4097 (15%)]\tLoss: 1.515154\n",
      "Train Epoch: 2 [1280/4097 (31%)]\tLoss: 1.429234\n",
      "Train Epoch: 2 [1920/4097 (46%)]\tLoss: 1.242855\n",
      "Train Epoch: 2 [2560/4097 (62%)]\tLoss: 1.490204\n",
      "Train Epoch: 2 [3200/4097 (77%)]\tLoss: 1.042174\n",
      "Train Epoch: 2 [3840/4097 (92%)]\tLoss: 1.376603\n",
      "Train Epoch: 3 [0/4097 (0%)]\tLoss: 1.905480\n",
      "Train Epoch: 3 [640/4097 (15%)]\tLoss: 1.901018\n",
      "Train Epoch: 3 [1280/4097 (31%)]\tLoss: 1.532783\n",
      "Train Epoch: 3 [1920/4097 (46%)]\tLoss: 1.676767\n",
      "Train Epoch: 3 [2560/4097 (62%)]\tLoss: 1.449604\n",
      "Train Epoch: 3 [3200/4097 (77%)]\tLoss: 1.490172\n",
      "Train Epoch: 3 [3840/4097 (92%)]\tLoss: 1.260216\n",
      "Train Epoch: 4 [0/4097 (0%)]\tLoss: 1.389504\n",
      "Train Epoch: 4 [640/4097 (15%)]\tLoss: 1.716723\n",
      "Train Epoch: 4 [1280/4097 (31%)]\tLoss: 1.294387\n",
      "Train Epoch: 4 [1920/4097 (46%)]\tLoss: 1.395253\n",
      "Train Epoch: 4 [2560/4097 (62%)]\tLoss: 1.416051\n",
      "Train Epoch: 4 [3200/4097 (77%)]\tLoss: 1.349925\n",
      "Train Epoch: 4 [3840/4097 (92%)]\tLoss: 1.334014\n",
      "Train Epoch: 5 [0/4097 (0%)]\tLoss: 1.549330\n",
      "Train Epoch: 5 [640/4097 (15%)]\tLoss: 1.485347\n",
      "Train Epoch: 5 [1280/4097 (31%)]\tLoss: 1.085048\n",
      "Train Epoch: 5 [1920/4097 (46%)]\tLoss: 1.435736\n",
      "Train Epoch: 5 [2560/4097 (62%)]\tLoss: 1.216344\n",
      "Train Epoch: 5 [3200/4097 (77%)]\tLoss: 1.473997\n",
      "Train Epoch: 5 [3840/4097 (92%)]\tLoss: 1.212384\n",
      "Train Epoch: 6 [0/4097 (0%)]\tLoss: 1.534149\n",
      "Train Epoch: 6 [640/4097 (15%)]\tLoss: 1.298050\n",
      "Train Epoch: 6 [1280/4097 (31%)]\tLoss: 1.475734\n",
      "Train Epoch: 6 [1920/4097 (46%)]\tLoss: 1.399813\n",
      "Train Epoch: 6 [2560/4097 (62%)]\tLoss: 1.219515\n",
      "Train Epoch: 6 [3200/4097 (77%)]\tLoss: 0.940280\n",
      "Train Epoch: 6 [3840/4097 (92%)]\tLoss: 1.202012\n",
      "Train Epoch: 7 [0/4097 (0%)]\tLoss: 1.882438\n",
      "Train Epoch: 7 [640/4097 (15%)]\tLoss: 1.533441\n",
      "Train Epoch: 7 [1280/4097 (31%)]\tLoss: 1.283509\n",
      "Train Epoch: 7 [1920/4097 (46%)]\tLoss: 1.140205\n",
      "Train Epoch: 7 [2560/4097 (62%)]\tLoss: 1.347304\n",
      "Train Epoch: 7 [3200/4097 (77%)]\tLoss: 1.273611\n",
      "Train Epoch: 7 [3840/4097 (92%)]\tLoss: 1.255366\n",
      "Train Epoch: 8 [0/4097 (0%)]\tLoss: 1.253625\n",
      "Train Epoch: 8 [640/4097 (15%)]\tLoss: 1.452887\n",
      "Train Epoch: 8 [1280/4097 (31%)]\tLoss: 1.391084\n",
      "Train Epoch: 8 [1920/4097 (46%)]\tLoss: 1.300696\n",
      "Train Epoch: 8 [2560/4097 (62%)]\tLoss: 1.385220\n",
      "Train Epoch: 8 [3200/4097 (77%)]\tLoss: 1.336410\n",
      "Train Epoch: 8 [3840/4097 (92%)]\tLoss: 1.044258\n",
      "Train Epoch: 9 [0/4097 (0%)]\tLoss: 1.180673\n",
      "Train Epoch: 9 [640/4097 (15%)]\tLoss: 1.190882\n",
      "Train Epoch: 9 [1280/4097 (31%)]\tLoss: 1.448644\n",
      "Train Epoch: 9 [1920/4097 (46%)]\tLoss: 1.414465\n",
      "Train Epoch: 9 [2560/4097 (62%)]\tLoss: 1.243883\n",
      "Train Epoch: 9 [3200/4097 (77%)]\tLoss: 1.403936\n",
      "Train Epoch: 9 [3840/4097 (92%)]\tLoss: 1.122563\n",
      "Train Epoch: 10 [0/4097 (0%)]\tLoss: 1.055496\n",
      "Train Epoch: 10 [640/4097 (15%)]\tLoss: 1.382142\n",
      "Train Epoch: 10 [1280/4097 (31%)]\tLoss: 1.116019\n",
      "Train Epoch: 10 [1920/4097 (46%)]\tLoss: 0.997565\n",
      "Train Epoch: 10 [2560/4097 (62%)]\tLoss: 1.191476\n",
      "Train Epoch: 10 [3200/4097 (77%)]\tLoss: 1.422837\n",
      "Train Epoch: 10 [3840/4097 (92%)]\tLoss: 1.132572\n",
      "Train Epoch: 11 [0/4097 (0%)]\tLoss: 1.462387\n",
      "Train Epoch: 11 [640/4097 (15%)]\tLoss: 1.221808\n",
      "Train Epoch: 11 [1280/4097 (31%)]\tLoss: 1.314735\n",
      "Train Epoch: 11 [1920/4097 (46%)]\tLoss: 1.328763\n",
      "Train Epoch: 11 [2560/4097 (62%)]\tLoss: 1.194830\n",
      "Train Epoch: 11 [3200/4097 (77%)]\tLoss: 1.344060\n",
      "Train Epoch: 11 [3840/4097 (92%)]\tLoss: 1.339689\n",
      "Train Epoch: 12 [0/4097 (0%)]\tLoss: 1.334364\n",
      "Train Epoch: 12 [640/4097 (15%)]\tLoss: 1.203118\n",
      "Train Epoch: 12 [1280/4097 (31%)]\tLoss: 1.260581\n",
      "Train Epoch: 12 [1920/4097 (46%)]\tLoss: 1.186950\n",
      "Train Epoch: 12 [2560/4097 (62%)]\tLoss: 1.235858\n",
      "Train Epoch: 12 [3200/4097 (77%)]\tLoss: 1.235379\n",
      "Train Epoch: 12 [3840/4097 (92%)]\tLoss: 1.294885\n",
      "Train Epoch: 13 [0/4097 (0%)]\tLoss: 1.261084\n",
      "Train Epoch: 13 [640/4097 (15%)]\tLoss: 1.236146\n",
      "Train Epoch: 13 [1280/4097 (31%)]\tLoss: 1.278395\n",
      "Train Epoch: 13 [1920/4097 (46%)]\tLoss: 1.179029\n",
      "Train Epoch: 13 [2560/4097 (62%)]\tLoss: 1.338743\n",
      "Train Epoch: 13 [3200/4097 (77%)]\tLoss: 1.223823\n",
      "Train Epoch: 13 [3840/4097 (92%)]\tLoss: 1.365357\n",
      "Train Epoch: 14 [0/4097 (0%)]\tLoss: 1.203016\n",
      "Train Epoch: 14 [640/4097 (15%)]\tLoss: 1.159870\n",
      "Train Epoch: 14 [1280/4097 (31%)]\tLoss: 1.216344\n",
      "Train Epoch: 14 [1920/4097 (46%)]\tLoss: 1.094119\n",
      "Train Epoch: 14 [2560/4097 (62%)]\tLoss: 0.964965\n",
      "Train Epoch: 14 [3200/4097 (77%)]\tLoss: 1.269136\n",
      "Train Epoch: 14 [3840/4097 (92%)]\tLoss: 1.152519\n",
      "Train Epoch: 15 [0/4097 (0%)]\tLoss: 1.282769\n",
      "Train Epoch: 15 [640/4097 (15%)]\tLoss: 1.433564\n",
      "Train Epoch: 15 [1280/4097 (31%)]\tLoss: 1.017295\n",
      "Train Epoch: 15 [1920/4097 (46%)]\tLoss: 1.383674\n",
      "Train Epoch: 15 [2560/4097 (62%)]\tLoss: 1.250015\n",
      "Train Epoch: 15 [3200/4097 (77%)]\tLoss: 1.294988\n",
      "Train Epoch: 15 [3840/4097 (92%)]\tLoss: 1.391918\n",
      "Train Epoch: 16 [0/4097 (0%)]\tLoss: 1.308139\n",
      "Train Epoch: 16 [640/4097 (15%)]\tLoss: 1.313395\n",
      "Train Epoch: 16 [1280/4097 (31%)]\tLoss: 1.236134\n",
      "Train Epoch: 16 [1920/4097 (46%)]\tLoss: 1.196833\n",
      "Train Epoch: 16 [2560/4097 (62%)]\tLoss: 1.180925\n",
      "Train Epoch: 16 [3200/4097 (77%)]\tLoss: 1.174606\n",
      "Train Epoch: 16 [3840/4097 (92%)]\tLoss: 1.257132\n",
      "Train Epoch: 17 [0/4097 (0%)]\tLoss: 1.309723\n",
      "Train Epoch: 17 [640/4097 (15%)]\tLoss: 1.196290\n",
      "Train Epoch: 17 [1280/4097 (31%)]\tLoss: 1.239315\n",
      "Train Epoch: 17 [1920/4097 (46%)]\tLoss: 1.425667\n",
      "Train Epoch: 17 [2560/4097 (62%)]\tLoss: 1.259154\n",
      "Train Epoch: 17 [3200/4097 (77%)]\tLoss: 1.249539\n",
      "Train Epoch: 17 [3840/4097 (92%)]\tLoss: 1.236210\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/8272 (0%)]\tLoss: 1.260685\n",
      "Train Epoch: 1 [640/8272 (8%)]\tLoss: 1.609765\n",
      "Train Epoch: 1 [1280/8272 (15%)]\tLoss: 1.413143\n",
      "Train Epoch: 1 [1920/8272 (23%)]\tLoss: 1.405120\n",
      "Train Epoch: 1 [2560/8272 (31%)]\tLoss: 1.167311\n",
      "Train Epoch: 1 [3200/8272 (38%)]\tLoss: 1.375016\n",
      "Train Epoch: 1 [3840/8272 (46%)]\tLoss: 1.504642\n",
      "Train Epoch: 1 [4480/8272 (54%)]\tLoss: 1.382651\n",
      "Train Epoch: 1 [5120/8272 (62%)]\tLoss: 1.322175\n",
      "Train Epoch: 1 [5760/8272 (69%)]\tLoss: 1.338312\n",
      "Train Epoch: 1 [6400/8272 (77%)]\tLoss: 1.397025\n",
      "Train Epoch: 1 [7040/8272 (85%)]\tLoss: 1.478709\n",
      "Train Epoch: 1 [7680/8272 (92%)]\tLoss: 1.392093\n",
      "Train Epoch: 2 [0/8272 (0%)]\tLoss: 1.268567\n",
      "Train Epoch: 2 [640/8272 (8%)]\tLoss: 1.475151\n",
      "Train Epoch: 2 [1280/8272 (15%)]\tLoss: 1.248859\n",
      "Train Epoch: 2 [1920/8272 (23%)]\tLoss: 1.569226\n",
      "Train Epoch: 2 [2560/8272 (31%)]\tLoss: 1.442574\n",
      "Train Epoch: 2 [3200/8272 (38%)]\tLoss: 1.348334\n",
      "Train Epoch: 2 [3840/8272 (46%)]\tLoss: 1.366407\n",
      "Train Epoch: 2 [4480/8272 (54%)]\tLoss: 1.432793\n",
      "Train Epoch: 2 [5120/8272 (62%)]\tLoss: 1.231039\n",
      "Train Epoch: 2 [5760/8272 (69%)]\tLoss: 1.306008\n",
      "Train Epoch: 2 [6400/8272 (77%)]\tLoss: 1.749939\n",
      "Train Epoch: 2 [7040/8272 (85%)]\tLoss: 1.183253\n",
      "Train Epoch: 2 [7680/8272 (92%)]\tLoss: 1.259882\n",
      "Train Epoch: 3 [0/8272 (0%)]\tLoss: 1.332100\n",
      "Train Epoch: 3 [640/8272 (8%)]\tLoss: 1.209586\n",
      "Train Epoch: 3 [1280/8272 (15%)]\tLoss: 1.236502\n",
      "Train Epoch: 3 [1920/8272 (23%)]\tLoss: 1.119852\n",
      "Train Epoch: 3 [2560/8272 (31%)]\tLoss: 1.274442\n",
      "Train Epoch: 3 [3200/8272 (38%)]\tLoss: 1.221787\n",
      "Train Epoch: 3 [3840/8272 (46%)]\tLoss: 1.322725\n",
      "Train Epoch: 3 [4480/8272 (54%)]\tLoss: 1.130164\n",
      "Train Epoch: 3 [5120/8272 (62%)]\tLoss: 1.135295\n",
      "Train Epoch: 3 [5760/8272 (69%)]\tLoss: 1.163836\n",
      "Train Epoch: 3 [6400/8272 (77%)]\tLoss: 1.194655\n",
      "Train Epoch: 3 [7040/8272 (85%)]\tLoss: 1.520098\n",
      "Train Epoch: 3 [7680/8272 (92%)]\tLoss: 1.173966\n",
      "Train Epoch: 4 [0/8272 (0%)]\tLoss: 1.367408\n",
      "Train Epoch: 4 [640/8272 (8%)]\tLoss: 1.317340\n",
      "Train Epoch: 4 [1280/8272 (15%)]\tLoss: 1.246723\n",
      "Train Epoch: 4 [1920/8272 (23%)]\tLoss: 1.249111\n",
      "Train Epoch: 4 [2560/8272 (31%)]\tLoss: 1.162064\n",
      "Train Epoch: 4 [3200/8272 (38%)]\tLoss: 1.260351\n",
      "Train Epoch: 4 [3840/8272 (46%)]\tLoss: 1.237002\n",
      "Train Epoch: 4 [4480/8272 (54%)]\tLoss: 1.475725\n",
      "Train Epoch: 4 [5120/8272 (62%)]\tLoss: 1.491218\n",
      "Train Epoch: 4 [5760/8272 (69%)]\tLoss: 1.464451\n",
      "Train Epoch: 4 [6400/8272 (77%)]\tLoss: 1.511770\n",
      "Train Epoch: 4 [7040/8272 (85%)]\tLoss: 1.525524\n",
      "Train Epoch: 4 [7680/8272 (92%)]\tLoss: 1.385230\n",
      "Train Epoch: 5 [0/8272 (0%)]\tLoss: 1.362394\n",
      "Train Epoch: 5 [640/8272 (8%)]\tLoss: 1.389559\n",
      "Train Epoch: 5 [1280/8272 (15%)]\tLoss: 1.560413\n",
      "Train Epoch: 5 [1920/8272 (23%)]\tLoss: 1.537483\n",
      "Train Epoch: 5 [2560/8272 (31%)]\tLoss: 1.177146\n",
      "Train Epoch: 5 [3200/8272 (38%)]\tLoss: 1.089560\n",
      "Train Epoch: 5 [3840/8272 (46%)]\tLoss: 1.415424\n",
      "Train Epoch: 5 [4480/8272 (54%)]\tLoss: 1.268873\n",
      "Train Epoch: 5 [5120/8272 (62%)]\tLoss: 1.390511\n",
      "Train Epoch: 5 [5760/8272 (69%)]\tLoss: 1.335731\n",
      "Train Epoch: 5 [6400/8272 (77%)]\tLoss: 1.599999\n",
      "Train Epoch: 5 [7040/8272 (85%)]\tLoss: 1.352669\n",
      "Train Epoch: 5 [7680/8272 (92%)]\tLoss: 1.108027\n",
      "Train Epoch: 6 [0/8272 (0%)]\tLoss: 1.417662\n",
      "Train Epoch: 6 [640/8272 (8%)]\tLoss: 1.442412\n",
      "Train Epoch: 6 [1280/8272 (15%)]\tLoss: 1.106512\n",
      "Train Epoch: 6 [1920/8272 (23%)]\tLoss: 1.103439\n",
      "Train Epoch: 6 [2560/8272 (31%)]\tLoss: 1.337400\n",
      "Train Epoch: 6 [3200/8272 (38%)]\tLoss: 1.429091\n",
      "Train Epoch: 6 [3840/8272 (46%)]\tLoss: 1.188162\n",
      "Train Epoch: 6 [4480/8272 (54%)]\tLoss: 1.652658\n",
      "Train Epoch: 6 [5120/8272 (62%)]\tLoss: 1.360513\n",
      "Train Epoch: 6 [5760/8272 (69%)]\tLoss: 1.188158\n",
      "Train Epoch: 6 [6400/8272 (77%)]\tLoss: 1.460382\n",
      "Train Epoch: 6 [7040/8272 (85%)]\tLoss: 1.441607\n",
      "Train Epoch: 6 [7680/8272 (92%)]\tLoss: 1.302998\n",
      "Train Epoch: 7 [0/8272 (0%)]\tLoss: 1.482498\n",
      "Train Epoch: 7 [640/8272 (8%)]\tLoss: 1.241563\n",
      "Train Epoch: 7 [1280/8272 (15%)]\tLoss: 1.299603\n",
      "Train Epoch: 7 [1920/8272 (23%)]\tLoss: 1.319296\n",
      "Train Epoch: 7 [2560/8272 (31%)]\tLoss: 1.304026\n",
      "Train Epoch: 7 [3200/8272 (38%)]\tLoss: 1.211740\n",
      "Train Epoch: 7 [3840/8272 (46%)]\tLoss: 1.261946\n",
      "Train Epoch: 7 [4480/8272 (54%)]\tLoss: 1.525510\n",
      "Train Epoch: 7 [5120/8272 (62%)]\tLoss: 1.385873\n",
      "Train Epoch: 7 [5760/8272 (69%)]\tLoss: 1.362995\n",
      "Train Epoch: 7 [6400/8272 (77%)]\tLoss: 1.291301\n",
      "Train Epoch: 7 [7040/8272 (85%)]\tLoss: 1.348665\n",
      "Train Epoch: 7 [7680/8272 (92%)]\tLoss: 1.051700\n",
      "Train Epoch: 8 [0/8272 (0%)]\tLoss: 1.482959\n",
      "Train Epoch: 8 [640/8272 (8%)]\tLoss: 1.367139\n",
      "Train Epoch: 8 [1280/8272 (15%)]\tLoss: 1.162882\n",
      "Train Epoch: 8 [1920/8272 (23%)]\tLoss: 1.384482\n",
      "Train Epoch: 8 [2560/8272 (31%)]\tLoss: 1.287135\n",
      "Train Epoch: 8 [3200/8272 (38%)]\tLoss: 1.301705\n",
      "Train Epoch: 8 [3840/8272 (46%)]\tLoss: 1.095231\n",
      "Train Epoch: 8 [4480/8272 (54%)]\tLoss: 1.190490\n",
      "Train Epoch: 8 [5120/8272 (62%)]\tLoss: 1.159460\n",
      "Train Epoch: 8 [5760/8272 (69%)]\tLoss: 1.403841\n",
      "Train Epoch: 8 [6400/8272 (77%)]\tLoss: 1.311362\n",
      "Train Epoch: 8 [7040/8272 (85%)]\tLoss: 1.170298\n",
      "Train Epoch: 8 [7680/8272 (92%)]\tLoss: 1.060747\n",
      "Train Epoch: 9 [0/8272 (0%)]\tLoss: 1.240474\n",
      "Train Epoch: 9 [640/8272 (8%)]\tLoss: 1.112724\n",
      "Train Epoch: 9 [1280/8272 (15%)]\tLoss: 1.392736\n",
      "Train Epoch: 9 [1920/8272 (23%)]\tLoss: 1.246112\n",
      "Train Epoch: 9 [2560/8272 (31%)]\tLoss: 1.429289\n",
      "Train Epoch: 9 [3200/8272 (38%)]\tLoss: 1.405895\n",
      "Train Epoch: 9 [3840/8272 (46%)]\tLoss: 1.272178\n",
      "Train Epoch: 9 [4480/8272 (54%)]\tLoss: 1.463297\n",
      "Train Epoch: 9 [5120/8272 (62%)]\tLoss: 1.290108\n",
      "Train Epoch: 9 [5760/8272 (69%)]\tLoss: 1.071880\n",
      "Train Epoch: 9 [6400/8272 (77%)]\tLoss: 1.362611\n",
      "Train Epoch: 9 [7040/8272 (85%)]\tLoss: 1.596225\n",
      "Train Epoch: 9 [7680/8272 (92%)]\tLoss: 1.178434\n",
      "Train Epoch: 10 [0/8272 (0%)]\tLoss: 1.298179\n",
      "Train Epoch: 10 [640/8272 (8%)]\tLoss: 1.035405\n",
      "Train Epoch: 10 [1280/8272 (15%)]\tLoss: 1.329899\n",
      "Train Epoch: 10 [1920/8272 (23%)]\tLoss: 1.528981\n",
      "Train Epoch: 10 [2560/8272 (31%)]\tLoss: 1.019584\n",
      "Train Epoch: 10 [3200/8272 (38%)]\tLoss: 1.504431\n",
      "Train Epoch: 10 [3840/8272 (46%)]\tLoss: 1.322597\n",
      "Train Epoch: 10 [4480/8272 (54%)]\tLoss: 1.261178\n",
      "Train Epoch: 10 [5120/8272 (62%)]\tLoss: 1.131278\n",
      "Train Epoch: 10 [5760/8272 (69%)]\tLoss: 1.346510\n",
      "Train Epoch: 10 [6400/8272 (77%)]\tLoss: 1.341622\n",
      "Train Epoch: 10 [7040/8272 (85%)]\tLoss: 1.104225\n",
      "Train Epoch: 10 [7680/8272 (92%)]\tLoss: 1.251474\n",
      "Train Epoch: 11 [0/8272 (0%)]\tLoss: 1.361215\n",
      "Train Epoch: 11 [640/8272 (8%)]\tLoss: 1.349228\n",
      "Train Epoch: 11 [1280/8272 (15%)]\tLoss: 1.246222\n",
      "Train Epoch: 11 [1920/8272 (23%)]\tLoss: 1.277363\n",
      "Train Epoch: 11 [2560/8272 (31%)]\tLoss: 1.440353\n",
      "Train Epoch: 11 [3200/8272 (38%)]\tLoss: 1.154371\n",
      "Train Epoch: 11 [3840/8272 (46%)]\tLoss: 1.262939\n",
      "Train Epoch: 11 [4480/8272 (54%)]\tLoss: 1.252329\n",
      "Train Epoch: 11 [5120/8272 (62%)]\tLoss: 1.282247\n",
      "Train Epoch: 11 [5760/8272 (69%)]\tLoss: 1.519414\n",
      "Train Epoch: 11 [6400/8272 (77%)]\tLoss: 1.240545\n",
      "Train Epoch: 11 [7040/8272 (85%)]\tLoss: 1.167988\n",
      "Train Epoch: 11 [7680/8272 (92%)]\tLoss: 1.294224\n",
      "Train Epoch: 12 [0/8272 (0%)]\tLoss: 1.379603\n",
      "Train Epoch: 12 [640/8272 (8%)]\tLoss: 1.318576\n",
      "Train Epoch: 12 [1280/8272 (15%)]\tLoss: 1.329981\n",
      "Train Epoch: 12 [1920/8272 (23%)]\tLoss: 1.381817\n",
      "Train Epoch: 12 [2560/8272 (31%)]\tLoss: 1.339945\n",
      "Train Epoch: 12 [3200/8272 (38%)]\tLoss: 1.592279\n",
      "Train Epoch: 12 [3840/8272 (46%)]\tLoss: 1.143482\n",
      "Train Epoch: 12 [4480/8272 (54%)]\tLoss: 1.033459\n",
      "Train Epoch: 12 [5120/8272 (62%)]\tLoss: 1.500492\n",
      "Train Epoch: 12 [5760/8272 (69%)]\tLoss: 1.435059\n",
      "Train Epoch: 12 [6400/8272 (77%)]\tLoss: 1.316827\n",
      "Train Epoch: 12 [7040/8272 (85%)]\tLoss: 1.462458\n",
      "Train Epoch: 12 [7680/8272 (92%)]\tLoss: 1.090347\n",
      "Train Epoch: 13 [0/8272 (0%)]\tLoss: 1.225051\n",
      "Train Epoch: 13 [640/8272 (8%)]\tLoss: 1.283678\n",
      "Train Epoch: 13 [1280/8272 (15%)]\tLoss: 1.105941\n",
      "Train Epoch: 13 [1920/8272 (23%)]\tLoss: 1.182571\n",
      "Train Epoch: 13 [2560/8272 (31%)]\tLoss: 1.061159\n",
      "Train Epoch: 13 [3200/8272 (38%)]\tLoss: 1.509031\n",
      "Train Epoch: 13 [3840/8272 (46%)]\tLoss: 1.032590\n",
      "Train Epoch: 13 [4480/8272 (54%)]\tLoss: 1.270853\n",
      "Train Epoch: 13 [5120/8272 (62%)]\tLoss: 1.277372\n",
      "Train Epoch: 13 [5760/8272 (69%)]\tLoss: 1.132612\n",
      "Train Epoch: 13 [6400/8272 (77%)]\tLoss: 1.189282\n",
      "Train Epoch: 13 [7040/8272 (85%)]\tLoss: 1.335484\n",
      "Train Epoch: 13 [7680/8272 (92%)]\tLoss: 1.260947\n",
      "Train Epoch: 14 [0/8272 (0%)]\tLoss: 1.254390\n",
      "Train Epoch: 14 [640/8272 (8%)]\tLoss: 1.251410\n",
      "Train Epoch: 14 [1280/8272 (15%)]\tLoss: 1.118807\n",
      "Train Epoch: 14 [1920/8272 (23%)]\tLoss: 1.203335\n",
      "Train Epoch: 14 [2560/8272 (31%)]\tLoss: 1.386999\n",
      "Train Epoch: 14 [3200/8272 (38%)]\tLoss: 1.195254\n",
      "Train Epoch: 14 [3840/8272 (46%)]\tLoss: 1.367636\n",
      "Train Epoch: 14 [4480/8272 (54%)]\tLoss: 1.361247\n",
      "Train Epoch: 14 [5120/8272 (62%)]\tLoss: 1.179137\n",
      "Train Epoch: 14 [5760/8272 (69%)]\tLoss: 1.278005\n",
      "Train Epoch: 14 [6400/8272 (77%)]\tLoss: 1.436567\n",
      "Train Epoch: 14 [7040/8272 (85%)]\tLoss: 1.286810\n",
      "Train Epoch: 14 [7680/8272 (92%)]\tLoss: 1.078883\n",
      "Train Epoch: 15 [0/8272 (0%)]\tLoss: 1.408832\n",
      "Train Epoch: 15 [640/8272 (8%)]\tLoss: 1.334326\n",
      "Train Epoch: 15 [1280/8272 (15%)]\tLoss: 1.347829\n",
      "Train Epoch: 15 [1920/8272 (23%)]\tLoss: 1.253439\n",
      "Train Epoch: 15 [2560/8272 (31%)]\tLoss: 1.722879\n",
      "Train Epoch: 15 [3200/8272 (38%)]\tLoss: 1.289241\n",
      "Train Epoch: 15 [3840/8272 (46%)]\tLoss: 1.271856\n",
      "Train Epoch: 15 [4480/8272 (54%)]\tLoss: 1.369075\n",
      "Train Epoch: 15 [5120/8272 (62%)]\tLoss: 1.362258\n",
      "Train Epoch: 15 [5760/8272 (69%)]\tLoss: 1.305177\n",
      "Train Epoch: 15 [6400/8272 (77%)]\tLoss: 0.951016\n",
      "Train Epoch: 15 [7040/8272 (85%)]\tLoss: 1.263166\n",
      "Train Epoch: 15 [7680/8272 (92%)]\tLoss: 1.161534\n",
      "Train Epoch: 16 [0/8272 (0%)]\tLoss: 1.263340\n",
      "Train Epoch: 16 [640/8272 (8%)]\tLoss: 1.564641\n",
      "Train Epoch: 16 [1280/8272 (15%)]\tLoss: 1.024877\n",
      "Train Epoch: 16 [1920/8272 (23%)]\tLoss: 1.361621\n",
      "Train Epoch: 16 [2560/8272 (31%)]\tLoss: 1.156996\n",
      "Train Epoch: 16 [3200/8272 (38%)]\tLoss: 1.259156\n",
      "Train Epoch: 16 [3840/8272 (46%)]\tLoss: 1.444728\n",
      "Train Epoch: 16 [4480/8272 (54%)]\tLoss: 1.078234\n",
      "Train Epoch: 16 [5120/8272 (62%)]\tLoss: 1.341710\n",
      "Train Epoch: 16 [5760/8272 (69%)]\tLoss: 1.402361\n",
      "Train Epoch: 16 [6400/8272 (77%)]\tLoss: 1.235651\n",
      "Train Epoch: 16 [7040/8272 (85%)]\tLoss: 1.207622\n",
      "Train Epoch: 16 [7680/8272 (92%)]\tLoss: 1.441848\n",
      "Train Epoch: 17 [0/8272 (0%)]\tLoss: 1.081733\n",
      "Train Epoch: 17 [640/8272 (8%)]\tLoss: 1.305910\n",
      "Train Epoch: 17 [1280/8272 (15%)]\tLoss: 1.169508\n",
      "Train Epoch: 17 [1920/8272 (23%)]\tLoss: 1.097198\n",
      "Train Epoch: 17 [2560/8272 (31%)]\tLoss: 1.275886\n",
      "Train Epoch: 17 [3200/8272 (38%)]\tLoss: 1.586090\n",
      "Train Epoch: 17 [3840/8272 (46%)]\tLoss: 1.338630\n",
      "Train Epoch: 17 [4480/8272 (54%)]\tLoss: 1.331261\n",
      "Train Epoch: 17 [5120/8272 (62%)]\tLoss: 1.282973\n",
      "Train Epoch: 17 [5760/8272 (69%)]\tLoss: 1.371671\n",
      "Train Epoch: 17 [6400/8272 (77%)]\tLoss: 1.279641\n",
      "Train Epoch: 17 [7040/8272 (85%)]\tLoss: 1.127364\n",
      "Train Epoch: 17 [7680/8272 (92%)]\tLoss: 1.328282\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/5100 (0%)]\tLoss: 1.534592\n",
      "Train Epoch: 1 [640/5100 (12%)]\tLoss: 1.350778\n",
      "Train Epoch: 1 [1280/5100 (25%)]\tLoss: 1.414964\n",
      "Train Epoch: 1 [1920/5100 (38%)]\tLoss: 1.311122\n",
      "Train Epoch: 1 [2560/5100 (50%)]\tLoss: 1.260778\n",
      "Train Epoch: 1 [3200/5100 (62%)]\tLoss: 1.189018\n",
      "Train Epoch: 1 [3840/5100 (75%)]\tLoss: 1.162072\n",
      "Train Epoch: 1 [4480/5100 (88%)]\tLoss: 1.247550\n",
      "Train Epoch: 2 [0/5100 (0%)]\tLoss: 1.196574\n",
      "Train Epoch: 2 [640/5100 (12%)]\tLoss: 1.176112\n",
      "Train Epoch: 2 [1280/5100 (25%)]\tLoss: 1.296239\n",
      "Train Epoch: 2 [1920/5100 (38%)]\tLoss: 1.406209\n",
      "Train Epoch: 2 [2560/5100 (50%)]\tLoss: 1.235054\n",
      "Train Epoch: 2 [3200/5100 (62%)]\tLoss: 1.291516\n",
      "Train Epoch: 2 [3840/5100 (75%)]\tLoss: 1.218772\n",
      "Train Epoch: 2 [4480/5100 (88%)]\tLoss: 1.165861\n",
      "Train Epoch: 3 [0/5100 (0%)]\tLoss: 1.047128\n",
      "Train Epoch: 3 [640/5100 (12%)]\tLoss: 1.323179\n",
      "Train Epoch: 3 [1280/5100 (25%)]\tLoss: 1.267475\n",
      "Train Epoch: 3 [1920/5100 (38%)]\tLoss: 1.285583\n",
      "Train Epoch: 3 [2560/5100 (50%)]\tLoss: 1.210357\n",
      "Train Epoch: 3 [3200/5100 (62%)]\tLoss: 1.035906\n",
      "Train Epoch: 3 [3840/5100 (75%)]\tLoss: 1.353634\n",
      "Train Epoch: 3 [4480/5100 (88%)]\tLoss: 1.028954\n",
      "Train Epoch: 4 [0/5100 (0%)]\tLoss: 1.453517\n",
      "Train Epoch: 4 [640/5100 (12%)]\tLoss: 1.179756\n",
      "Train Epoch: 4 [1280/5100 (25%)]\tLoss: 1.088032\n",
      "Train Epoch: 4 [1920/5100 (38%)]\tLoss: 1.136864\n",
      "Train Epoch: 4 [2560/5100 (50%)]\tLoss: 1.317716\n",
      "Train Epoch: 4 [3200/5100 (62%)]\tLoss: 1.318417\n",
      "Train Epoch: 4 [3840/5100 (75%)]\tLoss: 1.351920\n",
      "Train Epoch: 4 [4480/5100 (88%)]\tLoss: 1.014835\n",
      "Train Epoch: 5 [0/5100 (0%)]\tLoss: 1.267560\n",
      "Train Epoch: 5 [640/5100 (12%)]\tLoss: 1.262006\n",
      "Train Epoch: 5 [1280/5100 (25%)]\tLoss: 1.200876\n",
      "Train Epoch: 5 [1920/5100 (38%)]\tLoss: 1.164427\n",
      "Train Epoch: 5 [2560/5100 (50%)]\tLoss: 1.133308\n",
      "Train Epoch: 5 [3200/5100 (62%)]\tLoss: 0.999654\n",
      "Train Epoch: 5 [3840/5100 (75%)]\tLoss: 1.177825\n",
      "Train Epoch: 5 [4480/5100 (88%)]\tLoss: 1.347971\n",
      "Train Epoch: 6 [0/5100 (0%)]\tLoss: 1.365528\n",
      "Train Epoch: 6 [640/5100 (12%)]\tLoss: 1.014586\n",
      "Train Epoch: 6 [1280/5100 (25%)]\tLoss: 1.146392\n",
      "Train Epoch: 6 [1920/5100 (38%)]\tLoss: 1.480729\n",
      "Train Epoch: 6 [2560/5100 (50%)]\tLoss: 1.062848\n",
      "Train Epoch: 6 [3200/5100 (62%)]\tLoss: 1.203324\n",
      "Train Epoch: 6 [3840/5100 (75%)]\tLoss: 1.362581\n",
      "Train Epoch: 6 [4480/5100 (88%)]\tLoss: 0.907458\n",
      "Train Epoch: 7 [0/5100 (0%)]\tLoss: 1.163785\n",
      "Train Epoch: 7 [640/5100 (12%)]\tLoss: 0.976472\n",
      "Train Epoch: 7 [1280/5100 (25%)]\tLoss: 1.230940\n",
      "Train Epoch: 7 [1920/5100 (38%)]\tLoss: 1.081891\n",
      "Train Epoch: 7 [2560/5100 (50%)]\tLoss: 1.164845\n",
      "Train Epoch: 7 [3200/5100 (62%)]\tLoss: 1.197034\n",
      "Train Epoch: 7 [3840/5100 (75%)]\tLoss: 1.281947\n",
      "Train Epoch: 7 [4480/5100 (88%)]\tLoss: 1.202886\n",
      "Train Epoch: 8 [0/5100 (0%)]\tLoss: 1.260832\n",
      "Train Epoch: 8 [640/5100 (12%)]\tLoss: 1.269557\n",
      "Train Epoch: 8 [1280/5100 (25%)]\tLoss: 1.259830\n",
      "Train Epoch: 8 [1920/5100 (38%)]\tLoss: 1.197197\n",
      "Train Epoch: 8 [2560/5100 (50%)]\tLoss: 1.257378\n",
      "Train Epoch: 8 [3200/5100 (62%)]\tLoss: 1.227414\n",
      "Train Epoch: 8 [3840/5100 (75%)]\tLoss: 1.231031\n",
      "Train Epoch: 8 [4480/5100 (88%)]\tLoss: 1.226789\n",
      "Train Epoch: 9 [0/5100 (0%)]\tLoss: 1.063594\n",
      "Train Epoch: 9 [640/5100 (12%)]\tLoss: 0.997005\n",
      "Train Epoch: 9 [1280/5100 (25%)]\tLoss: 1.291931\n",
      "Train Epoch: 9 [1920/5100 (38%)]\tLoss: 1.443355\n",
      "Train Epoch: 9 [2560/5100 (50%)]\tLoss: 1.308538\n",
      "Train Epoch: 9 [3200/5100 (62%)]\tLoss: 1.138064\n",
      "Train Epoch: 9 [3840/5100 (75%)]\tLoss: 1.318174\n",
      "Train Epoch: 9 [4480/5100 (88%)]\tLoss: 1.115005\n",
      "Train Epoch: 10 [0/5100 (0%)]\tLoss: 1.291896\n",
      "Train Epoch: 10 [640/5100 (12%)]\tLoss: 1.205585\n",
      "Train Epoch: 10 [1280/5100 (25%)]\tLoss: 1.399158\n",
      "Train Epoch: 10 [1920/5100 (38%)]\tLoss: 1.142879\n",
      "Train Epoch: 10 [2560/5100 (50%)]\tLoss: 1.544008\n",
      "Train Epoch: 10 [3200/5100 (62%)]\tLoss: 0.998113\n",
      "Train Epoch: 10 [3840/5100 (75%)]\tLoss: 1.151569\n",
      "Train Epoch: 10 [4480/5100 (88%)]\tLoss: 1.092437\n",
      "Train Epoch: 11 [0/5100 (0%)]\tLoss: 1.093415\n",
      "Train Epoch: 11 [640/5100 (12%)]\tLoss: 1.142200\n",
      "Train Epoch: 11 [1280/5100 (25%)]\tLoss: 1.291935\n",
      "Train Epoch: 11 [1920/5100 (38%)]\tLoss: 1.141905\n",
      "Train Epoch: 11 [2560/5100 (50%)]\tLoss: 1.059667\n",
      "Train Epoch: 11 [3200/5100 (62%)]\tLoss: 1.179798\n",
      "Train Epoch: 11 [3840/5100 (75%)]\tLoss: 1.201039\n",
      "Train Epoch: 11 [4480/5100 (88%)]\tLoss: 1.076939\n",
      "Train Epoch: 12 [0/5100 (0%)]\tLoss: 1.235784\n",
      "Train Epoch: 12 [640/5100 (12%)]\tLoss: 1.126506\n",
      "Train Epoch: 12 [1280/5100 (25%)]\tLoss: 1.080115\n",
      "Train Epoch: 12 [1920/5100 (38%)]\tLoss: 1.155674\n",
      "Train Epoch: 12 [2560/5100 (50%)]\tLoss: 1.148776\n",
      "Train Epoch: 12 [3200/5100 (62%)]\tLoss: 1.157768\n",
      "Train Epoch: 12 [3840/5100 (75%)]\tLoss: 1.146723\n",
      "Train Epoch: 12 [4480/5100 (88%)]\tLoss: 1.247505\n",
      "Train Epoch: 13 [0/5100 (0%)]\tLoss: 1.457968\n",
      "Train Epoch: 13 [640/5100 (12%)]\tLoss: 1.120189\n",
      "Train Epoch: 13 [1280/5100 (25%)]\tLoss: 1.020561\n",
      "Train Epoch: 13 [1920/5100 (38%)]\tLoss: 1.302295\n",
      "Train Epoch: 13 [2560/5100 (50%)]\tLoss: 1.072313\n",
      "Train Epoch: 13 [3200/5100 (62%)]\tLoss: 1.031543\n",
      "Train Epoch: 13 [3840/5100 (75%)]\tLoss: 0.982868\n",
      "Train Epoch: 13 [4480/5100 (88%)]\tLoss: 1.221632\n",
      "Train Epoch: 14 [0/5100 (0%)]\tLoss: 1.096481\n",
      "Train Epoch: 14 [640/5100 (12%)]\tLoss: 1.131865\n",
      "Train Epoch: 14 [1280/5100 (25%)]\tLoss: 0.980052\n",
      "Train Epoch: 14 [1920/5100 (38%)]\tLoss: 1.238720\n",
      "Train Epoch: 14 [2560/5100 (50%)]\tLoss: 1.162081\n",
      "Train Epoch: 14 [3200/5100 (62%)]\tLoss: 1.050156\n",
      "Train Epoch: 14 [3840/5100 (75%)]\tLoss: 1.301260\n",
      "Train Epoch: 14 [4480/5100 (88%)]\tLoss: 1.119509\n",
      "Train Epoch: 15 [0/5100 (0%)]\tLoss: 1.254626\n",
      "Train Epoch: 15 [640/5100 (12%)]\tLoss: 1.070489\n",
      "Train Epoch: 15 [1280/5100 (25%)]\tLoss: 0.994729\n",
      "Train Epoch: 15 [1920/5100 (38%)]\tLoss: 0.933154\n",
      "Train Epoch: 15 [2560/5100 (50%)]\tLoss: 1.239913\n",
      "Train Epoch: 15 [3200/5100 (62%)]\tLoss: 1.195603\n",
      "Train Epoch: 15 [3840/5100 (75%)]\tLoss: 1.042160\n",
      "Train Epoch: 15 [4480/5100 (88%)]\tLoss: 1.180226\n",
      "Train Epoch: 16 [0/5100 (0%)]\tLoss: 0.958896\n",
      "Train Epoch: 16 [640/5100 (12%)]\tLoss: 1.362077\n",
      "Train Epoch: 16 [1280/5100 (25%)]\tLoss: 1.439602\n",
      "Train Epoch: 16 [1920/5100 (38%)]\tLoss: 1.092765\n",
      "Train Epoch: 16 [2560/5100 (50%)]\tLoss: 0.985035\n",
      "Train Epoch: 16 [3200/5100 (62%)]\tLoss: 1.194155\n",
      "Train Epoch: 16 [3840/5100 (75%)]\tLoss: 1.045212\n",
      "Train Epoch: 16 [4480/5100 (88%)]\tLoss: 1.363546\n",
      "Train Epoch: 17 [0/5100 (0%)]\tLoss: 1.216300\n",
      "Train Epoch: 17 [640/5100 (12%)]\tLoss: 1.057121\n",
      "Train Epoch: 17 [1280/5100 (25%)]\tLoss: 1.307229\n",
      "Train Epoch: 17 [1920/5100 (38%)]\tLoss: 1.241731\n",
      "Train Epoch: 17 [2560/5100 (50%)]\tLoss: 1.035787\n",
      "Train Epoch: 17 [3200/5100 (62%)]\tLoss: 1.358112\n",
      "Train Epoch: 17 [3840/5100 (75%)]\tLoss: 1.262122\n",
      "Train Epoch: 17 [4480/5100 (88%)]\tLoss: 1.219340\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1918, Accuracy: 5745/10000 (57%)\n",
      "\n",
      "Round 4/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.601751\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.168194\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.355760\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.233951\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.176991\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.364654\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.042432\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.477260\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.542428\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 1.239014\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.019820\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.034495\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 0.883853\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.182611\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.052849\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.263848\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.280838\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.297189\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.098597\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 0.909507\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.510350\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.188004\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.126180\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.075503\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.566976\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 1.268328\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.047543\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.326279\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.270833\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.058025\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.276718\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.088302\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.208421\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.174394\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.156509\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.121194\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.086304\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.202855\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.099727\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.252256\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 0.827019\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 1.268310\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 1.216139\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 0.929908\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.001833\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.336876\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 1.118080\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 1.347605\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.168164\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.202832\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.191563\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 1.238266\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 1.081834\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 1.246378\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 1.260964\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.079654\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 1.128497\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 0.961296\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.051329\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.120141\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.174542\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 0.926308\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 1.064219\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.020418\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.122021\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 1.096374\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 1.127180\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.238306\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 1.225454\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 0.980425\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.262339\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 0.998493\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 1.043227\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.060446\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.339244\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 1.055013\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.307788\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.063363\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 1.152931\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 1.159316\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.103785\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 1.141446\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.369098\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.247799\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 1.304897\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 1.243828\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.262257\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 0.941434\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.102723\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.116214\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.330205\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.143616\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 1.167844\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 1.257956\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 1.195146\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 0.952793\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.145187\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 0.977431\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 1.153754\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 0.889661\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.190725\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 0.774701\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 0.932311\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 0.886449\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 0.927542\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 1.104719\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 1.155787\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 0.918120\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 1.092755\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 1.398309\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 0.917501\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 1.235769\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 0.937735\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 1.083590\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.021940\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 1.040916\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 1.131958\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.037585\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 1.016057\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.486797\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.312970\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.256359\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.139845\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.262488\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.281571\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.408866\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.277916\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.329545\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.346136\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.233964\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.450914\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.367179\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.164329\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.127977\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.300395\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.209167\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.238430\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.007309\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 1.316964\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.182556\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.213042\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.381674\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.426226\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.217379\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.319427\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.178666\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.243070\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.457172\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.055224\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.148893\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.375870\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.028322\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.146237\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.568821\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.028600\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.365659\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.164973\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.175644\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.393795\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.321758\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.165091\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.165442\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.163949\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.336120\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.192758\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.252321\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.158352\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.024795\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.189968\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.301422\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.247569\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.127107\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 0.948983\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 1.400773\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.266838\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.236683\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.266767\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.026108\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 1.316684\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.113517\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.158404\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.050650\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.219007\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.055826\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.200610\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.278897\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.039196\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.282173\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.244189\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.242755\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.394054\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.123128\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.182693\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.058594\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.268426\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.305455\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.073856\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 1.166226\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.181526\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.114720\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.186163\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 0.822057\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.245445\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.292522\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.301702\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 1.269084\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.341008\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.157212\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.091565\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.134861\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.305232\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.143764\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.452845\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 0.995459\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 0.985132\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.322144\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 0.945378\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.096646\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.017462\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.418674\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.319927\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.126327\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.318619\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 0.959205\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.205799\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 1.098992\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.120670\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.191481\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.098866\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.072568\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.261029\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 1.020960\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.304068\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.131725\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.094154\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.060467\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.089116\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 0.995671\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.342522\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.014015\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.141972\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 1.303867\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.488954\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.012434\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.224798\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 1.061321\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.328072\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.025128\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.199498\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.085301\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.075792\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.021241\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 0.945686\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 1.305084\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 1.199314\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.487868\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.181466\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.113556\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.501396\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.275297\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.475060\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.268062\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.271071\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.223250\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.441041\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.476143\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.381315\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.126541\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.236745\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.052515\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.170407\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.300755\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.412253\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.296596\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.325409\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.295031\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.323698\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.472417\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.232318\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.297227\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.319381\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.453846\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.233232\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.281092\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.043145\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.358022\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.365545\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.313310\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.165352\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.146582\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.273997\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.159803\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.331470\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.236318\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.264152\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.180911\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.161938\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.268481\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.355075\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.313027\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.269045\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.283423\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.230042\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.298296\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.358564\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.284570\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.125929\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.485439\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.374702\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.068412\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.281961\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.142480\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.251634\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.337318\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.292530\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.280889\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.033245\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.276352\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.121306\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.137324\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.243316\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.336010\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.370929\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.275537\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.274662\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.144210\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.265959\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.276451\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.124951\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.268027\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.319108\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.150378\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.436720\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.163328\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 0.968897\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.300993\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.156220\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.332180\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.182903\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.320274\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.428349\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.003610\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.429585\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.364181\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.137683\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.356575\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.047231\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.248133\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.274286\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.050853\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 0.984093\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.149351\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.014686\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.158894\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 0.937693\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.137683\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.214630\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.207007\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.056527\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 1.170705\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.201138\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.136300\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.314105\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.131171\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.095744\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 0.959113\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.229558\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 0.953153\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.156126\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 0.909839\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 0.839162\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.190104\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.179708\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.199742\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.407486\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.176673\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.013537\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.320474\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.020322\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.365145\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.140154\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.102767\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.226806\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.380870\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.267295\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 1.290288\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.373683\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.199539\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.410090\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.051996\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 0.989968\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.401411\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.137328\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.317699\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.406572\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 1.212396\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 1.150877\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.173429\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 1.260425\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.401629\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.214069\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.302376\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.269783\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.074329\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.461192\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 1.171830\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.204551\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.400274\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 1.179614\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.198204\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.188074\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.026142\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.127438\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 0.995412\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.210582\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.149167\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.194279\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 1.181185\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.141279\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.102217\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.194540\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.273195\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.122686\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.240712\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.480107\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.305432\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 1.265624\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 1.258565\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 0.913524\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 0.884335\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 0.948828\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 0.820641\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 1.144630\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 0.939974\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 1.048385\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 0.893746\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 1.125737\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 1.102877\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 0.909935\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 0.940339\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 1.097340\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 1.038053\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.952831\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.896409\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.999233\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 1.020607\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 1.044150\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 0.835000\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 1.056785\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 0.895231\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 0.694872\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 1.031979\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 1.034426\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 1.053268\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 1.169431\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 0.717667\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 1.012588\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 1.129332\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 0.993897\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 1.044679\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 1.012940\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.818613\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 1.054815\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.793911\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.936605\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 1.128761\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.879368\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 1.040120\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.870140\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 1.209915\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 1.159968\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.882530\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.743110\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 0.787230\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 0.880152\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.864791\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 0.917976\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.939136\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 1.074915\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 0.925569\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.845565\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 1.075470\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.956832\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.844396\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.914204\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.811063\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 0.865807\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 0.911308\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.826654\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.909011\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.934348\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.681212\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.810390\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.906736\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.586899\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 0.868420\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.924111\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.864925\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.958322\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.830501\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.812113\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.790891\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.980268\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.879287\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.718992\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.730540\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.809558\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.789336\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.930446\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.977596\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 1.043393\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.834080\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.873791\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 0.680971\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.925435\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.868689\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.727674\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.754066\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.886932\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.753022\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.940886\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.860550\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 0.736851\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.840242\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 1.054940\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.608896\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.782901\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.296656\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.062512\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 0.935209\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 1.261569\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 1.072267\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.111200\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.102816\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 0.987012\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 1.467029\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 1.064545\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 0.979487\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 0.991784\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.123906\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.050110\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 0.898727\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 0.951297\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 0.875849\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 1.031388\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 1.160186\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 1.038259\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.034874\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 1.096595\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.002473\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 1.054340\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 1.213429\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.141614\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 1.119428\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 1.059387\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 0.898516\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 0.992239\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 0.964116\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 1.174280\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 1.444137\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 1.246208\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 0.988506\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 1.112890\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 1.035556\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 1.220452\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.191035\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 1.385502\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 0.920566\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 1.197828\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 0.861610\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 0.972435\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 1.128596\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 0.890228\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 0.981566\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 1.043589\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 1.256350\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 1.053903\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 0.998443\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 1.008961\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 1.165061\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 0.922684\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 1.120752\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 1.174989\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 1.321212\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 0.819815\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 0.917829\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 1.083286\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 0.823531\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 0.841937\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 1.011069\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 0.754180\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 0.787266\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 1.076460\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 0.967604\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 0.828812\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.937219\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 0.953083\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 1.090047\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 0.956558\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 0.880146\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 1.026385\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 1.341028\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 1.150218\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 1.041703\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 1.175111\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 0.890772\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 1.079169\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 1.303893\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 0.821046\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 1.072478\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 0.917645\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 1.038150\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 1.067734\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 0.987199\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 0.933398\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 0.808332\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 0.720916\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 0.976587\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 1.104794\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 1.131666\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 0.874342\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 0.836374\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 1.022231\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 1.098560\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 0.844563\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 0.958518\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 0.887627\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 0.820712\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 0.931861\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.350605\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.634256\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.246699\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.276130\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.101611\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.417168\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.432021\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.273751\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.276776\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.327081\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.324372\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.296505\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.171165\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 0.966878\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 0.985336\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.404004\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.019183\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.019885\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.242734\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.136094\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.507141\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.036558\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.432175\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 1.111208\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.333872\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.194547\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.263608\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.024050\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.361595\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.604570\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.041659\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 0.948122\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.019649\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.106037\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.153610\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.138941\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.160663\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.231933\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.353798\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.372601\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.406768\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.313073\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.230013\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.024636\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 1.065707\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.225113\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.303592\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.153027\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.266511\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.264434\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.335169\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.200965\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.130219\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 0.999244\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.104447\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.225317\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.124131\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.324217\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.268213\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.278816\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.187937\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.406230\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.360934\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.579287\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.289506\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.326096\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.150103\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.257765\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.101165\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.236333\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 0.971916\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.247373\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.067048\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 1.011332\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.158450\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.325477\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 1.057543\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.260844\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.095373\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 0.968304\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.330148\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.173533\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 0.923849\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.088855\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.323459\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.441159\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.092314\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.071753\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.255298\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.312698\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.095736\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.027610\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.188258\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 0.969199\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 1.122879\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.169452\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.204542\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.126614\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.301268\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.061959\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.313758\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.137881\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.195633\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.117344\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.617478\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.187459\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 1.159608\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 1.160197\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.128538\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 1.084723\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.396766\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.075612\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 1.107095\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 1.081040\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.024122\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 1.211364\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.338996\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.008412\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 0.925521\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.141080\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.028717\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 1.098871\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.093961\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.021120\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.177290\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.319848\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.137547\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.146600\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.196261\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.225179\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.143513\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.116595\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 1.270737\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.310523\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.319224\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 1.090789\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/5548 (0%)]\tLoss: 1.179598\n",
      "Train Epoch: 1 [640/5548 (11%)]\tLoss: 1.264528\n",
      "Train Epoch: 1 [1280/5548 (23%)]\tLoss: 1.175222\n",
      "Train Epoch: 1 [1920/5548 (34%)]\tLoss: 1.188923\n",
      "Train Epoch: 1 [2560/5548 (46%)]\tLoss: 1.304443\n",
      "Train Epoch: 1 [3200/5548 (57%)]\tLoss: 1.524375\n",
      "Train Epoch: 1 [3840/5548 (69%)]\tLoss: 0.988646\n",
      "Train Epoch: 1 [4480/5548 (80%)]\tLoss: 1.087941\n",
      "Train Epoch: 1 [5120/5548 (92%)]\tLoss: 1.041476\n",
      "Train Epoch: 2 [0/5548 (0%)]\tLoss: 1.114654\n",
      "Train Epoch: 2 [640/5548 (11%)]\tLoss: 1.013295\n",
      "Train Epoch: 2 [1280/5548 (23%)]\tLoss: 1.184853\n",
      "Train Epoch: 2 [1920/5548 (34%)]\tLoss: 1.201386\n",
      "Train Epoch: 2 [2560/5548 (46%)]\tLoss: 1.231143\n",
      "Train Epoch: 2 [3200/5548 (57%)]\tLoss: 1.532038\n",
      "Train Epoch: 2 [3840/5548 (69%)]\tLoss: 1.263972\n",
      "Train Epoch: 2 [4480/5548 (80%)]\tLoss: 1.210456\n",
      "Train Epoch: 2 [5120/5548 (92%)]\tLoss: 1.125038\n",
      "Train Epoch: 3 [0/5548 (0%)]\tLoss: 1.208071\n",
      "Train Epoch: 3 [640/5548 (11%)]\tLoss: 1.101362\n",
      "Train Epoch: 3 [1280/5548 (23%)]\tLoss: 1.195852\n",
      "Train Epoch: 3 [1920/5548 (34%)]\tLoss: 1.174414\n",
      "Train Epoch: 3 [2560/5548 (46%)]\tLoss: 0.927867\n",
      "Train Epoch: 3 [3200/5548 (57%)]\tLoss: 1.226216\n",
      "Train Epoch: 3 [3840/5548 (69%)]\tLoss: 1.103961\n",
      "Train Epoch: 3 [4480/5548 (80%)]\tLoss: 1.210530\n",
      "Train Epoch: 3 [5120/5548 (92%)]\tLoss: 1.100809\n",
      "Train Epoch: 4 [0/5548 (0%)]\tLoss: 0.885934\n",
      "Train Epoch: 4 [640/5548 (11%)]\tLoss: 0.981924\n",
      "Train Epoch: 4 [1280/5548 (23%)]\tLoss: 1.268763\n",
      "Train Epoch: 4 [1920/5548 (34%)]\tLoss: 1.041396\n",
      "Train Epoch: 4 [2560/5548 (46%)]\tLoss: 1.155366\n",
      "Train Epoch: 4 [3200/5548 (57%)]\tLoss: 1.115619\n",
      "Train Epoch: 4 [3840/5548 (69%)]\tLoss: 1.412305\n",
      "Train Epoch: 4 [4480/5548 (80%)]\tLoss: 0.963970\n",
      "Train Epoch: 4 [5120/5548 (92%)]\tLoss: 0.997563\n",
      "Train Epoch: 5 [0/5548 (0%)]\tLoss: 1.240694\n",
      "Train Epoch: 5 [640/5548 (11%)]\tLoss: 0.964338\n",
      "Train Epoch: 5 [1280/5548 (23%)]\tLoss: 1.040034\n",
      "Train Epoch: 5 [1920/5548 (34%)]\tLoss: 0.984691\n",
      "Train Epoch: 5 [2560/5548 (46%)]\tLoss: 1.242049\n",
      "Train Epoch: 5 [3200/5548 (57%)]\tLoss: 1.095461\n",
      "Train Epoch: 5 [3840/5548 (69%)]\tLoss: 1.169953\n",
      "Train Epoch: 5 [4480/5548 (80%)]\tLoss: 1.070363\n",
      "Train Epoch: 5 [5120/5548 (92%)]\tLoss: 1.116658\n",
      "Train Epoch: 6 [0/5548 (0%)]\tLoss: 1.296322\n",
      "Train Epoch: 6 [640/5548 (11%)]\tLoss: 0.950395\n",
      "Train Epoch: 6 [1280/5548 (23%)]\tLoss: 1.276125\n",
      "Train Epoch: 6 [1920/5548 (34%)]\tLoss: 1.069865\n",
      "Train Epoch: 6 [2560/5548 (46%)]\tLoss: 1.387102\n",
      "Train Epoch: 6 [3200/5548 (57%)]\tLoss: 1.145588\n",
      "Train Epoch: 6 [3840/5548 (69%)]\tLoss: 0.789022\n",
      "Train Epoch: 6 [4480/5548 (80%)]\tLoss: 1.293563\n",
      "Train Epoch: 6 [5120/5548 (92%)]\tLoss: 1.170267\n",
      "Train Epoch: 7 [0/5548 (0%)]\tLoss: 1.055594\n",
      "Train Epoch: 7 [640/5548 (11%)]\tLoss: 1.081961\n",
      "Train Epoch: 7 [1280/5548 (23%)]\tLoss: 1.072884\n",
      "Train Epoch: 7 [1920/5548 (34%)]\tLoss: 1.101735\n",
      "Train Epoch: 7 [2560/5548 (46%)]\tLoss: 1.077470\n",
      "Train Epoch: 7 [3200/5548 (57%)]\tLoss: 1.125837\n",
      "Train Epoch: 7 [3840/5548 (69%)]\tLoss: 0.974219\n",
      "Train Epoch: 7 [4480/5548 (80%)]\tLoss: 1.257524\n",
      "Train Epoch: 7 [5120/5548 (92%)]\tLoss: 0.958037\n",
      "Train Epoch: 8 [0/5548 (0%)]\tLoss: 1.245251\n",
      "Train Epoch: 8 [640/5548 (11%)]\tLoss: 1.161013\n",
      "Train Epoch: 8 [1280/5548 (23%)]\tLoss: 1.030927\n",
      "Train Epoch: 8 [1920/5548 (34%)]\tLoss: 0.938101\n",
      "Train Epoch: 8 [2560/5548 (46%)]\tLoss: 1.318910\n",
      "Train Epoch: 8 [3200/5548 (57%)]\tLoss: 1.162219\n",
      "Train Epoch: 8 [3840/5548 (69%)]\tLoss: 0.849674\n",
      "Train Epoch: 8 [4480/5548 (80%)]\tLoss: 1.278473\n",
      "Train Epoch: 8 [5120/5548 (92%)]\tLoss: 0.907216\n",
      "Train Epoch: 9 [0/5548 (0%)]\tLoss: 0.947126\n",
      "Train Epoch: 9 [640/5548 (11%)]\tLoss: 1.079535\n",
      "Train Epoch: 9 [1280/5548 (23%)]\tLoss: 1.062693\n",
      "Train Epoch: 9 [1920/5548 (34%)]\tLoss: 0.981513\n",
      "Train Epoch: 9 [2560/5548 (46%)]\tLoss: 0.950703\n",
      "Train Epoch: 9 [3200/5548 (57%)]\tLoss: 1.147781\n",
      "Train Epoch: 9 [3840/5548 (69%)]\tLoss: 1.124586\n",
      "Train Epoch: 9 [4480/5548 (80%)]\tLoss: 1.124114\n",
      "Train Epoch: 9 [5120/5548 (92%)]\tLoss: 0.979440\n",
      "Train Epoch: 10 [0/5548 (0%)]\tLoss: 1.094525\n",
      "Train Epoch: 10 [640/5548 (11%)]\tLoss: 1.114113\n",
      "Train Epoch: 10 [1280/5548 (23%)]\tLoss: 1.062883\n",
      "Train Epoch: 10 [1920/5548 (34%)]\tLoss: 1.091830\n",
      "Train Epoch: 10 [2560/5548 (46%)]\tLoss: 1.251125\n",
      "Train Epoch: 10 [3200/5548 (57%)]\tLoss: 1.050327\n",
      "Train Epoch: 10 [3840/5548 (69%)]\tLoss: 0.985730\n",
      "Train Epoch: 10 [4480/5548 (80%)]\tLoss: 1.114400\n",
      "Train Epoch: 10 [5120/5548 (92%)]\tLoss: 1.097720\n",
      "Train Epoch: 11 [0/5548 (0%)]\tLoss: 1.190717\n",
      "Train Epoch: 11 [640/5548 (11%)]\tLoss: 0.996703\n",
      "Train Epoch: 11 [1280/5548 (23%)]\tLoss: 0.769419\n",
      "Train Epoch: 11 [1920/5548 (34%)]\tLoss: 0.921454\n",
      "Train Epoch: 11 [2560/5548 (46%)]\tLoss: 1.185926\n",
      "Train Epoch: 11 [3200/5548 (57%)]\tLoss: 1.080723\n",
      "Train Epoch: 11 [3840/5548 (69%)]\tLoss: 0.896515\n",
      "Train Epoch: 11 [4480/5548 (80%)]\tLoss: 1.157201\n",
      "Train Epoch: 11 [5120/5548 (92%)]\tLoss: 0.939463\n",
      "Train Epoch: 12 [0/5548 (0%)]\tLoss: 0.948291\n",
      "Train Epoch: 12 [640/5548 (11%)]\tLoss: 0.863513\n",
      "Train Epoch: 12 [1280/5548 (23%)]\tLoss: 1.196655\n",
      "Train Epoch: 12 [1920/5548 (34%)]\tLoss: 1.003322\n",
      "Train Epoch: 12 [2560/5548 (46%)]\tLoss: 1.289784\n",
      "Train Epoch: 12 [3200/5548 (57%)]\tLoss: 1.083937\n",
      "Train Epoch: 12 [3840/5548 (69%)]\tLoss: 1.281866\n",
      "Train Epoch: 12 [4480/5548 (80%)]\tLoss: 1.029873\n",
      "Train Epoch: 12 [5120/5548 (92%)]\tLoss: 0.789094\n",
      "Train Epoch: 13 [0/5548 (0%)]\tLoss: 1.042172\n",
      "Train Epoch: 13 [640/5548 (11%)]\tLoss: 0.955740\n",
      "Train Epoch: 13 [1280/5548 (23%)]\tLoss: 1.065383\n",
      "Train Epoch: 13 [1920/5548 (34%)]\tLoss: 0.942121\n",
      "Train Epoch: 13 [2560/5548 (46%)]\tLoss: 1.181008\n",
      "Train Epoch: 13 [3200/5548 (57%)]\tLoss: 0.961386\n",
      "Train Epoch: 13 [3840/5548 (69%)]\tLoss: 1.260703\n",
      "Train Epoch: 13 [4480/5548 (80%)]\tLoss: 1.019089\n",
      "Train Epoch: 13 [5120/5548 (92%)]\tLoss: 1.149703\n",
      "Train Epoch: 14 [0/5548 (0%)]\tLoss: 1.114588\n",
      "Train Epoch: 14 [640/5548 (11%)]\tLoss: 1.004910\n",
      "Train Epoch: 14 [1280/5548 (23%)]\tLoss: 1.027722\n",
      "Train Epoch: 14 [1920/5548 (34%)]\tLoss: 1.052637\n",
      "Train Epoch: 14 [2560/5548 (46%)]\tLoss: 1.048921\n",
      "Train Epoch: 14 [3200/5548 (57%)]\tLoss: 0.967744\n",
      "Train Epoch: 14 [3840/5548 (69%)]\tLoss: 0.846223\n",
      "Train Epoch: 14 [4480/5548 (80%)]\tLoss: 0.867799\n",
      "Train Epoch: 14 [5120/5548 (92%)]\tLoss: 0.928433\n",
      "Train Epoch: 15 [0/5548 (0%)]\tLoss: 0.875938\n",
      "Train Epoch: 15 [640/5548 (11%)]\tLoss: 1.139931\n",
      "Train Epoch: 15 [1280/5548 (23%)]\tLoss: 1.014419\n",
      "Train Epoch: 15 [1920/5548 (34%)]\tLoss: 1.110810\n",
      "Train Epoch: 15 [2560/5548 (46%)]\tLoss: 1.105321\n",
      "Train Epoch: 15 [3200/5548 (57%)]\tLoss: 1.076678\n",
      "Train Epoch: 15 [3840/5548 (69%)]\tLoss: 1.205050\n",
      "Train Epoch: 15 [4480/5548 (80%)]\tLoss: 0.831106\n",
      "Train Epoch: 15 [5120/5548 (92%)]\tLoss: 1.099600\n",
      "Train Epoch: 16 [0/5548 (0%)]\tLoss: 0.918363\n",
      "Train Epoch: 16 [640/5548 (11%)]\tLoss: 1.199211\n",
      "Train Epoch: 16 [1280/5548 (23%)]\tLoss: 0.998207\n",
      "Train Epoch: 16 [1920/5548 (34%)]\tLoss: 1.023642\n",
      "Train Epoch: 16 [2560/5548 (46%)]\tLoss: 0.970852\n",
      "Train Epoch: 16 [3200/5548 (57%)]\tLoss: 0.963071\n",
      "Train Epoch: 16 [3840/5548 (69%)]\tLoss: 0.902983\n",
      "Train Epoch: 16 [4480/5548 (80%)]\tLoss: 0.933127\n",
      "Train Epoch: 16 [5120/5548 (92%)]\tLoss: 1.256820\n",
      "Train Epoch: 17 [0/5548 (0%)]\tLoss: 0.984580\n",
      "Train Epoch: 17 [640/5548 (11%)]\tLoss: 1.142108\n",
      "Train Epoch: 17 [1280/5548 (23%)]\tLoss: 0.917855\n",
      "Train Epoch: 17 [1920/5548 (34%)]\tLoss: 1.018164\n",
      "Train Epoch: 17 [2560/5548 (46%)]\tLoss: 1.202787\n",
      "Train Epoch: 17 [3200/5548 (57%)]\tLoss: 1.185471\n",
      "Train Epoch: 17 [3840/5548 (69%)]\tLoss: 0.953231\n",
      "Train Epoch: 17 [4480/5548 (80%)]\tLoss: 0.811679\n",
      "Train Epoch: 17 [5120/5548 (92%)]\tLoss: 1.041740\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/4097 (0%)]\tLoss: 1.391286\n",
      "Train Epoch: 1 [640/4097 (15%)]\tLoss: 1.350639\n",
      "Train Epoch: 1 [1280/4097 (31%)]\tLoss: 1.097826\n",
      "Train Epoch: 1 [1920/4097 (46%)]\tLoss: 1.260802\n",
      "Train Epoch: 1 [2560/4097 (62%)]\tLoss: 1.796310\n",
      "Train Epoch: 1 [3200/4097 (77%)]\tLoss: 1.448270\n",
      "Train Epoch: 1 [3840/4097 (92%)]\tLoss: 1.447358\n",
      "Train Epoch: 2 [0/4097 (0%)]\tLoss: 1.162509\n",
      "Train Epoch: 2 [640/4097 (15%)]\tLoss: 1.574957\n",
      "Train Epoch: 2 [1280/4097 (31%)]\tLoss: 1.197243\n",
      "Train Epoch: 2 [1920/4097 (46%)]\tLoss: 1.532088\n",
      "Train Epoch: 2 [2560/4097 (62%)]\tLoss: 1.399240\n",
      "Train Epoch: 2 [3200/4097 (77%)]\tLoss: 1.235202\n",
      "Train Epoch: 2 [3840/4097 (92%)]\tLoss: 1.421973\n",
      "Train Epoch: 3 [0/4097 (0%)]\tLoss: 1.317863\n",
      "Train Epoch: 3 [640/4097 (15%)]\tLoss: 1.116276\n",
      "Train Epoch: 3 [1280/4097 (31%)]\tLoss: 0.842148\n",
      "Train Epoch: 3 [1920/4097 (46%)]\tLoss: 0.913687\n",
      "Train Epoch: 3 [2560/4097 (62%)]\tLoss: 1.217705\n",
      "Train Epoch: 3 [3200/4097 (77%)]\tLoss: 1.404079\n",
      "Train Epoch: 3 [3840/4097 (92%)]\tLoss: 1.417693\n",
      "Train Epoch: 4 [0/4097 (0%)]\tLoss: 1.327149\n",
      "Train Epoch: 4 [640/4097 (15%)]\tLoss: 1.312034\n",
      "Train Epoch: 4 [1280/4097 (31%)]\tLoss: 0.937666\n",
      "Train Epoch: 4 [1920/4097 (46%)]\tLoss: 1.462934\n",
      "Train Epoch: 4 [2560/4097 (62%)]\tLoss: 1.156252\n",
      "Train Epoch: 4 [3200/4097 (77%)]\tLoss: 0.995449\n",
      "Train Epoch: 4 [3840/4097 (92%)]\tLoss: 1.308332\n",
      "Train Epoch: 5 [0/4097 (0%)]\tLoss: 1.207654\n",
      "Train Epoch: 5 [640/4097 (15%)]\tLoss: 1.227693\n",
      "Train Epoch: 5 [1280/4097 (31%)]\tLoss: 1.423953\n",
      "Train Epoch: 5 [1920/4097 (46%)]\tLoss: 1.327106\n",
      "Train Epoch: 5 [2560/4097 (62%)]\tLoss: 1.285256\n",
      "Train Epoch: 5 [3200/4097 (77%)]\tLoss: 1.002444\n",
      "Train Epoch: 5 [3840/4097 (92%)]\tLoss: 1.470122\n",
      "Train Epoch: 6 [0/4097 (0%)]\tLoss: 1.232830\n",
      "Train Epoch: 6 [640/4097 (15%)]\tLoss: 1.267112\n",
      "Train Epoch: 6 [1280/4097 (31%)]\tLoss: 1.430056\n",
      "Train Epoch: 6 [1920/4097 (46%)]\tLoss: 1.291789\n",
      "Train Epoch: 6 [2560/4097 (62%)]\tLoss: 1.536621\n",
      "Train Epoch: 6 [3200/4097 (77%)]\tLoss: 1.274861\n",
      "Train Epoch: 6 [3840/4097 (92%)]\tLoss: 1.187685\n",
      "Train Epoch: 7 [0/4097 (0%)]\tLoss: 1.102815\n",
      "Train Epoch: 7 [640/4097 (15%)]\tLoss: 1.415634\n",
      "Train Epoch: 7 [1280/4097 (31%)]\tLoss: 1.314098\n",
      "Train Epoch: 7 [1920/4097 (46%)]\tLoss: 1.310984\n",
      "Train Epoch: 7 [2560/4097 (62%)]\tLoss: 1.285019\n",
      "Train Epoch: 7 [3200/4097 (77%)]\tLoss: 1.059295\n",
      "Train Epoch: 7 [3840/4097 (92%)]\tLoss: 1.116408\n",
      "Train Epoch: 8 [0/4097 (0%)]\tLoss: 1.735123\n",
      "Train Epoch: 8 [640/4097 (15%)]\tLoss: 1.572681\n",
      "Train Epoch: 8 [1280/4097 (31%)]\tLoss: 1.346117\n",
      "Train Epoch: 8 [1920/4097 (46%)]\tLoss: 1.346325\n",
      "Train Epoch: 8 [2560/4097 (62%)]\tLoss: 1.231194\n",
      "Train Epoch: 8 [3200/4097 (77%)]\tLoss: 1.315079\n",
      "Train Epoch: 8 [3840/4097 (92%)]\tLoss: 1.207905\n",
      "Train Epoch: 9 [0/4097 (0%)]\tLoss: 1.738513\n",
      "Train Epoch: 9 [640/4097 (15%)]\tLoss: 1.253216\n",
      "Train Epoch: 9 [1280/4097 (31%)]\tLoss: 1.280006\n",
      "Train Epoch: 9 [1920/4097 (46%)]\tLoss: 1.448088\n",
      "Train Epoch: 9 [2560/4097 (62%)]\tLoss: 1.417059\n",
      "Train Epoch: 9 [3200/4097 (77%)]\tLoss: 1.222751\n",
      "Train Epoch: 9 [3840/4097 (92%)]\tLoss: 1.343297\n",
      "Train Epoch: 10 [0/4097 (0%)]\tLoss: 1.354751\n",
      "Train Epoch: 10 [640/4097 (15%)]\tLoss: 1.233802\n",
      "Train Epoch: 10 [1280/4097 (31%)]\tLoss: 1.329260\n",
      "Train Epoch: 10 [1920/4097 (46%)]\tLoss: 1.290394\n",
      "Train Epoch: 10 [2560/4097 (62%)]\tLoss: 1.293723\n",
      "Train Epoch: 10 [3200/4097 (77%)]\tLoss: 1.231136\n",
      "Train Epoch: 10 [3840/4097 (92%)]\tLoss: 1.189139\n",
      "Train Epoch: 11 [0/4097 (0%)]\tLoss: 1.485104\n",
      "Train Epoch: 11 [640/4097 (15%)]\tLoss: 1.326505\n",
      "Train Epoch: 11 [1280/4097 (31%)]\tLoss: 1.233972\n",
      "Train Epoch: 11 [1920/4097 (46%)]\tLoss: 1.192498\n",
      "Train Epoch: 11 [2560/4097 (62%)]\tLoss: 1.386078\n",
      "Train Epoch: 11 [3200/4097 (77%)]\tLoss: 1.247499\n",
      "Train Epoch: 11 [3840/4097 (92%)]\tLoss: 1.493346\n",
      "Train Epoch: 12 [0/4097 (0%)]\tLoss: 1.389739\n",
      "Train Epoch: 12 [640/4097 (15%)]\tLoss: 1.083195\n",
      "Train Epoch: 12 [1280/4097 (31%)]\tLoss: 1.202689\n",
      "Train Epoch: 12 [1920/4097 (46%)]\tLoss: 1.249961\n",
      "Train Epoch: 12 [2560/4097 (62%)]\tLoss: 1.454318\n",
      "Train Epoch: 12 [3200/4097 (77%)]\tLoss: 1.276648\n",
      "Train Epoch: 12 [3840/4097 (92%)]\tLoss: 1.222806\n",
      "Train Epoch: 13 [0/4097 (0%)]\tLoss: 1.210712\n",
      "Train Epoch: 13 [640/4097 (15%)]\tLoss: 1.505292\n",
      "Train Epoch: 13 [1280/4097 (31%)]\tLoss: 1.208154\n",
      "Train Epoch: 13 [1920/4097 (46%)]\tLoss: 0.963514\n",
      "Train Epoch: 13 [2560/4097 (62%)]\tLoss: 1.142248\n",
      "Train Epoch: 13 [3200/4097 (77%)]\tLoss: 1.175606\n",
      "Train Epoch: 13 [3840/4097 (92%)]\tLoss: 1.451060\n",
      "Train Epoch: 14 [0/4097 (0%)]\tLoss: 1.487383\n",
      "Train Epoch: 14 [640/4097 (15%)]\tLoss: 1.252903\n",
      "Train Epoch: 14 [1280/4097 (31%)]\tLoss: 1.177066\n",
      "Train Epoch: 14 [1920/4097 (46%)]\tLoss: 1.098252\n",
      "Train Epoch: 14 [2560/4097 (62%)]\tLoss: 1.387025\n",
      "Train Epoch: 14 [3200/4097 (77%)]\tLoss: 1.114620\n",
      "Train Epoch: 14 [3840/4097 (92%)]\tLoss: 1.222418\n",
      "Train Epoch: 15 [0/4097 (0%)]\tLoss: 1.296576\n",
      "Train Epoch: 15 [640/4097 (15%)]\tLoss: 1.221549\n",
      "Train Epoch: 15 [1280/4097 (31%)]\tLoss: 1.386540\n",
      "Train Epoch: 15 [1920/4097 (46%)]\tLoss: 1.064021\n",
      "Train Epoch: 15 [2560/4097 (62%)]\tLoss: 1.227977\n",
      "Train Epoch: 15 [3200/4097 (77%)]\tLoss: 1.104419\n",
      "Train Epoch: 15 [3840/4097 (92%)]\tLoss: 1.294445\n",
      "Train Epoch: 16 [0/4097 (0%)]\tLoss: 1.281818\n",
      "Train Epoch: 16 [640/4097 (15%)]\tLoss: 1.121671\n",
      "Train Epoch: 16 [1280/4097 (31%)]\tLoss: 1.248054\n",
      "Train Epoch: 16 [1920/4097 (46%)]\tLoss: 1.382555\n",
      "Train Epoch: 16 [2560/4097 (62%)]\tLoss: 1.199742\n",
      "Train Epoch: 16 [3200/4097 (77%)]\tLoss: 1.217434\n",
      "Train Epoch: 16 [3840/4097 (92%)]\tLoss: 1.229187\n",
      "Train Epoch: 17 [0/4097 (0%)]\tLoss: 1.138433\n",
      "Train Epoch: 17 [640/4097 (15%)]\tLoss: 1.108030\n",
      "Train Epoch: 17 [1280/4097 (31%)]\tLoss: 1.584511\n",
      "Train Epoch: 17 [1920/4097 (46%)]\tLoss: 1.185763\n",
      "Train Epoch: 17 [2560/4097 (62%)]\tLoss: 1.496292\n",
      "Train Epoch: 17 [3200/4097 (77%)]\tLoss: 1.333930\n",
      "Train Epoch: 17 [3840/4097 (92%)]\tLoss: 1.251617\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/8272 (0%)]\tLoss: 1.607132\n",
      "Train Epoch: 1 [640/8272 (8%)]\tLoss: 1.579780\n",
      "Train Epoch: 1 [1280/8272 (15%)]\tLoss: 1.691078\n",
      "Train Epoch: 1 [1920/8272 (23%)]\tLoss: 1.437204\n",
      "Train Epoch: 1 [2560/8272 (31%)]\tLoss: 1.140818\n",
      "Train Epoch: 1 [3200/8272 (38%)]\tLoss: 1.384937\n",
      "Train Epoch: 1 [3840/8272 (46%)]\tLoss: 1.454685\n",
      "Train Epoch: 1 [4480/8272 (54%)]\tLoss: 1.528408\n",
      "Train Epoch: 1 [5120/8272 (62%)]\tLoss: 1.748337\n",
      "Train Epoch: 1 [5760/8272 (69%)]\tLoss: 1.418362\n",
      "Train Epoch: 1 [6400/8272 (77%)]\tLoss: 1.105797\n",
      "Train Epoch: 1 [7040/8272 (85%)]\tLoss: 1.130790\n",
      "Train Epoch: 1 [7680/8272 (92%)]\tLoss: 1.307602\n",
      "Train Epoch: 2 [0/8272 (0%)]\tLoss: 1.365921\n",
      "Train Epoch: 2 [640/8272 (8%)]\tLoss: 1.307088\n",
      "Train Epoch: 2 [1280/8272 (15%)]\tLoss: 1.234725\n",
      "Train Epoch: 2 [1920/8272 (23%)]\tLoss: 1.413643\n",
      "Train Epoch: 2 [2560/8272 (31%)]\tLoss: 1.334922\n",
      "Train Epoch: 2 [3200/8272 (38%)]\tLoss: 1.293811\n",
      "Train Epoch: 2 [3840/8272 (46%)]\tLoss: 1.329321\n",
      "Train Epoch: 2 [4480/8272 (54%)]\tLoss: 1.401875\n",
      "Train Epoch: 2 [5120/8272 (62%)]\tLoss: 1.361402\n",
      "Train Epoch: 2 [5760/8272 (69%)]\tLoss: 1.335262\n",
      "Train Epoch: 2 [6400/8272 (77%)]\tLoss: 1.399573\n",
      "Train Epoch: 2 [7040/8272 (85%)]\tLoss: 1.479535\n",
      "Train Epoch: 2 [7680/8272 (92%)]\tLoss: 1.377654\n",
      "Train Epoch: 3 [0/8272 (0%)]\tLoss: 1.122394\n",
      "Train Epoch: 3 [640/8272 (8%)]\tLoss: 1.172236\n",
      "Train Epoch: 3 [1280/8272 (15%)]\tLoss: 1.481949\n",
      "Train Epoch: 3 [1920/8272 (23%)]\tLoss: 1.507965\n",
      "Train Epoch: 3 [2560/8272 (31%)]\tLoss: 1.280481\n",
      "Train Epoch: 3 [3200/8272 (38%)]\tLoss: 1.242483\n",
      "Train Epoch: 3 [3840/8272 (46%)]\tLoss: 1.397073\n",
      "Train Epoch: 3 [4480/8272 (54%)]\tLoss: 1.348229\n",
      "Train Epoch: 3 [5120/8272 (62%)]\tLoss: 1.118544\n",
      "Train Epoch: 3 [5760/8272 (69%)]\tLoss: 1.124966\n",
      "Train Epoch: 3 [6400/8272 (77%)]\tLoss: 1.155719\n",
      "Train Epoch: 3 [7040/8272 (85%)]\tLoss: 1.468740\n",
      "Train Epoch: 3 [7680/8272 (92%)]\tLoss: 1.377455\n",
      "Train Epoch: 4 [0/8272 (0%)]\tLoss: 1.634622\n",
      "Train Epoch: 4 [640/8272 (8%)]\tLoss: 1.139195\n",
      "Train Epoch: 4 [1280/8272 (15%)]\tLoss: 1.444077\n",
      "Train Epoch: 4 [1920/8272 (23%)]\tLoss: 1.351637\n",
      "Train Epoch: 4 [2560/8272 (31%)]\tLoss: 1.278366\n",
      "Train Epoch: 4 [3200/8272 (38%)]\tLoss: 1.140266\n",
      "Train Epoch: 4 [3840/8272 (46%)]\tLoss: 1.493670\n",
      "Train Epoch: 4 [4480/8272 (54%)]\tLoss: 1.137715\n",
      "Train Epoch: 4 [5120/8272 (62%)]\tLoss: 1.102017\n",
      "Train Epoch: 4 [5760/8272 (69%)]\tLoss: 1.223814\n",
      "Train Epoch: 4 [6400/8272 (77%)]\tLoss: 1.410599\n",
      "Train Epoch: 4 [7040/8272 (85%)]\tLoss: 1.480975\n",
      "Train Epoch: 4 [7680/8272 (92%)]\tLoss: 1.066050\n",
      "Train Epoch: 5 [0/8272 (0%)]\tLoss: 1.085301\n",
      "Train Epoch: 5 [640/8272 (8%)]\tLoss: 1.341314\n",
      "Train Epoch: 5 [1280/8272 (15%)]\tLoss: 1.390292\n",
      "Train Epoch: 5 [1920/8272 (23%)]\tLoss: 1.299002\n",
      "Train Epoch: 5 [2560/8272 (31%)]\tLoss: 1.245070\n",
      "Train Epoch: 5 [3200/8272 (38%)]\tLoss: 1.229831\n",
      "Train Epoch: 5 [3840/8272 (46%)]\tLoss: 1.332764\n",
      "Train Epoch: 5 [4480/8272 (54%)]\tLoss: 1.400605\n",
      "Train Epoch: 5 [5120/8272 (62%)]\tLoss: 1.469725\n",
      "Train Epoch: 5 [5760/8272 (69%)]\tLoss: 1.204075\n",
      "Train Epoch: 5 [6400/8272 (77%)]\tLoss: 1.186962\n",
      "Train Epoch: 5 [7040/8272 (85%)]\tLoss: 1.137933\n",
      "Train Epoch: 5 [7680/8272 (92%)]\tLoss: 1.421774\n",
      "Train Epoch: 6 [0/8272 (0%)]\tLoss: 1.337434\n",
      "Train Epoch: 6 [640/8272 (8%)]\tLoss: 1.483359\n",
      "Train Epoch: 6 [1280/8272 (15%)]\tLoss: 1.586323\n",
      "Train Epoch: 6 [1920/8272 (23%)]\tLoss: 1.325006\n",
      "Train Epoch: 6 [2560/8272 (31%)]\tLoss: 1.592450\n",
      "Train Epoch: 6 [3200/8272 (38%)]\tLoss: 1.223551\n",
      "Train Epoch: 6 [3840/8272 (46%)]\tLoss: 1.130113\n",
      "Train Epoch: 6 [4480/8272 (54%)]\tLoss: 1.434860\n",
      "Train Epoch: 6 [5120/8272 (62%)]\tLoss: 1.181738\n",
      "Train Epoch: 6 [5760/8272 (69%)]\tLoss: 1.119683\n",
      "Train Epoch: 6 [6400/8272 (77%)]\tLoss: 1.242659\n",
      "Train Epoch: 6 [7040/8272 (85%)]\tLoss: 1.465830\n",
      "Train Epoch: 6 [7680/8272 (92%)]\tLoss: 1.430927\n",
      "Train Epoch: 7 [0/8272 (0%)]\tLoss: 1.259938\n",
      "Train Epoch: 7 [640/8272 (8%)]\tLoss: 1.246961\n",
      "Train Epoch: 7 [1280/8272 (15%)]\tLoss: 1.153769\n",
      "Train Epoch: 7 [1920/8272 (23%)]\tLoss: 1.336161\n",
      "Train Epoch: 7 [2560/8272 (31%)]\tLoss: 1.021913\n",
      "Train Epoch: 7 [3200/8272 (38%)]\tLoss: 1.196035\n",
      "Train Epoch: 7 [3840/8272 (46%)]\tLoss: 1.339538\n",
      "Train Epoch: 7 [4480/8272 (54%)]\tLoss: 1.359514\n",
      "Train Epoch: 7 [5120/8272 (62%)]\tLoss: 1.385447\n",
      "Train Epoch: 7 [5760/8272 (69%)]\tLoss: 1.246709\n",
      "Train Epoch: 7 [6400/8272 (77%)]\tLoss: 1.539994\n",
      "Train Epoch: 7 [7040/8272 (85%)]\tLoss: 1.246912\n",
      "Train Epoch: 7 [7680/8272 (92%)]\tLoss: 1.186621\n",
      "Train Epoch: 8 [0/8272 (0%)]\tLoss: 1.686391\n",
      "Train Epoch: 8 [640/8272 (8%)]\tLoss: 1.467433\n",
      "Train Epoch: 8 [1280/8272 (15%)]\tLoss: 1.376357\n",
      "Train Epoch: 8 [1920/8272 (23%)]\tLoss: 1.177539\n",
      "Train Epoch: 8 [2560/8272 (31%)]\tLoss: 1.128231\n",
      "Train Epoch: 8 [3200/8272 (38%)]\tLoss: 1.280977\n",
      "Train Epoch: 8 [3840/8272 (46%)]\tLoss: 1.872594\n",
      "Train Epoch: 8 [4480/8272 (54%)]\tLoss: 1.422120\n",
      "Train Epoch: 8 [5120/8272 (62%)]\tLoss: 1.308102\n",
      "Train Epoch: 8 [5760/8272 (69%)]\tLoss: 1.539696\n",
      "Train Epoch: 8 [6400/8272 (77%)]\tLoss: 1.222882\n",
      "Train Epoch: 8 [7040/8272 (85%)]\tLoss: 1.126270\n",
      "Train Epoch: 8 [7680/8272 (92%)]\tLoss: 1.179130\n",
      "Train Epoch: 9 [0/8272 (0%)]\tLoss: 1.538098\n",
      "Train Epoch: 9 [640/8272 (8%)]\tLoss: 0.968462\n",
      "Train Epoch: 9 [1280/8272 (15%)]\tLoss: 1.244106\n",
      "Train Epoch: 9 [1920/8272 (23%)]\tLoss: 1.109052\n",
      "Train Epoch: 9 [2560/8272 (31%)]\tLoss: 1.303602\n",
      "Train Epoch: 9 [3200/8272 (38%)]\tLoss: 1.353521\n",
      "Train Epoch: 9 [3840/8272 (46%)]\tLoss: 1.161102\n",
      "Train Epoch: 9 [4480/8272 (54%)]\tLoss: 1.225730\n",
      "Train Epoch: 9 [5120/8272 (62%)]\tLoss: 1.218264\n",
      "Train Epoch: 9 [5760/8272 (69%)]\tLoss: 1.434913\n",
      "Train Epoch: 9 [6400/8272 (77%)]\tLoss: 1.150440\n",
      "Train Epoch: 9 [7040/8272 (85%)]\tLoss: 1.424445\n",
      "Train Epoch: 9 [7680/8272 (92%)]\tLoss: 1.187269\n",
      "Train Epoch: 10 [0/8272 (0%)]\tLoss: 1.236266\n",
      "Train Epoch: 10 [640/8272 (8%)]\tLoss: 1.297336\n",
      "Train Epoch: 10 [1280/8272 (15%)]\tLoss: 1.076990\n",
      "Train Epoch: 10 [1920/8272 (23%)]\tLoss: 1.122575\n",
      "Train Epoch: 10 [2560/8272 (31%)]\tLoss: 1.497139\n",
      "Train Epoch: 10 [3200/8272 (38%)]\tLoss: 1.360618\n",
      "Train Epoch: 10 [3840/8272 (46%)]\tLoss: 1.388828\n",
      "Train Epoch: 10 [4480/8272 (54%)]\tLoss: 1.202618\n",
      "Train Epoch: 10 [5120/8272 (62%)]\tLoss: 1.197475\n",
      "Train Epoch: 10 [5760/8272 (69%)]\tLoss: 1.483720\n",
      "Train Epoch: 10 [6400/8272 (77%)]\tLoss: 1.373551\n",
      "Train Epoch: 10 [7040/8272 (85%)]\tLoss: 1.241022\n",
      "Train Epoch: 10 [7680/8272 (92%)]\tLoss: 1.238324\n",
      "Train Epoch: 11 [0/8272 (0%)]\tLoss: 1.399229\n",
      "Train Epoch: 11 [640/8272 (8%)]\tLoss: 1.219351\n",
      "Train Epoch: 11 [1280/8272 (15%)]\tLoss: 1.312154\n",
      "Train Epoch: 11 [1920/8272 (23%)]\tLoss: 1.295537\n",
      "Train Epoch: 11 [2560/8272 (31%)]\tLoss: 1.251276\n",
      "Train Epoch: 11 [3200/8272 (38%)]\tLoss: 1.463271\n",
      "Train Epoch: 11 [3840/8272 (46%)]\tLoss: 0.990639\n",
      "Train Epoch: 11 [4480/8272 (54%)]\tLoss: 1.240232\n",
      "Train Epoch: 11 [5120/8272 (62%)]\tLoss: 1.365140\n",
      "Train Epoch: 11 [5760/8272 (69%)]\tLoss: 1.040376\n",
      "Train Epoch: 11 [6400/8272 (77%)]\tLoss: 0.931157\n",
      "Train Epoch: 11 [7040/8272 (85%)]\tLoss: 1.323498\n",
      "Train Epoch: 11 [7680/8272 (92%)]\tLoss: 1.631736\n",
      "Train Epoch: 12 [0/8272 (0%)]\tLoss: 1.084808\n",
      "Train Epoch: 12 [640/8272 (8%)]\tLoss: 1.210539\n",
      "Train Epoch: 12 [1280/8272 (15%)]\tLoss: 1.242962\n",
      "Train Epoch: 12 [1920/8272 (23%)]\tLoss: 1.177565\n",
      "Train Epoch: 12 [2560/8272 (31%)]\tLoss: 1.254735\n",
      "Train Epoch: 12 [3200/8272 (38%)]\tLoss: 1.226791\n",
      "Train Epoch: 12 [3840/8272 (46%)]\tLoss: 1.371637\n",
      "Train Epoch: 12 [4480/8272 (54%)]\tLoss: 1.135563\n",
      "Train Epoch: 12 [5120/8272 (62%)]\tLoss: 1.637237\n",
      "Train Epoch: 12 [5760/8272 (69%)]\tLoss: 1.185735\n",
      "Train Epoch: 12 [6400/8272 (77%)]\tLoss: 1.455076\n",
      "Train Epoch: 12 [7040/8272 (85%)]\tLoss: 1.226919\n",
      "Train Epoch: 12 [7680/8272 (92%)]\tLoss: 1.396964\n",
      "Train Epoch: 13 [0/8272 (0%)]\tLoss: 1.064570\n",
      "Train Epoch: 13 [640/8272 (8%)]\tLoss: 1.465335\n",
      "Train Epoch: 13 [1280/8272 (15%)]\tLoss: 1.393054\n",
      "Train Epoch: 13 [1920/8272 (23%)]\tLoss: 1.348625\n",
      "Train Epoch: 13 [2560/8272 (31%)]\tLoss: 1.442891\n",
      "Train Epoch: 13 [3200/8272 (38%)]\tLoss: 1.496992\n",
      "Train Epoch: 13 [3840/8272 (46%)]\tLoss: 1.057219\n",
      "Train Epoch: 13 [4480/8272 (54%)]\tLoss: 1.419017\n",
      "Train Epoch: 13 [5120/8272 (62%)]\tLoss: 1.195507\n",
      "Train Epoch: 13 [5760/8272 (69%)]\tLoss: 1.211863\n",
      "Train Epoch: 13 [6400/8272 (77%)]\tLoss: 1.570491\n",
      "Train Epoch: 13 [7040/8272 (85%)]\tLoss: 1.177191\n",
      "Train Epoch: 13 [7680/8272 (92%)]\tLoss: 1.175216\n",
      "Train Epoch: 14 [0/8272 (0%)]\tLoss: 1.491106\n",
      "Train Epoch: 14 [640/8272 (8%)]\tLoss: 1.428119\n",
      "Train Epoch: 14 [1280/8272 (15%)]\tLoss: 1.367809\n",
      "Train Epoch: 14 [1920/8272 (23%)]\tLoss: 1.263669\n",
      "Train Epoch: 14 [2560/8272 (31%)]\tLoss: 1.068852\n",
      "Train Epoch: 14 [3200/8272 (38%)]\tLoss: 1.331395\n",
      "Train Epoch: 14 [3840/8272 (46%)]\tLoss: 1.133349\n",
      "Train Epoch: 14 [4480/8272 (54%)]\tLoss: 0.966470\n",
      "Train Epoch: 14 [5120/8272 (62%)]\tLoss: 1.359862\n",
      "Train Epoch: 14 [5760/8272 (69%)]\tLoss: 1.302112\n",
      "Train Epoch: 14 [6400/8272 (77%)]\tLoss: 1.055024\n",
      "Train Epoch: 14 [7040/8272 (85%)]\tLoss: 1.275841\n",
      "Train Epoch: 14 [7680/8272 (92%)]\tLoss: 1.054823\n",
      "Train Epoch: 15 [0/8272 (0%)]\tLoss: 1.221816\n",
      "Train Epoch: 15 [640/8272 (8%)]\tLoss: 1.265964\n",
      "Train Epoch: 15 [1280/8272 (15%)]\tLoss: 1.168653\n",
      "Train Epoch: 15 [1920/8272 (23%)]\tLoss: 1.034557\n",
      "Train Epoch: 15 [2560/8272 (31%)]\tLoss: 1.139728\n",
      "Train Epoch: 15 [3200/8272 (38%)]\tLoss: 1.364650\n",
      "Train Epoch: 15 [3840/8272 (46%)]\tLoss: 1.223947\n",
      "Train Epoch: 15 [4480/8272 (54%)]\tLoss: 1.078011\n",
      "Train Epoch: 15 [5120/8272 (62%)]\tLoss: 0.935090\n",
      "Train Epoch: 15 [5760/8272 (69%)]\tLoss: 1.326819\n",
      "Train Epoch: 15 [6400/8272 (77%)]\tLoss: 1.347431\n",
      "Train Epoch: 15 [7040/8272 (85%)]\tLoss: 1.275048\n",
      "Train Epoch: 15 [7680/8272 (92%)]\tLoss: 1.251947\n",
      "Train Epoch: 16 [0/8272 (0%)]\tLoss: 1.176603\n",
      "Train Epoch: 16 [640/8272 (8%)]\tLoss: 1.116278\n",
      "Train Epoch: 16 [1280/8272 (15%)]\tLoss: 1.522763\n",
      "Train Epoch: 16 [1920/8272 (23%)]\tLoss: 1.159350\n",
      "Train Epoch: 16 [2560/8272 (31%)]\tLoss: 1.298071\n",
      "Train Epoch: 16 [3200/8272 (38%)]\tLoss: 1.267386\n",
      "Train Epoch: 16 [3840/8272 (46%)]\tLoss: 1.468446\n",
      "Train Epoch: 16 [4480/8272 (54%)]\tLoss: 1.421444\n",
      "Train Epoch: 16 [5120/8272 (62%)]\tLoss: 1.260700\n",
      "Train Epoch: 16 [5760/8272 (69%)]\tLoss: 1.424358\n",
      "Train Epoch: 16 [6400/8272 (77%)]\tLoss: 1.648160\n",
      "Train Epoch: 16 [7040/8272 (85%)]\tLoss: 1.152595\n",
      "Train Epoch: 16 [7680/8272 (92%)]\tLoss: 1.326107\n",
      "Train Epoch: 17 [0/8272 (0%)]\tLoss: 1.316944\n",
      "Train Epoch: 17 [640/8272 (8%)]\tLoss: 1.534980\n",
      "Train Epoch: 17 [1280/8272 (15%)]\tLoss: 1.055769\n",
      "Train Epoch: 17 [1920/8272 (23%)]\tLoss: 1.314736\n",
      "Train Epoch: 17 [2560/8272 (31%)]\tLoss: 1.160109\n",
      "Train Epoch: 17 [3200/8272 (38%)]\tLoss: 1.262626\n",
      "Train Epoch: 17 [3840/8272 (46%)]\tLoss: 1.172188\n",
      "Train Epoch: 17 [4480/8272 (54%)]\tLoss: 1.388504\n",
      "Train Epoch: 17 [5120/8272 (62%)]\tLoss: 1.188150\n",
      "Train Epoch: 17 [5760/8272 (69%)]\tLoss: 0.973665\n",
      "Train Epoch: 17 [6400/8272 (77%)]\tLoss: 1.387257\n",
      "Train Epoch: 17 [7040/8272 (85%)]\tLoss: 1.341096\n",
      "Train Epoch: 17 [7680/8272 (92%)]\tLoss: 1.163675\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/5100 (0%)]\tLoss: 1.524139\n",
      "Train Epoch: 1 [640/5100 (12%)]\tLoss: 1.427342\n",
      "Train Epoch: 1 [1280/5100 (25%)]\tLoss: 1.294908\n",
      "Train Epoch: 1 [1920/5100 (38%)]\tLoss: 1.171470\n",
      "Train Epoch: 1 [2560/5100 (50%)]\tLoss: 1.142733\n",
      "Train Epoch: 1 [3200/5100 (62%)]\tLoss: 1.375241\n",
      "Train Epoch: 1 [3840/5100 (75%)]\tLoss: 1.054973\n",
      "Train Epoch: 1 [4480/5100 (88%)]\tLoss: 1.259107\n",
      "Train Epoch: 2 [0/5100 (0%)]\tLoss: 1.233996\n",
      "Train Epoch: 2 [640/5100 (12%)]\tLoss: 1.213594\n",
      "Train Epoch: 2 [1280/5100 (25%)]\tLoss: 1.272380\n",
      "Train Epoch: 2 [1920/5100 (38%)]\tLoss: 1.420602\n",
      "Train Epoch: 2 [2560/5100 (50%)]\tLoss: 1.184191\n",
      "Train Epoch: 2 [3200/5100 (62%)]\tLoss: 1.405609\n",
      "Train Epoch: 2 [3840/5100 (75%)]\tLoss: 1.256647\n",
      "Train Epoch: 2 [4480/5100 (88%)]\tLoss: 1.346696\n",
      "Train Epoch: 3 [0/5100 (0%)]\tLoss: 1.022020\n",
      "Train Epoch: 3 [640/5100 (12%)]\tLoss: 1.025093\n",
      "Train Epoch: 3 [1280/5100 (25%)]\tLoss: 1.146819\n",
      "Train Epoch: 3 [1920/5100 (38%)]\tLoss: 1.224653\n",
      "Train Epoch: 3 [2560/5100 (50%)]\tLoss: 1.210270\n",
      "Train Epoch: 3 [3200/5100 (62%)]\tLoss: 1.101315\n",
      "Train Epoch: 3 [3840/5100 (75%)]\tLoss: 1.142266\n",
      "Train Epoch: 3 [4480/5100 (88%)]\tLoss: 1.482595\n",
      "Train Epoch: 4 [0/5100 (0%)]\tLoss: 1.379540\n",
      "Train Epoch: 4 [640/5100 (12%)]\tLoss: 1.317574\n",
      "Train Epoch: 4 [1280/5100 (25%)]\tLoss: 1.224479\n",
      "Train Epoch: 4 [1920/5100 (38%)]\tLoss: 0.922696\n",
      "Train Epoch: 4 [2560/5100 (50%)]\tLoss: 1.214927\n",
      "Train Epoch: 4 [3200/5100 (62%)]\tLoss: 1.062349\n",
      "Train Epoch: 4 [3840/5100 (75%)]\tLoss: 1.350388\n",
      "Train Epoch: 4 [4480/5100 (88%)]\tLoss: 1.038568\n",
      "Train Epoch: 5 [0/5100 (0%)]\tLoss: 1.317686\n",
      "Train Epoch: 5 [640/5100 (12%)]\tLoss: 1.262203\n",
      "Train Epoch: 5 [1280/5100 (25%)]\tLoss: 1.260072\n",
      "Train Epoch: 5 [1920/5100 (38%)]\tLoss: 1.380120\n",
      "Train Epoch: 5 [2560/5100 (50%)]\tLoss: 1.176620\n",
      "Train Epoch: 5 [3200/5100 (62%)]\tLoss: 1.143063\n",
      "Train Epoch: 5 [3840/5100 (75%)]\tLoss: 1.031067\n",
      "Train Epoch: 5 [4480/5100 (88%)]\tLoss: 1.114490\n",
      "Train Epoch: 6 [0/5100 (0%)]\tLoss: 1.333468\n",
      "Train Epoch: 6 [640/5100 (12%)]\tLoss: 1.086831\n",
      "Train Epoch: 6 [1280/5100 (25%)]\tLoss: 1.139185\n",
      "Train Epoch: 6 [1920/5100 (38%)]\tLoss: 1.147591\n",
      "Train Epoch: 6 [2560/5100 (50%)]\tLoss: 1.024357\n",
      "Train Epoch: 6 [3200/5100 (62%)]\tLoss: 0.962956\n",
      "Train Epoch: 6 [3840/5100 (75%)]\tLoss: 0.913675\n",
      "Train Epoch: 6 [4480/5100 (88%)]\tLoss: 1.443032\n",
      "Train Epoch: 7 [0/5100 (0%)]\tLoss: 1.581139\n",
      "Train Epoch: 7 [640/5100 (12%)]\tLoss: 1.287191\n",
      "Train Epoch: 7 [1280/5100 (25%)]\tLoss: 1.221172\n",
      "Train Epoch: 7 [1920/5100 (38%)]\tLoss: 1.129931\n",
      "Train Epoch: 7 [2560/5100 (50%)]\tLoss: 1.209457\n",
      "Train Epoch: 7 [3200/5100 (62%)]\tLoss: 1.310212\n",
      "Train Epoch: 7 [3840/5100 (75%)]\tLoss: 0.961658\n",
      "Train Epoch: 7 [4480/5100 (88%)]\tLoss: 1.129322\n",
      "Train Epoch: 8 [0/5100 (0%)]\tLoss: 1.165513\n",
      "Train Epoch: 8 [640/5100 (12%)]\tLoss: 1.464092\n",
      "Train Epoch: 8 [1280/5100 (25%)]\tLoss: 1.116636\n",
      "Train Epoch: 8 [1920/5100 (38%)]\tLoss: 1.220895\n",
      "Train Epoch: 8 [2560/5100 (50%)]\tLoss: 1.098341\n",
      "Train Epoch: 8 [3200/5100 (62%)]\tLoss: 1.086417\n",
      "Train Epoch: 8 [3840/5100 (75%)]\tLoss: 1.360116\n",
      "Train Epoch: 8 [4480/5100 (88%)]\tLoss: 1.406763\n",
      "Train Epoch: 9 [0/5100 (0%)]\tLoss: 1.124186\n",
      "Train Epoch: 9 [640/5100 (12%)]\tLoss: 1.117761\n",
      "Train Epoch: 9 [1280/5100 (25%)]\tLoss: 1.155745\n",
      "Train Epoch: 9 [1920/5100 (38%)]\tLoss: 1.210495\n",
      "Train Epoch: 9 [2560/5100 (50%)]\tLoss: 1.108915\n",
      "Train Epoch: 9 [3200/5100 (62%)]\tLoss: 1.230632\n",
      "Train Epoch: 9 [3840/5100 (75%)]\tLoss: 1.275456\n",
      "Train Epoch: 9 [4480/5100 (88%)]\tLoss: 1.200214\n",
      "Train Epoch: 10 [0/5100 (0%)]\tLoss: 1.159271\n",
      "Train Epoch: 10 [640/5100 (12%)]\tLoss: 1.118031\n",
      "Train Epoch: 10 [1280/5100 (25%)]\tLoss: 1.210120\n",
      "Train Epoch: 10 [1920/5100 (38%)]\tLoss: 1.124345\n",
      "Train Epoch: 10 [2560/5100 (50%)]\tLoss: 1.115295\n",
      "Train Epoch: 10 [3200/5100 (62%)]\tLoss: 1.239985\n",
      "Train Epoch: 10 [3840/5100 (75%)]\tLoss: 1.278216\n",
      "Train Epoch: 10 [4480/5100 (88%)]\tLoss: 0.917370\n",
      "Train Epoch: 11 [0/5100 (0%)]\tLoss: 1.254808\n",
      "Train Epoch: 11 [640/5100 (12%)]\tLoss: 1.274337\n",
      "Train Epoch: 11 [1280/5100 (25%)]\tLoss: 0.990611\n",
      "Train Epoch: 11 [1920/5100 (38%)]\tLoss: 0.968487\n",
      "Train Epoch: 11 [2560/5100 (50%)]\tLoss: 1.398746\n",
      "Train Epoch: 11 [3200/5100 (62%)]\tLoss: 1.187317\n",
      "Train Epoch: 11 [3840/5100 (75%)]\tLoss: 1.329860\n",
      "Train Epoch: 11 [4480/5100 (88%)]\tLoss: 1.191912\n",
      "Train Epoch: 12 [0/5100 (0%)]\tLoss: 1.222740\n",
      "Train Epoch: 12 [640/5100 (12%)]\tLoss: 1.320214\n",
      "Train Epoch: 12 [1280/5100 (25%)]\tLoss: 1.132684\n",
      "Train Epoch: 12 [1920/5100 (38%)]\tLoss: 1.188494\n",
      "Train Epoch: 12 [2560/5100 (50%)]\tLoss: 0.993498\n",
      "Train Epoch: 12 [3200/5100 (62%)]\tLoss: 1.171724\n",
      "Train Epoch: 12 [3840/5100 (75%)]\tLoss: 1.477718\n",
      "Train Epoch: 12 [4480/5100 (88%)]\tLoss: 1.229569\n",
      "Train Epoch: 13 [0/5100 (0%)]\tLoss: 1.003932\n",
      "Train Epoch: 13 [640/5100 (12%)]\tLoss: 1.009489\n",
      "Train Epoch: 13 [1280/5100 (25%)]\tLoss: 1.446948\n",
      "Train Epoch: 13 [1920/5100 (38%)]\tLoss: 1.197916\n",
      "Train Epoch: 13 [2560/5100 (50%)]\tLoss: 1.241238\n",
      "Train Epoch: 13 [3200/5100 (62%)]\tLoss: 0.848964\n",
      "Train Epoch: 13 [3840/5100 (75%)]\tLoss: 1.264253\n",
      "Train Epoch: 13 [4480/5100 (88%)]\tLoss: 1.081860\n",
      "Train Epoch: 14 [0/5100 (0%)]\tLoss: 1.099911\n",
      "Train Epoch: 14 [640/5100 (12%)]\tLoss: 1.120534\n",
      "Train Epoch: 14 [1280/5100 (25%)]\tLoss: 0.925352\n",
      "Train Epoch: 14 [1920/5100 (38%)]\tLoss: 1.348431\n",
      "Train Epoch: 14 [2560/5100 (50%)]\tLoss: 1.253616\n",
      "Train Epoch: 14 [3200/5100 (62%)]\tLoss: 1.290111\n",
      "Train Epoch: 14 [3840/5100 (75%)]\tLoss: 0.953706\n",
      "Train Epoch: 14 [4480/5100 (88%)]\tLoss: 1.225026\n",
      "Train Epoch: 15 [0/5100 (0%)]\tLoss: 1.295142\n",
      "Train Epoch: 15 [640/5100 (12%)]\tLoss: 1.296287\n",
      "Train Epoch: 15 [1280/5100 (25%)]\tLoss: 1.073133\n",
      "Train Epoch: 15 [1920/5100 (38%)]\tLoss: 1.113096\n",
      "Train Epoch: 15 [2560/5100 (50%)]\tLoss: 1.095161\n",
      "Train Epoch: 15 [3200/5100 (62%)]\tLoss: 0.767628\n",
      "Train Epoch: 15 [3840/5100 (75%)]\tLoss: 0.941500\n",
      "Train Epoch: 15 [4480/5100 (88%)]\tLoss: 1.010790\n",
      "Train Epoch: 16 [0/5100 (0%)]\tLoss: 1.163700\n",
      "Train Epoch: 16 [640/5100 (12%)]\tLoss: 1.300225\n",
      "Train Epoch: 16 [1280/5100 (25%)]\tLoss: 1.237175\n",
      "Train Epoch: 16 [1920/5100 (38%)]\tLoss: 1.211050\n",
      "Train Epoch: 16 [2560/5100 (50%)]\tLoss: 1.127888\n",
      "Train Epoch: 16 [3200/5100 (62%)]\tLoss: 1.154778\n",
      "Train Epoch: 16 [3840/5100 (75%)]\tLoss: 1.154796\n",
      "Train Epoch: 16 [4480/5100 (88%)]\tLoss: 1.019822\n",
      "Train Epoch: 17 [0/5100 (0%)]\tLoss: 1.149254\n",
      "Train Epoch: 17 [640/5100 (12%)]\tLoss: 0.922978\n",
      "Train Epoch: 17 [1280/5100 (25%)]\tLoss: 1.214625\n",
      "Train Epoch: 17 [1920/5100 (38%)]\tLoss: 1.299782\n",
      "Train Epoch: 17 [2560/5100 (50%)]\tLoss: 0.973474\n",
      "Train Epoch: 17 [3200/5100 (62%)]\tLoss: 0.999862\n",
      "Train Epoch: 17 [3840/5100 (75%)]\tLoss: 1.330332\n",
      "Train Epoch: 17 [4480/5100 (88%)]\tLoss: 1.119147\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1830, Accuracy: 5792/10000 (58%)\n",
      "\n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.666402\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.349121\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.302829\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.532846\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.222535\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.425152\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 0.997389\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.262527\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.183555\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 1.319913\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.292296\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.198327\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.167596\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.419672\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 0.993853\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.377542\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.184551\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.084382\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.190948\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.505383\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 0.967050\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 0.950857\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.209395\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.257857\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.213730\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 1.099676\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.027061\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.340722\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.158871\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.449792\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.067800\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.158287\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.149999\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.154914\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.409019\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.125808\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.297556\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.016546\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.068477\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.393084\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 1.116562\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 1.480897\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 1.133988\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 1.011692\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.163506\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.017298\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 1.263544\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 1.372468\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.023954\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.156997\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.141970\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 1.068056\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 0.938628\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 0.904282\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 0.933323\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.253704\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 0.954255\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 1.107091\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.188962\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.447802\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.066012\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 0.960168\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 1.025430\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.196139\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.354725\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 1.040637\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 1.110685\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.011267\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 1.314859\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 1.003425\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 0.924735\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 1.068797\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 0.966790\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.025293\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 0.976922\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 0.957032\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.022288\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.167761\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 1.043876\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 1.290944\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 0.945512\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 0.930298\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 0.926489\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.117755\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 1.074072\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 1.084839\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.303392\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 0.905820\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.105637\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.101140\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.319455\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.047801\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 1.003505\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 1.114372\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 0.805177\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 1.009022\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.266222\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 1.056584\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 0.970238\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 1.042109\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.097766\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 1.206258\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 1.173086\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 0.991980\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 1.260242\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 0.931677\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 1.171441\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.192573\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 1.244054\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 1.094240\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.060180\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 1.135330\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 1.138315\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 1.075331\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.138374\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 0.965470\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 1.046102\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.176329\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 1.145476\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.282408\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.454752\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.482993\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.007796\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.275952\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.390305\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.143647\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.035251\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.284910\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.099191\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.215743\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.178880\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.478810\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.319383\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.098571\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.386049\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.024077\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.364464\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.238137\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 1.316906\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.401790\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.353889\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.238268\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.194291\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.323952\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.336419\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.188279\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.319408\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.198483\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.323377\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.281186\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.271048\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.179066\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.200913\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.002548\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.221700\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.160986\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.194815\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.234277\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.218205\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.221686\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.467236\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.396191\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.098172\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.324256\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.216517\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.246490\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.219771\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.460333\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.239394\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.229812\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.137197\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.248370\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.261143\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 1.126004\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.106804\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.218302\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.086942\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.323333\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 1.261694\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.513109\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.021347\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.222525\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.226547\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.008616\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.079727\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.196587\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.048262\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.478896\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 0.916975\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.026390\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.366281\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.190349\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.218917\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.071009\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.274451\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 0.939546\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.183686\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 1.275524\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.137076\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 0.871589\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.202883\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.320677\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.091191\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.138910\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.007036\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 1.212623\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.171579\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.070532\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.140745\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.176149\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.326790\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.211674\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.250609\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 0.979361\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 0.938323\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.217409\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.218286\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.028578\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.364398\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.244555\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.254192\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.087783\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 0.977888\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 1.006246\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.058405\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 1.068014\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.371951\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.204315\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.198170\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.114341\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.117682\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 1.186200\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.098859\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.274579\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.075147\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.190771\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.082499\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 1.295155\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.297718\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.111770\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.150679\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 1.375106\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.124273\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.154082\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.326600\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 0.973007\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.059534\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.308956\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.044387\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 0.979422\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.184177\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.034405\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 1.148557\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 1.106175\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 0.883276\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.527887\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.501739\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.485114\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.235158\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.338491\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.422003\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.405180\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.139557\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.017902\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.247115\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.172853\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.289619\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.072002\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.210393\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.276934\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.370058\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.376273\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.436585\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.353096\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 0.944221\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.219576\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.401533\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.380816\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.287482\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.120084\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.170439\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.292931\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.437139\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.229566\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.014297\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.503321\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.201342\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.486740\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.089706\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.120801\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.506806\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.421200\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.044101\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.215315\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.213175\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.032796\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.338115\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.207356\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.319615\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.299293\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.056445\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.244450\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.155064\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.214876\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.410950\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.334033\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.197056\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.384929\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.130274\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.365511\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.276106\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.023890\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.332763\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.291969\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.411714\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.214944\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.224569\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.056393\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.221675\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.036889\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 0.917641\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.103698\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.198631\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.215436\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.059652\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.146521\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.297304\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.269140\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.344447\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.213500\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.142820\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.214362\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.129533\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.137535\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 1.431199\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.264089\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.212108\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.236820\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.103469\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.298751\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.523396\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.620984\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.103065\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.130432\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.159745\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.155864\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.272586\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.220297\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.330764\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.312475\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.313198\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.105068\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.321986\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.109602\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 1.152191\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.186387\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.402629\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.182157\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.403570\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 1.167661\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.366006\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.279444\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.311207\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.352226\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.152605\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 1.173057\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.045833\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.176688\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.282716\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.129262\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.279404\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.282853\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 0.875813\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.220126\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.377671\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.089934\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.654087\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.013662\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.082545\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 0.946833\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.052038\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.196293\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.081429\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.133868\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.194654\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 0.990548\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.179514\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 0.960835\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.082055\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.214813\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.122801\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.258029\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.323913\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 0.962523\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.165207\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 1.070429\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 1.083898\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.177002\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 0.982949\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.088418\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.187583\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.321345\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.123903\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.071913\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.342823\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 1.281686\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.126706\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.261680\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 1.144238\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 0.889730\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.024903\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.131522\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.128040\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.141870\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.277353\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.101163\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.092631\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 1.376238\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.168650\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.045319\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.153451\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.008970\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.101724\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.046314\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.223284\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.005712\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 1.225075\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 1.072507\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 0.885825\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 0.846706\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 0.929041\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 1.022606\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 0.904017\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 1.007764\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 0.669556\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 1.076436\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 0.861639\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 0.866433\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 1.350856\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 0.990835\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 0.984323\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 0.704580\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.864998\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.829495\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 1.089449\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 0.870875\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 1.034867\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 1.036540\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 0.978932\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 0.887687\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 0.620468\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 1.146098\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 0.756567\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 0.841233\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 0.967228\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 0.967065\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 0.859476\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.836577\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 1.274098\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 1.154399\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 0.974862\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.778534\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 0.835488\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 1.053821\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.919442\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 1.146903\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.841312\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.788481\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.837341\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 0.869435\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 0.667001\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.689374\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.761639\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 0.850353\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 0.756399\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.730782\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 0.706231\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.893017\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 1.012073\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 0.721972\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.586670\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 0.836668\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.745615\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.715515\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.867069\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 1.015550\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 0.961133\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 0.829082\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 1.108209\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.821221\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.628393\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.854831\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.797196\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.873380\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.994970\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 0.767460\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.952106\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.783704\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.965419\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.928469\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.930701\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.834245\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.823468\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.880852\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.735836\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.989424\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 1.027888\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.851379\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.990298\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.587999\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 0.937234\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.944939\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.628666\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 1.054663\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.794250\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 1.108631\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.750384\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.982245\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.765051\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.613502\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.537930\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.816625\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 0.817040\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.647115\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 0.912720\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 1.083733\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.621820\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.256194\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.130138\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.244173\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 0.987657\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 1.210471\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.193745\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.163591\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 1.052656\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 0.909098\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 0.962182\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 1.005593\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 1.025604\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.220964\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.155427\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.089706\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 0.956157\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 1.083829\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 0.940574\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 0.847940\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 1.018973\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.010683\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 1.161461\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.124357\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 0.965921\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 1.061494\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.016301\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 0.813846\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 1.047655\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 1.119078\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 1.046196\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 0.954814\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 0.895577\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 1.136926\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 0.992697\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.104615\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 1.252439\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 1.164407\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.922673\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.075824\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 0.837620\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 1.313948\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 1.137023\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 1.264114\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 1.024206\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 1.127126\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 1.182313\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 1.131978\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 1.033525\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 0.961164\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 0.987708\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 1.160524\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 0.993588\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 0.717115\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 1.138622\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 0.954639\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 0.983006\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 0.957732\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 1.273519\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 0.860908\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 0.737692\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 1.386425\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 0.946000\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 0.934560\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 1.136031\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 1.058021\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 1.099376\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 1.077505\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 1.009369\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.875530\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 1.043798\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 0.833317\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 0.772793\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 0.853039\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 0.875015\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 0.828755\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 1.224828\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 0.928931\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 1.036184\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 0.901965\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 0.854178\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 0.901088\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 0.809335\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 0.982646\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 0.960520\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 0.952882\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 1.031812\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 1.045360\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 1.065229\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 0.882651\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 1.017107\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 1.015056\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 0.895911\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 0.987097\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 0.707291\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 1.011569\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 0.736106\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 0.875498\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 1.169429\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 0.871063\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 0.978653\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 1.113698\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 0.906960\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.473906\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.359987\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.182649\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.750974\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 0.957038\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.197052\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.542724\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.274952\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.193721\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.088542\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.351599\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.240536\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.344454\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.158083\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.407336\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.176034\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.117580\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.180420\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.140674\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.099451\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.118494\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.300089\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.082455\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 1.306670\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.105741\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.243085\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 0.997216\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.149098\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.311608\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.226650\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.087380\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.221270\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.141791\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.291576\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.158316\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.104929\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.316475\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.186687\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.282873\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.156170\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.296911\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.186002\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.050146\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.169900\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 0.906237\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.244779\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.145286\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.176398\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.150350\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.120099\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.235430\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.094850\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.175924\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.281704\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.102578\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.057340\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.043142\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.114112\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.028421\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.121403\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.188625\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.107522\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.271672\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.240278\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.357582\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.160781\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.204838\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.337458\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.297066\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.300665\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.233508\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.051307\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.073529\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 1.434534\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.138963\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.278240\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 1.331462\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.364918\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.077113\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.066965\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.286011\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.039725\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.297258\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.066000\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.029504\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.387139\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.278283\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.273224\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.100129\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.123682\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.064718\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.031934\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.118195\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 1.381392\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 1.090160\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.087458\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.280379\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.196129\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.110472\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.217748\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.342834\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.218688\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.017562\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 0.982624\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 0.984445\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.076616\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 1.003260\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 1.068221\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.161455\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 1.099304\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.164029\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.163607\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 1.106122\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 1.277527\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.094107\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 1.107358\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.218929\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.239420\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 1.127088\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.086440\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.138376\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 0.934656\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.183499\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.245005\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.222840\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.181756\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.169420\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.152563\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.080437\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.064814\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 0.975898\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.135755\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 1.240312\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.178980\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.359591\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 0.947109\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1925, Accuracy: 5643/10000 (56%)\n",
      "\n",
      "Round 2/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.661151\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.213729\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.069448\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.195512\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.260330\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.141269\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.263429\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.434327\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.484498\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 1.141759\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.244217\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.255222\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.282698\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.113684\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.291940\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.154167\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.012598\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.159211\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.442837\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.163149\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.376069\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.010055\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.097834\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.286335\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.318597\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 0.997342\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.208479\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.056454\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.065521\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.215031\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.255918\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.089388\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.241506\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.205119\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.389575\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.039995\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.101913\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.151223\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.488427\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.420360\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 0.865627\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 1.380251\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 1.084264\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 1.254501\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.014523\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.099506\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 1.127620\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 0.956253\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.074541\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.324722\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.113810\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 1.181339\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 0.975913\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 1.117521\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 1.181641\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.262365\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 0.939182\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 0.987376\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.129882\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.310541\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.293684\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 1.043338\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 1.348766\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.097176\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 0.864332\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 1.275732\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 1.098129\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.015025\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 0.987128\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 1.213959\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.062388\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 0.867757\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 1.273424\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.244327\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.157046\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 1.176356\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 0.887829\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 0.966400\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 1.127311\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 0.941374\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.088063\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 0.993520\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.155092\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 0.924230\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 0.827037\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 1.169426\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.189135\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 1.027637\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.274975\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.192430\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 0.960172\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 0.878123\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 1.356483\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 1.034474\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 0.933534\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 0.930024\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.028764\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 1.085541\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 1.124358\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 1.383186\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.033808\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 0.854829\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 1.297516\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 1.284031\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 0.938132\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 0.983268\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 0.999783\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.096906\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 0.954042\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 0.929421\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.539737\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 1.162152\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 1.191788\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 0.954472\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.063135\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 1.334859\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 0.956306\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.318594\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 1.169735\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.559647\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.206048\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.407530\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.377090\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.226654\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.068675\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.269342\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.299480\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.051727\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.448221\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.293496\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 0.981248\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.362533\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.238664\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.230404\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.139567\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.199566\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.203507\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.344809\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 1.091365\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.181080\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.150013\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.445770\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.138308\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.177054\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 0.928018\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.300544\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.038442\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.222260\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.340935\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.062322\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.180248\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.001757\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.232928\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.266093\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.352400\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.083830\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.105632\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.234497\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.125381\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.196040\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.276788\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.242132\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.236628\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.295340\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.213949\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.247947\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.061637\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.166420\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.088562\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.154356\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.237542\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.054456\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.071159\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 1.169036\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.020710\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.446519\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.348850\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.288067\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 1.125145\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.036769\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.249965\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.231754\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.380026\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.116807\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.236618\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.198838\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.027167\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.373214\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.111237\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.293347\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.190412\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.023785\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.262720\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.192953\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.554196\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.256067\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.371235\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 1.292313\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.117457\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.307974\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 0.952102\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.064688\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.294169\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.131105\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.291600\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 0.939357\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.128547\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.111855\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.277084\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.328367\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.113577\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.029727\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.202176\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 1.260907\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 1.180826\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.200074\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.130207\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.009067\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.206952\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.316212\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.086938\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.160731\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.059439\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 1.119410\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.167727\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 1.018187\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.243109\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.296234\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.055408\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.118322\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.212815\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 0.890612\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 0.902710\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.081216\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.372574\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.579134\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.070781\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 1.228550\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.170647\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.214911\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.163014\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 0.881279\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.260090\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.064269\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.207798\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 1.067423\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.336558\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.018151\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.042348\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.147463\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.081468\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.343574\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 1.203086\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 1.244086\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 1.087030\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.360342\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.513543\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.410373\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.337587\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.479226\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.289254\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.259923\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.276321\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.189911\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.232794\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.322431\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.387674\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.136102\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.269224\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.177800\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.319940\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.194901\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.139428\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.187097\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.017664\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.255057\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.346447\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.164479\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 0.899980\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 0.991290\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.228814\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.015292\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.226608\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.121787\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.499150\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.372013\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 0.995061\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.284630\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.259697\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.309104\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.340346\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.422972\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.355302\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.166229\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.373984\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.068342\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.374359\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.136227\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.314047\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.531836\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.341578\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.093551\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.257143\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.361359\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.231743\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.102205\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.400206\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.141660\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.177386\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.084340\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.055468\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.222301\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.657794\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.407307\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.229154\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.308574\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.334545\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.467930\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.126558\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.351763\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.124852\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.375802\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.190614\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.479926\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.171527\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.029538\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.165397\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.254245\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.207226\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.248612\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.101459\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.449049\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.326662\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.286154\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 1.301323\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 0.961365\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.084990\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.234247\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.217020\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.311735\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.397130\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.283461\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.196316\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.096606\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.017177\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.157314\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.202822\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.363886\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.453324\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.285937\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.245845\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.172723\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.115329\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.210963\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 1.235006\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.131452\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.416910\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.108519\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.197316\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 1.219798\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.092719\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.191438\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.259025\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.228348\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.334676\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 1.149491\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.148960\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.044875\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.103068\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.190178\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.168506\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.057470\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.151778\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.055729\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.331969\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.251258\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.249714\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.169444\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 0.965957\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.043938\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.625021\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 0.975217\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.154431\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.354623\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.356152\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 1.192587\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 0.952778\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.063461\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.152836\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.158524\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.234334\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.281750\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.100345\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.566925\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 0.976572\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 0.996513\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 0.990162\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 0.930333\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 0.882973\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.166878\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.058503\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.341566\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.027934\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.025923\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.203939\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 0.864311\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 0.968514\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.071846\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 1.042746\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.278770\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 0.849711\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.217968\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.067885\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.019581\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.062301\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.121189\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.136074\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 1.237458\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.432503\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.070537\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.220487\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.154145\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.360317\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.016887\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.004443\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.370189\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 1.116226\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 0.956381\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 1.015550\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 0.893589\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 1.078709\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 0.776461\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 0.872611\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 1.065150\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 1.046758\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 0.876784\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 0.878666\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 0.799504\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 1.109809\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 1.128816\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 0.811084\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 0.778202\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.993770\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.960150\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.916822\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 0.872783\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 1.061037\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 1.152616\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 1.101872\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 0.893824\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 0.882983\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 0.979851\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 0.779840\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 0.995485\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 1.067834\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 1.019085\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 1.056211\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.749498\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 1.156813\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 0.924902\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 0.750687\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.741890\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 0.848933\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.900970\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.717270\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 0.889244\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.735046\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.708420\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.654039\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 0.677774\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 0.861207\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.952473\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.553453\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 0.825941\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 1.042339\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.897536\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 0.851113\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.742823\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 0.784252\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 0.745323\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.651795\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 0.667401\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.773906\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.924831\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.915279\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.996936\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 0.900689\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 1.008636\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.991117\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.480990\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.624543\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.857703\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.679700\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.987361\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.808532\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 0.869080\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.836367\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.667753\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 1.026499\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.663542\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.700287\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.792224\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.943952\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.979169\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.881928\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.744891\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.914018\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.696298\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.788503\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.811352\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 0.875593\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.826582\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.790449\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 0.904140\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.809855\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.632851\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.818490\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.763112\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.676981\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.792984\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.976041\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.692797\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 0.742279\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.769958\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 0.588336\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.705300\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.788038\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.336844\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.036601\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.300104\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 1.138816\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 0.803456\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.113562\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.092827\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 1.074004\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 0.958749\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 0.924530\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 0.900911\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 0.988255\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.145146\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.119559\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.356627\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 1.116769\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 1.207304\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 0.983752\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 0.936996\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 1.281307\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 0.989182\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 1.023088\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.200383\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 1.142264\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 0.874080\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.022496\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 1.098368\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 0.962243\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 1.046353\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 1.224743\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 1.157831\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 1.061464\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 0.850984\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 1.055322\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.204033\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 0.859728\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 0.979032\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.911772\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.027043\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 0.890681\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 1.032139\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 0.903913\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 0.964202\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 1.083391\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 1.089175\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 1.031780\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 0.937223\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 0.969567\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 1.080855\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 0.883676\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 1.044076\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 1.016779\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 1.103158\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 0.761960\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 0.869874\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 1.091287\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 1.140503\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 0.916482\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 0.903604\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 1.072491\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 0.834432\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 0.811336\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 0.950954\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 0.928181\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 0.762943\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 1.361914\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 1.015574\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 0.893427\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.903584\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 0.917646\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 0.889972\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 1.062998\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 1.133064\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 0.961122\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 1.008441\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 1.130303\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 0.771559\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 0.859364\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 0.912207\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 1.029252\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 0.845290\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 1.035030\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 0.935726\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 0.940201\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 1.120712\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 1.220200\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 0.938045\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 1.067505\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 1.150042\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 0.867760\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 0.966922\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 1.005854\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 0.803188\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 1.050938\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 0.906314\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 0.974672\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 0.743189\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 0.955900\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 1.020361\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 1.086380\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 1.000575\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 1.063267\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.640533\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.291813\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.255103\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.167217\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.212457\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.177231\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.162626\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.247300\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.234529\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.155760\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.165078\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.133942\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.108755\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.154793\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.338964\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.216561\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.337978\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.104032\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.326442\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.311925\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.316929\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.146241\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.131572\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 1.244724\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.255732\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.179183\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.079683\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.065876\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.340621\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.153192\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.320230\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.071714\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.200251\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.148721\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.199753\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.221755\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.149892\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.034303\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.231748\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.150305\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.485261\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.273472\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.187113\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.401337\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 1.276703\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.309425\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.185154\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.215451\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.105828\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.112581\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.107835\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.224001\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.215361\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.182237\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.139192\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.092469\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.154575\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 0.998450\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.363790\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.245922\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.315358\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.071982\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.255651\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.049774\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.340217\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.190119\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.263780\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.164902\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.264632\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.342942\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.488423\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.238533\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.305981\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 1.121908\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.319774\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.289389\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 1.315199\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.222324\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.163658\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.279939\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.053063\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.241479\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.203524\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.069899\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.191821\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.145239\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.166054\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.082228\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.193869\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.384124\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.200263\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.153424\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.327783\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 1.192772\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 0.975048\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.202566\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.053610\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.316769\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.013584\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 0.893912\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.303950\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.020960\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.214361\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.312575\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.080569\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.122565\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 1.259250\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 1.056651\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.094707\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 1.015801\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.267930\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.014325\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 0.994778\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 1.166873\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.039835\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 1.162286\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.244618\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 0.905042\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 1.290871\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.139599\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.152575\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 1.134354\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.195807\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.254647\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 0.973445\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.079993\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.080115\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.333284\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.135795\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.171527\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.083806\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 0.972399\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 1.130376\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.085991\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.251516\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 1.220060\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1915, Accuracy: 5621/10000 (56%)\n",
      "\n",
      "Round 3/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.514063\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.271380\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.285299\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.200892\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.280473\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 0.957853\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.136393\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.223130\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.428376\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 0.960199\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.090057\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.270018\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.161319\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.104068\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.234793\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.190684\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.099519\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.045313\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.040121\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.211737\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.292298\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.329286\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 0.978680\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.024658\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.320850\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 1.161607\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 0.982457\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.134718\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.161312\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.088667\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 0.982813\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.131241\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.386581\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.241327\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.064300\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.110849\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.153041\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.184639\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.327149\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.397957\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 1.160052\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 0.947664\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 0.953875\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 1.037030\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 0.781587\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.288956\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 1.184469\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 0.953963\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.089712\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.074396\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.116831\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 1.201883\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 1.446563\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 0.964750\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 1.344465\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.113635\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 1.030980\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 0.907813\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.242555\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.147737\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.169151\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 1.245595\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 0.908670\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.090351\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.002936\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 1.225052\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 1.168694\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 1.220815\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 0.944681\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 1.191934\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.176255\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 1.122911\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 0.970002\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.222433\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.075490\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 1.405570\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.041616\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.107523\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 0.944819\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 1.210156\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.072635\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 0.900241\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.095780\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.066119\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 0.964875\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 0.944625\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.143846\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 0.917292\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.028645\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.272137\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.012651\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.228204\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 1.232338\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 1.275860\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 1.241260\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 1.347630\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 0.863986\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 0.917596\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 0.912764\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 0.835171\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.050730\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 1.097274\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 0.904719\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 1.022342\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 1.070716\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 1.057860\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 0.944049\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.196697\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 0.996848\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 1.137085\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.121595\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 0.909816\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 0.980147\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 1.187656\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.058456\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 0.792861\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 1.024102\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.052736\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 1.085366\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.239650\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.468966\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.263446\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.136743\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.368241\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.294579\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.268967\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.050540\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.302467\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 0.989625\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.147299\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.269553\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.573983\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.015586\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.274859\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.364986\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.314062\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.092905\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.217130\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 0.892184\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.156808\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.247460\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.159987\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.124019\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.127222\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.160102\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.384908\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.039924\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.177245\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 1.036998\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.234681\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.232248\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.103636\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.210547\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.149632\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.027994\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.195247\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.469278\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.327244\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.221033\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 0.970004\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 0.980068\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.200439\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.239732\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.328170\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.300227\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 1.304060\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.006130\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.157429\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.059648\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.345639\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.003198\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 1.062018\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.214990\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 1.025092\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 0.999296\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.081098\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.306174\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.118432\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 0.972792\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.070797\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.107828\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.315694\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.188209\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.095798\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.193940\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.504482\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 1.083504\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.336385\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.234372\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.133410\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.156734\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.325898\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.213477\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.088124\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.008723\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.368238\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.196884\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 0.957555\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.156503\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.163770\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.222291\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.257568\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.024633\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.149843\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.152250\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 1.087225\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.127412\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.264627\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.063822\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.092476\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.045959\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 1.236821\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 1.276064\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 1.065994\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 1.248468\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.203391\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.055066\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 1.057337\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.066047\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.138731\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.190681\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.047995\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.277099\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 0.923471\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 1.164823\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 1.138827\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.149776\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 1.290291\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.369179\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.182731\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.026567\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 1.283505\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.247887\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 0.941618\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.143657\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.291737\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 0.897427\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 1.272257\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.037603\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.110058\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 1.102044\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 0.931290\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.092695\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.121786\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.107259\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 1.303732\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.145034\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.236500\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.091346\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.096396\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 0.838713\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.159544\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 0.970120\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 0.985075\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 1.098444\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.632483\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.075004\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.453855\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.488504\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.407014\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.371682\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.269502\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.313388\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.146262\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.242856\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.125735\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.232356\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.070910\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 0.985382\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.192187\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 0.981132\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.258394\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.182664\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.248745\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.433282\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.419449\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.519292\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.265122\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.129762\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.325365\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.283261\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.269741\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.317765\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.245677\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 1.350975\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.126744\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.104981\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.222118\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.072274\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.110231\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.146900\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 0.949666\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.323797\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.151262\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.200938\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 1.193563\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.201073\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.423893\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 1.173784\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.031135\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.247528\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.221191\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.171780\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.383703\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 1.230543\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.153061\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.272918\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.064002\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.262411\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.376552\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.167916\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.049326\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.065084\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 1.235822\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.045171\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.239212\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.179938\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.259393\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.101272\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.338735\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.146938\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.082916\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.581568\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.233984\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 1.128425\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.267687\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.252041\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.181226\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 0.929128\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.122806\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.057123\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 1.344937\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.135292\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.207965\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 0.951535\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.362518\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.071367\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.253994\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.203091\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.540525\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.195858\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.206141\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.158971\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.217932\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.483635\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.074073\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.146089\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.040653\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.201342\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.281667\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.174209\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.298386\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.313444\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.141971\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 0.998399\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.303277\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.067642\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.243849\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.061667\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 0.972273\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.103015\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 0.880890\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.188797\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 1.189198\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.458822\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 0.947683\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.208277\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.124741\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.208494\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.157662\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.160138\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.356130\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.270449\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.171221\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.364702\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.005189\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.049284\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.118929\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.211131\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.304446\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.135330\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.569097\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.094156\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 0.915103\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.368718\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 1.208338\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.117579\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.272840\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 1.339387\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.168217\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.088401\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.099859\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.088866\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.190371\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.220532\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 1.122851\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 0.949401\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.012461\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 1.317033\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 1.206671\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.094622\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.092402\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 0.914403\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.103135\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.222528\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 1.003681\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.383060\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 1.104314\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 0.968033\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.205886\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.257024\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.304673\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.098664\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.253887\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 1.182103\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.323858\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.034808\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 0.890721\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.233809\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.272339\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 1.132866\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 1.311688\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.048830\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 1.252091\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.209878\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.072466\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 0.918892\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 0.782386\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 1.376877\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 0.989712\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 0.856388\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 0.843313\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 1.014884\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 1.111656\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 1.124918\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 0.664836\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 1.054325\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 0.639477\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 0.726258\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 0.727095\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 1.097847\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 1.110386\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.725042\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.752255\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.742658\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 0.837998\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 0.711053\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 1.097326\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 0.789198\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 0.969733\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 1.066801\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 0.872275\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 0.974860\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 0.706994\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 0.850416\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 0.849008\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 0.803565\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.870283\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 0.912145\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 0.811245\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 0.885559\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.768738\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 0.897961\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.918816\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.714214\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 0.719350\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.802172\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.824669\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.991639\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 0.874960\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 0.726362\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.642709\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 1.112665\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 0.768021\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 1.101406\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.895253\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 0.974903\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.836193\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 1.201152\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 1.070135\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.626691\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 0.781630\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.765798\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 1.078614\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.742790\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.841341\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 0.643048\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 0.783750\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.842400\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.821026\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.997556\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.927393\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.788265\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.638133\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.776936\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 0.973492\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.866571\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.681742\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.771475\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.592332\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 1.020463\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 1.369371\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.853966\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.617430\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 0.946505\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.959621\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.731715\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 0.938659\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.922347\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.893144\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 0.992643\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.563751\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.974062\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 0.668499\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.672004\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.645480\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.643003\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.709188\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.688232\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 0.914969\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.655366\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.668452\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 0.994815\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.840990\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 0.783537\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.983728\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.956656\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.262822\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.001991\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.040284\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 1.354738\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 0.936703\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.030266\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.113548\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 1.186225\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 0.917116\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 1.064152\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 1.128346\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 0.964590\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 1.179789\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 0.959266\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.044425\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 1.083755\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 0.997228\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 1.290297\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 1.167693\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 1.079054\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.305126\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 1.048158\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.202925\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 1.107600\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 1.038428\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 0.861035\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 0.870106\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 1.289842\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 0.934204\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 1.246731\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 0.862414\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 1.046263\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 1.101723\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 1.261195\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.009935\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 0.883858\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 1.053265\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.985130\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.019768\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 0.797427\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 1.157655\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 0.855056\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 1.029357\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 0.837909\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 0.809433\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 1.024422\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 0.955551\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 1.077503\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 1.226269\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 1.068714\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 1.031274\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 1.237770\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 1.047246\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 1.050714\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 1.059717\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 1.280093\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 1.132798\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 0.876458\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 1.161652\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 1.181625\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 0.931852\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 0.838292\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 0.947983\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 1.032050\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 0.902514\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 0.906288\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 1.177942\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 0.979434\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.789050\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 1.030260\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 1.106028\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 0.776101\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 0.942364\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 0.898300\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 0.695884\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 1.149317\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 0.819382\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 0.940814\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 1.071350\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 1.066043\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 0.619194\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 1.001824\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 1.020139\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 1.079527\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 0.779562\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 0.795131\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 0.834432\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 1.030659\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 0.925460\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 0.910465\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 0.792055\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 1.013593\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 0.875715\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 1.194920\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 0.994146\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 0.889764\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 0.871673\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 1.050345\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 0.859753\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 0.860147\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 1.055589\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 0.865275\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.396312\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.303713\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.296402\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.253258\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.204018\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.016356\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.211855\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.190304\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.215557\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.098783\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.619954\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 1.092495\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.149898\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.018131\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.216181\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.152590\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.210282\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.119246\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.003649\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.289632\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.183793\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.241756\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.129285\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 0.951712\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.277306\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.300912\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.276595\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.199209\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.258436\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.087330\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.105162\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.116556\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.121682\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.249285\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 1.119555\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.162284\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.225402\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.092334\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.037882\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.505328\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.060793\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.102352\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 1.274403\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 1.050897\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 0.978966\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 0.901713\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.224134\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.324312\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.225655\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.109052\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.131271\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.226406\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.222849\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.211479\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.176715\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 1.249474\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.063771\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.257640\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.073498\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.181076\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.256907\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.086372\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.157352\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.132277\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.243892\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.300773\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 0.999484\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.122921\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.042513\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.267841\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.321779\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.220273\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.370139\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 1.119782\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.175046\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.312701\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 0.954687\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.039298\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.004657\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.157779\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.306806\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.094300\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.081117\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.186818\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.116203\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.237415\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.002880\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.069562\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.129151\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.059018\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.187922\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.319002\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.049133\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 1.167368\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 0.723079\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.233897\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 0.854681\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.212373\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 1.174813\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.102460\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 0.917793\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 1.057925\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.196875\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.225681\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.018385\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 0.928513\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 1.110721\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 0.995023\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.009209\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 0.819571\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.134936\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 0.932892\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 1.160776\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 0.962494\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 1.155183\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 0.930257\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 0.834676\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.527904\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 1.082723\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.302625\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.007615\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 1.031612\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.118054\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 1.079890\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.134237\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.298401\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.293683\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.186431\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 1.204920\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.069583\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.107373\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.209818\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 1.221523\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.161394\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.012617\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 0.902991\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1902, Accuracy: 5617/10000 (56%)\n",
      "\n",
      "Round 4/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4323 (0%)]\tLoss: 1.384344\n",
      "Train Epoch: 1 [640/4323 (15%)]\tLoss: 1.073919\n",
      "Train Epoch: 1 [1280/4323 (29%)]\tLoss: 1.412687\n",
      "Train Epoch: 1 [1920/4323 (44%)]\tLoss: 1.403862\n",
      "Train Epoch: 1 [2560/4323 (59%)]\tLoss: 1.255514\n",
      "Train Epoch: 1 [3200/4323 (74%)]\tLoss: 1.026899\n",
      "Train Epoch: 1 [3840/4323 (88%)]\tLoss: 1.035182\n",
      "Train Epoch: 2 [0/4323 (0%)]\tLoss: 1.327024\n",
      "Train Epoch: 2 [640/4323 (15%)]\tLoss: 1.047018\n",
      "Train Epoch: 2 [1280/4323 (29%)]\tLoss: 0.999103\n",
      "Train Epoch: 2 [1920/4323 (44%)]\tLoss: 1.017771\n",
      "Train Epoch: 2 [2560/4323 (59%)]\tLoss: 1.624027\n",
      "Train Epoch: 2 [3200/4323 (74%)]\tLoss: 1.463630\n",
      "Train Epoch: 2 [3840/4323 (88%)]\tLoss: 1.099709\n",
      "Train Epoch: 3 [0/4323 (0%)]\tLoss: 1.302939\n",
      "Train Epoch: 3 [640/4323 (15%)]\tLoss: 1.136541\n",
      "Train Epoch: 3 [1280/4323 (29%)]\tLoss: 1.181896\n",
      "Train Epoch: 3 [1920/4323 (44%)]\tLoss: 1.121154\n",
      "Train Epoch: 3 [2560/4323 (59%)]\tLoss: 1.013626\n",
      "Train Epoch: 3 [3200/4323 (74%)]\tLoss: 1.142385\n",
      "Train Epoch: 3 [3840/4323 (88%)]\tLoss: 1.210325\n",
      "Train Epoch: 4 [0/4323 (0%)]\tLoss: 1.072149\n",
      "Train Epoch: 4 [640/4323 (15%)]\tLoss: 1.059228\n",
      "Train Epoch: 4 [1280/4323 (29%)]\tLoss: 1.256131\n",
      "Train Epoch: 4 [1920/4323 (44%)]\tLoss: 1.075898\n",
      "Train Epoch: 4 [2560/4323 (59%)]\tLoss: 0.976914\n",
      "Train Epoch: 4 [3200/4323 (74%)]\tLoss: 1.028388\n",
      "Train Epoch: 4 [3840/4323 (88%)]\tLoss: 1.446800\n",
      "Train Epoch: 5 [0/4323 (0%)]\tLoss: 1.085548\n",
      "Train Epoch: 5 [640/4323 (15%)]\tLoss: 1.216533\n",
      "Train Epoch: 5 [1280/4323 (29%)]\tLoss: 1.277039\n",
      "Train Epoch: 5 [1920/4323 (44%)]\tLoss: 1.098644\n",
      "Train Epoch: 5 [2560/4323 (59%)]\tLoss: 1.421145\n",
      "Train Epoch: 5 [3200/4323 (74%)]\tLoss: 1.209182\n",
      "Train Epoch: 5 [3840/4323 (88%)]\tLoss: 1.159279\n",
      "Train Epoch: 6 [0/4323 (0%)]\tLoss: 1.006730\n",
      "Train Epoch: 6 [640/4323 (15%)]\tLoss: 1.208494\n",
      "Train Epoch: 6 [1280/4323 (29%)]\tLoss: 1.138723\n",
      "Train Epoch: 6 [1920/4323 (44%)]\tLoss: 1.119112\n",
      "Train Epoch: 6 [2560/4323 (59%)]\tLoss: 1.095981\n",
      "Train Epoch: 6 [3200/4323 (74%)]\tLoss: 1.058063\n",
      "Train Epoch: 6 [3840/4323 (88%)]\tLoss: 0.977116\n",
      "Train Epoch: 7 [0/4323 (0%)]\tLoss: 1.097751\n",
      "Train Epoch: 7 [640/4323 (15%)]\tLoss: 0.960839\n",
      "Train Epoch: 7 [1280/4323 (29%)]\tLoss: 1.155510\n",
      "Train Epoch: 7 [1920/4323 (44%)]\tLoss: 1.114316\n",
      "Train Epoch: 7 [2560/4323 (59%)]\tLoss: 0.999797\n",
      "Train Epoch: 7 [3200/4323 (74%)]\tLoss: 0.998228\n",
      "Train Epoch: 7 [3840/4323 (88%)]\tLoss: 1.071996\n",
      "Train Epoch: 8 [0/4323 (0%)]\tLoss: 1.209638\n",
      "Train Epoch: 8 [640/4323 (15%)]\tLoss: 1.001199\n",
      "Train Epoch: 8 [1280/4323 (29%)]\tLoss: 0.994156\n",
      "Train Epoch: 8 [1920/4323 (44%)]\tLoss: 1.009304\n",
      "Train Epoch: 8 [2560/4323 (59%)]\tLoss: 1.304665\n",
      "Train Epoch: 8 [3200/4323 (74%)]\tLoss: 0.872356\n",
      "Train Epoch: 8 [3840/4323 (88%)]\tLoss: 1.134172\n",
      "Train Epoch: 9 [0/4323 (0%)]\tLoss: 0.936321\n",
      "Train Epoch: 9 [640/4323 (15%)]\tLoss: 1.271277\n",
      "Train Epoch: 9 [1280/4323 (29%)]\tLoss: 1.136271\n",
      "Train Epoch: 9 [1920/4323 (44%)]\tLoss: 1.073334\n",
      "Train Epoch: 9 [2560/4323 (59%)]\tLoss: 1.030711\n",
      "Train Epoch: 9 [3200/4323 (74%)]\tLoss: 1.145323\n",
      "Train Epoch: 9 [3840/4323 (88%)]\tLoss: 0.963891\n",
      "Train Epoch: 10 [0/4323 (0%)]\tLoss: 1.071800\n",
      "Train Epoch: 10 [640/4323 (15%)]\tLoss: 1.139433\n",
      "Train Epoch: 10 [1280/4323 (29%)]\tLoss: 0.872157\n",
      "Train Epoch: 10 [1920/4323 (44%)]\tLoss: 1.198780\n",
      "Train Epoch: 10 [2560/4323 (59%)]\tLoss: 0.844101\n",
      "Train Epoch: 10 [3200/4323 (74%)]\tLoss: 1.045082\n",
      "Train Epoch: 10 [3840/4323 (88%)]\tLoss: 0.947730\n",
      "Train Epoch: 11 [0/4323 (0%)]\tLoss: 1.208229\n",
      "Train Epoch: 11 [640/4323 (15%)]\tLoss: 1.128588\n",
      "Train Epoch: 11 [1280/4323 (29%)]\tLoss: 1.101618\n",
      "Train Epoch: 11 [1920/4323 (44%)]\tLoss: 1.179013\n",
      "Train Epoch: 11 [2560/4323 (59%)]\tLoss: 1.081145\n",
      "Train Epoch: 11 [3200/4323 (74%)]\tLoss: 0.855419\n",
      "Train Epoch: 11 [3840/4323 (88%)]\tLoss: 1.106541\n",
      "Train Epoch: 12 [0/4323 (0%)]\tLoss: 1.059034\n",
      "Train Epoch: 12 [640/4323 (15%)]\tLoss: 0.990816\n",
      "Train Epoch: 12 [1280/4323 (29%)]\tLoss: 1.105514\n",
      "Train Epoch: 12 [1920/4323 (44%)]\tLoss: 1.162572\n",
      "Train Epoch: 12 [2560/4323 (59%)]\tLoss: 1.157691\n",
      "Train Epoch: 12 [3200/4323 (74%)]\tLoss: 1.057103\n",
      "Train Epoch: 12 [3840/4323 (88%)]\tLoss: 1.216985\n",
      "Train Epoch: 13 [0/4323 (0%)]\tLoss: 1.464149\n",
      "Train Epoch: 13 [640/4323 (15%)]\tLoss: 0.996454\n",
      "Train Epoch: 13 [1280/4323 (29%)]\tLoss: 1.042397\n",
      "Train Epoch: 13 [1920/4323 (44%)]\tLoss: 1.039992\n",
      "Train Epoch: 13 [2560/4323 (59%)]\tLoss: 1.075349\n",
      "Train Epoch: 13 [3200/4323 (74%)]\tLoss: 1.008747\n",
      "Train Epoch: 13 [3840/4323 (88%)]\tLoss: 1.038656\n",
      "Train Epoch: 14 [0/4323 (0%)]\tLoss: 1.217332\n",
      "Train Epoch: 14 [640/4323 (15%)]\tLoss: 0.940963\n",
      "Train Epoch: 14 [1280/4323 (29%)]\tLoss: 0.761857\n",
      "Train Epoch: 14 [1920/4323 (44%)]\tLoss: 1.060763\n",
      "Train Epoch: 14 [2560/4323 (59%)]\tLoss: 1.056865\n",
      "Train Epoch: 14 [3200/4323 (74%)]\tLoss: 1.192732\n",
      "Train Epoch: 14 [3840/4323 (88%)]\tLoss: 1.138841\n",
      "Train Epoch: 15 [0/4323 (0%)]\tLoss: 1.073707\n",
      "Train Epoch: 15 [640/4323 (15%)]\tLoss: 1.009636\n",
      "Train Epoch: 15 [1280/4323 (29%)]\tLoss: 1.273996\n",
      "Train Epoch: 15 [1920/4323 (44%)]\tLoss: 1.027023\n",
      "Train Epoch: 15 [2560/4323 (59%)]\tLoss: 0.835566\n",
      "Train Epoch: 15 [3200/4323 (74%)]\tLoss: 0.891376\n",
      "Train Epoch: 15 [3840/4323 (88%)]\tLoss: 1.032271\n",
      "Train Epoch: 16 [0/4323 (0%)]\tLoss: 1.106211\n",
      "Train Epoch: 16 [640/4323 (15%)]\tLoss: 1.058340\n",
      "Train Epoch: 16 [1280/4323 (29%)]\tLoss: 1.275517\n",
      "Train Epoch: 16 [1920/4323 (44%)]\tLoss: 1.097916\n",
      "Train Epoch: 16 [2560/4323 (59%)]\tLoss: 0.939206\n",
      "Train Epoch: 16 [3200/4323 (74%)]\tLoss: 1.146307\n",
      "Train Epoch: 16 [3840/4323 (88%)]\tLoss: 1.049566\n",
      "Train Epoch: 17 [0/4323 (0%)]\tLoss: 1.168582\n",
      "Train Epoch: 17 [640/4323 (15%)]\tLoss: 0.951310\n",
      "Train Epoch: 17 [1280/4323 (29%)]\tLoss: 1.206047\n",
      "Train Epoch: 17 [1920/4323 (44%)]\tLoss: 1.048228\n",
      "Train Epoch: 17 [2560/4323 (59%)]\tLoss: 1.589443\n",
      "Train Epoch: 17 [3200/4323 (74%)]\tLoss: 1.132192\n",
      "Train Epoch: 17 [3840/4323 (88%)]\tLoss: 0.932644\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/4641 (0%)]\tLoss: 1.446306\n",
      "Train Epoch: 1 [640/4641 (14%)]\tLoss: 1.329049\n",
      "Train Epoch: 1 [1280/4641 (27%)]\tLoss: 1.133129\n",
      "Train Epoch: 1 [1920/4641 (41%)]\tLoss: 1.309036\n",
      "Train Epoch: 1 [2560/4641 (55%)]\tLoss: 1.280603\n",
      "Train Epoch: 1 [3200/4641 (68%)]\tLoss: 1.146043\n",
      "Train Epoch: 1 [3840/4641 (82%)]\tLoss: 1.162098\n",
      "Train Epoch: 1 [4480/4641 (96%)]\tLoss: 1.287380\n",
      "Train Epoch: 2 [0/4641 (0%)]\tLoss: 1.383951\n",
      "Train Epoch: 2 [640/4641 (14%)]\tLoss: 1.389062\n",
      "Train Epoch: 2 [1280/4641 (27%)]\tLoss: 1.122650\n",
      "Train Epoch: 2 [1920/4641 (41%)]\tLoss: 1.118227\n",
      "Train Epoch: 2 [2560/4641 (55%)]\tLoss: 1.189089\n",
      "Train Epoch: 2 [3200/4641 (68%)]\tLoss: 1.490635\n",
      "Train Epoch: 2 [3840/4641 (82%)]\tLoss: 1.311741\n",
      "Train Epoch: 2 [4480/4641 (96%)]\tLoss: 1.232062\n",
      "Train Epoch: 3 [0/4641 (0%)]\tLoss: 1.188308\n",
      "Train Epoch: 3 [640/4641 (14%)]\tLoss: 1.190488\n",
      "Train Epoch: 3 [1280/4641 (27%)]\tLoss: 1.291738\n",
      "Train Epoch: 3 [1920/4641 (41%)]\tLoss: 1.131093\n",
      "Train Epoch: 3 [2560/4641 (55%)]\tLoss: 1.432681\n",
      "Train Epoch: 3 [3200/4641 (68%)]\tLoss: 1.301584\n",
      "Train Epoch: 3 [3840/4641 (82%)]\tLoss: 1.321757\n",
      "Train Epoch: 3 [4480/4641 (96%)]\tLoss: 1.056520\n",
      "Train Epoch: 4 [0/4641 (0%)]\tLoss: 1.409471\n",
      "Train Epoch: 4 [640/4641 (14%)]\tLoss: 1.387639\n",
      "Train Epoch: 4 [1280/4641 (27%)]\tLoss: 1.025385\n",
      "Train Epoch: 4 [1920/4641 (41%)]\tLoss: 1.284167\n",
      "Train Epoch: 4 [2560/4641 (55%)]\tLoss: 1.025919\n",
      "Train Epoch: 4 [3200/4641 (68%)]\tLoss: 0.991286\n",
      "Train Epoch: 4 [3840/4641 (82%)]\tLoss: 1.296244\n",
      "Train Epoch: 4 [4480/4641 (96%)]\tLoss: 1.274650\n",
      "Train Epoch: 5 [0/4641 (0%)]\tLoss: 1.250049\n",
      "Train Epoch: 5 [640/4641 (14%)]\tLoss: 1.141815\n",
      "Train Epoch: 5 [1280/4641 (27%)]\tLoss: 1.285795\n",
      "Train Epoch: 5 [1920/4641 (41%)]\tLoss: 1.211071\n",
      "Train Epoch: 5 [2560/4641 (55%)]\tLoss: 1.176816\n",
      "Train Epoch: 5 [3200/4641 (68%)]\tLoss: 1.199929\n",
      "Train Epoch: 5 [3840/4641 (82%)]\tLoss: 1.348719\n",
      "Train Epoch: 5 [4480/4641 (96%)]\tLoss: 1.269922\n",
      "Train Epoch: 6 [0/4641 (0%)]\tLoss: 1.287368\n",
      "Train Epoch: 6 [640/4641 (14%)]\tLoss: 1.156724\n",
      "Train Epoch: 6 [1280/4641 (27%)]\tLoss: 1.125412\n",
      "Train Epoch: 6 [1920/4641 (41%)]\tLoss: 1.189758\n",
      "Train Epoch: 6 [2560/4641 (55%)]\tLoss: 1.227951\n",
      "Train Epoch: 6 [3200/4641 (68%)]\tLoss: 1.321556\n",
      "Train Epoch: 6 [3840/4641 (82%)]\tLoss: 0.992477\n",
      "Train Epoch: 6 [4480/4641 (96%)]\tLoss: 1.129255\n",
      "Train Epoch: 7 [0/4641 (0%)]\tLoss: 1.044053\n",
      "Train Epoch: 7 [640/4641 (14%)]\tLoss: 1.271800\n",
      "Train Epoch: 7 [1280/4641 (27%)]\tLoss: 1.260668\n",
      "Train Epoch: 7 [1920/4641 (41%)]\tLoss: 1.319467\n",
      "Train Epoch: 7 [2560/4641 (55%)]\tLoss: 0.865213\n",
      "Train Epoch: 7 [3200/4641 (68%)]\tLoss: 1.154784\n",
      "Train Epoch: 7 [3840/4641 (82%)]\tLoss: 1.253590\n",
      "Train Epoch: 7 [4480/4641 (96%)]\tLoss: 1.161180\n",
      "Train Epoch: 8 [0/4641 (0%)]\tLoss: 1.015888\n",
      "Train Epoch: 8 [640/4641 (14%)]\tLoss: 1.001369\n",
      "Train Epoch: 8 [1280/4641 (27%)]\tLoss: 1.210820\n",
      "Train Epoch: 8 [1920/4641 (41%)]\tLoss: 1.051272\n",
      "Train Epoch: 8 [2560/4641 (55%)]\tLoss: 1.292966\n",
      "Train Epoch: 8 [3200/4641 (68%)]\tLoss: 1.267905\n",
      "Train Epoch: 8 [3840/4641 (82%)]\tLoss: 1.173229\n",
      "Train Epoch: 8 [4480/4641 (96%)]\tLoss: 1.227446\n",
      "Train Epoch: 9 [0/4641 (0%)]\tLoss: 1.373657\n",
      "Train Epoch: 9 [640/4641 (14%)]\tLoss: 1.245981\n",
      "Train Epoch: 9 [1280/4641 (27%)]\tLoss: 1.055879\n",
      "Train Epoch: 9 [1920/4641 (41%)]\tLoss: 0.974761\n",
      "Train Epoch: 9 [2560/4641 (55%)]\tLoss: 1.450854\n",
      "Train Epoch: 9 [3200/4641 (68%)]\tLoss: 1.149765\n",
      "Train Epoch: 9 [3840/4641 (82%)]\tLoss: 1.140962\n",
      "Train Epoch: 9 [4480/4641 (96%)]\tLoss: 1.130537\n",
      "Train Epoch: 10 [0/4641 (0%)]\tLoss: 1.033074\n",
      "Train Epoch: 10 [640/4641 (14%)]\tLoss: 1.118383\n",
      "Train Epoch: 10 [1280/4641 (27%)]\tLoss: 1.055057\n",
      "Train Epoch: 10 [1920/4641 (41%)]\tLoss: 1.031245\n",
      "Train Epoch: 10 [2560/4641 (55%)]\tLoss: 1.094274\n",
      "Train Epoch: 10 [3200/4641 (68%)]\tLoss: 1.295436\n",
      "Train Epoch: 10 [3840/4641 (82%)]\tLoss: 1.034434\n",
      "Train Epoch: 10 [4480/4641 (96%)]\tLoss: 1.129641\n",
      "Train Epoch: 11 [0/4641 (0%)]\tLoss: 1.368676\n",
      "Train Epoch: 11 [640/4641 (14%)]\tLoss: 1.095417\n",
      "Train Epoch: 11 [1280/4641 (27%)]\tLoss: 1.408441\n",
      "Train Epoch: 11 [1920/4641 (41%)]\tLoss: 1.250280\n",
      "Train Epoch: 11 [2560/4641 (55%)]\tLoss: 1.332244\n",
      "Train Epoch: 11 [3200/4641 (68%)]\tLoss: 1.020066\n",
      "Train Epoch: 11 [3840/4641 (82%)]\tLoss: 0.997865\n",
      "Train Epoch: 11 [4480/4641 (96%)]\tLoss: 1.275282\n",
      "Train Epoch: 12 [0/4641 (0%)]\tLoss: 1.241622\n",
      "Train Epoch: 12 [640/4641 (14%)]\tLoss: 1.085784\n",
      "Train Epoch: 12 [1280/4641 (27%)]\tLoss: 1.136617\n",
      "Train Epoch: 12 [1920/4641 (41%)]\tLoss: 1.442188\n",
      "Train Epoch: 12 [2560/4641 (55%)]\tLoss: 0.987229\n",
      "Train Epoch: 12 [3200/4641 (68%)]\tLoss: 0.979557\n",
      "Train Epoch: 12 [3840/4641 (82%)]\tLoss: 0.872040\n",
      "Train Epoch: 12 [4480/4641 (96%)]\tLoss: 1.285537\n",
      "Train Epoch: 13 [0/4641 (0%)]\tLoss: 1.031204\n",
      "Train Epoch: 13 [640/4641 (14%)]\tLoss: 1.113656\n",
      "Train Epoch: 13 [1280/4641 (27%)]\tLoss: 0.981531\n",
      "Train Epoch: 13 [1920/4641 (41%)]\tLoss: 1.003478\n",
      "Train Epoch: 13 [2560/4641 (55%)]\tLoss: 1.095924\n",
      "Train Epoch: 13 [3200/4641 (68%)]\tLoss: 1.278111\n",
      "Train Epoch: 13 [3840/4641 (82%)]\tLoss: 1.101740\n",
      "Train Epoch: 13 [4480/4641 (96%)]\tLoss: 1.295423\n",
      "Train Epoch: 14 [0/4641 (0%)]\tLoss: 1.026231\n",
      "Train Epoch: 14 [640/4641 (14%)]\tLoss: 0.976845\n",
      "Train Epoch: 14 [1280/4641 (27%)]\tLoss: 1.173238\n",
      "Train Epoch: 14 [1920/4641 (41%)]\tLoss: 1.208572\n",
      "Train Epoch: 14 [2560/4641 (55%)]\tLoss: 0.929821\n",
      "Train Epoch: 14 [3200/4641 (68%)]\tLoss: 1.178437\n",
      "Train Epoch: 14 [3840/4641 (82%)]\tLoss: 1.198886\n",
      "Train Epoch: 14 [4480/4641 (96%)]\tLoss: 1.050272\n",
      "Train Epoch: 15 [0/4641 (0%)]\tLoss: 1.053437\n",
      "Train Epoch: 15 [640/4641 (14%)]\tLoss: 1.321411\n",
      "Train Epoch: 15 [1280/4641 (27%)]\tLoss: 1.101101\n",
      "Train Epoch: 15 [1920/4641 (41%)]\tLoss: 1.137001\n",
      "Train Epoch: 15 [2560/4641 (55%)]\tLoss: 1.132755\n",
      "Train Epoch: 15 [3200/4641 (68%)]\tLoss: 1.177356\n",
      "Train Epoch: 15 [3840/4641 (82%)]\tLoss: 0.989459\n",
      "Train Epoch: 15 [4480/4641 (96%)]\tLoss: 1.023609\n",
      "Train Epoch: 16 [0/4641 (0%)]\tLoss: 1.050619\n",
      "Train Epoch: 16 [640/4641 (14%)]\tLoss: 0.910912\n",
      "Train Epoch: 16 [1280/4641 (27%)]\tLoss: 1.206955\n",
      "Train Epoch: 16 [1920/4641 (41%)]\tLoss: 1.189014\n",
      "Train Epoch: 16 [2560/4641 (55%)]\tLoss: 1.047215\n",
      "Train Epoch: 16 [3200/4641 (68%)]\tLoss: 1.015353\n",
      "Train Epoch: 16 [3840/4641 (82%)]\tLoss: 1.139352\n",
      "Train Epoch: 16 [4480/4641 (96%)]\tLoss: 1.290219\n",
      "Train Epoch: 17 [0/4641 (0%)]\tLoss: 1.021198\n",
      "Train Epoch: 17 [640/4641 (14%)]\tLoss: 1.110530\n",
      "Train Epoch: 17 [1280/4641 (27%)]\tLoss: 1.108727\n",
      "Train Epoch: 17 [1920/4641 (41%)]\tLoss: 1.015699\n",
      "Train Epoch: 17 [2560/4641 (55%)]\tLoss: 1.115798\n",
      "Train Epoch: 17 [3200/4641 (68%)]\tLoss: 1.317949\n",
      "Train Epoch: 17 [3840/4641 (82%)]\tLoss: 1.137185\n",
      "Train Epoch: 17 [4480/4641 (96%)]\tLoss: 0.947892\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/6073 (0%)]\tLoss: 1.421834\n",
      "Train Epoch: 1 [640/6073 (11%)]\tLoss: 1.248762\n",
      "Train Epoch: 1 [1280/6073 (21%)]\tLoss: 1.604096\n",
      "Train Epoch: 1 [1920/6073 (32%)]\tLoss: 1.443009\n",
      "Train Epoch: 1 [2560/6073 (42%)]\tLoss: 1.219567\n",
      "Train Epoch: 1 [3200/6073 (53%)]\tLoss: 1.342191\n",
      "Train Epoch: 1 [3840/6073 (63%)]\tLoss: 1.245361\n",
      "Train Epoch: 1 [4480/6073 (74%)]\tLoss: 1.114601\n",
      "Train Epoch: 1 [5120/6073 (84%)]\tLoss: 1.331325\n",
      "Train Epoch: 1 [5760/6073 (95%)]\tLoss: 1.510838\n",
      "Train Epoch: 2 [0/6073 (0%)]\tLoss: 1.071875\n",
      "Train Epoch: 2 [640/6073 (11%)]\tLoss: 1.315620\n",
      "Train Epoch: 2 [1280/6073 (21%)]\tLoss: 1.254584\n",
      "Train Epoch: 2 [1920/6073 (32%)]\tLoss: 1.065845\n",
      "Train Epoch: 2 [2560/6073 (42%)]\tLoss: 1.076243\n",
      "Train Epoch: 2 [3200/6073 (53%)]\tLoss: 1.142901\n",
      "Train Epoch: 2 [3840/6073 (63%)]\tLoss: 1.320845\n",
      "Train Epoch: 2 [4480/6073 (74%)]\tLoss: 1.280604\n",
      "Train Epoch: 2 [5120/6073 (84%)]\tLoss: 1.135533\n",
      "Train Epoch: 2 [5760/6073 (95%)]\tLoss: 1.267949\n",
      "Train Epoch: 3 [0/6073 (0%)]\tLoss: 1.278956\n",
      "Train Epoch: 3 [640/6073 (11%)]\tLoss: 1.123278\n",
      "Train Epoch: 3 [1280/6073 (21%)]\tLoss: 1.373846\n",
      "Train Epoch: 3 [1920/6073 (32%)]\tLoss: 1.213528\n",
      "Train Epoch: 3 [2560/6073 (42%)]\tLoss: 1.280582\n",
      "Train Epoch: 3 [3200/6073 (53%)]\tLoss: 1.219982\n",
      "Train Epoch: 3 [3840/6073 (63%)]\tLoss: 1.170732\n",
      "Train Epoch: 3 [4480/6073 (74%)]\tLoss: 1.352669\n",
      "Train Epoch: 3 [5120/6073 (84%)]\tLoss: 1.189141\n",
      "Train Epoch: 3 [5760/6073 (95%)]\tLoss: 0.848862\n",
      "Train Epoch: 4 [0/6073 (0%)]\tLoss: 1.366891\n",
      "Train Epoch: 4 [640/6073 (11%)]\tLoss: 1.083779\n",
      "Train Epoch: 4 [1280/6073 (21%)]\tLoss: 1.340722\n",
      "Train Epoch: 4 [1920/6073 (32%)]\tLoss: 1.365148\n",
      "Train Epoch: 4 [2560/6073 (42%)]\tLoss: 1.287698\n",
      "Train Epoch: 4 [3200/6073 (53%)]\tLoss: 1.273949\n",
      "Train Epoch: 4 [3840/6073 (63%)]\tLoss: 1.231510\n",
      "Train Epoch: 4 [4480/6073 (74%)]\tLoss: 1.324284\n",
      "Train Epoch: 4 [5120/6073 (84%)]\tLoss: 1.016106\n",
      "Train Epoch: 4 [5760/6073 (95%)]\tLoss: 1.264329\n",
      "Train Epoch: 5 [0/6073 (0%)]\tLoss: 0.994053\n",
      "Train Epoch: 5 [640/6073 (11%)]\tLoss: 1.146790\n",
      "Train Epoch: 5 [1280/6073 (21%)]\tLoss: 1.221465\n",
      "Train Epoch: 5 [1920/6073 (32%)]\tLoss: 0.933863\n",
      "Train Epoch: 5 [2560/6073 (42%)]\tLoss: 1.050391\n",
      "Train Epoch: 5 [3200/6073 (53%)]\tLoss: 1.239630\n",
      "Train Epoch: 5 [3840/6073 (63%)]\tLoss: 1.076680\n",
      "Train Epoch: 5 [4480/6073 (74%)]\tLoss: 1.430635\n",
      "Train Epoch: 5 [5120/6073 (84%)]\tLoss: 1.290748\n",
      "Train Epoch: 5 [5760/6073 (95%)]\tLoss: 0.938171\n",
      "Train Epoch: 6 [0/6073 (0%)]\tLoss: 1.024413\n",
      "Train Epoch: 6 [640/6073 (11%)]\tLoss: 1.409102\n",
      "Train Epoch: 6 [1280/6073 (21%)]\tLoss: 1.385761\n",
      "Train Epoch: 6 [1920/6073 (32%)]\tLoss: 1.264470\n",
      "Train Epoch: 6 [2560/6073 (42%)]\tLoss: 1.099775\n",
      "Train Epoch: 6 [3200/6073 (53%)]\tLoss: 1.112054\n",
      "Train Epoch: 6 [3840/6073 (63%)]\tLoss: 1.367864\n",
      "Train Epoch: 6 [4480/6073 (74%)]\tLoss: 1.313885\n",
      "Train Epoch: 6 [5120/6073 (84%)]\tLoss: 0.962453\n",
      "Train Epoch: 6 [5760/6073 (95%)]\tLoss: 1.318302\n",
      "Train Epoch: 7 [0/6073 (0%)]\tLoss: 1.149202\n",
      "Train Epoch: 7 [640/6073 (11%)]\tLoss: 1.158820\n",
      "Train Epoch: 7 [1280/6073 (21%)]\tLoss: 1.176017\n",
      "Train Epoch: 7 [1920/6073 (32%)]\tLoss: 1.165586\n",
      "Train Epoch: 7 [2560/6073 (42%)]\tLoss: 1.048864\n",
      "Train Epoch: 7 [3200/6073 (53%)]\tLoss: 1.260221\n",
      "Train Epoch: 7 [3840/6073 (63%)]\tLoss: 1.019607\n",
      "Train Epoch: 7 [4480/6073 (74%)]\tLoss: 1.066931\n",
      "Train Epoch: 7 [5120/6073 (84%)]\tLoss: 1.174327\n",
      "Train Epoch: 7 [5760/6073 (95%)]\tLoss: 0.932928\n",
      "Train Epoch: 8 [0/6073 (0%)]\tLoss: 1.053575\n",
      "Train Epoch: 8 [640/6073 (11%)]\tLoss: 1.020754\n",
      "Train Epoch: 8 [1280/6073 (21%)]\tLoss: 1.084040\n",
      "Train Epoch: 8 [1920/6073 (32%)]\tLoss: 1.234815\n",
      "Train Epoch: 8 [2560/6073 (42%)]\tLoss: 1.269568\n",
      "Train Epoch: 8 [3200/6073 (53%)]\tLoss: 1.316407\n",
      "Train Epoch: 8 [3840/6073 (63%)]\tLoss: 0.951097\n",
      "Train Epoch: 8 [4480/6073 (74%)]\tLoss: 1.050585\n",
      "Train Epoch: 8 [5120/6073 (84%)]\tLoss: 1.175302\n",
      "Train Epoch: 8 [5760/6073 (95%)]\tLoss: 1.204359\n",
      "Train Epoch: 9 [0/6073 (0%)]\tLoss: 1.423674\n",
      "Train Epoch: 9 [640/6073 (11%)]\tLoss: 1.311054\n",
      "Train Epoch: 9 [1280/6073 (21%)]\tLoss: 1.371131\n",
      "Train Epoch: 9 [1920/6073 (32%)]\tLoss: 1.141131\n",
      "Train Epoch: 9 [2560/6073 (42%)]\tLoss: 1.272883\n",
      "Train Epoch: 9 [3200/6073 (53%)]\tLoss: 1.204755\n",
      "Train Epoch: 9 [3840/6073 (63%)]\tLoss: 1.353634\n",
      "Train Epoch: 9 [4480/6073 (74%)]\tLoss: 1.048018\n",
      "Train Epoch: 9 [5120/6073 (84%)]\tLoss: 1.091102\n",
      "Train Epoch: 9 [5760/6073 (95%)]\tLoss: 1.239374\n",
      "Train Epoch: 10 [0/6073 (0%)]\tLoss: 1.174868\n",
      "Train Epoch: 10 [640/6073 (11%)]\tLoss: 1.293562\n",
      "Train Epoch: 10 [1280/6073 (21%)]\tLoss: 1.491472\n",
      "Train Epoch: 10 [1920/6073 (32%)]\tLoss: 1.000205\n",
      "Train Epoch: 10 [2560/6073 (42%)]\tLoss: 1.264942\n",
      "Train Epoch: 10 [3200/6073 (53%)]\tLoss: 1.173006\n",
      "Train Epoch: 10 [3840/6073 (63%)]\tLoss: 1.272386\n",
      "Train Epoch: 10 [4480/6073 (74%)]\tLoss: 1.251925\n",
      "Train Epoch: 10 [5120/6073 (84%)]\tLoss: 1.203764\n",
      "Train Epoch: 10 [5760/6073 (95%)]\tLoss: 1.102658\n",
      "Train Epoch: 11 [0/6073 (0%)]\tLoss: 1.024461\n",
      "Train Epoch: 11 [640/6073 (11%)]\tLoss: 1.119615\n",
      "Train Epoch: 11 [1280/6073 (21%)]\tLoss: 1.257704\n",
      "Train Epoch: 11 [1920/6073 (32%)]\tLoss: 1.342157\n",
      "Train Epoch: 11 [2560/6073 (42%)]\tLoss: 1.030754\n",
      "Train Epoch: 11 [3200/6073 (53%)]\tLoss: 1.343611\n",
      "Train Epoch: 11 [3840/6073 (63%)]\tLoss: 1.089620\n",
      "Train Epoch: 11 [4480/6073 (74%)]\tLoss: 1.054704\n",
      "Train Epoch: 11 [5120/6073 (84%)]\tLoss: 0.931827\n",
      "Train Epoch: 11 [5760/6073 (95%)]\tLoss: 1.340124\n",
      "Train Epoch: 12 [0/6073 (0%)]\tLoss: 1.182329\n",
      "Train Epoch: 12 [640/6073 (11%)]\tLoss: 1.367601\n",
      "Train Epoch: 12 [1280/6073 (21%)]\tLoss: 1.408730\n",
      "Train Epoch: 12 [1920/6073 (32%)]\tLoss: 1.276995\n",
      "Train Epoch: 12 [2560/6073 (42%)]\tLoss: 1.008524\n",
      "Train Epoch: 12 [3200/6073 (53%)]\tLoss: 1.209328\n",
      "Train Epoch: 12 [3840/6073 (63%)]\tLoss: 1.070729\n",
      "Train Epoch: 12 [4480/6073 (74%)]\tLoss: 1.315977\n",
      "Train Epoch: 12 [5120/6073 (84%)]\tLoss: 1.083947\n",
      "Train Epoch: 12 [5760/6073 (95%)]\tLoss: 1.350147\n",
      "Train Epoch: 13 [0/6073 (0%)]\tLoss: 1.127168\n",
      "Train Epoch: 13 [640/6073 (11%)]\tLoss: 1.042911\n",
      "Train Epoch: 13 [1280/6073 (21%)]\tLoss: 1.658318\n",
      "Train Epoch: 13 [1920/6073 (32%)]\tLoss: 1.306664\n",
      "Train Epoch: 13 [2560/6073 (42%)]\tLoss: 1.064949\n",
      "Train Epoch: 13 [3200/6073 (53%)]\tLoss: 1.071817\n",
      "Train Epoch: 13 [3840/6073 (63%)]\tLoss: 1.107244\n",
      "Train Epoch: 13 [4480/6073 (74%)]\tLoss: 1.193465\n",
      "Train Epoch: 13 [5120/6073 (84%)]\tLoss: 1.347554\n",
      "Train Epoch: 13 [5760/6073 (95%)]\tLoss: 1.226517\n",
      "Train Epoch: 14 [0/6073 (0%)]\tLoss: 1.142286\n",
      "Train Epoch: 14 [640/6073 (11%)]\tLoss: 1.407029\n",
      "Train Epoch: 14 [1280/6073 (21%)]\tLoss: 1.054003\n",
      "Train Epoch: 14 [1920/6073 (32%)]\tLoss: 0.964393\n",
      "Train Epoch: 14 [2560/6073 (42%)]\tLoss: 1.270781\n",
      "Train Epoch: 14 [3200/6073 (53%)]\tLoss: 1.123147\n",
      "Train Epoch: 14 [3840/6073 (63%)]\tLoss: 1.027033\n",
      "Train Epoch: 14 [4480/6073 (74%)]\tLoss: 1.184476\n",
      "Train Epoch: 14 [5120/6073 (84%)]\tLoss: 1.298273\n",
      "Train Epoch: 14 [5760/6073 (95%)]\tLoss: 1.101378\n",
      "Train Epoch: 15 [0/6073 (0%)]\tLoss: 1.097161\n",
      "Train Epoch: 15 [640/6073 (11%)]\tLoss: 1.061097\n",
      "Train Epoch: 15 [1280/6073 (21%)]\tLoss: 1.297832\n",
      "Train Epoch: 15 [1920/6073 (32%)]\tLoss: 1.204698\n",
      "Train Epoch: 15 [2560/6073 (42%)]\tLoss: 0.906834\n",
      "Train Epoch: 15 [3200/6073 (53%)]\tLoss: 1.196384\n",
      "Train Epoch: 15 [3840/6073 (63%)]\tLoss: 1.188195\n",
      "Train Epoch: 15 [4480/6073 (74%)]\tLoss: 1.138018\n",
      "Train Epoch: 15 [5120/6073 (84%)]\tLoss: 1.330696\n",
      "Train Epoch: 15 [5760/6073 (95%)]\tLoss: 1.245333\n",
      "Train Epoch: 16 [0/6073 (0%)]\tLoss: 1.166329\n",
      "Train Epoch: 16 [640/6073 (11%)]\tLoss: 1.165680\n",
      "Train Epoch: 16 [1280/6073 (21%)]\tLoss: 0.916733\n",
      "Train Epoch: 16 [1920/6073 (32%)]\tLoss: 1.070043\n",
      "Train Epoch: 16 [2560/6073 (42%)]\tLoss: 1.196565\n",
      "Train Epoch: 16 [3200/6073 (53%)]\tLoss: 1.211336\n",
      "Train Epoch: 16 [3840/6073 (63%)]\tLoss: 1.155464\n",
      "Train Epoch: 16 [4480/6073 (74%)]\tLoss: 1.111394\n",
      "Train Epoch: 16 [5120/6073 (84%)]\tLoss: 1.090764\n",
      "Train Epoch: 16 [5760/6073 (95%)]\tLoss: 0.905892\n",
      "Train Epoch: 17 [0/6073 (0%)]\tLoss: 1.016098\n",
      "Train Epoch: 17 [640/6073 (11%)]\tLoss: 1.189523\n",
      "Train Epoch: 17 [1280/6073 (21%)]\tLoss: 0.941048\n",
      "Train Epoch: 17 [1920/6073 (32%)]\tLoss: 1.359787\n",
      "Train Epoch: 17 [2560/6073 (42%)]\tLoss: 1.143905\n",
      "Train Epoch: 17 [3200/6073 (53%)]\tLoss: 0.887292\n",
      "Train Epoch: 17 [3840/6073 (63%)]\tLoss: 0.844827\n",
      "Train Epoch: 17 [4480/6073 (74%)]\tLoss: 1.013403\n",
      "Train Epoch: 17 [5120/6073 (84%)]\tLoss: 0.987355\n",
      "Train Epoch: 17 [5760/6073 (95%)]\tLoss: 1.210479\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/3439 (0%)]\tLoss: 1.164999\n",
      "Train Epoch: 1 [640/3439 (19%)]\tLoss: 1.318118\n",
      "Train Epoch: 1 [1280/3439 (37%)]\tLoss: 0.936034\n",
      "Train Epoch: 1 [1920/3439 (56%)]\tLoss: 1.099381\n",
      "Train Epoch: 1 [2560/3439 (74%)]\tLoss: 0.976260\n",
      "Train Epoch: 1 [3200/3439 (93%)]\tLoss: 1.267151\n",
      "Train Epoch: 2 [0/3439 (0%)]\tLoss: 1.052777\n",
      "Train Epoch: 2 [640/3439 (19%)]\tLoss: 1.216063\n",
      "Train Epoch: 2 [1280/3439 (37%)]\tLoss: 0.964691\n",
      "Train Epoch: 2 [1920/3439 (56%)]\tLoss: 1.045663\n",
      "Train Epoch: 2 [2560/3439 (74%)]\tLoss: 1.086994\n",
      "Train Epoch: 2 [3200/3439 (93%)]\tLoss: 0.716834\n",
      "Train Epoch: 3 [0/3439 (0%)]\tLoss: 0.923170\n",
      "Train Epoch: 3 [640/3439 (19%)]\tLoss: 0.816292\n",
      "Train Epoch: 3 [1280/3439 (37%)]\tLoss: 1.017323\n",
      "Train Epoch: 3 [1920/3439 (56%)]\tLoss: 0.839530\n",
      "Train Epoch: 3 [2560/3439 (74%)]\tLoss: 0.841994\n",
      "Train Epoch: 3 [3200/3439 (93%)]\tLoss: 0.682453\n",
      "Train Epoch: 4 [0/3439 (0%)]\tLoss: 0.813277\n",
      "Train Epoch: 4 [640/3439 (19%)]\tLoss: 0.826583\n",
      "Train Epoch: 4 [1280/3439 (37%)]\tLoss: 0.878072\n",
      "Train Epoch: 4 [1920/3439 (56%)]\tLoss: 0.915053\n",
      "Train Epoch: 4 [2560/3439 (74%)]\tLoss: 0.906464\n",
      "Train Epoch: 4 [3200/3439 (93%)]\tLoss: 1.021120\n",
      "Train Epoch: 5 [0/3439 (0%)]\tLoss: 0.709797\n",
      "Train Epoch: 5 [640/3439 (19%)]\tLoss: 0.731967\n",
      "Train Epoch: 5 [1280/3439 (37%)]\tLoss: 0.844807\n",
      "Train Epoch: 5 [1920/3439 (56%)]\tLoss: 0.839292\n",
      "Train Epoch: 5 [2560/3439 (74%)]\tLoss: 0.903323\n",
      "Train Epoch: 5 [3200/3439 (93%)]\tLoss: 0.894828\n",
      "Train Epoch: 6 [0/3439 (0%)]\tLoss: 0.758259\n",
      "Train Epoch: 6 [640/3439 (19%)]\tLoss: 0.667225\n",
      "Train Epoch: 6 [1280/3439 (37%)]\tLoss: 0.912328\n",
      "Train Epoch: 6 [1920/3439 (56%)]\tLoss: 1.064288\n",
      "Train Epoch: 6 [2560/3439 (74%)]\tLoss: 0.954843\n",
      "Train Epoch: 6 [3200/3439 (93%)]\tLoss: 1.035265\n",
      "Train Epoch: 7 [0/3439 (0%)]\tLoss: 0.778215\n",
      "Train Epoch: 7 [640/3439 (19%)]\tLoss: 0.921713\n",
      "Train Epoch: 7 [1280/3439 (37%)]\tLoss: 0.713284\n",
      "Train Epoch: 7 [1920/3439 (56%)]\tLoss: 0.666361\n",
      "Train Epoch: 7 [2560/3439 (74%)]\tLoss: 0.706428\n",
      "Train Epoch: 7 [3200/3439 (93%)]\tLoss: 0.827436\n",
      "Train Epoch: 8 [0/3439 (0%)]\tLoss: 0.832267\n",
      "Train Epoch: 8 [640/3439 (19%)]\tLoss: 0.850830\n",
      "Train Epoch: 8 [1280/3439 (37%)]\tLoss: 0.860334\n",
      "Train Epoch: 8 [1920/3439 (56%)]\tLoss: 1.045520\n",
      "Train Epoch: 8 [2560/3439 (74%)]\tLoss: 0.743847\n",
      "Train Epoch: 8 [3200/3439 (93%)]\tLoss: 0.910662\n",
      "Train Epoch: 9 [0/3439 (0%)]\tLoss: 0.786781\n",
      "Train Epoch: 9 [640/3439 (19%)]\tLoss: 0.748391\n",
      "Train Epoch: 9 [1280/3439 (37%)]\tLoss: 0.811881\n",
      "Train Epoch: 9 [1920/3439 (56%)]\tLoss: 0.692031\n",
      "Train Epoch: 9 [2560/3439 (74%)]\tLoss: 0.745380\n",
      "Train Epoch: 9 [3200/3439 (93%)]\tLoss: 0.686636\n",
      "Train Epoch: 10 [0/3439 (0%)]\tLoss: 1.122284\n",
      "Train Epoch: 10 [640/3439 (19%)]\tLoss: 0.874935\n",
      "Train Epoch: 10 [1280/3439 (37%)]\tLoss: 0.977138\n",
      "Train Epoch: 10 [1920/3439 (56%)]\tLoss: 0.748969\n",
      "Train Epoch: 10 [2560/3439 (74%)]\tLoss: 0.987540\n",
      "Train Epoch: 10 [3200/3439 (93%)]\tLoss: 0.812319\n",
      "Train Epoch: 11 [0/3439 (0%)]\tLoss: 0.717376\n",
      "Train Epoch: 11 [640/3439 (19%)]\tLoss: 1.001694\n",
      "Train Epoch: 11 [1280/3439 (37%)]\tLoss: 0.956920\n",
      "Train Epoch: 11 [1920/3439 (56%)]\tLoss: 0.857255\n",
      "Train Epoch: 11 [2560/3439 (74%)]\tLoss: 0.587117\n",
      "Train Epoch: 11 [3200/3439 (93%)]\tLoss: 0.895007\n",
      "Train Epoch: 12 [0/3439 (0%)]\tLoss: 0.794042\n",
      "Train Epoch: 12 [640/3439 (19%)]\tLoss: 0.785390\n",
      "Train Epoch: 12 [1280/3439 (37%)]\tLoss: 0.905489\n",
      "Train Epoch: 12 [1920/3439 (56%)]\tLoss: 0.742179\n",
      "Train Epoch: 12 [2560/3439 (74%)]\tLoss: 0.893344\n",
      "Train Epoch: 12 [3200/3439 (93%)]\tLoss: 0.845862\n",
      "Train Epoch: 13 [0/3439 (0%)]\tLoss: 0.843508\n",
      "Train Epoch: 13 [640/3439 (19%)]\tLoss: 0.648773\n",
      "Train Epoch: 13 [1280/3439 (37%)]\tLoss: 0.759809\n",
      "Train Epoch: 13 [1920/3439 (56%)]\tLoss: 0.900939\n",
      "Train Epoch: 13 [2560/3439 (74%)]\tLoss: 0.835037\n",
      "Train Epoch: 13 [3200/3439 (93%)]\tLoss: 0.647572\n",
      "Train Epoch: 14 [0/3439 (0%)]\tLoss: 0.838227\n",
      "Train Epoch: 14 [640/3439 (19%)]\tLoss: 1.018876\n",
      "Train Epoch: 14 [1280/3439 (37%)]\tLoss: 0.635127\n",
      "Train Epoch: 14 [1920/3439 (56%)]\tLoss: 0.973225\n",
      "Train Epoch: 14 [2560/3439 (74%)]\tLoss: 1.241360\n",
      "Train Epoch: 14 [3200/3439 (93%)]\tLoss: 0.574166\n",
      "Train Epoch: 15 [0/3439 (0%)]\tLoss: 0.666003\n",
      "Train Epoch: 15 [640/3439 (19%)]\tLoss: 0.780829\n",
      "Train Epoch: 15 [1280/3439 (37%)]\tLoss: 0.871989\n",
      "Train Epoch: 15 [1920/3439 (56%)]\tLoss: 0.877890\n",
      "Train Epoch: 15 [2560/3439 (74%)]\tLoss: 0.887947\n",
      "Train Epoch: 15 [3200/3439 (93%)]\tLoss: 0.796034\n",
      "Train Epoch: 16 [0/3439 (0%)]\tLoss: 0.865646\n",
      "Train Epoch: 16 [640/3439 (19%)]\tLoss: 0.640707\n",
      "Train Epoch: 16 [1280/3439 (37%)]\tLoss: 0.616753\n",
      "Train Epoch: 16 [1920/3439 (56%)]\tLoss: 0.818577\n",
      "Train Epoch: 16 [2560/3439 (74%)]\tLoss: 1.401745\n",
      "Train Epoch: 16 [3200/3439 (93%)]\tLoss: 0.868119\n",
      "Train Epoch: 17 [0/3439 (0%)]\tLoss: 0.635730\n",
      "Train Epoch: 17 [640/3439 (19%)]\tLoss: 0.719806\n",
      "Train Epoch: 17 [1280/3439 (37%)]\tLoss: 0.730780\n",
      "Train Epoch: 17 [1920/3439 (56%)]\tLoss: 0.878203\n",
      "Train Epoch: 17 [2560/3439 (74%)]\tLoss: 0.742138\n",
      "Train Epoch: 17 [3200/3439 (93%)]\tLoss: 0.975757\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/3683 (0%)]\tLoss: 1.128220\n",
      "Train Epoch: 1 [640/3683 (17%)]\tLoss: 1.178881\n",
      "Train Epoch: 1 [1280/3683 (34%)]\tLoss: 1.139076\n",
      "Train Epoch: 1 [1920/3683 (52%)]\tLoss: 0.986564\n",
      "Train Epoch: 1 [2560/3683 (69%)]\tLoss: 0.998136\n",
      "Train Epoch: 1 [3200/3683 (86%)]\tLoss: 1.327726\n",
      "Train Epoch: 2 [0/3683 (0%)]\tLoss: 1.073937\n",
      "Train Epoch: 2 [640/3683 (17%)]\tLoss: 0.841108\n",
      "Train Epoch: 2 [1280/3683 (34%)]\tLoss: 0.939520\n",
      "Train Epoch: 2 [1920/3683 (52%)]\tLoss: 1.079765\n",
      "Train Epoch: 2 [2560/3683 (69%)]\tLoss: 1.082611\n",
      "Train Epoch: 2 [3200/3683 (86%)]\tLoss: 1.242063\n",
      "Train Epoch: 3 [0/3683 (0%)]\tLoss: 0.934394\n",
      "Train Epoch: 3 [640/3683 (17%)]\tLoss: 1.031557\n",
      "Train Epoch: 3 [1280/3683 (34%)]\tLoss: 1.150614\n",
      "Train Epoch: 3 [1920/3683 (52%)]\tLoss: 1.243573\n",
      "Train Epoch: 3 [2560/3683 (69%)]\tLoss: 1.133516\n",
      "Train Epoch: 3 [3200/3683 (86%)]\tLoss: 1.235905\n",
      "Train Epoch: 4 [0/3683 (0%)]\tLoss: 0.963642\n",
      "Train Epoch: 4 [640/3683 (17%)]\tLoss: 0.934193\n",
      "Train Epoch: 4 [1280/3683 (34%)]\tLoss: 1.362154\n",
      "Train Epoch: 4 [1920/3683 (52%)]\tLoss: 0.918296\n",
      "Train Epoch: 4 [2560/3683 (69%)]\tLoss: 1.074360\n",
      "Train Epoch: 4 [3200/3683 (86%)]\tLoss: 1.010705\n",
      "Train Epoch: 5 [0/3683 (0%)]\tLoss: 0.962610\n",
      "Train Epoch: 5 [640/3683 (17%)]\tLoss: 1.079542\n",
      "Train Epoch: 5 [1280/3683 (34%)]\tLoss: 1.141895\n",
      "Train Epoch: 5 [1920/3683 (52%)]\tLoss: 0.943589\n",
      "Train Epoch: 5 [2560/3683 (69%)]\tLoss: 0.761609\n",
      "Train Epoch: 5 [3200/3683 (86%)]\tLoss: 1.215905\n",
      "Train Epoch: 6 [0/3683 (0%)]\tLoss: 0.844142\n",
      "Train Epoch: 6 [640/3683 (17%)]\tLoss: 0.842747\n",
      "Train Epoch: 6 [1280/3683 (34%)]\tLoss: 0.858773\n",
      "Train Epoch: 6 [1920/3683 (52%)]\tLoss: 0.789904\n",
      "Train Epoch: 6 [2560/3683 (69%)]\tLoss: 1.128061\n",
      "Train Epoch: 6 [3200/3683 (86%)]\tLoss: 0.880677\n",
      "Train Epoch: 7 [0/3683 (0%)]\tLoss: 0.828007\n",
      "Train Epoch: 7 [640/3683 (17%)]\tLoss: 0.841767\n",
      "Train Epoch: 7 [1280/3683 (34%)]\tLoss: 1.163051\n",
      "Train Epoch: 7 [1920/3683 (52%)]\tLoss: 0.985131\n",
      "Train Epoch: 7 [2560/3683 (69%)]\tLoss: 0.993362\n",
      "Train Epoch: 7 [3200/3683 (86%)]\tLoss: 0.878397\n",
      "Train Epoch: 8 [0/3683 (0%)]\tLoss: 0.825711\n",
      "Train Epoch: 8 [640/3683 (17%)]\tLoss: 0.939740\n",
      "Train Epoch: 8 [1280/3683 (34%)]\tLoss: 1.078666\n",
      "Train Epoch: 8 [1920/3683 (52%)]\tLoss: 0.740902\n",
      "Train Epoch: 8 [2560/3683 (69%)]\tLoss: 0.949133\n",
      "Train Epoch: 8 [3200/3683 (86%)]\tLoss: 0.809178\n",
      "Train Epoch: 9 [0/3683 (0%)]\tLoss: 0.932170\n",
      "Train Epoch: 9 [640/3683 (17%)]\tLoss: 0.828728\n",
      "Train Epoch: 9 [1280/3683 (34%)]\tLoss: 0.823585\n",
      "Train Epoch: 9 [1920/3683 (52%)]\tLoss: 0.974382\n",
      "Train Epoch: 9 [2560/3683 (69%)]\tLoss: 1.040791\n",
      "Train Epoch: 9 [3200/3683 (86%)]\tLoss: 1.022465\n",
      "Train Epoch: 10 [0/3683 (0%)]\tLoss: 0.934859\n",
      "Train Epoch: 10 [640/3683 (17%)]\tLoss: 1.319588\n",
      "Train Epoch: 10 [1280/3683 (34%)]\tLoss: 1.139885\n",
      "Train Epoch: 10 [1920/3683 (52%)]\tLoss: 0.965615\n",
      "Train Epoch: 10 [2560/3683 (69%)]\tLoss: 1.055074\n",
      "Train Epoch: 10 [3200/3683 (86%)]\tLoss: 0.955883\n",
      "Train Epoch: 11 [0/3683 (0%)]\tLoss: 1.021612\n",
      "Train Epoch: 11 [640/3683 (17%)]\tLoss: 0.934783\n",
      "Train Epoch: 11 [1280/3683 (34%)]\tLoss: 1.041711\n",
      "Train Epoch: 11 [1920/3683 (52%)]\tLoss: 1.048792\n",
      "Train Epoch: 11 [2560/3683 (69%)]\tLoss: 0.757877\n",
      "Train Epoch: 11 [3200/3683 (86%)]\tLoss: 1.277401\n",
      "Train Epoch: 12 [0/3683 (0%)]\tLoss: 1.087032\n",
      "Train Epoch: 12 [640/3683 (17%)]\tLoss: 0.894045\n",
      "Train Epoch: 12 [1280/3683 (34%)]\tLoss: 0.787843\n",
      "Train Epoch: 12 [1920/3683 (52%)]\tLoss: 1.114227\n",
      "Train Epoch: 12 [2560/3683 (69%)]\tLoss: 1.002932\n",
      "Train Epoch: 12 [3200/3683 (86%)]\tLoss: 0.733414\n",
      "Train Epoch: 13 [0/3683 (0%)]\tLoss: 1.107216\n",
      "Train Epoch: 13 [640/3683 (17%)]\tLoss: 1.039918\n",
      "Train Epoch: 13 [1280/3683 (34%)]\tLoss: 1.090093\n",
      "Train Epoch: 13 [1920/3683 (52%)]\tLoss: 0.783524\n",
      "Train Epoch: 13 [2560/3683 (69%)]\tLoss: 1.030926\n",
      "Train Epoch: 13 [3200/3683 (86%)]\tLoss: 0.867689\n",
      "Train Epoch: 14 [0/3683 (0%)]\tLoss: 0.886911\n",
      "Train Epoch: 14 [640/3683 (17%)]\tLoss: 1.004133\n",
      "Train Epoch: 14 [1280/3683 (34%)]\tLoss: 1.048518\n",
      "Train Epoch: 14 [1920/3683 (52%)]\tLoss: 1.084691\n",
      "Train Epoch: 14 [2560/3683 (69%)]\tLoss: 1.048892\n",
      "Train Epoch: 14 [3200/3683 (86%)]\tLoss: 0.872873\n",
      "Train Epoch: 15 [0/3683 (0%)]\tLoss: 0.711828\n",
      "Train Epoch: 15 [640/3683 (17%)]\tLoss: 0.848479\n",
      "Train Epoch: 15 [1280/3683 (34%)]\tLoss: 1.055726\n",
      "Train Epoch: 15 [1920/3683 (52%)]\tLoss: 1.139029\n",
      "Train Epoch: 15 [2560/3683 (69%)]\tLoss: 0.742344\n",
      "Train Epoch: 15 [3200/3683 (86%)]\tLoss: 0.922841\n",
      "Train Epoch: 16 [0/3683 (0%)]\tLoss: 0.896624\n",
      "Train Epoch: 16 [640/3683 (17%)]\tLoss: 0.813639\n",
      "Train Epoch: 16 [1280/3683 (34%)]\tLoss: 0.955543\n",
      "Train Epoch: 16 [1920/3683 (52%)]\tLoss: 0.848518\n",
      "Train Epoch: 16 [2560/3683 (69%)]\tLoss: 0.981770\n",
      "Train Epoch: 16 [3200/3683 (86%)]\tLoss: 0.916923\n",
      "Train Epoch: 17 [0/3683 (0%)]\tLoss: 1.051348\n",
      "Train Epoch: 17 [640/3683 (17%)]\tLoss: 0.929103\n",
      "Train Epoch: 17 [1280/3683 (34%)]\tLoss: 0.853559\n",
      "Train Epoch: 17 [1920/3683 (52%)]\tLoss: 0.740955\n",
      "Train Epoch: 17 [2560/3683 (69%)]\tLoss: 0.911070\n",
      "Train Epoch: 17 [3200/3683 (86%)]\tLoss: 1.074160\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4824 (0%)]\tLoss: 1.245346\n",
      "Train Epoch: 1 [640/4824 (13%)]\tLoss: 1.240808\n",
      "Train Epoch: 1 [1280/4824 (26%)]\tLoss: 1.372901\n",
      "Train Epoch: 1 [1920/4824 (39%)]\tLoss: 1.550061\n",
      "Train Epoch: 1 [2560/4824 (53%)]\tLoss: 1.232007\n",
      "Train Epoch: 1 [3200/4824 (66%)]\tLoss: 1.167491\n",
      "Train Epoch: 1 [3840/4824 (79%)]\tLoss: 1.140889\n",
      "Train Epoch: 1 [4480/4824 (92%)]\tLoss: 1.470667\n",
      "Train Epoch: 2 [0/4824 (0%)]\tLoss: 1.435768\n",
      "Train Epoch: 2 [640/4824 (13%)]\tLoss: 1.031082\n",
      "Train Epoch: 2 [1280/4824 (26%)]\tLoss: 1.202383\n",
      "Train Epoch: 2 [1920/4824 (39%)]\tLoss: 0.985412\n",
      "Train Epoch: 2 [2560/4824 (53%)]\tLoss: 1.014316\n",
      "Train Epoch: 2 [3200/4824 (66%)]\tLoss: 1.376017\n",
      "Train Epoch: 2 [3840/4824 (79%)]\tLoss: 1.026141\n",
      "Train Epoch: 2 [4480/4824 (92%)]\tLoss: 1.175586\n",
      "Train Epoch: 3 [0/4824 (0%)]\tLoss: 1.099504\n",
      "Train Epoch: 3 [640/4824 (13%)]\tLoss: 1.236310\n",
      "Train Epoch: 3 [1280/4824 (26%)]\tLoss: 1.323588\n",
      "Train Epoch: 3 [1920/4824 (39%)]\tLoss: 1.135583\n",
      "Train Epoch: 3 [2560/4824 (53%)]\tLoss: 1.573300\n",
      "Train Epoch: 3 [3200/4824 (66%)]\tLoss: 1.261487\n",
      "Train Epoch: 3 [3840/4824 (79%)]\tLoss: 1.213120\n",
      "Train Epoch: 3 [4480/4824 (92%)]\tLoss: 0.962689\n",
      "Train Epoch: 4 [0/4824 (0%)]\tLoss: 1.209200\n",
      "Train Epoch: 4 [640/4824 (13%)]\tLoss: 1.008223\n",
      "Train Epoch: 4 [1280/4824 (26%)]\tLoss: 1.157491\n",
      "Train Epoch: 4 [1920/4824 (39%)]\tLoss: 1.326799\n",
      "Train Epoch: 4 [2560/4824 (53%)]\tLoss: 1.382087\n",
      "Train Epoch: 4 [3200/4824 (66%)]\tLoss: 1.253658\n",
      "Train Epoch: 4 [3840/4824 (79%)]\tLoss: 1.026651\n",
      "Train Epoch: 4 [4480/4824 (92%)]\tLoss: 1.242326\n",
      "Train Epoch: 5 [0/4824 (0%)]\tLoss: 1.102284\n",
      "Train Epoch: 5 [640/4824 (13%)]\tLoss: 1.402975\n",
      "Train Epoch: 5 [1280/4824 (26%)]\tLoss: 0.934812\n",
      "Train Epoch: 5 [1920/4824 (39%)]\tLoss: 1.310146\n",
      "Train Epoch: 5 [2560/4824 (53%)]\tLoss: 1.237537\n",
      "Train Epoch: 5 [3200/4824 (66%)]\tLoss: 1.217693\n",
      "Train Epoch: 5 [3840/4824 (79%)]\tLoss: 1.149350\n",
      "Train Epoch: 5 [4480/4824 (92%)]\tLoss: 1.002554\n",
      "Train Epoch: 6 [0/4824 (0%)]\tLoss: 1.124300\n",
      "Train Epoch: 6 [640/4824 (13%)]\tLoss: 1.125987\n",
      "Train Epoch: 6 [1280/4824 (26%)]\tLoss: 0.970498\n",
      "Train Epoch: 6 [1920/4824 (39%)]\tLoss: 0.968737\n",
      "Train Epoch: 6 [2560/4824 (53%)]\tLoss: 1.244283\n",
      "Train Epoch: 6 [3200/4824 (66%)]\tLoss: 1.069124\n",
      "Train Epoch: 6 [3840/4824 (79%)]\tLoss: 1.156986\n",
      "Train Epoch: 6 [4480/4824 (92%)]\tLoss: 1.184177\n",
      "Train Epoch: 7 [0/4824 (0%)]\tLoss: 1.290101\n",
      "Train Epoch: 7 [640/4824 (13%)]\tLoss: 1.370329\n",
      "Train Epoch: 7 [1280/4824 (26%)]\tLoss: 1.036531\n",
      "Train Epoch: 7 [1920/4824 (39%)]\tLoss: 1.136124\n",
      "Train Epoch: 7 [2560/4824 (53%)]\tLoss: 1.031254\n",
      "Train Epoch: 7 [3200/4824 (66%)]\tLoss: 1.135372\n",
      "Train Epoch: 7 [3840/4824 (79%)]\tLoss: 1.326060\n",
      "Train Epoch: 7 [4480/4824 (92%)]\tLoss: 0.968409\n",
      "Train Epoch: 8 [0/4824 (0%)]\tLoss: 1.119304\n",
      "Train Epoch: 8 [640/4824 (13%)]\tLoss: 1.077348\n",
      "Train Epoch: 8 [1280/4824 (26%)]\tLoss: 1.270470\n",
      "Train Epoch: 8 [1920/4824 (39%)]\tLoss: 1.165725\n",
      "Train Epoch: 8 [2560/4824 (53%)]\tLoss: 1.067137\n",
      "Train Epoch: 8 [3200/4824 (66%)]\tLoss: 1.204382\n",
      "Train Epoch: 8 [3840/4824 (79%)]\tLoss: 1.422544\n",
      "Train Epoch: 8 [4480/4824 (92%)]\tLoss: 1.212265\n",
      "Train Epoch: 9 [0/4824 (0%)]\tLoss: 1.119496\n",
      "Train Epoch: 9 [640/4824 (13%)]\tLoss: 1.321512\n",
      "Train Epoch: 9 [1280/4824 (26%)]\tLoss: 1.246888\n",
      "Train Epoch: 9 [1920/4824 (39%)]\tLoss: 1.394175\n",
      "Train Epoch: 9 [2560/4824 (53%)]\tLoss: 1.188149\n",
      "Train Epoch: 9 [3200/4824 (66%)]\tLoss: 1.076685\n",
      "Train Epoch: 9 [3840/4824 (79%)]\tLoss: 1.131439\n",
      "Train Epoch: 9 [4480/4824 (92%)]\tLoss: 1.053780\n",
      "Train Epoch: 10 [0/4824 (0%)]\tLoss: 1.244663\n",
      "Train Epoch: 10 [640/4824 (13%)]\tLoss: 0.997689\n",
      "Train Epoch: 10 [1280/4824 (26%)]\tLoss: 1.258757\n",
      "Train Epoch: 10 [1920/4824 (39%)]\tLoss: 1.274517\n",
      "Train Epoch: 10 [2560/4824 (53%)]\tLoss: 1.127410\n",
      "Train Epoch: 10 [3200/4824 (66%)]\tLoss: 1.093089\n",
      "Train Epoch: 10 [3840/4824 (79%)]\tLoss: 1.128462\n",
      "Train Epoch: 10 [4480/4824 (92%)]\tLoss: 1.048310\n",
      "Train Epoch: 11 [0/4824 (0%)]\tLoss: 1.231929\n",
      "Train Epoch: 11 [640/4824 (13%)]\tLoss: 1.107859\n",
      "Train Epoch: 11 [1280/4824 (26%)]\tLoss: 1.081549\n",
      "Train Epoch: 11 [1920/4824 (39%)]\tLoss: 1.009694\n",
      "Train Epoch: 11 [2560/4824 (53%)]\tLoss: 1.022154\n",
      "Train Epoch: 11 [3200/4824 (66%)]\tLoss: 1.205423\n",
      "Train Epoch: 11 [3840/4824 (79%)]\tLoss: 1.002541\n",
      "Train Epoch: 11 [4480/4824 (92%)]\tLoss: 1.065994\n",
      "Train Epoch: 12 [0/4824 (0%)]\tLoss: 1.067984\n",
      "Train Epoch: 12 [640/4824 (13%)]\tLoss: 1.352714\n",
      "Train Epoch: 12 [1280/4824 (26%)]\tLoss: 1.163692\n",
      "Train Epoch: 12 [1920/4824 (39%)]\tLoss: 1.108449\n",
      "Train Epoch: 12 [2560/4824 (53%)]\tLoss: 1.015950\n",
      "Train Epoch: 12 [3200/4824 (66%)]\tLoss: 1.119248\n",
      "Train Epoch: 12 [3840/4824 (79%)]\tLoss: 1.114516\n",
      "Train Epoch: 12 [4480/4824 (92%)]\tLoss: 1.241840\n",
      "Train Epoch: 13 [0/4824 (0%)]\tLoss: 1.306194\n",
      "Train Epoch: 13 [640/4824 (13%)]\tLoss: 1.185472\n",
      "Train Epoch: 13 [1280/4824 (26%)]\tLoss: 0.865030\n",
      "Train Epoch: 13 [1920/4824 (39%)]\tLoss: 1.346946\n",
      "Train Epoch: 13 [2560/4824 (53%)]\tLoss: 1.218488\n",
      "Train Epoch: 13 [3200/4824 (66%)]\tLoss: 0.874450\n",
      "Train Epoch: 13 [3840/4824 (79%)]\tLoss: 1.177669\n",
      "Train Epoch: 13 [4480/4824 (92%)]\tLoss: 1.210825\n",
      "Train Epoch: 14 [0/4824 (0%)]\tLoss: 1.012700\n",
      "Train Epoch: 14 [640/4824 (13%)]\tLoss: 1.140601\n",
      "Train Epoch: 14 [1280/4824 (26%)]\tLoss: 0.937140\n",
      "Train Epoch: 14 [1920/4824 (39%)]\tLoss: 1.025815\n",
      "Train Epoch: 14 [2560/4824 (53%)]\tLoss: 1.195827\n",
      "Train Epoch: 14 [3200/4824 (66%)]\tLoss: 0.979912\n",
      "Train Epoch: 14 [3840/4824 (79%)]\tLoss: 1.035805\n",
      "Train Epoch: 14 [4480/4824 (92%)]\tLoss: 1.133100\n",
      "Train Epoch: 15 [0/4824 (0%)]\tLoss: 1.408239\n",
      "Train Epoch: 15 [640/4824 (13%)]\tLoss: 0.933839\n",
      "Train Epoch: 15 [1280/4824 (26%)]\tLoss: 0.999145\n",
      "Train Epoch: 15 [1920/4824 (39%)]\tLoss: 0.892191\n",
      "Train Epoch: 15 [2560/4824 (53%)]\tLoss: 1.074430\n",
      "Train Epoch: 15 [3200/4824 (66%)]\tLoss: 1.009748\n",
      "Train Epoch: 15 [3840/4824 (79%)]\tLoss: 1.118619\n",
      "Train Epoch: 15 [4480/4824 (92%)]\tLoss: 1.099275\n",
      "Train Epoch: 16 [0/4824 (0%)]\tLoss: 1.178556\n",
      "Train Epoch: 16 [640/4824 (13%)]\tLoss: 0.986463\n",
      "Train Epoch: 16 [1280/4824 (26%)]\tLoss: 1.332898\n",
      "Train Epoch: 16 [1920/4824 (39%)]\tLoss: 0.997494\n",
      "Train Epoch: 16 [2560/4824 (53%)]\tLoss: 1.062036\n",
      "Train Epoch: 16 [3200/4824 (66%)]\tLoss: 1.146148\n",
      "Train Epoch: 16 [3840/4824 (79%)]\tLoss: 1.002566\n",
      "Train Epoch: 16 [4480/4824 (92%)]\tLoss: 1.070343\n",
      "Train Epoch: 17 [0/4824 (0%)]\tLoss: 0.999558\n",
      "Train Epoch: 17 [640/4824 (13%)]\tLoss: 1.215979\n",
      "Train Epoch: 17 [1280/4824 (26%)]\tLoss: 1.071529\n",
      "Train Epoch: 17 [1920/4824 (39%)]\tLoss: 1.076539\n",
      "Train Epoch: 17 [2560/4824 (53%)]\tLoss: 1.275958\n",
      "Train Epoch: 17 [3200/4824 (66%)]\tLoss: 1.104818\n",
      "Train Epoch: 17 [3840/4824 (79%)]\tLoss: 1.160840\n",
      "Train Epoch: 17 [4480/4824 (92%)]\tLoss: 1.230938\n",
      "after training[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "after fedaveraging[cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      "), cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "10\n",
      "local_models in the distribute function [cifar_classification_model(\n",
      "  (conv1): Conv2d(3, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1): Linear(in_features=500, out_features=50, bias=True)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")]\n",
      "1\n",
      "\n",
      "Test set: Avg. loss: 1.1818, Accuracy: 5641/10000 (56%)\n",
      "\n",
      "Running experiment with alpha: 5 \n",
      "Round 1/4\n",
      "Training client 1\n",
      "Train Epoch: 1 [0/4982 (0%)]\tLoss: 1.423119\n",
      "Train Epoch: 1 [640/4982 (13%)]\tLoss: 1.236630\n",
      "Train Epoch: 1 [1280/4982 (26%)]\tLoss: 1.304619\n",
      "Train Epoch: 1 [1920/4982 (38%)]\tLoss: 1.486477\n",
      "Train Epoch: 1 [2560/4982 (51%)]\tLoss: 1.393377\n",
      "Train Epoch: 1 [3200/4982 (64%)]\tLoss: 1.263056\n",
      "Train Epoch: 1 [3840/4982 (77%)]\tLoss: 1.111785\n",
      "Train Epoch: 1 [4480/4982 (90%)]\tLoss: 1.352087\n",
      "Train Epoch: 2 [0/4982 (0%)]\tLoss: 1.296983\n",
      "Train Epoch: 2 [640/4982 (13%)]\tLoss: 1.212888\n",
      "Train Epoch: 2 [1280/4982 (26%)]\tLoss: 1.147368\n",
      "Train Epoch: 2 [1920/4982 (38%)]\tLoss: 1.235781\n",
      "Train Epoch: 2 [2560/4982 (51%)]\tLoss: 1.038249\n",
      "Train Epoch: 2 [3200/4982 (64%)]\tLoss: 1.133836\n",
      "Train Epoch: 2 [3840/4982 (77%)]\tLoss: 1.442493\n",
      "Train Epoch: 2 [4480/4982 (90%)]\tLoss: 1.327251\n",
      "Train Epoch: 3 [0/4982 (0%)]\tLoss: 1.428074\n",
      "Train Epoch: 3 [640/4982 (13%)]\tLoss: 1.161594\n",
      "Train Epoch: 3 [1280/4982 (26%)]\tLoss: 0.982744\n",
      "Train Epoch: 3 [1920/4982 (38%)]\tLoss: 1.344741\n",
      "Train Epoch: 3 [2560/4982 (51%)]\tLoss: 1.614021\n",
      "Train Epoch: 3 [3200/4982 (64%)]\tLoss: 1.449287\n",
      "Train Epoch: 3 [3840/4982 (77%)]\tLoss: 1.203684\n",
      "Train Epoch: 3 [4480/4982 (90%)]\tLoss: 1.350803\n",
      "Train Epoch: 4 [0/4982 (0%)]\tLoss: 1.363583\n",
      "Train Epoch: 4 [640/4982 (13%)]\tLoss: 1.313178\n",
      "Train Epoch: 4 [1280/4982 (26%)]\tLoss: 1.281943\n",
      "Train Epoch: 4 [1920/4982 (38%)]\tLoss: 1.337098\n",
      "Train Epoch: 4 [2560/4982 (51%)]\tLoss: 1.184414\n",
      "Train Epoch: 4 [3200/4982 (64%)]\tLoss: 1.196069\n",
      "Train Epoch: 4 [3840/4982 (77%)]\tLoss: 1.205302\n",
      "Train Epoch: 4 [4480/4982 (90%)]\tLoss: 1.005738\n",
      "Train Epoch: 5 [0/4982 (0%)]\tLoss: 1.270016\n",
      "Train Epoch: 5 [640/4982 (13%)]\tLoss: 1.190894\n",
      "Train Epoch: 5 [1280/4982 (26%)]\tLoss: 1.384366\n",
      "Train Epoch: 5 [1920/4982 (38%)]\tLoss: 1.117377\n",
      "Train Epoch: 5 [2560/4982 (51%)]\tLoss: 1.010132\n",
      "Train Epoch: 5 [3200/4982 (64%)]\tLoss: 1.226249\n",
      "Train Epoch: 5 [3840/4982 (77%)]\tLoss: 1.305000\n",
      "Train Epoch: 5 [4480/4982 (90%)]\tLoss: 1.240823\n",
      "Train Epoch: 6 [0/4982 (0%)]\tLoss: 1.134565\n",
      "Train Epoch: 6 [640/4982 (13%)]\tLoss: 1.206867\n",
      "Train Epoch: 6 [1280/4982 (26%)]\tLoss: 1.092021\n",
      "Train Epoch: 6 [1920/4982 (38%)]\tLoss: 1.168507\n",
      "Train Epoch: 6 [2560/4982 (51%)]\tLoss: 0.984465\n",
      "Train Epoch: 6 [3200/4982 (64%)]\tLoss: 1.080002\n",
      "Train Epoch: 6 [3840/4982 (77%)]\tLoss: 1.109043\n",
      "Train Epoch: 6 [4480/4982 (90%)]\tLoss: 1.331239\n",
      "Train Epoch: 7 [0/4982 (0%)]\tLoss: 1.114947\n",
      "Train Epoch: 7 [640/4982 (13%)]\tLoss: 1.371178\n",
      "Train Epoch: 7 [1280/4982 (26%)]\tLoss: 1.356240\n",
      "Train Epoch: 7 [1920/4982 (38%)]\tLoss: 1.067166\n",
      "Train Epoch: 7 [2560/4982 (51%)]\tLoss: 1.225444\n",
      "Train Epoch: 7 [3200/4982 (64%)]\tLoss: 1.370020\n",
      "Train Epoch: 7 [3840/4982 (77%)]\tLoss: 0.987469\n",
      "Train Epoch: 7 [4480/4982 (90%)]\tLoss: 1.271860\n",
      "Train Epoch: 8 [0/4982 (0%)]\tLoss: 1.208807\n",
      "Train Epoch: 8 [640/4982 (13%)]\tLoss: 0.945591\n",
      "Train Epoch: 8 [1280/4982 (26%)]\tLoss: 1.361504\n",
      "Train Epoch: 8 [1920/4982 (38%)]\tLoss: 1.227109\n",
      "Train Epoch: 8 [2560/4982 (51%)]\tLoss: 1.019861\n",
      "Train Epoch: 8 [3200/4982 (64%)]\tLoss: 1.141177\n",
      "Train Epoch: 8 [3840/4982 (77%)]\tLoss: 1.245411\n",
      "Train Epoch: 8 [4480/4982 (90%)]\tLoss: 1.206981\n",
      "Train Epoch: 9 [0/4982 (0%)]\tLoss: 1.232529\n",
      "Train Epoch: 9 [640/4982 (13%)]\tLoss: 1.161249\n",
      "Train Epoch: 9 [1280/4982 (26%)]\tLoss: 1.310320\n",
      "Train Epoch: 9 [1920/4982 (38%)]\tLoss: 1.384243\n",
      "Train Epoch: 9 [2560/4982 (51%)]\tLoss: 1.042980\n",
      "Train Epoch: 9 [3200/4982 (64%)]\tLoss: 1.148697\n",
      "Train Epoch: 9 [3840/4982 (77%)]\tLoss: 1.101446\n",
      "Train Epoch: 9 [4480/4982 (90%)]\tLoss: 1.261165\n",
      "Train Epoch: 10 [0/4982 (0%)]\tLoss: 1.041185\n",
      "Train Epoch: 10 [640/4982 (13%)]\tLoss: 1.128081\n",
      "Train Epoch: 10 [1280/4982 (26%)]\tLoss: 1.062149\n",
      "Train Epoch: 10 [1920/4982 (38%)]\tLoss: 1.098326\n",
      "Train Epoch: 10 [2560/4982 (51%)]\tLoss: 1.010149\n",
      "Train Epoch: 10 [3200/4982 (64%)]\tLoss: 1.207997\n",
      "Train Epoch: 10 [3840/4982 (77%)]\tLoss: 1.302280\n",
      "Train Epoch: 10 [4480/4982 (90%)]\tLoss: 1.488983\n",
      "Train Epoch: 11 [0/4982 (0%)]\tLoss: 1.550948\n",
      "Train Epoch: 11 [640/4982 (13%)]\tLoss: 1.210888\n",
      "Train Epoch: 11 [1280/4982 (26%)]\tLoss: 1.152871\n",
      "Train Epoch: 11 [1920/4982 (38%)]\tLoss: 1.076078\n",
      "Train Epoch: 11 [2560/4982 (51%)]\tLoss: 1.237119\n",
      "Train Epoch: 11 [3200/4982 (64%)]\tLoss: 1.388389\n",
      "Train Epoch: 11 [3840/4982 (77%)]\tLoss: 1.374150\n",
      "Train Epoch: 11 [4480/4982 (90%)]\tLoss: 1.132849\n",
      "Train Epoch: 12 [0/4982 (0%)]\tLoss: 1.157089\n",
      "Train Epoch: 12 [640/4982 (13%)]\tLoss: 1.210534\n",
      "Train Epoch: 12 [1280/4982 (26%)]\tLoss: 1.116109\n",
      "Train Epoch: 12 [1920/4982 (38%)]\tLoss: 1.319516\n",
      "Train Epoch: 12 [2560/4982 (51%)]\tLoss: 1.056413\n",
      "Train Epoch: 12 [3200/4982 (64%)]\tLoss: 1.036131\n",
      "Train Epoch: 12 [3840/4982 (77%)]\tLoss: 1.037839\n",
      "Train Epoch: 12 [4480/4982 (90%)]\tLoss: 1.046019\n",
      "Train Epoch: 13 [0/4982 (0%)]\tLoss: 1.213855\n",
      "Train Epoch: 13 [640/4982 (13%)]\tLoss: 0.866688\n",
      "Train Epoch: 13 [1280/4982 (26%)]\tLoss: 1.448547\n",
      "Train Epoch: 13 [1920/4982 (38%)]\tLoss: 1.172318\n",
      "Train Epoch: 13 [2560/4982 (51%)]\tLoss: 1.244815\n",
      "Train Epoch: 13 [3200/4982 (64%)]\tLoss: 1.354653\n",
      "Train Epoch: 13 [3840/4982 (77%)]\tLoss: 1.190159\n",
      "Train Epoch: 13 [4480/4982 (90%)]\tLoss: 1.083717\n",
      "Train Epoch: 14 [0/4982 (0%)]\tLoss: 1.342776\n",
      "Train Epoch: 14 [640/4982 (13%)]\tLoss: 1.131613\n",
      "Train Epoch: 14 [1280/4982 (26%)]\tLoss: 1.073068\n",
      "Train Epoch: 14 [1920/4982 (38%)]\tLoss: 1.144783\n",
      "Train Epoch: 14 [2560/4982 (51%)]\tLoss: 1.257208\n",
      "Train Epoch: 14 [3200/4982 (64%)]\tLoss: 1.147422\n",
      "Train Epoch: 14 [3840/4982 (77%)]\tLoss: 0.971310\n",
      "Train Epoch: 14 [4480/4982 (90%)]\tLoss: 1.277933\n",
      "Train Epoch: 15 [0/4982 (0%)]\tLoss: 1.057337\n",
      "Train Epoch: 15 [640/4982 (13%)]\tLoss: 1.141387\n",
      "Train Epoch: 15 [1280/4982 (26%)]\tLoss: 1.211998\n",
      "Train Epoch: 15 [1920/4982 (38%)]\tLoss: 1.089499\n",
      "Train Epoch: 15 [2560/4982 (51%)]\tLoss: 1.115674\n",
      "Train Epoch: 15 [3200/4982 (64%)]\tLoss: 1.204997\n",
      "Train Epoch: 15 [3840/4982 (77%)]\tLoss: 1.167665\n",
      "Train Epoch: 15 [4480/4982 (90%)]\tLoss: 1.360161\n",
      "Train Epoch: 16 [0/4982 (0%)]\tLoss: 0.997319\n",
      "Train Epoch: 16 [640/4982 (13%)]\tLoss: 1.144361\n",
      "Train Epoch: 16 [1280/4982 (26%)]\tLoss: 1.225481\n",
      "Train Epoch: 16 [1920/4982 (38%)]\tLoss: 1.197181\n",
      "Train Epoch: 16 [2560/4982 (51%)]\tLoss: 1.060770\n",
      "Train Epoch: 16 [3200/4982 (64%)]\tLoss: 1.117954\n",
      "Train Epoch: 16 [3840/4982 (77%)]\tLoss: 1.138909\n",
      "Train Epoch: 16 [4480/4982 (90%)]\tLoss: 1.387261\n",
      "Train Epoch: 17 [0/4982 (0%)]\tLoss: 1.165243\n",
      "Train Epoch: 17 [640/4982 (13%)]\tLoss: 1.104973\n",
      "Train Epoch: 17 [1280/4982 (26%)]\tLoss: 1.330309\n",
      "Train Epoch: 17 [1920/4982 (38%)]\tLoss: 1.217755\n",
      "Train Epoch: 17 [2560/4982 (51%)]\tLoss: 1.275203\n",
      "Train Epoch: 17 [3200/4982 (64%)]\tLoss: 1.121602\n",
      "Train Epoch: 17 [3840/4982 (77%)]\tLoss: 1.107256\n",
      "Train Epoch: 17 [4480/4982 (90%)]\tLoss: 1.098665\n",
      "Training client 2\n",
      "Train Epoch: 1 [0/5150 (0%)]\tLoss: 1.499539\n",
      "Train Epoch: 1 [640/5150 (12%)]\tLoss: 1.304746\n",
      "Train Epoch: 1 [1280/5150 (25%)]\tLoss: 1.452565\n",
      "Train Epoch: 1 [1920/5150 (37%)]\tLoss: 1.393933\n",
      "Train Epoch: 1 [2560/5150 (49%)]\tLoss: 1.220930\n",
      "Train Epoch: 1 [3200/5150 (62%)]\tLoss: 1.267345\n",
      "Train Epoch: 1 [3840/5150 (74%)]\tLoss: 1.298835\n",
      "Train Epoch: 1 [4480/5150 (86%)]\tLoss: 1.476316\n",
      "Train Epoch: 1 [2400/5150 (99%)]\tLoss: 1.349550\n",
      "Train Epoch: 2 [0/5150 (0%)]\tLoss: 1.411885\n",
      "Train Epoch: 2 [640/5150 (12%)]\tLoss: 1.200203\n",
      "Train Epoch: 2 [1280/5150 (25%)]\tLoss: 1.279660\n",
      "Train Epoch: 2 [1920/5150 (37%)]\tLoss: 1.448661\n",
      "Train Epoch: 2 [2560/5150 (49%)]\tLoss: 1.364108\n",
      "Train Epoch: 2 [3200/5150 (62%)]\tLoss: 1.104628\n",
      "Train Epoch: 2 [3840/5150 (74%)]\tLoss: 1.215180\n",
      "Train Epoch: 2 [4480/5150 (86%)]\tLoss: 1.449073\n",
      "Train Epoch: 2 [2400/5150 (99%)]\tLoss: 1.217846\n",
      "Train Epoch: 3 [0/5150 (0%)]\tLoss: 1.090511\n",
      "Train Epoch: 3 [640/5150 (12%)]\tLoss: 1.076894\n",
      "Train Epoch: 3 [1280/5150 (25%)]\tLoss: 1.338664\n",
      "Train Epoch: 3 [1920/5150 (37%)]\tLoss: 0.983310\n",
      "Train Epoch: 3 [2560/5150 (49%)]\tLoss: 1.219232\n",
      "Train Epoch: 3 [3200/5150 (62%)]\tLoss: 1.341465\n",
      "Train Epoch: 3 [3840/5150 (74%)]\tLoss: 1.181513\n",
      "Train Epoch: 3 [4480/5150 (86%)]\tLoss: 1.283319\n",
      "Train Epoch: 3 [2400/5150 (99%)]\tLoss: 1.579188\n",
      "Train Epoch: 4 [0/5150 (0%)]\tLoss: 1.332103\n",
      "Train Epoch: 4 [640/5150 (12%)]\tLoss: 1.415130\n",
      "Train Epoch: 4 [1280/5150 (25%)]\tLoss: 1.215950\n",
      "Train Epoch: 4 [1920/5150 (37%)]\tLoss: 1.241108\n",
      "Train Epoch: 4 [2560/5150 (49%)]\tLoss: 1.117641\n",
      "Train Epoch: 4 [3200/5150 (62%)]\tLoss: 1.363778\n",
      "Train Epoch: 4 [3840/5150 (74%)]\tLoss: 1.278482\n",
      "Train Epoch: 4 [4480/5150 (86%)]\tLoss: 1.374533\n",
      "Train Epoch: 4 [2400/5150 (99%)]\tLoss: 1.369693\n",
      "Train Epoch: 5 [0/5150 (0%)]\tLoss: 1.095097\n",
      "Train Epoch: 5 [640/5150 (12%)]\tLoss: 1.183611\n",
      "Train Epoch: 5 [1280/5150 (25%)]\tLoss: 1.180321\n",
      "Train Epoch: 5 [1920/5150 (37%)]\tLoss: 1.196975\n",
      "Train Epoch: 5 [2560/5150 (49%)]\tLoss: 1.412625\n",
      "Train Epoch: 5 [3200/5150 (62%)]\tLoss: 1.392208\n",
      "Train Epoch: 5 [3840/5150 (74%)]\tLoss: 1.375548\n",
      "Train Epoch: 5 [4480/5150 (86%)]\tLoss: 1.142267\n",
      "Train Epoch: 5 [2400/5150 (99%)]\tLoss: 1.084104\n",
      "Train Epoch: 6 [0/5150 (0%)]\tLoss: 1.139029\n",
      "Train Epoch: 6 [640/5150 (12%)]\tLoss: 1.134098\n",
      "Train Epoch: 6 [1280/5150 (25%)]\tLoss: 1.404343\n",
      "Train Epoch: 6 [1920/5150 (37%)]\tLoss: 1.361818\n",
      "Train Epoch: 6 [2560/5150 (49%)]\tLoss: 1.248441\n",
      "Train Epoch: 6 [3200/5150 (62%)]\tLoss: 1.332951\n",
      "Train Epoch: 6 [3840/5150 (74%)]\tLoss: 1.221196\n",
      "Train Epoch: 6 [4480/5150 (86%)]\tLoss: 1.139444\n",
      "Train Epoch: 6 [2400/5150 (99%)]\tLoss: 1.613720\n",
      "Train Epoch: 7 [0/5150 (0%)]\tLoss: 1.132976\n",
      "Train Epoch: 7 [640/5150 (12%)]\tLoss: 1.120164\n",
      "Train Epoch: 7 [1280/5150 (25%)]\tLoss: 1.361628\n",
      "Train Epoch: 7 [1920/5150 (37%)]\tLoss: 1.439001\n",
      "Train Epoch: 7 [2560/5150 (49%)]\tLoss: 1.342990\n",
      "Train Epoch: 7 [3200/5150 (62%)]\tLoss: 1.376030\n",
      "Train Epoch: 7 [3840/5150 (74%)]\tLoss: 1.308933\n",
      "Train Epoch: 7 [4480/5150 (86%)]\tLoss: 1.347808\n",
      "Train Epoch: 7 [2400/5150 (99%)]\tLoss: 1.424882\n",
      "Train Epoch: 8 [0/5150 (0%)]\tLoss: 1.086873\n",
      "Train Epoch: 8 [640/5150 (12%)]\tLoss: 1.246463\n",
      "Train Epoch: 8 [1280/5150 (25%)]\tLoss: 1.157670\n",
      "Train Epoch: 8 [1920/5150 (37%)]\tLoss: 1.199663\n",
      "Train Epoch: 8 [2560/5150 (49%)]\tLoss: 1.317671\n",
      "Train Epoch: 8 [3200/5150 (62%)]\tLoss: 1.329065\n",
      "Train Epoch: 8 [3840/5150 (74%)]\tLoss: 1.156136\n",
      "Train Epoch: 8 [4480/5150 (86%)]\tLoss: 1.299680\n",
      "Train Epoch: 8 [2400/5150 (99%)]\tLoss: 1.643822\n",
      "Train Epoch: 9 [0/5150 (0%)]\tLoss: 1.272634\n",
      "Train Epoch: 9 [640/5150 (12%)]\tLoss: 1.440592\n",
      "Train Epoch: 9 [1280/5150 (25%)]\tLoss: 1.076700\n",
      "Train Epoch: 9 [1920/5150 (37%)]\tLoss: 1.188742\n",
      "Train Epoch: 9 [2560/5150 (49%)]\tLoss: 1.313706\n",
      "Train Epoch: 9 [3200/5150 (62%)]\tLoss: 1.097284\n",
      "Train Epoch: 9 [3840/5150 (74%)]\tLoss: 1.368659\n",
      "Train Epoch: 9 [4480/5150 (86%)]\tLoss: 1.312586\n",
      "Train Epoch: 9 [2400/5150 (99%)]\tLoss: 1.317721\n",
      "Train Epoch: 10 [0/5150 (0%)]\tLoss: 1.108078\n",
      "Train Epoch: 10 [640/5150 (12%)]\tLoss: 1.446409\n",
      "Train Epoch: 10 [1280/5150 (25%)]\tLoss: 1.081208\n",
      "Train Epoch: 10 [1920/5150 (37%)]\tLoss: 1.150807\n",
      "Train Epoch: 10 [2560/5150 (49%)]\tLoss: 1.080299\n",
      "Train Epoch: 10 [3200/5150 (62%)]\tLoss: 1.161966\n",
      "Train Epoch: 10 [3840/5150 (74%)]\tLoss: 1.060610\n",
      "Train Epoch: 10 [4480/5150 (86%)]\tLoss: 1.366408\n",
      "Train Epoch: 10 [2400/5150 (99%)]\tLoss: 1.525036\n",
      "Train Epoch: 11 [0/5150 (0%)]\tLoss: 1.295477\n",
      "Train Epoch: 11 [640/5150 (12%)]\tLoss: 1.155102\n",
      "Train Epoch: 11 [1280/5150 (25%)]\tLoss: 1.231446\n",
      "Train Epoch: 11 [1920/5150 (37%)]\tLoss: 1.118608\n",
      "Train Epoch: 11 [2560/5150 (49%)]\tLoss: 1.118690\n",
      "Train Epoch: 11 [3200/5150 (62%)]\tLoss: 1.116562\n",
      "Train Epoch: 11 [3840/5150 (74%)]\tLoss: 1.174084\n",
      "Train Epoch: 11 [4480/5150 (86%)]\tLoss: 1.092444\n",
      "Train Epoch: 11 [2400/5150 (99%)]\tLoss: 0.917828\n",
      "Train Epoch: 12 [0/5150 (0%)]\tLoss: 1.156253\n",
      "Train Epoch: 12 [640/5150 (12%)]\tLoss: 1.202636\n",
      "Train Epoch: 12 [1280/5150 (25%)]\tLoss: 1.410628\n",
      "Train Epoch: 12 [1920/5150 (37%)]\tLoss: 1.394999\n",
      "Train Epoch: 12 [2560/5150 (49%)]\tLoss: 1.279699\n",
      "Train Epoch: 12 [3200/5150 (62%)]\tLoss: 1.215280\n",
      "Train Epoch: 12 [3840/5150 (74%)]\tLoss: 1.056246\n",
      "Train Epoch: 12 [4480/5150 (86%)]\tLoss: 1.122601\n",
      "Train Epoch: 12 [2400/5150 (99%)]\tLoss: 0.982503\n",
      "Train Epoch: 13 [0/5150 (0%)]\tLoss: 1.232121\n",
      "Train Epoch: 13 [640/5150 (12%)]\tLoss: 1.469437\n",
      "Train Epoch: 13 [1280/5150 (25%)]\tLoss: 1.028633\n",
      "Train Epoch: 13 [1920/5150 (37%)]\tLoss: 1.264469\n",
      "Train Epoch: 13 [2560/5150 (49%)]\tLoss: 1.510948\n",
      "Train Epoch: 13 [3200/5150 (62%)]\tLoss: 1.258712\n",
      "Train Epoch: 13 [3840/5150 (74%)]\tLoss: 1.272982\n",
      "Train Epoch: 13 [4480/5150 (86%)]\tLoss: 1.122746\n",
      "Train Epoch: 13 [2400/5150 (99%)]\tLoss: 1.209508\n",
      "Train Epoch: 14 [0/5150 (0%)]\tLoss: 1.105540\n",
      "Train Epoch: 14 [640/5150 (12%)]\tLoss: 1.107869\n",
      "Train Epoch: 14 [1280/5150 (25%)]\tLoss: 1.397700\n",
      "Train Epoch: 14 [1920/5150 (37%)]\tLoss: 1.171285\n",
      "Train Epoch: 14 [2560/5150 (49%)]\tLoss: 1.253236\n",
      "Train Epoch: 14 [3200/5150 (62%)]\tLoss: 1.312314\n",
      "Train Epoch: 14 [3840/5150 (74%)]\tLoss: 1.039720\n",
      "Train Epoch: 14 [4480/5150 (86%)]\tLoss: 1.206757\n",
      "Train Epoch: 14 [2400/5150 (99%)]\tLoss: 0.915952\n",
      "Train Epoch: 15 [0/5150 (0%)]\tLoss: 1.300984\n",
      "Train Epoch: 15 [640/5150 (12%)]\tLoss: 1.077182\n",
      "Train Epoch: 15 [1280/5150 (25%)]\tLoss: 1.267936\n",
      "Train Epoch: 15 [1920/5150 (37%)]\tLoss: 1.442129\n",
      "Train Epoch: 15 [2560/5150 (49%)]\tLoss: 1.227009\n",
      "Train Epoch: 15 [3200/5150 (62%)]\tLoss: 1.345250\n",
      "Train Epoch: 15 [3840/5150 (74%)]\tLoss: 1.443825\n",
      "Train Epoch: 15 [4480/5150 (86%)]\tLoss: 1.054011\n",
      "Train Epoch: 15 [2400/5150 (99%)]\tLoss: 1.264110\n",
      "Train Epoch: 16 [0/5150 (0%)]\tLoss: 1.091679\n",
      "Train Epoch: 16 [640/5150 (12%)]\tLoss: 1.144448\n",
      "Train Epoch: 16 [1280/5150 (25%)]\tLoss: 1.062643\n",
      "Train Epoch: 16 [1920/5150 (37%)]\tLoss: 1.395717\n",
      "Train Epoch: 16 [2560/5150 (49%)]\tLoss: 1.091061\n",
      "Train Epoch: 16 [3200/5150 (62%)]\tLoss: 1.110617\n",
      "Train Epoch: 16 [3840/5150 (74%)]\tLoss: 1.091100\n",
      "Train Epoch: 16 [4480/5150 (86%)]\tLoss: 1.021103\n",
      "Train Epoch: 16 [2400/5150 (99%)]\tLoss: 1.369328\n",
      "Train Epoch: 17 [0/5150 (0%)]\tLoss: 1.027704\n",
      "Train Epoch: 17 [640/5150 (12%)]\tLoss: 1.251143\n",
      "Train Epoch: 17 [1280/5150 (25%)]\tLoss: 1.122886\n",
      "Train Epoch: 17 [1920/5150 (37%)]\tLoss: 1.107256\n",
      "Train Epoch: 17 [2560/5150 (49%)]\tLoss: 0.995335\n",
      "Train Epoch: 17 [3200/5150 (62%)]\tLoss: 1.210234\n",
      "Train Epoch: 17 [3840/5150 (74%)]\tLoss: 1.043867\n",
      "Train Epoch: 17 [4480/5150 (86%)]\tLoss: 1.048807\n",
      "Train Epoch: 17 [2400/5150 (99%)]\tLoss: 1.098700\n",
      "Training client 3\n",
      "Train Epoch: 1 [0/5542 (0%)]\tLoss: 1.401690\n",
      "Train Epoch: 1 [640/5542 (11%)]\tLoss: 1.495961\n",
      "Train Epoch: 1 [1280/5542 (23%)]\tLoss: 1.341406\n",
      "Train Epoch: 1 [1920/5542 (34%)]\tLoss: 1.243253\n",
      "Train Epoch: 1 [2560/5542 (46%)]\tLoss: 1.153117\n",
      "Train Epoch: 1 [3200/5542 (57%)]\tLoss: 1.607458\n",
      "Train Epoch: 1 [3840/5542 (69%)]\tLoss: 1.209234\n",
      "Train Epoch: 1 [4480/5542 (80%)]\tLoss: 1.200303\n",
      "Train Epoch: 1 [5120/5542 (92%)]\tLoss: 1.248229\n",
      "Train Epoch: 2 [0/5542 (0%)]\tLoss: 1.216786\n",
      "Train Epoch: 2 [640/5542 (11%)]\tLoss: 1.449606\n",
      "Train Epoch: 2 [1280/5542 (23%)]\tLoss: 1.363048\n",
      "Train Epoch: 2 [1920/5542 (34%)]\tLoss: 1.311651\n",
      "Train Epoch: 2 [2560/5542 (46%)]\tLoss: 1.385348\n",
      "Train Epoch: 2 [3200/5542 (57%)]\tLoss: 1.415984\n",
      "Train Epoch: 2 [3840/5542 (69%)]\tLoss: 1.297902\n",
      "Train Epoch: 2 [4480/5542 (80%)]\tLoss: 1.409158\n",
      "Train Epoch: 2 [5120/5542 (92%)]\tLoss: 1.157347\n",
      "Train Epoch: 3 [0/5542 (0%)]\tLoss: 1.420386\n",
      "Train Epoch: 3 [640/5542 (11%)]\tLoss: 1.253423\n",
      "Train Epoch: 3 [1280/5542 (23%)]\tLoss: 1.603781\n",
      "Train Epoch: 3 [1920/5542 (34%)]\tLoss: 1.208123\n",
      "Train Epoch: 3 [2560/5542 (46%)]\tLoss: 1.376730\n",
      "Train Epoch: 3 [3200/5542 (57%)]\tLoss: 1.155086\n",
      "Train Epoch: 3 [3840/5542 (69%)]\tLoss: 1.371124\n",
      "Train Epoch: 3 [4480/5542 (80%)]\tLoss: 1.451212\n",
      "Train Epoch: 3 [5120/5542 (92%)]\tLoss: 1.399617\n",
      "Train Epoch: 4 [0/5542 (0%)]\tLoss: 1.041491\n",
      "Train Epoch: 4 [640/5542 (11%)]\tLoss: 1.486940\n",
      "Train Epoch: 4 [1280/5542 (23%)]\tLoss: 1.558773\n",
      "Train Epoch: 4 [1920/5542 (34%)]\tLoss: 1.417150\n",
      "Train Epoch: 4 [2560/5542 (46%)]\tLoss: 1.261669\n",
      "Train Epoch: 4 [3200/5542 (57%)]\tLoss: 1.308594\n",
      "Train Epoch: 4 [3840/5542 (69%)]\tLoss: 1.382183\n",
      "Train Epoch: 4 [4480/5542 (80%)]\tLoss: 1.413847\n",
      "Train Epoch: 4 [5120/5542 (92%)]\tLoss: 1.025826\n",
      "Train Epoch: 5 [0/5542 (0%)]\tLoss: 1.327367\n",
      "Train Epoch: 5 [640/5542 (11%)]\tLoss: 1.315564\n",
      "Train Epoch: 5 [1280/5542 (23%)]\tLoss: 1.337106\n",
      "Train Epoch: 5 [1920/5542 (34%)]\tLoss: 1.231570\n",
      "Train Epoch: 5 [2560/5542 (46%)]\tLoss: 1.001698\n",
      "Train Epoch: 5 [3200/5542 (57%)]\tLoss: 1.223123\n",
      "Train Epoch: 5 [3840/5542 (69%)]\tLoss: 1.281612\n",
      "Train Epoch: 5 [4480/5542 (80%)]\tLoss: 1.520620\n",
      "Train Epoch: 5 [5120/5542 (92%)]\tLoss: 1.172888\n",
      "Train Epoch: 6 [0/5542 (0%)]\tLoss: 1.329904\n",
      "Train Epoch: 6 [640/5542 (11%)]\tLoss: 1.218257\n",
      "Train Epoch: 6 [1280/5542 (23%)]\tLoss: 1.333680\n",
      "Train Epoch: 6 [1920/5542 (34%)]\tLoss: 1.500862\n",
      "Train Epoch: 6 [2560/5542 (46%)]\tLoss: 1.269364\n",
      "Train Epoch: 6 [3200/5542 (57%)]\tLoss: 1.392190\n",
      "Train Epoch: 6 [3840/5542 (69%)]\tLoss: 1.165255\n",
      "Train Epoch: 6 [4480/5542 (80%)]\tLoss: 1.547346\n",
      "Train Epoch: 6 [5120/5542 (92%)]\tLoss: 1.200136\n",
      "Train Epoch: 7 [0/5542 (0%)]\tLoss: 1.116185\n",
      "Train Epoch: 7 [640/5542 (11%)]\tLoss: 1.375465\n",
      "Train Epoch: 7 [1280/5542 (23%)]\tLoss: 1.257383\n",
      "Train Epoch: 7 [1920/5542 (34%)]\tLoss: 1.355942\n",
      "Train Epoch: 7 [2560/5542 (46%)]\tLoss: 1.120758\n",
      "Train Epoch: 7 [3200/5542 (57%)]\tLoss: 0.925999\n",
      "Train Epoch: 7 [3840/5542 (69%)]\tLoss: 1.674035\n",
      "Train Epoch: 7 [4480/5542 (80%)]\tLoss: 1.474555\n",
      "Train Epoch: 7 [5120/5542 (92%)]\tLoss: 1.387116\n",
      "Train Epoch: 8 [0/5542 (0%)]\tLoss: 1.282999\n",
      "Train Epoch: 8 [640/5542 (11%)]\tLoss: 1.261244\n",
      "Train Epoch: 8 [1280/5542 (23%)]\tLoss: 1.311778\n",
      "Train Epoch: 8 [1920/5542 (34%)]\tLoss: 1.240452\n",
      "Train Epoch: 8 [2560/5542 (46%)]\tLoss: 1.260835\n",
      "Train Epoch: 8 [3200/5542 (57%)]\tLoss: 1.021851\n",
      "Train Epoch: 8 [3840/5542 (69%)]\tLoss: 1.231647\n",
      "Train Epoch: 8 [4480/5542 (80%)]\tLoss: 1.124913\n",
      "Train Epoch: 8 [5120/5542 (92%)]\tLoss: 1.411510\n",
      "Train Epoch: 9 [0/5542 (0%)]\tLoss: 1.511491\n",
      "Train Epoch: 9 [640/5542 (11%)]\tLoss: 1.241047\n",
      "Train Epoch: 9 [1280/5542 (23%)]\tLoss: 1.418047\n",
      "Train Epoch: 9 [1920/5542 (34%)]\tLoss: 1.419777\n",
      "Train Epoch: 9 [2560/5542 (46%)]\tLoss: 1.496680\n",
      "Train Epoch: 9 [3200/5542 (57%)]\tLoss: 1.288164\n",
      "Train Epoch: 9 [3840/5542 (69%)]\tLoss: 1.164365\n",
      "Train Epoch: 9 [4480/5542 (80%)]\tLoss: 1.432054\n",
      "Train Epoch: 9 [5120/5542 (92%)]\tLoss: 1.367042\n",
      "Train Epoch: 10 [0/5542 (0%)]\tLoss: 1.063664\n",
      "Train Epoch: 10 [640/5542 (11%)]\tLoss: 1.164271\n",
      "Train Epoch: 10 [1280/5542 (23%)]\tLoss: 1.037567\n",
      "Train Epoch: 10 [1920/5542 (34%)]\tLoss: 1.149677\n",
      "Train Epoch: 10 [2560/5542 (46%)]\tLoss: 1.331511\n",
      "Train Epoch: 10 [3200/5542 (57%)]\tLoss: 1.119220\n",
      "Train Epoch: 10 [3840/5542 (69%)]\tLoss: 1.367265\n",
      "Train Epoch: 10 [4480/5542 (80%)]\tLoss: 1.342291\n",
      "Train Epoch: 10 [5120/5542 (92%)]\tLoss: 1.344329\n",
      "Train Epoch: 11 [0/5542 (0%)]\tLoss: 1.352470\n",
      "Train Epoch: 11 [640/5542 (11%)]\tLoss: 1.168945\n",
      "Train Epoch: 11 [1280/5542 (23%)]\tLoss: 1.094663\n",
      "Train Epoch: 11 [1920/5542 (34%)]\tLoss: 1.274724\n",
      "Train Epoch: 11 [2560/5542 (46%)]\tLoss: 1.212267\n",
      "Train Epoch: 11 [3200/5542 (57%)]\tLoss: 1.197062\n",
      "Train Epoch: 11 [3840/5542 (69%)]\tLoss: 1.370740\n",
      "Train Epoch: 11 [4480/5542 (80%)]\tLoss: 1.337148\n",
      "Train Epoch: 11 [5120/5542 (92%)]\tLoss: 1.328007\n",
      "Train Epoch: 12 [0/5542 (0%)]\tLoss: 1.191536\n",
      "Train Epoch: 12 [640/5542 (11%)]\tLoss: 1.247857\n",
      "Train Epoch: 12 [1280/5542 (23%)]\tLoss: 1.308872\n",
      "Train Epoch: 12 [1920/5542 (34%)]\tLoss: 1.193784\n",
      "Train Epoch: 12 [2560/5542 (46%)]\tLoss: 1.382625\n",
      "Train Epoch: 12 [3200/5542 (57%)]\tLoss: 1.219571\n",
      "Train Epoch: 12 [3840/5542 (69%)]\tLoss: 1.410245\n",
      "Train Epoch: 12 [4480/5542 (80%)]\tLoss: 1.161797\n",
      "Train Epoch: 12 [5120/5542 (92%)]\tLoss: 1.069704\n",
      "Train Epoch: 13 [0/5542 (0%)]\tLoss: 0.947220\n",
      "Train Epoch: 13 [640/5542 (11%)]\tLoss: 1.401434\n",
      "Train Epoch: 13 [1280/5542 (23%)]\tLoss: 1.273126\n",
      "Train Epoch: 13 [1920/5542 (34%)]\tLoss: 1.400652\n",
      "Train Epoch: 13 [2560/5542 (46%)]\tLoss: 1.230396\n",
      "Train Epoch: 13 [3200/5542 (57%)]\tLoss: 1.090587\n",
      "Train Epoch: 13 [3840/5542 (69%)]\tLoss: 1.275870\n",
      "Train Epoch: 13 [4480/5542 (80%)]\tLoss: 1.244336\n",
      "Train Epoch: 13 [5120/5542 (92%)]\tLoss: 1.452353\n",
      "Train Epoch: 14 [0/5542 (0%)]\tLoss: 1.173655\n",
      "Train Epoch: 14 [640/5542 (11%)]\tLoss: 1.223510\n",
      "Train Epoch: 14 [1280/5542 (23%)]\tLoss: 1.141744\n",
      "Train Epoch: 14 [1920/5542 (34%)]\tLoss: 1.083439\n",
      "Train Epoch: 14 [2560/5542 (46%)]\tLoss: 1.322512\n",
      "Train Epoch: 14 [3200/5542 (57%)]\tLoss: 1.326080\n",
      "Train Epoch: 14 [3840/5542 (69%)]\tLoss: 1.268790\n",
      "Train Epoch: 14 [4480/5542 (80%)]\tLoss: 1.174014\n",
      "Train Epoch: 14 [5120/5542 (92%)]\tLoss: 1.224620\n",
      "Train Epoch: 15 [0/5542 (0%)]\tLoss: 1.180996\n",
      "Train Epoch: 15 [640/5542 (11%)]\tLoss: 1.423600\n",
      "Train Epoch: 15 [1280/5542 (23%)]\tLoss: 1.234861\n",
      "Train Epoch: 15 [1920/5542 (34%)]\tLoss: 1.319485\n",
      "Train Epoch: 15 [2560/5542 (46%)]\tLoss: 1.175953\n",
      "Train Epoch: 15 [3200/5542 (57%)]\tLoss: 1.358398\n",
      "Train Epoch: 15 [3840/5542 (69%)]\tLoss: 1.270441\n",
      "Train Epoch: 15 [4480/5542 (80%)]\tLoss: 1.044461\n",
      "Train Epoch: 15 [5120/5542 (92%)]\tLoss: 1.200789\n",
      "Train Epoch: 16 [0/5542 (0%)]\tLoss: 1.308803\n",
      "Train Epoch: 16 [640/5542 (11%)]\tLoss: 1.334519\n",
      "Train Epoch: 16 [1280/5542 (23%)]\tLoss: 1.227735\n",
      "Train Epoch: 16 [1920/5542 (34%)]\tLoss: 1.082271\n",
      "Train Epoch: 16 [2560/5542 (46%)]\tLoss: 1.262980\n",
      "Train Epoch: 16 [3200/5542 (57%)]\tLoss: 1.269008\n",
      "Train Epoch: 16 [3840/5542 (69%)]\tLoss: 1.313461\n",
      "Train Epoch: 16 [4480/5542 (80%)]\tLoss: 1.188543\n",
      "Train Epoch: 16 [5120/5542 (92%)]\tLoss: 1.166554\n",
      "Train Epoch: 17 [0/5542 (0%)]\tLoss: 1.234949\n",
      "Train Epoch: 17 [640/5542 (11%)]\tLoss: 1.214168\n",
      "Train Epoch: 17 [1280/5542 (23%)]\tLoss: 1.120415\n",
      "Train Epoch: 17 [1920/5542 (34%)]\tLoss: 1.203388\n",
      "Train Epoch: 17 [2560/5542 (46%)]\tLoss: 1.565951\n",
      "Train Epoch: 17 [3200/5542 (57%)]\tLoss: 1.112245\n",
      "Train Epoch: 17 [3840/5542 (69%)]\tLoss: 1.162191\n",
      "Train Epoch: 17 [4480/5542 (80%)]\tLoss: 1.506900\n",
      "Train Epoch: 17 [5120/5542 (92%)]\tLoss: 1.284602\n",
      "Training client 4\n",
      "Train Epoch: 1 [0/4979 (0%)]\tLoss: 1.261494\n",
      "Train Epoch: 1 [640/4979 (13%)]\tLoss: 1.136482\n",
      "Train Epoch: 1 [1280/4979 (26%)]\tLoss: 1.228977\n",
      "Train Epoch: 1 [1920/4979 (38%)]\tLoss: 1.226249\n",
      "Train Epoch: 1 [2560/4979 (51%)]\tLoss: 1.509639\n",
      "Train Epoch: 1 [3200/4979 (64%)]\tLoss: 1.417321\n",
      "Train Epoch: 1 [3840/4979 (77%)]\tLoss: 1.385439\n",
      "Train Epoch: 1 [4480/4979 (90%)]\tLoss: 1.610022\n",
      "Train Epoch: 2 [0/4979 (0%)]\tLoss: 1.187811\n",
      "Train Epoch: 2 [640/4979 (13%)]\tLoss: 1.129293\n",
      "Train Epoch: 2 [1280/4979 (26%)]\tLoss: 1.178860\n",
      "Train Epoch: 2 [1920/4979 (38%)]\tLoss: 1.072684\n",
      "Train Epoch: 2 [2560/4979 (51%)]\tLoss: 1.387899\n",
      "Train Epoch: 2 [3200/4979 (64%)]\tLoss: 1.246576\n",
      "Train Epoch: 2 [3840/4979 (77%)]\tLoss: 1.525446\n",
      "Train Epoch: 2 [4480/4979 (90%)]\tLoss: 1.297431\n",
      "Train Epoch: 3 [0/4979 (0%)]\tLoss: 1.459396\n",
      "Train Epoch: 3 [640/4979 (13%)]\tLoss: 1.198295\n",
      "Train Epoch: 3 [1280/4979 (26%)]\tLoss: 1.341857\n",
      "Train Epoch: 3 [1920/4979 (38%)]\tLoss: 1.101458\n",
      "Train Epoch: 3 [2560/4979 (51%)]\tLoss: 1.435404\n",
      "Train Epoch: 3 [3200/4979 (64%)]\tLoss: 1.257245\n",
      "Train Epoch: 3 [3840/4979 (77%)]\tLoss: 1.557350\n",
      "Train Epoch: 3 [4480/4979 (90%)]\tLoss: 1.417982\n",
      "Train Epoch: 4 [0/4979 (0%)]\tLoss: 1.722785\n",
      "Train Epoch: 4 [640/4979 (13%)]\tLoss: 1.123476\n",
      "Train Epoch: 4 [1280/4979 (26%)]\tLoss: 1.708811\n",
      "Train Epoch: 4 [1920/4979 (38%)]\tLoss: 1.187832\n",
      "Train Epoch: 4 [2560/4979 (51%)]\tLoss: 1.383688\n",
      "Train Epoch: 4 [3200/4979 (64%)]\tLoss: 1.223813\n",
      "Train Epoch: 4 [3840/4979 (77%)]\tLoss: 1.222875\n",
      "Train Epoch: 4 [4480/4979 (90%)]\tLoss: 1.229216\n",
      "Train Epoch: 5 [0/4979 (0%)]\tLoss: 1.343209\n",
      "Train Epoch: 5 [640/4979 (13%)]\tLoss: 1.288916\n",
      "Train Epoch: 5 [1280/4979 (26%)]\tLoss: 1.110729\n",
      "Train Epoch: 5 [1920/4979 (38%)]\tLoss: 1.562231\n",
      "Train Epoch: 5 [2560/4979 (51%)]\tLoss: 1.256137\n",
      "Train Epoch: 5 [3200/4979 (64%)]\tLoss: 1.562112\n",
      "Train Epoch: 5 [3840/4979 (77%)]\tLoss: 1.179972\n",
      "Train Epoch: 5 [4480/4979 (90%)]\tLoss: 1.135359\n",
      "Train Epoch: 6 [0/4979 (0%)]\tLoss: 1.270993\n",
      "Train Epoch: 6 [640/4979 (13%)]\tLoss: 1.399532\n",
      "Train Epoch: 6 [1280/4979 (26%)]\tLoss: 1.271011\n",
      "Train Epoch: 6 [1920/4979 (38%)]\tLoss: 1.179865\n",
      "Train Epoch: 6 [2560/4979 (51%)]\tLoss: 1.492019\n",
      "Train Epoch: 6 [3200/4979 (64%)]\tLoss: 1.239646\n",
      "Train Epoch: 6 [3840/4979 (77%)]\tLoss: 1.292737\n",
      "Train Epoch: 6 [4480/4979 (90%)]\tLoss: 1.423233\n",
      "Train Epoch: 7 [0/4979 (0%)]\tLoss: 1.244176\n",
      "Train Epoch: 7 [640/4979 (13%)]\tLoss: 1.303788\n",
      "Train Epoch: 7 [1280/4979 (26%)]\tLoss: 1.363391\n",
      "Train Epoch: 7 [1920/4979 (38%)]\tLoss: 1.255544\n",
      "Train Epoch: 7 [2560/4979 (51%)]\tLoss: 1.299742\n",
      "Train Epoch: 7 [3200/4979 (64%)]\tLoss: 1.429742\n",
      "Train Epoch: 7 [3840/4979 (77%)]\tLoss: 1.306240\n",
      "Train Epoch: 7 [4480/4979 (90%)]\tLoss: 1.292303\n",
      "Train Epoch: 8 [0/4979 (0%)]\tLoss: 1.102435\n",
      "Train Epoch: 8 [640/4979 (13%)]\tLoss: 1.149127\n",
      "Train Epoch: 8 [1280/4979 (26%)]\tLoss: 1.232298\n",
      "Train Epoch: 8 [1920/4979 (38%)]\tLoss: 1.258885\n",
      "Train Epoch: 8 [2560/4979 (51%)]\tLoss: 1.081664\n",
      "Train Epoch: 8 [3200/4979 (64%)]\tLoss: 1.249338\n",
      "Train Epoch: 8 [3840/4979 (77%)]\tLoss: 1.260113\n",
      "Train Epoch: 8 [4480/4979 (90%)]\tLoss: 1.225659\n",
      "Train Epoch: 9 [0/4979 (0%)]\tLoss: 1.267763\n",
      "Train Epoch: 9 [640/4979 (13%)]\tLoss: 1.560554\n",
      "Train Epoch: 9 [1280/4979 (26%)]\tLoss: 1.043431\n",
      "Train Epoch: 9 [1920/4979 (38%)]\tLoss: 1.149789\n",
      "Train Epoch: 9 [2560/4979 (51%)]\tLoss: 1.096093\n",
      "Train Epoch: 9 [3200/4979 (64%)]\tLoss: 1.409212\n",
      "Train Epoch: 9 [3840/4979 (77%)]\tLoss: 1.335395\n",
      "Train Epoch: 9 [4480/4979 (90%)]\tLoss: 1.146857\n",
      "Train Epoch: 10 [0/4979 (0%)]\tLoss: 1.118826\n",
      "Train Epoch: 10 [640/4979 (13%)]\tLoss: 1.268905\n",
      "Train Epoch: 10 [1280/4979 (26%)]\tLoss: 1.155348\n",
      "Train Epoch: 10 [1920/4979 (38%)]\tLoss: 1.229473\n",
      "Train Epoch: 10 [2560/4979 (51%)]\tLoss: 1.231825\n",
      "Train Epoch: 10 [3200/4979 (64%)]\tLoss: 1.483902\n",
      "Train Epoch: 10 [3840/4979 (77%)]\tLoss: 1.125745\n",
      "Train Epoch: 10 [4480/4979 (90%)]\tLoss: 1.265528\n",
      "Train Epoch: 11 [0/4979 (0%)]\tLoss: 1.108289\n",
      "Train Epoch: 11 [640/4979 (13%)]\tLoss: 1.168484\n",
      "Train Epoch: 11 [1280/4979 (26%)]\tLoss: 0.945322\n",
      "Train Epoch: 11 [1920/4979 (38%)]\tLoss: 1.144216\n",
      "Train Epoch: 11 [2560/4979 (51%)]\tLoss: 1.274601\n",
      "Train Epoch: 11 [3200/4979 (64%)]\tLoss: 1.378271\n",
      "Train Epoch: 11 [3840/4979 (77%)]\tLoss: 1.146100\n",
      "Train Epoch: 11 [4480/4979 (90%)]\tLoss: 1.248113\n",
      "Train Epoch: 12 [0/4979 (0%)]\tLoss: 1.422073\n",
      "Train Epoch: 12 [640/4979 (13%)]\tLoss: 1.064715\n",
      "Train Epoch: 12 [1280/4979 (26%)]\tLoss: 1.212846\n",
      "Train Epoch: 12 [1920/4979 (38%)]\tLoss: 1.518539\n",
      "Train Epoch: 12 [2560/4979 (51%)]\tLoss: 1.288559\n",
      "Train Epoch: 12 [3200/4979 (64%)]\tLoss: 1.030195\n",
      "Train Epoch: 12 [3840/4979 (77%)]\tLoss: 1.121117\n",
      "Train Epoch: 12 [4480/4979 (90%)]\tLoss: 1.160420\n",
      "Train Epoch: 13 [0/4979 (0%)]\tLoss: 1.382390\n",
      "Train Epoch: 13 [640/4979 (13%)]\tLoss: 1.140345\n",
      "Train Epoch: 13 [1280/4979 (26%)]\tLoss: 1.378288\n",
      "Train Epoch: 13 [1920/4979 (38%)]\tLoss: 1.205780\n",
      "Train Epoch: 13 [2560/4979 (51%)]\tLoss: 1.196093\n",
      "Train Epoch: 13 [3200/4979 (64%)]\tLoss: 1.276546\n",
      "Train Epoch: 13 [3840/4979 (77%)]\tLoss: 1.274041\n",
      "Train Epoch: 13 [4480/4979 (90%)]\tLoss: 1.230204\n",
      "Train Epoch: 14 [0/4979 (0%)]\tLoss: 1.223904\n",
      "Train Epoch: 14 [640/4979 (13%)]\tLoss: 1.289925\n",
      "Train Epoch: 14 [1280/4979 (26%)]\tLoss: 1.106282\n",
      "Train Epoch: 14 [1920/4979 (38%)]\tLoss: 1.152740\n",
      "Train Epoch: 14 [2560/4979 (51%)]\tLoss: 1.159838\n",
      "Train Epoch: 14 [3200/4979 (64%)]\tLoss: 1.278987\n",
      "Train Epoch: 14 [3840/4979 (77%)]\tLoss: 1.240047\n",
      "Train Epoch: 14 [4480/4979 (90%)]\tLoss: 1.431736\n",
      "Train Epoch: 15 [0/4979 (0%)]\tLoss: 1.407573\n",
      "Train Epoch: 15 [640/4979 (13%)]\tLoss: 1.063963\n",
      "Train Epoch: 15 [1280/4979 (26%)]\tLoss: 0.959200\n",
      "Train Epoch: 15 [1920/4979 (38%)]\tLoss: 1.093712\n",
      "Train Epoch: 15 [2560/4979 (51%)]\tLoss: 1.321445\n",
      "Train Epoch: 15 [3200/4979 (64%)]\tLoss: 1.073291\n",
      "Train Epoch: 15 [3840/4979 (77%)]\tLoss: 1.215459\n",
      "Train Epoch: 15 [4480/4979 (90%)]\tLoss: 1.495496\n",
      "Train Epoch: 16 [0/4979 (0%)]\tLoss: 1.104645\n",
      "Train Epoch: 16 [640/4979 (13%)]\tLoss: 1.389685\n",
      "Train Epoch: 16 [1280/4979 (26%)]\tLoss: 1.025306\n",
      "Train Epoch: 16 [1920/4979 (38%)]\tLoss: 0.988433\n",
      "Train Epoch: 16 [2560/4979 (51%)]\tLoss: 1.145695\n",
      "Train Epoch: 16 [3200/4979 (64%)]\tLoss: 1.063228\n",
      "Train Epoch: 16 [3840/4979 (77%)]\tLoss: 1.131637\n",
      "Train Epoch: 16 [4480/4979 (90%)]\tLoss: 1.012096\n",
      "Train Epoch: 17 [0/4979 (0%)]\tLoss: 1.194694\n",
      "Train Epoch: 17 [640/4979 (13%)]\tLoss: 1.053127\n",
      "Train Epoch: 17 [1280/4979 (26%)]\tLoss: 1.016299\n",
      "Train Epoch: 17 [1920/4979 (38%)]\tLoss: 1.221243\n",
      "Train Epoch: 17 [2560/4979 (51%)]\tLoss: 1.321068\n",
      "Train Epoch: 17 [3200/4979 (64%)]\tLoss: 1.331199\n",
      "Train Epoch: 17 [3840/4979 (77%)]\tLoss: 1.659582\n",
      "Train Epoch: 17 [4480/4979 (90%)]\tLoss: 1.095128\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/4033 (0%)]\tLoss: 1.386156\n",
      "Train Epoch: 1 [640/4033 (16%)]\tLoss: 1.225077\n",
      "Train Epoch: 1 [1280/4033 (31%)]\tLoss: 1.228176\n",
      "Train Epoch: 1 [1920/4033 (47%)]\tLoss: 1.411259\n",
      "Train Epoch: 1 [2560/4033 (62%)]\tLoss: 1.441523\n",
      "Train Epoch: 1 [3200/4033 (78%)]\tLoss: 0.967493\n",
      "Train Epoch: 1 [3840/4033 (94%)]\tLoss: 1.194356\n",
      "Train Epoch: 2 [0/4033 (0%)]\tLoss: 1.591456\n",
      "Train Epoch: 2 [640/4033 (16%)]\tLoss: 1.603253\n",
      "Train Epoch: 2 [1280/4033 (31%)]\tLoss: 1.674748\n",
      "Train Epoch: 2 [1920/4033 (47%)]\tLoss: 1.253535\n",
      "Train Epoch: 2 [2560/4033 (62%)]\tLoss: 1.169214\n",
      "Train Epoch: 2 [3200/4033 (78%)]\tLoss: 1.322886\n",
      "Train Epoch: 2 [3840/4033 (94%)]\tLoss: 1.475186\n",
      "Train Epoch: 3 [0/4033 (0%)]\tLoss: 1.480334\n",
      "Train Epoch: 3 [640/4033 (16%)]\tLoss: 1.296677\n",
      "Train Epoch: 3 [1280/4033 (31%)]\tLoss: 1.044558\n",
      "Train Epoch: 3 [1920/4033 (47%)]\tLoss: 1.282346\n",
      "Train Epoch: 3 [2560/4033 (62%)]\tLoss: 1.486669\n",
      "Train Epoch: 3 [3200/4033 (78%)]\tLoss: 1.488599\n",
      "Train Epoch: 3 [3840/4033 (94%)]\tLoss: 1.167348\n",
      "Train Epoch: 4 [0/4033 (0%)]\tLoss: 1.389885\n",
      "Train Epoch: 4 [640/4033 (16%)]\tLoss: 0.975069\n",
      "Train Epoch: 4 [1280/4033 (31%)]\tLoss: 1.398362\n",
      "Train Epoch: 4 [1920/4033 (47%)]\tLoss: 1.118830\n",
      "Train Epoch: 4 [2560/4033 (62%)]\tLoss: 1.500005\n",
      "Train Epoch: 4 [3200/4033 (78%)]\tLoss: 1.082385\n",
      "Train Epoch: 4 [3840/4033 (94%)]\tLoss: 1.274563\n",
      "Train Epoch: 5 [0/4033 (0%)]\tLoss: 1.205698\n",
      "Train Epoch: 5 [640/4033 (16%)]\tLoss: 1.316593\n",
      "Train Epoch: 5 [1280/4033 (31%)]\tLoss: 1.503901\n",
      "Train Epoch: 5 [1920/4033 (47%)]\tLoss: 1.291231\n",
      "Train Epoch: 5 [2560/4033 (62%)]\tLoss: 1.135444\n",
      "Train Epoch: 5 [3200/4033 (78%)]\tLoss: 1.328279\n",
      "Train Epoch: 5 [3840/4033 (94%)]\tLoss: 1.253290\n",
      "Train Epoch: 6 [0/4033 (0%)]\tLoss: 1.267058\n",
      "Train Epoch: 6 [640/4033 (16%)]\tLoss: 1.056789\n",
      "Train Epoch: 6 [1280/4033 (31%)]\tLoss: 1.376445\n",
      "Train Epoch: 6 [1920/4033 (47%)]\tLoss: 1.181665\n",
      "Train Epoch: 6 [2560/4033 (62%)]\tLoss: 1.514322\n",
      "Train Epoch: 6 [3200/4033 (78%)]\tLoss: 1.203698\n",
      "Train Epoch: 6 [3840/4033 (94%)]\tLoss: 1.169120\n",
      "Train Epoch: 7 [0/4033 (0%)]\tLoss: 1.229396\n",
      "Train Epoch: 7 [640/4033 (16%)]\tLoss: 1.343561\n",
      "Train Epoch: 7 [1280/4033 (31%)]\tLoss: 1.163422\n",
      "Train Epoch: 7 [1920/4033 (47%)]\tLoss: 1.185640\n",
      "Train Epoch: 7 [2560/4033 (62%)]\tLoss: 1.156347\n",
      "Train Epoch: 7 [3200/4033 (78%)]\tLoss: 1.302574\n",
      "Train Epoch: 7 [3840/4033 (94%)]\tLoss: 1.134548\n",
      "Train Epoch: 8 [0/4033 (0%)]\tLoss: 1.392238\n",
      "Train Epoch: 8 [640/4033 (16%)]\tLoss: 1.138203\n",
      "Train Epoch: 8 [1280/4033 (31%)]\tLoss: 1.094632\n",
      "Train Epoch: 8 [1920/4033 (47%)]\tLoss: 1.227760\n",
      "Train Epoch: 8 [2560/4033 (62%)]\tLoss: 1.276870\n",
      "Train Epoch: 8 [3200/4033 (78%)]\tLoss: 1.135232\n",
      "Train Epoch: 8 [3840/4033 (94%)]\tLoss: 1.264837\n",
      "Train Epoch: 9 [0/4033 (0%)]\tLoss: 2.936593\n",
      "Train Epoch: 9 [640/4033 (16%)]\tLoss: 1.394381\n",
      "Train Epoch: 9 [1280/4033 (31%)]\tLoss: 1.620211\n",
      "Train Epoch: 9 [1920/4033 (47%)]\tLoss: 1.307917\n",
      "Train Epoch: 9 [2560/4033 (62%)]\tLoss: 1.759250\n",
      "Train Epoch: 9 [3200/4033 (78%)]\tLoss: 1.432657\n",
      "Train Epoch: 9 [3840/4033 (94%)]\tLoss: 1.267149\n",
      "Train Epoch: 10 [0/4033 (0%)]\tLoss: 1.692725\n",
      "Train Epoch: 10 [640/4033 (16%)]\tLoss: 1.199836\n",
      "Train Epoch: 10 [1280/4033 (31%)]\tLoss: 1.425474\n",
      "Train Epoch: 10 [1920/4033 (47%)]\tLoss: 1.414599\n",
      "Train Epoch: 10 [2560/4033 (62%)]\tLoss: 1.350017\n",
      "Train Epoch: 10 [3200/4033 (78%)]\tLoss: 1.081455\n",
      "Train Epoch: 10 [3840/4033 (94%)]\tLoss: 1.337330\n",
      "Train Epoch: 11 [0/4033 (0%)]\tLoss: 1.232413\n",
      "Train Epoch: 11 [640/4033 (16%)]\tLoss: 1.252923\n",
      "Train Epoch: 11 [1280/4033 (31%)]\tLoss: 1.200537\n",
      "Train Epoch: 11 [1920/4033 (47%)]\tLoss: 1.658622\n",
      "Train Epoch: 11 [2560/4033 (62%)]\tLoss: 1.416645\n",
      "Train Epoch: 11 [3200/4033 (78%)]\tLoss: 1.194816\n",
      "Train Epoch: 11 [3840/4033 (94%)]\tLoss: 1.231592\n",
      "Train Epoch: 12 [0/4033 (0%)]\tLoss: 1.239986\n",
      "Train Epoch: 12 [640/4033 (16%)]\tLoss: 1.380512\n",
      "Train Epoch: 12 [1280/4033 (31%)]\tLoss: 1.304465\n",
      "Train Epoch: 12 [1920/4033 (47%)]\tLoss: 1.356590\n",
      "Train Epoch: 12 [2560/4033 (62%)]\tLoss: 1.330509\n",
      "Train Epoch: 12 [3200/4033 (78%)]\tLoss: 1.329610\n",
      "Train Epoch: 12 [3840/4033 (94%)]\tLoss: 1.203636\n",
      "Train Epoch: 13 [0/4033 (0%)]\tLoss: 1.228354\n",
      "Train Epoch: 13 [640/4033 (16%)]\tLoss: 1.256102\n",
      "Train Epoch: 13 [1280/4033 (31%)]\tLoss: 1.546320\n",
      "Train Epoch: 13 [1920/4033 (47%)]\tLoss: 1.487049\n",
      "Train Epoch: 13 [2560/4033 (62%)]\tLoss: 1.091392\n",
      "Train Epoch: 13 [3200/4033 (78%)]\tLoss: 1.150651\n",
      "Train Epoch: 13 [3840/4033 (94%)]\tLoss: 1.356147\n",
      "Train Epoch: 14 [0/4033 (0%)]\tLoss: 1.819505\n",
      "Train Epoch: 14 [640/4033 (16%)]\tLoss: 1.301794\n",
      "Train Epoch: 14 [1280/4033 (31%)]\tLoss: 1.238079\n",
      "Train Epoch: 14 [1920/4033 (47%)]\tLoss: 1.328797\n",
      "Train Epoch: 14 [2560/4033 (62%)]\tLoss: 1.523615\n",
      "Train Epoch: 14 [3200/4033 (78%)]\tLoss: 1.453368\n",
      "Train Epoch: 14 [3840/4033 (94%)]\tLoss: 1.328334\n",
      "Train Epoch: 15 [0/4033 (0%)]\tLoss: 1.327203\n",
      "Train Epoch: 15 [640/4033 (16%)]\tLoss: 1.219918\n",
      "Train Epoch: 15 [1280/4033 (31%)]\tLoss: 1.101690\n",
      "Train Epoch: 15 [1920/4033 (47%)]\tLoss: 1.333366\n",
      "Train Epoch: 15 [2560/4033 (62%)]\tLoss: 1.532874\n",
      "Train Epoch: 15 [3200/4033 (78%)]\tLoss: 1.083709\n",
      "Train Epoch: 15 [3840/4033 (94%)]\tLoss: 1.350682\n",
      "Train Epoch: 16 [0/4033 (0%)]\tLoss: 1.614018\n",
      "Train Epoch: 16 [640/4033 (16%)]\tLoss: 1.280876\n",
      "Train Epoch: 16 [1280/4033 (31%)]\tLoss: 1.163092\n",
      "Train Epoch: 16 [1920/4033 (47%)]\tLoss: 1.410260\n",
      "Train Epoch: 16 [2560/4033 (62%)]\tLoss: 1.254566\n",
      "Train Epoch: 16 [3200/4033 (78%)]\tLoss: 1.135442\n",
      "Train Epoch: 16 [3840/4033 (94%)]\tLoss: 1.146711\n",
      "Train Epoch: 17 [0/4033 (0%)]\tLoss: 1.635962\n",
      "Train Epoch: 17 [640/4033 (16%)]\tLoss: 1.432744\n",
      "Train Epoch: 17 [1280/4033 (31%)]\tLoss: 1.458315\n",
      "Train Epoch: 17 [1920/4033 (47%)]\tLoss: 1.418064\n",
      "Train Epoch: 17 [2560/4033 (62%)]\tLoss: 1.289605\n",
      "Train Epoch: 17 [3200/4033 (78%)]\tLoss: 1.449695\n",
      "Train Epoch: 17 [3840/4033 (94%)]\tLoss: 1.132695\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/3705 (0%)]\tLoss: 1.323482\n",
      "Train Epoch: 1 [640/3705 (17%)]\tLoss: 1.482039\n",
      "Train Epoch: 1 [1280/3705 (34%)]\tLoss: 1.327904\n",
      "Train Epoch: 1 [1920/3705 (52%)]\tLoss: 1.370565\n",
      "Train Epoch: 1 [2560/3705 (69%)]\tLoss: 1.398841\n",
      "Train Epoch: 1 [3200/3705 (86%)]\tLoss: 1.378319\n",
      "Train Epoch: 2 [0/3705 (0%)]\tLoss: 1.096197\n",
      "Train Epoch: 2 [640/3705 (17%)]\tLoss: 1.434874\n",
      "Train Epoch: 2 [1280/3705 (34%)]\tLoss: 1.350467\n",
      "Train Epoch: 2 [1920/3705 (52%)]\tLoss: 1.298732\n",
      "Train Epoch: 2 [2560/3705 (69%)]\tLoss: 1.377418\n",
      "Train Epoch: 2 [3200/3705 (86%)]\tLoss: 1.213762\n",
      "Train Epoch: 3 [0/3705 (0%)]\tLoss: 1.260594\n",
      "Train Epoch: 3 [640/3705 (17%)]\tLoss: 1.251978\n",
      "Train Epoch: 3 [1280/3705 (34%)]\tLoss: 1.215249\n",
      "Train Epoch: 3 [1920/3705 (52%)]\tLoss: 1.325262\n",
      "Train Epoch: 3 [2560/3705 (69%)]\tLoss: 1.348532\n",
      "Train Epoch: 3 [3200/3705 (86%)]\tLoss: 1.374625\n",
      "Train Epoch: 4 [0/3705 (0%)]\tLoss: 1.424433\n",
      "Train Epoch: 4 [640/3705 (17%)]\tLoss: 1.526998\n",
      "Train Epoch: 4 [1280/3705 (34%)]\tLoss: 1.319263\n",
      "Train Epoch: 4 [1920/3705 (52%)]\tLoss: 1.467949\n",
      "Train Epoch: 4 [2560/3705 (69%)]\tLoss: 1.276465\n",
      "Train Epoch: 4 [3200/3705 (86%)]\tLoss: 1.296160\n",
      "Train Epoch: 5 [0/3705 (0%)]\tLoss: 1.324116\n",
      "Train Epoch: 5 [640/3705 (17%)]\tLoss: 1.318068\n",
      "Train Epoch: 5 [1280/3705 (34%)]\tLoss: 1.449582\n",
      "Train Epoch: 5 [1920/3705 (52%)]\tLoss: 1.586480\n",
      "Train Epoch: 5 [2560/3705 (69%)]\tLoss: 1.212273\n",
      "Train Epoch: 5 [3200/3705 (86%)]\tLoss: 1.322339\n",
      "Train Epoch: 6 [0/3705 (0%)]\tLoss: 1.226878\n",
      "Train Epoch: 6 [640/3705 (17%)]\tLoss: 1.415383\n",
      "Train Epoch: 6 [1280/3705 (34%)]\tLoss: 1.080000\n",
      "Train Epoch: 6 [1920/3705 (52%)]\tLoss: 1.358971\n",
      "Train Epoch: 6 [2560/3705 (69%)]\tLoss: 1.308553\n",
      "Train Epoch: 6 [3200/3705 (86%)]\tLoss: 1.093177\n",
      "Train Epoch: 7 [0/3705 (0%)]\tLoss: 1.317890\n",
      "Train Epoch: 7 [640/3705 (17%)]\tLoss: 1.320446\n",
      "Train Epoch: 7 [1280/3705 (34%)]\tLoss: 1.182409\n",
      "Train Epoch: 7 [1920/3705 (52%)]\tLoss: 1.385957\n",
      "Train Epoch: 7 [2560/3705 (69%)]\tLoss: 1.061687\n",
      "Train Epoch: 7 [3200/3705 (86%)]\tLoss: 1.121286\n",
      "Train Epoch: 8 [0/3705 (0%)]\tLoss: 1.340902\n",
      "Train Epoch: 8 [640/3705 (17%)]\tLoss: 1.346749\n",
      "Train Epoch: 8 [1280/3705 (34%)]\tLoss: 1.333798\n",
      "Train Epoch: 8 [1920/3705 (52%)]\tLoss: 1.242892\n",
      "Train Epoch: 8 [2560/3705 (69%)]\tLoss: 1.357986\n",
      "Train Epoch: 8 [3200/3705 (86%)]\tLoss: 1.412885\n",
      "Train Epoch: 9 [0/3705 (0%)]\tLoss: 1.252723\n",
      "Train Epoch: 9 [640/3705 (17%)]\tLoss: 1.137358\n",
      "Train Epoch: 9 [1280/3705 (34%)]\tLoss: 1.003650\n",
      "Train Epoch: 9 [1920/3705 (52%)]\tLoss: 1.120825\n",
      "Train Epoch: 9 [2560/3705 (69%)]\tLoss: 1.457879\n",
      "Train Epoch: 9 [3200/3705 (86%)]\tLoss: 1.262040\n",
      "Train Epoch: 10 [0/3705 (0%)]\tLoss: 1.189358\n",
      "Train Epoch: 10 [640/3705 (17%)]\tLoss: 1.086659\n",
      "Train Epoch: 10 [1280/3705 (34%)]\tLoss: 1.272634\n",
      "Train Epoch: 10 [1920/3705 (52%)]\tLoss: 1.128009\n",
      "Train Epoch: 10 [2560/3705 (69%)]\tLoss: 1.228580\n",
      "Train Epoch: 10 [3200/3705 (86%)]\tLoss: 1.202811\n",
      "Train Epoch: 11 [0/3705 (0%)]\tLoss: 1.215825\n",
      "Train Epoch: 11 [640/3705 (17%)]\tLoss: 1.021543\n",
      "Train Epoch: 11 [1280/3705 (34%)]\tLoss: 1.111993\n",
      "Train Epoch: 11 [1920/3705 (52%)]\tLoss: 1.352981\n",
      "Train Epoch: 11 [2560/3705 (69%)]\tLoss: 1.154088\n",
      "Train Epoch: 11 [3200/3705 (86%)]\tLoss: 1.282141\n",
      "Train Epoch: 12 [0/3705 (0%)]\tLoss: 1.214410\n",
      "Train Epoch: 12 [640/3705 (17%)]\tLoss: 1.126371\n",
      "Train Epoch: 12 [1280/3705 (34%)]\tLoss: 1.605676\n",
      "Train Epoch: 12 [1920/3705 (52%)]\tLoss: 1.003061\n",
      "Train Epoch: 12 [2560/3705 (69%)]\tLoss: 1.383463\n",
      "Train Epoch: 12 [3200/3705 (86%)]\tLoss: 1.121372\n",
      "Train Epoch: 13 [0/3705 (0%)]\tLoss: 1.328416\n",
      "Train Epoch: 13 [640/3705 (17%)]\tLoss: 1.112769\n",
      "Train Epoch: 13 [1280/3705 (34%)]\tLoss: 1.219727\n",
      "Train Epoch: 13 [1920/3705 (52%)]\tLoss: 1.118598\n",
      "Train Epoch: 13 [2560/3705 (69%)]\tLoss: 1.189787\n",
      "Train Epoch: 13 [3200/3705 (86%)]\tLoss: 1.265250\n",
      "Train Epoch: 14 [0/3705 (0%)]\tLoss: 1.320406\n",
      "Train Epoch: 14 [640/3705 (17%)]\tLoss: 1.133949\n",
      "Train Epoch: 14 [1280/3705 (34%)]\tLoss: 1.369832\n",
      "Train Epoch: 14 [1920/3705 (52%)]\tLoss: 1.179633\n",
      "Train Epoch: 14 [2560/3705 (69%)]\tLoss: 1.392280\n",
      "Train Epoch: 14 [3200/3705 (86%)]\tLoss: 1.291834\n",
      "Train Epoch: 15 [0/3705 (0%)]\tLoss: 1.159468\n",
      "Train Epoch: 15 [640/3705 (17%)]\tLoss: 1.326579\n",
      "Train Epoch: 15 [1280/3705 (34%)]\tLoss: 1.234304\n",
      "Train Epoch: 15 [1920/3705 (52%)]\tLoss: 1.208303\n",
      "Train Epoch: 15 [2560/3705 (69%)]\tLoss: 1.340716\n",
      "Train Epoch: 15 [3200/3705 (86%)]\tLoss: 1.172748\n",
      "Train Epoch: 16 [0/3705 (0%)]\tLoss: 1.239351\n",
      "Train Epoch: 16 [640/3705 (17%)]\tLoss: 1.206830\n",
      "Train Epoch: 16 [1280/3705 (34%)]\tLoss: 0.997823\n",
      "Train Epoch: 16 [1920/3705 (52%)]\tLoss: 1.215629\n",
      "Train Epoch: 16 [2560/3705 (69%)]\tLoss: 1.086223\n",
      "Train Epoch: 16 [3200/3705 (86%)]\tLoss: 1.581571\n",
      "Train Epoch: 17 [0/3705 (0%)]\tLoss: 1.210815\n",
      "Train Epoch: 17 [640/3705 (17%)]\tLoss: 1.258244\n",
      "Train Epoch: 17 [1280/3705 (34%)]\tLoss: 1.228123\n",
      "Train Epoch: 17 [1920/3705 (52%)]\tLoss: 1.369606\n",
      "Train Epoch: 17 [2560/3705 (69%)]\tLoss: 1.241209\n",
      "Train Epoch: 17 [3200/3705 (86%)]\tLoss: 1.271730\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/5759 (0%)]\tLoss: 1.580434\n",
      "Train Epoch: 1 [640/5759 (11%)]\tLoss: 1.175920\n",
      "Train Epoch: 1 [1280/5759 (22%)]\tLoss: 1.607841\n",
      "Train Epoch: 1 [1920/5759 (33%)]\tLoss: 1.533139\n",
      "Train Epoch: 1 [2560/5759 (44%)]\tLoss: 1.274535\n",
      "Train Epoch: 1 [3200/5759 (56%)]\tLoss: 1.458270\n",
      "Train Epoch: 1 [3840/5759 (67%)]\tLoss: 1.323471\n",
      "Train Epoch: 1 [4480/5759 (78%)]\tLoss: 1.419377\n",
      "Train Epoch: 1 [5120/5759 (89%)]\tLoss: 1.280712\n",
      "Train Epoch: 2 [0/5759 (0%)]\tLoss: 1.233616\n",
      "Train Epoch: 2 [640/5759 (11%)]\tLoss: 1.106266\n",
      "Train Epoch: 2 [1280/5759 (22%)]\tLoss: 1.062214\n",
      "Train Epoch: 2 [1920/5759 (33%)]\tLoss: 1.580530\n",
      "Train Epoch: 2 [2560/5759 (44%)]\tLoss: 1.406786\n",
      "Train Epoch: 2 [3200/5759 (56%)]\tLoss: 1.454098\n",
      "Train Epoch: 2 [3840/5759 (67%)]\tLoss: 1.361557\n",
      "Train Epoch: 2 [4480/5759 (78%)]\tLoss: 1.464079\n",
      "Train Epoch: 2 [5120/5759 (89%)]\tLoss: 1.519825\n",
      "Train Epoch: 3 [0/5759 (0%)]\tLoss: 1.451456\n",
      "Train Epoch: 3 [640/5759 (11%)]\tLoss: 1.212235\n",
      "Train Epoch: 3 [1280/5759 (22%)]\tLoss: 1.442695\n",
      "Train Epoch: 3 [1920/5759 (33%)]\tLoss: 1.316103\n",
      "Train Epoch: 3 [2560/5759 (44%)]\tLoss: 1.453820\n",
      "Train Epoch: 3 [3200/5759 (56%)]\tLoss: 1.472697\n",
      "Train Epoch: 3 [3840/5759 (67%)]\tLoss: 1.648355\n",
      "Train Epoch: 3 [4480/5759 (78%)]\tLoss: 1.578370\n",
      "Train Epoch: 3 [5120/5759 (89%)]\tLoss: 1.334816\n",
      "Train Epoch: 4 [0/5759 (0%)]\tLoss: 1.204360\n",
      "Train Epoch: 4 [640/5759 (11%)]\tLoss: 1.157663\n",
      "Train Epoch: 4 [1280/5759 (22%)]\tLoss: 1.327390\n",
      "Train Epoch: 4 [1920/5759 (33%)]\tLoss: 1.394104\n",
      "Train Epoch: 4 [2560/5759 (44%)]\tLoss: 1.435342\n",
      "Train Epoch: 4 [3200/5759 (56%)]\tLoss: 1.248247\n",
      "Train Epoch: 4 [3840/5759 (67%)]\tLoss: 1.556638\n",
      "Train Epoch: 4 [4480/5759 (78%)]\tLoss: 1.699773\n",
      "Train Epoch: 4 [5120/5759 (89%)]\tLoss: 1.533044\n",
      "Train Epoch: 5 [0/5759 (0%)]\tLoss: 1.356727\n",
      "Train Epoch: 5 [640/5759 (11%)]\tLoss: 1.366528\n",
      "Train Epoch: 5 [1280/5759 (22%)]\tLoss: 1.338596\n",
      "Train Epoch: 5 [1920/5759 (33%)]\tLoss: 1.408549\n",
      "Train Epoch: 5 [2560/5759 (44%)]\tLoss: 1.327969\n",
      "Train Epoch: 5 [3200/5759 (56%)]\tLoss: 1.401424\n",
      "Train Epoch: 5 [3840/5759 (67%)]\tLoss: 1.156449\n",
      "Train Epoch: 5 [4480/5759 (78%)]\tLoss: 1.115190\n",
      "Train Epoch: 5 [5120/5759 (89%)]\tLoss: 1.352492\n",
      "Train Epoch: 6 [0/5759 (0%)]\tLoss: 1.413826\n",
      "Train Epoch: 6 [640/5759 (11%)]\tLoss: 1.340606\n",
      "Train Epoch: 6 [1280/5759 (22%)]\tLoss: 1.369636\n",
      "Train Epoch: 6 [1920/5759 (33%)]\tLoss: 1.280217\n",
      "Train Epoch: 6 [2560/5759 (44%)]\tLoss: 1.506046\n",
      "Train Epoch: 6 [3200/5759 (56%)]\tLoss: 1.270744\n",
      "Train Epoch: 6 [3840/5759 (67%)]\tLoss: 1.356377\n",
      "Train Epoch: 6 [4480/5759 (78%)]\tLoss: 1.620233\n",
      "Train Epoch: 6 [5120/5759 (89%)]\tLoss: 1.382611\n",
      "Train Epoch: 7 [0/5759 (0%)]\tLoss: 1.410456\n",
      "Train Epoch: 7 [640/5759 (11%)]\tLoss: 1.287324\n",
      "Train Epoch: 7 [1280/5759 (22%)]\tLoss: 1.309033\n",
      "Train Epoch: 7 [1920/5759 (33%)]\tLoss: 1.292443\n",
      "Train Epoch: 7 [2560/5759 (44%)]\tLoss: 1.355746\n",
      "Train Epoch: 7 [3200/5759 (56%)]\tLoss: 1.389753\n",
      "Train Epoch: 7 [3840/5759 (67%)]\tLoss: 1.322078\n",
      "Train Epoch: 7 [4480/5759 (78%)]\tLoss: 1.274277\n",
      "Train Epoch: 7 [5120/5759 (89%)]\tLoss: 1.401334\n",
      "Train Epoch: 8 [0/5759 (0%)]\tLoss: 1.327266\n",
      "Train Epoch: 8 [640/5759 (11%)]\tLoss: 1.305026\n",
      "Train Epoch: 8 [1280/5759 (22%)]\tLoss: 1.297350\n",
      "Train Epoch: 8 [1920/5759 (33%)]\tLoss: 1.199522\n",
      "Train Epoch: 8 [2560/5759 (44%)]\tLoss: 1.528513\n",
      "Train Epoch: 8 [3200/5759 (56%)]\tLoss: 1.113543\n",
      "Train Epoch: 8 [3840/5759 (67%)]\tLoss: 1.152238\n",
      "Train Epoch: 8 [4480/5759 (78%)]\tLoss: 1.265476\n",
      "Train Epoch: 8 [5120/5759 (89%)]\tLoss: 1.304492\n",
      "Train Epoch: 9 [0/5759 (0%)]\tLoss: 1.265475\n",
      "Train Epoch: 9 [640/5759 (11%)]\tLoss: 1.261301\n",
      "Train Epoch: 9 [1280/5759 (22%)]\tLoss: 1.170519\n",
      "Train Epoch: 9 [1920/5759 (33%)]\tLoss: 1.320658\n",
      "Train Epoch: 9 [2560/5759 (44%)]\tLoss: 1.168884\n",
      "Train Epoch: 9 [3200/5759 (56%)]\tLoss: 1.290782\n",
      "Train Epoch: 9 [3840/5759 (67%)]\tLoss: 1.433381\n",
      "Train Epoch: 9 [4480/5759 (78%)]\tLoss: 1.227474\n",
      "Train Epoch: 9 [5120/5759 (89%)]\tLoss: 1.476118\n",
      "Train Epoch: 10 [0/5759 (0%)]\tLoss: 1.260930\n",
      "Train Epoch: 10 [640/5759 (11%)]\tLoss: 1.251813\n",
      "Train Epoch: 10 [1280/5759 (22%)]\tLoss: 1.278631\n",
      "Train Epoch: 10 [1920/5759 (33%)]\tLoss: 1.412058\n",
      "Train Epoch: 10 [2560/5759 (44%)]\tLoss: 1.069337\n",
      "Train Epoch: 10 [3200/5759 (56%)]\tLoss: 1.255955\n",
      "Train Epoch: 10 [3840/5759 (67%)]\tLoss: 1.359383\n",
      "Train Epoch: 10 [4480/5759 (78%)]\tLoss: 1.657138\n",
      "Train Epoch: 10 [5120/5759 (89%)]\tLoss: 1.131919\n",
      "Train Epoch: 11 [0/5759 (0%)]\tLoss: 1.252558\n",
      "Train Epoch: 11 [640/5759 (11%)]\tLoss: 1.341959\n",
      "Train Epoch: 11 [1280/5759 (22%)]\tLoss: 1.539804\n",
      "Train Epoch: 11 [1920/5759 (33%)]\tLoss: 1.343918\n",
      "Train Epoch: 11 [2560/5759 (44%)]\tLoss: 1.460408\n",
      "Train Epoch: 11 [3200/5759 (56%)]\tLoss: 1.244853\n",
      "Train Epoch: 11 [3840/5759 (67%)]\tLoss: 1.454037\n",
      "Train Epoch: 11 [4480/5759 (78%)]\tLoss: 1.099952\n",
      "Train Epoch: 11 [5120/5759 (89%)]\tLoss: 1.329587\n",
      "Train Epoch: 12 [0/5759 (0%)]\tLoss: 1.522993\n",
      "Train Epoch: 12 [640/5759 (11%)]\tLoss: 1.302131\n",
      "Train Epoch: 12 [1280/5759 (22%)]\tLoss: 1.365831\n",
      "Train Epoch: 12 [1920/5759 (33%)]\tLoss: 1.434675\n",
      "Train Epoch: 12 [2560/5759 (44%)]\tLoss: 1.226077\n",
      "Train Epoch: 12 [3200/5759 (56%)]\tLoss: 1.280586\n",
      "Train Epoch: 12 [3840/5759 (67%)]\tLoss: 1.194051\n",
      "Train Epoch: 12 [4480/5759 (78%)]\tLoss: 1.334302\n",
      "Train Epoch: 12 [5120/5759 (89%)]\tLoss: 1.212479\n",
      "Train Epoch: 13 [0/5759 (0%)]\tLoss: 1.221609\n",
      "Train Epoch: 13 [640/5759 (11%)]\tLoss: 1.325136\n",
      "Train Epoch: 13 [1280/5759 (22%)]\tLoss: 1.367047\n",
      "Train Epoch: 13 [1920/5759 (33%)]\tLoss: 0.884411\n",
      "Train Epoch: 13 [2560/5759 (44%)]\tLoss: 1.674229\n",
      "Train Epoch: 13 [3200/5759 (56%)]\tLoss: 1.297170\n",
      "Train Epoch: 13 [3840/5759 (67%)]\tLoss: 1.184792\n",
      "Train Epoch: 13 [4480/5759 (78%)]\tLoss: 1.494019\n",
      "Train Epoch: 13 [5120/5759 (89%)]\tLoss: 1.308858\n",
      "Train Epoch: 14 [0/5759 (0%)]\tLoss: 1.065945\n",
      "Train Epoch: 14 [640/5759 (11%)]\tLoss: 1.207378\n",
      "Train Epoch: 14 [1280/5759 (22%)]\tLoss: 1.415352\n",
      "Train Epoch: 14 [1920/5759 (33%)]\tLoss: 1.591326\n",
      "Train Epoch: 14 [2560/5759 (44%)]\tLoss: 1.185193\n",
      "Train Epoch: 14 [3200/5759 (56%)]\tLoss: 1.277955\n",
      "Train Epoch: 14 [3840/5759 (67%)]\tLoss: 1.075624\n",
      "Train Epoch: 14 [4480/5759 (78%)]\tLoss: 1.266619\n",
      "Train Epoch: 14 [5120/5759 (89%)]\tLoss: 1.136814\n",
      "Train Epoch: 15 [0/5759 (0%)]\tLoss: 1.276809\n",
      "Train Epoch: 15 [640/5759 (11%)]\tLoss: 1.298171\n",
      "Train Epoch: 15 [1280/5759 (22%)]\tLoss: 1.143027\n",
      "Train Epoch: 15 [1920/5759 (33%)]\tLoss: 1.260794\n",
      "Train Epoch: 15 [2560/5759 (44%)]\tLoss: 1.158154\n",
      "Train Epoch: 15 [3200/5759 (56%)]\tLoss: 1.271088\n",
      "Train Epoch: 15 [3840/5759 (67%)]\tLoss: 1.143195\n",
      "Train Epoch: 15 [4480/5759 (78%)]\tLoss: 1.415873\n",
      "Train Epoch: 15 [5120/5759 (89%)]\tLoss: 1.240118\n",
      "Train Epoch: 16 [0/5759 (0%)]\tLoss: 1.055421\n",
      "Train Epoch: 16 [640/5759 (11%)]\tLoss: 1.254944\n",
      "Train Epoch: 16 [1280/5759 (22%)]\tLoss: 1.242325\n",
      "Train Epoch: 16 [1920/5759 (33%)]\tLoss: 1.346089\n",
      "Train Epoch: 16 [2560/5759 (44%)]\tLoss: 1.098210\n",
      "Train Epoch: 16 [3200/5759 (56%)]\tLoss: 1.224975\n",
      "Train Epoch: 16 [3840/5759 (67%)]\tLoss: 1.413612\n",
      "Train Epoch: 16 [4480/5759 (78%)]\tLoss: 1.358885\n",
      "Train Epoch: 16 [5120/5759 (89%)]\tLoss: 1.208203\n",
      "Train Epoch: 17 [0/5759 (0%)]\tLoss: 1.322299\n",
      "Train Epoch: 17 [640/5759 (11%)]\tLoss: 1.099480\n",
      "Train Epoch: 17 [1280/5759 (22%)]\tLoss: 1.227306\n",
      "Train Epoch: 17 [1920/5759 (33%)]\tLoss: 1.074188\n",
      "Train Epoch: 17 [2560/5759 (44%)]\tLoss: 1.195069\n",
      "Train Epoch: 17 [3200/5759 (56%)]\tLoss: 1.229471\n",
      "Train Epoch: 17 [3840/5759 (67%)]\tLoss: 1.298793\n",
      "Train Epoch: 17 [4480/5759 (78%)]\tLoss: 1.257053\n",
      "Train Epoch: 17 [5120/5759 (89%)]\tLoss: 1.306027\n",
      "Training client 8\n",
      "Train Epoch: 1 [0/4328 (0%)]\tLoss: 1.393565\n",
      "Train Epoch: 1 [640/4328 (15%)]\tLoss: 1.559331\n",
      "Train Epoch: 1 [1280/4328 (29%)]\tLoss: 1.702548\n",
      "Train Epoch: 1 [1920/4328 (44%)]\tLoss: 1.523112\n",
      "Train Epoch: 1 [2560/4328 (59%)]\tLoss: 1.328077\n",
      "Train Epoch: 1 [3200/4328 (74%)]\tLoss: 1.246132\n",
      "Train Epoch: 1 [3840/4328 (88%)]\tLoss: 1.330559\n",
      "Train Epoch: 2 [0/4328 (0%)]\tLoss: 1.444024\n",
      "Train Epoch: 2 [640/4328 (15%)]\tLoss: 1.253649\n",
      "Train Epoch: 2 [1280/4328 (29%)]\tLoss: 1.279439\n",
      "Train Epoch: 2 [1920/4328 (44%)]\tLoss: 1.285783\n",
      "Train Epoch: 2 [2560/4328 (59%)]\tLoss: 1.006113\n",
      "Train Epoch: 2 [3200/4328 (74%)]\tLoss: 1.464229\n",
      "Train Epoch: 2 [3840/4328 (88%)]\tLoss: 1.190538\n",
      "Train Epoch: 3 [0/4328 (0%)]\tLoss: 1.296974\n",
      "Train Epoch: 3 [640/4328 (15%)]\tLoss: 1.292171\n",
      "Train Epoch: 3 [1280/4328 (29%)]\tLoss: 1.222754\n",
      "Train Epoch: 3 [1920/4328 (44%)]\tLoss: 1.209396\n",
      "Train Epoch: 3 [2560/4328 (59%)]\tLoss: 1.455058\n",
      "Train Epoch: 3 [3200/4328 (74%)]\tLoss: 1.402622\n",
      "Train Epoch: 3 [3840/4328 (88%)]\tLoss: 1.347273\n",
      "Train Epoch: 4 [0/4328 (0%)]\tLoss: 1.172468\n",
      "Train Epoch: 4 [640/4328 (15%)]\tLoss: 1.400453\n",
      "Train Epoch: 4 [1280/4328 (29%)]\tLoss: 1.243902\n",
      "Train Epoch: 4 [1920/4328 (44%)]\tLoss: 1.363228\n",
      "Train Epoch: 4 [2560/4328 (59%)]\tLoss: 1.324787\n",
      "Train Epoch: 4 [3200/4328 (74%)]\tLoss: 1.266034\n",
      "Train Epoch: 4 [3840/4328 (88%)]\tLoss: 1.372261\n",
      "Train Epoch: 5 [0/4328 (0%)]\tLoss: 1.402631\n",
      "Train Epoch: 5 [640/4328 (15%)]\tLoss: 1.438751\n",
      "Train Epoch: 5 [1280/4328 (29%)]\tLoss: 1.353048\n",
      "Train Epoch: 5 [1920/4328 (44%)]\tLoss: 1.181317\n",
      "Train Epoch: 5 [2560/4328 (59%)]\tLoss: 1.483287\n",
      "Train Epoch: 5 [3200/4328 (74%)]\tLoss: 1.364685\n",
      "Train Epoch: 5 [3840/4328 (88%)]\tLoss: 1.566622\n",
      "Train Epoch: 6 [0/4328 (0%)]\tLoss: 1.510814\n",
      "Train Epoch: 6 [640/4328 (15%)]\tLoss: 1.543684\n",
      "Train Epoch: 6 [1280/4328 (29%)]\tLoss: 1.222354\n",
      "Train Epoch: 6 [1920/4328 (44%)]\tLoss: 1.316143\n",
      "Train Epoch: 6 [2560/4328 (59%)]\tLoss: 1.129058\n",
      "Train Epoch: 6 [3200/4328 (74%)]\tLoss: 1.635066\n",
      "Train Epoch: 6 [3840/4328 (88%)]\tLoss: 1.060759\n",
      "Train Epoch: 7 [0/4328 (0%)]\tLoss: 1.492269\n",
      "Train Epoch: 7 [640/4328 (15%)]\tLoss: 1.323338\n",
      "Train Epoch: 7 [1280/4328 (29%)]\tLoss: 1.137719\n",
      "Train Epoch: 7 [1920/4328 (44%)]\tLoss: 1.463646\n",
      "Train Epoch: 7 [2560/4328 (59%)]\tLoss: 1.174473\n",
      "Train Epoch: 7 [3200/4328 (74%)]\tLoss: 1.268182\n",
      "Train Epoch: 7 [3840/4328 (88%)]\tLoss: 1.523694\n",
      "Train Epoch: 8 [0/4328 (0%)]\tLoss: 1.267299\n",
      "Train Epoch: 8 [640/4328 (15%)]\tLoss: 1.343515\n",
      "Train Epoch: 8 [1280/4328 (29%)]\tLoss: 1.330495\n",
      "Train Epoch: 8 [1920/4328 (44%)]\tLoss: 1.414288\n",
      "Train Epoch: 8 [2560/4328 (59%)]\tLoss: 1.507955\n",
      "Train Epoch: 8 [3200/4328 (74%)]\tLoss: 1.295518\n",
      "Train Epoch: 8 [3840/4328 (88%)]\tLoss: 1.371840\n",
      "Train Epoch: 9 [0/4328 (0%)]\tLoss: 1.405984\n",
      "Train Epoch: 9 [640/4328 (15%)]\tLoss: 1.144524\n",
      "Train Epoch: 9 [1280/4328 (29%)]\tLoss: 1.306657\n",
      "Train Epoch: 9 [1920/4328 (44%)]\tLoss: 1.250809\n",
      "Train Epoch: 9 [2560/4328 (59%)]\tLoss: 1.407225\n",
      "Train Epoch: 9 [3200/4328 (74%)]\tLoss: 1.630409\n",
      "Train Epoch: 9 [3840/4328 (88%)]\tLoss: 1.424597\n",
      "Train Epoch: 10 [0/4328 (0%)]\tLoss: 1.340717\n",
      "Train Epoch: 10 [640/4328 (15%)]\tLoss: 1.170889\n",
      "Train Epoch: 10 [1280/4328 (29%)]\tLoss: 1.297112\n",
      "Train Epoch: 10 [1920/4328 (44%)]\tLoss: 1.217776\n",
      "Train Epoch: 10 [2560/4328 (59%)]\tLoss: 1.234543\n",
      "Train Epoch: 10 [3200/4328 (74%)]\tLoss: 1.381293\n",
      "Train Epoch: 10 [3840/4328 (88%)]\tLoss: 1.396301\n",
      "Train Epoch: 11 [0/4328 (0%)]\tLoss: 1.286830\n",
      "Train Epoch: 11 [640/4328 (15%)]\tLoss: 1.516285\n",
      "Train Epoch: 11 [1280/4328 (29%)]\tLoss: 1.204714\n",
      "Train Epoch: 11 [1920/4328 (44%)]\tLoss: 1.309395\n",
      "Train Epoch: 11 [2560/4328 (59%)]\tLoss: 1.465656\n",
      "Train Epoch: 11 [3200/4328 (74%)]\tLoss: 1.354940\n",
      "Train Epoch: 11 [3840/4328 (88%)]\tLoss: 1.316431\n",
      "Train Epoch: 12 [0/4328 (0%)]\tLoss: 1.276503\n",
      "Train Epoch: 12 [640/4328 (15%)]\tLoss: 0.972817\n",
      "Train Epoch: 12 [1280/4328 (29%)]\tLoss: 1.501936\n",
      "Train Epoch: 12 [1920/4328 (44%)]\tLoss: 1.244463\n",
      "Train Epoch: 12 [2560/4328 (59%)]\tLoss: 1.143799\n",
      "Train Epoch: 12 [3200/4328 (74%)]\tLoss: 1.328997\n",
      "Train Epoch: 12 [3840/4328 (88%)]\tLoss: 1.208182\n",
      "Train Epoch: 13 [0/4328 (0%)]\tLoss: 1.234129\n",
      "Train Epoch: 13 [640/4328 (15%)]\tLoss: 1.312076\n",
      "Train Epoch: 13 [1280/4328 (29%)]\tLoss: 1.391382\n",
      "Train Epoch: 13 [1920/4328 (44%)]\tLoss: 1.311774\n",
      "Train Epoch: 13 [2560/4328 (59%)]\tLoss: 1.167302\n",
      "Train Epoch: 13 [3200/4328 (74%)]\tLoss: 1.459131\n",
      "Train Epoch: 13 [3840/4328 (88%)]\tLoss: 1.304683\n",
      "Train Epoch: 14 [0/4328 (0%)]\tLoss: 1.377810\n",
      "Train Epoch: 14 [640/4328 (15%)]\tLoss: 1.475245\n",
      "Train Epoch: 14 [1280/4328 (29%)]\tLoss: 1.296652\n",
      "Train Epoch: 14 [1920/4328 (44%)]\tLoss: 1.294600\n",
      "Train Epoch: 14 [2560/4328 (59%)]\tLoss: 1.376064\n",
      "Train Epoch: 14 [3200/4328 (74%)]\tLoss: 1.191850\n",
      "Train Epoch: 14 [3840/4328 (88%)]\tLoss: 1.349975\n",
      "Train Epoch: 15 [0/4328 (0%)]\tLoss: 1.038681\n",
      "Train Epoch: 15 [640/4328 (15%)]\tLoss: 1.484787\n",
      "Train Epoch: 15 [1280/4328 (29%)]\tLoss: 1.077182\n",
      "Train Epoch: 15 [1920/4328 (44%)]\tLoss: 1.391236\n",
      "Train Epoch: 15 [2560/4328 (59%)]\tLoss: 1.208313\n",
      "Train Epoch: 15 [3200/4328 (74%)]\tLoss: 1.543095\n",
      "Train Epoch: 15 [3840/4328 (88%)]\tLoss: 1.381372\n",
      "Train Epoch: 16 [0/4328 (0%)]\tLoss: 1.227913\n",
      "Train Epoch: 16 [640/4328 (15%)]\tLoss: 1.328598\n",
      "Train Epoch: 16 [1280/4328 (29%)]\tLoss: 1.175506\n",
      "Train Epoch: 16 [1920/4328 (44%)]\tLoss: 1.241765\n",
      "Train Epoch: 16 [2560/4328 (59%)]\tLoss: 1.392643\n",
      "Train Epoch: 16 [3200/4328 (74%)]\tLoss: 1.193488\n",
      "Train Epoch: 16 [3840/4328 (88%)]\tLoss: 1.433017\n",
      "Train Epoch: 17 [0/4328 (0%)]\tLoss: 1.215381\n",
      "Train Epoch: 17 [640/4328 (15%)]\tLoss: 1.243387\n",
      "Train Epoch: 17 [1280/4328 (29%)]\tLoss: 1.389572\n",
      "Train Epoch: 17 [1920/4328 (44%)]\tLoss: 1.364033\n",
      "Train Epoch: 17 [2560/4328 (59%)]\tLoss: 1.218973\n",
      "Train Epoch: 17 [3200/4328 (74%)]\tLoss: 1.178215\n",
      "Train Epoch: 17 [3840/4328 (88%)]\tLoss: 1.175334\n",
      "Training client 9\n",
      "Train Epoch: 1 [0/6128 (0%)]\tLoss: 1.682152\n",
      "Train Epoch: 1 [640/6128 (10%)]\tLoss: 1.359014\n",
      "Train Epoch: 1 [1280/6128 (21%)]\tLoss: 1.690795\n",
      "Train Epoch: 1 [1920/6128 (31%)]\tLoss: 1.784296\n",
      "Train Epoch: 1 [2560/6128 (42%)]\tLoss: 1.492028\n",
      "Train Epoch: 1 [3200/6128 (52%)]\tLoss: 1.474095\n",
      "Train Epoch: 1 [3840/6128 (62%)]\tLoss: 1.471180\n",
      "Train Epoch: 1 [4480/6128 (73%)]\tLoss: 1.657692\n",
      "Train Epoch: 1 [5120/6128 (83%)]\tLoss: 1.359491\n",
      "Train Epoch: 1 [5760/6128 (94%)]\tLoss: 1.516997\n",
      "Train Epoch: 2 [0/6128 (0%)]\tLoss: 1.422358\n",
      "Train Epoch: 2 [640/6128 (10%)]\tLoss: 1.544836\n",
      "Train Epoch: 2 [1280/6128 (21%)]\tLoss: 1.519828\n",
      "Train Epoch: 2 [1920/6128 (31%)]\tLoss: 1.461202\n",
      "Train Epoch: 2 [2560/6128 (42%)]\tLoss: 1.329538\n",
      "Train Epoch: 2 [3200/6128 (52%)]\tLoss: 1.608116\n",
      "Train Epoch: 2 [3840/6128 (62%)]\tLoss: 1.584981\n",
      "Train Epoch: 2 [4480/6128 (73%)]\tLoss: 1.510857\n",
      "Train Epoch: 2 [5120/6128 (83%)]\tLoss: 1.115694\n",
      "Train Epoch: 2 [5760/6128 (94%)]\tLoss: 1.525400\n",
      "Train Epoch: 3 [0/6128 (0%)]\tLoss: 1.812150\n",
      "Train Epoch: 3 [640/6128 (10%)]\tLoss: 1.241840\n",
      "Train Epoch: 3 [1280/6128 (21%)]\tLoss: 1.868581\n",
      "Train Epoch: 3 [1920/6128 (31%)]\tLoss: 1.372496\n",
      "Train Epoch: 3 [2560/6128 (42%)]\tLoss: 1.403895\n",
      "Train Epoch: 3 [3200/6128 (52%)]\tLoss: 1.475850\n",
      "Train Epoch: 3 [3840/6128 (62%)]\tLoss: 1.417142\n",
      "Train Epoch: 3 [4480/6128 (73%)]\tLoss: 1.106804\n",
      "Train Epoch: 3 [5120/6128 (83%)]\tLoss: 1.288684\n",
      "Train Epoch: 3 [5760/6128 (94%)]\tLoss: 1.232322\n",
      "Train Epoch: 4 [0/6128 (0%)]\tLoss: 1.450125\n",
      "Train Epoch: 4 [640/6128 (10%)]\tLoss: 1.415640\n",
      "Train Epoch: 4 [1280/6128 (21%)]\tLoss: 1.336162\n",
      "Train Epoch: 4 [1920/6128 (31%)]\tLoss: 1.486216\n",
      "Train Epoch: 4 [2560/6128 (42%)]\tLoss: 1.307447\n",
      "Train Epoch: 4 [3200/6128 (52%)]\tLoss: 1.324822\n",
      "Train Epoch: 4 [3840/6128 (62%)]\tLoss: 1.469735\n",
      "Train Epoch: 4 [4480/6128 (73%)]\tLoss: 1.643766\n",
      "Train Epoch: 4 [5120/6128 (83%)]\tLoss: 1.484973\n",
      "Train Epoch: 4 [5760/6128 (94%)]\tLoss: 1.353757\n",
      "Train Epoch: 5 [0/6128 (0%)]\tLoss: 1.289007\n",
      "Train Epoch: 5 [640/6128 (10%)]\tLoss: 1.322538\n",
      "Train Epoch: 5 [1280/6128 (21%)]\tLoss: 1.272221\n",
      "Train Epoch: 5 [1920/6128 (31%)]\tLoss: 1.293282\n",
      "Train Epoch: 5 [2560/6128 (42%)]\tLoss: 1.352107\n",
      "Train Epoch: 5 [3200/6128 (52%)]\tLoss: 1.352205\n",
      "Train Epoch: 5 [3840/6128 (62%)]\tLoss: 1.273152\n",
      "Train Epoch: 5 [4480/6128 (73%)]\tLoss: 1.367224\n",
      "Train Epoch: 5 [5120/6128 (83%)]\tLoss: 1.474940\n",
      "Train Epoch: 5 [5760/6128 (94%)]\tLoss: 1.364286\n",
      "Train Epoch: 6 [0/6128 (0%)]\tLoss: 1.480854\n",
      "Train Epoch: 6 [640/6128 (10%)]\tLoss: 1.629552\n",
      "Train Epoch: 6 [1280/6128 (21%)]\tLoss: 1.144261\n",
      "Train Epoch: 6 [1920/6128 (31%)]\tLoss: 1.362459\n",
      "Train Epoch: 6 [2560/6128 (42%)]\tLoss: 1.447940\n",
      "Train Epoch: 6 [3200/6128 (52%)]\tLoss: 1.251816\n",
      "Train Epoch: 6 [3840/6128 (62%)]\tLoss: 1.312531\n",
      "Train Epoch: 6 [4480/6128 (73%)]\tLoss: 1.590956\n",
      "Train Epoch: 6 [5120/6128 (83%)]\tLoss: 1.355956\n",
      "Train Epoch: 6 [5760/6128 (94%)]\tLoss: 1.423686\n",
      "Train Epoch: 7 [0/6128 (0%)]\tLoss: 1.133039\n",
      "Train Epoch: 7 [640/6128 (10%)]\tLoss: 1.549314\n",
      "Train Epoch: 7 [1280/6128 (21%)]\tLoss: 1.285554\n",
      "Train Epoch: 7 [1920/6128 (31%)]\tLoss: 1.385902\n",
      "Train Epoch: 7 [2560/6128 (42%)]\tLoss: 1.202622\n",
      "Train Epoch: 7 [3200/6128 (52%)]\tLoss: 1.429198\n",
      "Train Epoch: 7 [3840/6128 (62%)]\tLoss: 1.337239\n",
      "Train Epoch: 7 [4480/6128 (73%)]\tLoss: 1.277265\n",
      "Train Epoch: 7 [5120/6128 (83%)]\tLoss: 1.543716\n",
      "Train Epoch: 7 [5760/6128 (94%)]\tLoss: 1.144920\n",
      "Train Epoch: 8 [0/6128 (0%)]\tLoss: 1.554336\n",
      "Train Epoch: 8 [640/6128 (10%)]\tLoss: 1.462425\n",
      "Train Epoch: 8 [1280/6128 (21%)]\tLoss: 1.505731\n",
      "Train Epoch: 8 [1920/6128 (31%)]\tLoss: 1.348058\n",
      "Train Epoch: 8 [2560/6128 (42%)]\tLoss: 1.616413\n",
      "Train Epoch: 8 [3200/6128 (52%)]\tLoss: 1.278715\n",
      "Train Epoch: 8 [3840/6128 (62%)]\tLoss: 1.472387\n",
      "Train Epoch: 8 [4480/6128 (73%)]\tLoss: 1.405659\n",
      "Train Epoch: 8 [5120/6128 (83%)]\tLoss: 1.491444\n",
      "Train Epoch: 8 [5760/6128 (94%)]\tLoss: 1.351576\n",
      "Train Epoch: 9 [0/6128 (0%)]\tLoss: 1.306521\n",
      "Train Epoch: 9 [640/6128 (10%)]\tLoss: 1.067145\n",
      "Train Epoch: 9 [1280/6128 (21%)]\tLoss: 1.275367\n",
      "Train Epoch: 9 [1920/6128 (31%)]\tLoss: 1.189675\n",
      "Train Epoch: 9 [2560/6128 (42%)]\tLoss: 1.454499\n",
      "Train Epoch: 9 [3200/6128 (52%)]\tLoss: 1.418585\n",
      "Train Epoch: 9 [3840/6128 (62%)]\tLoss: 1.351588\n",
      "Train Epoch: 9 [4480/6128 (73%)]\tLoss: 1.278571\n",
      "Train Epoch: 9 [5120/6128 (83%)]\tLoss: 1.372747\n",
      "Train Epoch: 9 [5760/6128 (94%)]\tLoss: 1.113090\n",
      "Train Epoch: 10 [0/6128 (0%)]\tLoss: 1.368886\n",
      "Train Epoch: 10 [640/6128 (10%)]\tLoss: 1.113491\n",
      "Train Epoch: 10 [1280/6128 (21%)]\tLoss: 1.299524\n",
      "Train Epoch: 10 [1920/6128 (31%)]\tLoss: 1.395706\n",
      "Train Epoch: 10 [2560/6128 (42%)]\tLoss: 1.453748\n",
      "Train Epoch: 10 [3200/6128 (52%)]\tLoss: 1.372961\n",
      "Train Epoch: 10 [3840/6128 (62%)]\tLoss: 1.364566\n",
      "Train Epoch: 10 [4480/6128 (73%)]\tLoss: 1.257128\n",
      "Train Epoch: 10 [5120/6128 (83%)]\tLoss: 1.390297\n",
      "Train Epoch: 10 [5760/6128 (94%)]\tLoss: 1.473459\n",
      "Train Epoch: 11 [0/6128 (0%)]\tLoss: 1.297736\n",
      "Train Epoch: 11 [640/6128 (10%)]\tLoss: 1.581478\n",
      "Train Epoch: 11 [1280/6128 (21%)]\tLoss: 1.319694\n",
      "Train Epoch: 11 [1920/6128 (31%)]\tLoss: 1.110054\n",
      "Train Epoch: 11 [2560/6128 (42%)]\tLoss: 1.163513\n",
      "Train Epoch: 11 [3200/6128 (52%)]\tLoss: 1.359950\n",
      "Train Epoch: 11 [3840/6128 (62%)]\tLoss: 1.432846\n",
      "Train Epoch: 11 [4480/6128 (73%)]\tLoss: 1.351645\n",
      "Train Epoch: 11 [5120/6128 (83%)]\tLoss: 1.331055\n",
      "Train Epoch: 11 [5760/6128 (94%)]\tLoss: 1.306739\n",
      "Train Epoch: 12 [0/6128 (0%)]\tLoss: 1.513320\n",
      "Train Epoch: 12 [640/6128 (10%)]\tLoss: 1.426297\n",
      "Train Epoch: 12 [1280/6128 (21%)]\tLoss: 1.397710\n",
      "Train Epoch: 12 [1920/6128 (31%)]\tLoss: 1.423587\n",
      "Train Epoch: 12 [2560/6128 (42%)]\tLoss: 1.470326\n",
      "Train Epoch: 12 [3200/6128 (52%)]\tLoss: 1.258292\n",
      "Train Epoch: 12 [3840/6128 (62%)]\tLoss: 1.245685\n",
      "Train Epoch: 12 [4480/6128 (73%)]\tLoss: 1.410262\n",
      "Train Epoch: 12 [5120/6128 (83%)]\tLoss: 1.671549\n",
      "Train Epoch: 12 [5760/6128 (94%)]\tLoss: 1.437074\n",
      "Train Epoch: 13 [0/6128 (0%)]\tLoss: 0.987580\n",
      "Train Epoch: 13 [640/6128 (10%)]\tLoss: 1.436980\n",
      "Train Epoch: 13 [1280/6128 (21%)]\tLoss: 1.440857\n",
      "Train Epoch: 13 [1920/6128 (31%)]\tLoss: 1.321945\n",
      "Train Epoch: 13 [2560/6128 (42%)]\tLoss: 1.023124\n",
      "Train Epoch: 13 [3200/6128 (52%)]\tLoss: 1.409302\n",
      "Train Epoch: 13 [3840/6128 (62%)]\tLoss: 1.213239\n",
      "Train Epoch: 13 [4480/6128 (73%)]\tLoss: 1.273168\n",
      "Train Epoch: 13 [5120/6128 (83%)]\tLoss: 1.380665\n",
      "Train Epoch: 13 [5760/6128 (94%)]\tLoss: 1.513624\n",
      "Train Epoch: 14 [0/6128 (0%)]\tLoss: 1.433599\n",
      "Train Epoch: 14 [640/6128 (10%)]\tLoss: 1.415665\n",
      "Train Epoch: 14 [1280/6128 (21%)]\tLoss: 1.654049\n",
      "Train Epoch: 14 [1920/6128 (31%)]\tLoss: 1.445132\n",
      "Train Epoch: 14 [2560/6128 (42%)]\tLoss: 1.600439\n",
      "Train Epoch: 14 [3200/6128 (52%)]\tLoss: 1.543550\n",
      "Train Epoch: 14 [3840/6128 (62%)]\tLoss: 1.529428\n",
      "Train Epoch: 14 [4480/6128 (73%)]\tLoss: 1.449426\n",
      "Train Epoch: 14 [5120/6128 (83%)]\tLoss: 1.332014\n",
      "Train Epoch: 14 [5760/6128 (94%)]\tLoss: 1.312256\n",
      "Train Epoch: 15 [0/6128 (0%)]\tLoss: 1.568038\n",
      "Train Epoch: 15 [640/6128 (10%)]\tLoss: 1.438872\n",
      "Train Epoch: 15 [1280/6128 (21%)]\tLoss: 1.372863\n",
      "Train Epoch: 15 [1920/6128 (31%)]\tLoss: 1.404085\n",
      "Train Epoch: 15 [2560/6128 (42%)]\tLoss: 1.231868\n",
      "Train Epoch: 15 [3200/6128 (52%)]\tLoss: 1.262542\n",
      "Train Epoch: 15 [3840/6128 (62%)]\tLoss: 1.307366\n",
      "Train Epoch: 15 [4480/6128 (73%)]\tLoss: 1.378270\n",
      "Train Epoch: 15 [5120/6128 (83%)]\tLoss: 1.402084\n",
      "Train Epoch: 15 [5760/6128 (94%)]\tLoss: 1.431461\n",
      "Train Epoch: 16 [0/6128 (0%)]\tLoss: 1.291090\n",
      "Train Epoch: 16 [640/6128 (10%)]\tLoss: 1.565462\n",
      "Train Epoch: 16 [1280/6128 (21%)]\tLoss: 1.272025\n",
      "Train Epoch: 16 [1920/6128 (31%)]\tLoss: 1.381606\n",
      "Train Epoch: 16 [2560/6128 (42%)]\tLoss: 1.306694\n",
      "Train Epoch: 16 [3200/6128 (52%)]\tLoss: 1.051471\n",
      "Train Epoch: 16 [3840/6128 (62%)]\tLoss: 1.347946\n",
      "Train Epoch: 16 [4480/6128 (73%)]\tLoss: 1.378369\n",
      "Train Epoch: 16 [5120/6128 (83%)]\tLoss: 1.498719\n",
      "Train Epoch: 16 [5760/6128 (94%)]\tLoss: 1.190032\n",
      "Train Epoch: 17 [0/6128 (0%)]\tLoss: 1.124495\n",
      "Train Epoch: 17 [640/6128 (10%)]\tLoss: 1.218651\n",
      "Train Epoch: 17 [1280/6128 (21%)]\tLoss: 1.098741\n",
      "Train Epoch: 17 [1920/6128 (31%)]\tLoss: 1.463691\n",
      "Train Epoch: 17 [2560/6128 (42%)]\tLoss: 1.183773\n",
      "Train Epoch: 17 [3200/6128 (52%)]\tLoss: 1.254816\n",
      "Train Epoch: 17 [3840/6128 (62%)]\tLoss: 1.328328\n",
      "Train Epoch: 17 [4480/6128 (73%)]\tLoss: 1.365763\n",
      "Train Epoch: 17 [5120/6128 (83%)]\tLoss: 1.316877\n",
      "Train Epoch: 17 [5760/6128 (94%)]\tLoss: 1.153698\n",
      "Training client 10\n",
      "Train Epoch: 1 [0/5394 (0%)]\tLoss: 1.577765\n",
      "Train Epoch: 1 [640/5394 (12%)]\tLoss: 1.387727\n",
      "Train Epoch: 1 [1280/5394 (24%)]\tLoss: 1.534356\n",
      "Train Epoch: 1 [1920/5394 (35%)]\tLoss: 1.434585\n",
      "Train Epoch: 1 [2560/5394 (47%)]\tLoss: 1.289533\n",
      "Train Epoch: 1 [3200/5394 (59%)]\tLoss: 1.345988\n",
      "Train Epoch: 1 [3840/5394 (71%)]\tLoss: 1.429676\n",
      "Train Epoch: 1 [4480/5394 (82%)]\tLoss: 1.381426\n",
      "Train Epoch: 1 [5120/5394 (94%)]\tLoss: 1.227183\n",
      "Train Epoch: 2 [0/5394 (0%)]\tLoss: 1.323518\n",
      "Train Epoch: 2 [640/5394 (12%)]\tLoss: 1.488758\n",
      "Train Epoch: 2 [1280/5394 (24%)]\tLoss: 1.598576\n",
      "Train Epoch: 2 [1920/5394 (35%)]\tLoss: 1.231618\n",
      "Train Epoch: 2 [2560/5394 (47%)]\tLoss: 1.325657\n",
      "Train Epoch: 2 [3200/5394 (59%)]\tLoss: 1.296717\n",
      "Train Epoch: 11 [1920/5017 (38%)]\tLoss: 1.312365\n",
      "Train Epoch: 11 [2560/5017 (51%)]\tLoss: 1.247644\n",
      "Train Epoch: 11 [3200/5017 (63%)]\tLoss: 1.217164\n",
      "Train Epoch: 11 [3840/5017 (76%)]\tLoss: 1.001524\n",
      "Train Epoch: 11 [4480/5017 (89%)]\tLoss: 1.060192\n",
      "Train Epoch: 12 [0/5017 (0%)]\tLoss: 0.897458\n",
      "Train Epoch: 12 [640/5017 (13%)]\tLoss: 0.977136\n",
      "Train Epoch: 12 [1280/5017 (25%)]\tLoss: 1.369199\n",
      "Train Epoch: 12 [1920/5017 (38%)]\tLoss: 1.423534\n",
      "Train Epoch: 12 [2560/5017 (51%)]\tLoss: 1.098009\n",
      "Train Epoch: 12 [3200/5017 (63%)]\tLoss: 1.197105\n",
      "Train Epoch: 12 [3840/5017 (76%)]\tLoss: 1.469615\n",
      "Train Epoch: 12 [4480/5017 (89%)]\tLoss: 1.393678\n",
      "Train Epoch: 13 [0/5017 (0%)]\tLoss: 1.234118\n",
      "Train Epoch: 13 [640/5017 (13%)]\tLoss: 0.963429\n",
      "Train Epoch: 13 [1280/5017 (25%)]\tLoss: 1.602355\n",
      "Train Epoch: 13 [1920/5017 (38%)]\tLoss: 0.838571\n",
      "Train Epoch: 13 [2560/5017 (51%)]\tLoss: 1.204887\n",
      "Train Epoch: 13 [3200/5017 (63%)]\tLoss: 1.500125\n",
      "Train Epoch: 13 [3840/5017 (76%)]\tLoss: 1.378692\n",
      "Train Epoch: 13 [4480/5017 (89%)]\tLoss: 1.195077\n",
      "Train Epoch: 14 [0/5017 (0%)]\tLoss: 1.161690\n",
      "Train Epoch: 14 [640/5017 (13%)]\tLoss: 1.141001\n",
      "Train Epoch: 14 [1280/5017 (25%)]\tLoss: 1.395189\n",
      "Train Epoch: 14 [1920/5017 (38%)]\tLoss: 1.523977\n",
      "Train Epoch: 14 [2560/5017 (51%)]\tLoss: 1.077427\n",
      "Train Epoch: 14 [3200/5017 (63%)]\tLoss: 1.343673\n",
      "Train Epoch: 14 [3840/5017 (76%)]\tLoss: 1.057565\n",
      "Train Epoch: 14 [4480/5017 (89%)]\tLoss: 1.125568\n",
      "Train Epoch: 15 [0/5017 (0%)]\tLoss: 1.037610\n",
      "Train Epoch: 15 [640/5017 (13%)]\tLoss: 1.056069\n",
      "Train Epoch: 15 [1280/5017 (25%)]\tLoss: 1.234872\n",
      "Train Epoch: 15 [1920/5017 (38%)]\tLoss: 1.349308\n",
      "Train Epoch: 15 [2560/5017 (51%)]\tLoss: 1.118140\n",
      "Train Epoch: 15 [3200/5017 (63%)]\tLoss: 1.126699\n",
      "Train Epoch: 15 [3840/5017 (76%)]\tLoss: 1.154481\n",
      "Train Epoch: 15 [4480/5017 (89%)]\tLoss: 1.248702\n",
      "Train Epoch: 16 [0/5017 (0%)]\tLoss: 1.267695\n",
      "Train Epoch: 16 [640/5017 (13%)]\tLoss: 1.294795\n",
      "Train Epoch: 16 [1280/5017 (25%)]\tLoss: 1.227528\n",
      "Train Epoch: 16 [1920/5017 (38%)]\tLoss: 1.304517\n",
      "Train Epoch: 16 [2560/5017 (51%)]\tLoss: 1.500066\n",
      "Train Epoch: 16 [3200/5017 (63%)]\tLoss: 1.094861\n",
      "Train Epoch: 16 [3840/5017 (76%)]\tLoss: 1.062208\n",
      "Train Epoch: 16 [4480/5017 (89%)]\tLoss: 1.253155\n",
      "Train Epoch: 17 [0/5017 (0%)]\tLoss: 1.184177\n",
      "Train Epoch: 17 [640/5017 (13%)]\tLoss: 1.186629\n",
      "Train Epoch: 17 [1280/5017 (25%)]\tLoss: 1.034109\n",
      "Train Epoch: 17 [1920/5017 (38%)]\tLoss: 1.120091\n",
      "Train Epoch: 17 [2560/5017 (51%)]\tLoss: 1.123516\n",
      "Train Epoch: 17 [3200/5017 (63%)]\tLoss: 1.335796\n",
      "Train Epoch: 17 [3840/5017 (76%)]\tLoss: 1.177467\n",
      "Train Epoch: 17 [4480/5017 (89%)]\tLoss: 1.121510\n",
      "Training client 5\n",
      "Train Epoch: 1 [0/4336 (0%)]\tLoss: 1.195233\n",
      "Train Epoch: 1 [640/4336 (15%)]\tLoss: 1.184287\n",
      "Train Epoch: 1 [1280/4336 (29%)]\tLoss: 1.171799\n",
      "Train Epoch: 1 [1920/4336 (44%)]\tLoss: 1.190343\n",
      "Train Epoch: 1 [2560/4336 (59%)]\tLoss: 1.259349\n",
      "Train Epoch: 1 [3200/4336 (74%)]\tLoss: 1.152741\n",
      "Train Epoch: 1 [3840/4336 (88%)]\tLoss: 1.422215\n",
      "Train Epoch: 2 [0/4336 (0%)]\tLoss: 1.409343\n",
      "Train Epoch: 2 [640/4336 (15%)]\tLoss: 1.361695\n",
      "Train Epoch: 2 [1280/4336 (29%)]\tLoss: 1.058600\n",
      "Train Epoch: 2 [1920/4336 (44%)]\tLoss: 1.221469\n",
      "Train Epoch: 2 [2560/4336 (59%)]\tLoss: 1.422922\n",
      "Train Epoch: 2 [3200/4336 (74%)]\tLoss: 1.253364\n",
      "Train Epoch: 2 [3840/4336 (88%)]\tLoss: 1.289708\n",
      "Train Epoch: 3 [0/4336 (0%)]\tLoss: 1.022962\n",
      "Train Epoch: 3 [640/4336 (15%)]\tLoss: 1.227197\n",
      "Train Epoch: 3 [1280/4336 (29%)]\tLoss: 1.172525\n",
      "Train Epoch: 3 [1920/4336 (44%)]\tLoss: 1.379267\n",
      "Train Epoch: 3 [2560/4336 (59%)]\tLoss: 1.318982\n",
      "Train Epoch: 3 [3200/4336 (74%)]\tLoss: 1.392978\n",
      "Train Epoch: 3 [3840/4336 (88%)]\tLoss: 1.279705\n",
      "Train Epoch: 4 [0/4336 (0%)]\tLoss: 1.193438\n",
      "Train Epoch: 4 [640/4336 (15%)]\tLoss: 1.341512\n",
      "Train Epoch: 4 [1280/4336 (29%)]\tLoss: 1.143652\n",
      "Train Epoch: 4 [1920/4336 (44%)]\tLoss: 1.213237\n",
      "Train Epoch: 4 [2560/4336 (59%)]\tLoss: 1.248114\n",
      "Train Epoch: 4 [3200/4336 (74%)]\tLoss: 1.128895\n",
      "Train Epoch: 4 [3840/4336 (88%)]\tLoss: 1.087553\n",
      "Train Epoch: 5 [0/4336 (0%)]\tLoss: 1.235417\n",
      "Train Epoch: 5 [640/4336 (15%)]\tLoss: 1.258210\n",
      "Train Epoch: 5 [1280/4336 (29%)]\tLoss: 1.248477\n",
      "Train Epoch: 5 [1920/4336 (44%)]\tLoss: 1.084530\n",
      "Train Epoch: 5 [2560/4336 (59%)]\tLoss: 1.560113\n",
      "Train Epoch: 5 [3200/4336 (74%)]\tLoss: 1.255944\n",
      "Train Epoch: 5 [3840/4336 (88%)]\tLoss: 1.257077\n",
      "Train Epoch: 6 [0/4336 (0%)]\tLoss: 1.072652\n",
      "Train Epoch: 6 [640/4336 (15%)]\tLoss: 1.112857\n",
      "Train Epoch: 6 [1280/4336 (29%)]\tLoss: 1.094297\n",
      "Train Epoch: 6 [1920/4336 (44%)]\tLoss: 1.217729\n",
      "Train Epoch: 6 [2560/4336 (59%)]\tLoss: 1.469634\n",
      "Train Epoch: 6 [3200/4336 (74%)]\tLoss: 1.239622\n",
      "Train Epoch: 6 [3840/4336 (88%)]\tLoss: 1.458158\n",
      "Train Epoch: 7 [0/4336 (0%)]\tLoss: 1.413432\n",
      "Train Epoch: 7 [640/4336 (15%)]\tLoss: 1.493512\n",
      "Train Epoch: 7 [1280/4336 (29%)]\tLoss: 1.000928\n",
      "Train Epoch: 7 [1920/4336 (44%)]\tLoss: 1.418049\n",
      "Train Epoch: 7 [2560/4336 (59%)]\tLoss: 1.330584\n",
      "Train Epoch: 7 [3200/4336 (74%)]\tLoss: 1.266445\n",
      "Train Epoch: 7 [3840/4336 (88%)]\tLoss: 1.367250\n",
      "Train Epoch: 8 [0/4336 (0%)]\tLoss: 1.008153\n",
      "Train Epoch: 8 [640/4336 (15%)]\tLoss: 1.238486\n",
      "Train Epoch: 8 [1280/4336 (29%)]\tLoss: 1.165408\n",
      "Train Epoch: 8 [1920/4336 (44%)]\tLoss: 1.181302\n",
      "Train Epoch: 8 [2560/4336 (59%)]\tLoss: 1.352506\n",
      "Train Epoch: 8 [3200/4336 (74%)]\tLoss: 1.252656\n",
      "Train Epoch: 8 [3840/4336 (88%)]\tLoss: 1.242372\n",
      "Train Epoch: 9 [0/4336 (0%)]\tLoss: 1.172572\n",
      "Train Epoch: 9 [640/4336 (15%)]\tLoss: 1.058969\n",
      "Train Epoch: 9 [1280/4336 (29%)]\tLoss: 1.387065\n",
      "Train Epoch: 9 [1920/4336 (44%)]\tLoss: 1.417985\n",
      "Train Epoch: 9 [2560/4336 (59%)]\tLoss: 1.267805\n",
      "Train Epoch: 9 [3200/4336 (74%)]\tLoss: 1.161106\n",
      "Train Epoch: 9 [3840/4336 (88%)]\tLoss: 1.105363\n",
      "Train Epoch: 10 [0/4336 (0%)]\tLoss: 1.299532\n",
      "Train Epoch: 10 [640/4336 (15%)]\tLoss: 1.260216\n",
      "Train Epoch: 10 [1280/4336 (29%)]\tLoss: 1.149562\n",
      "Train Epoch: 10 [1920/4336 (44%)]\tLoss: 1.326418\n",
      "Train Epoch: 10 [2560/4336 (59%)]\tLoss: 1.453865\n",
      "Train Epoch: 10 [3200/4336 (74%)]\tLoss: 1.309332\n",
      "Train Epoch: 10 [3840/4336 (88%)]\tLoss: 0.871594\n",
      "Train Epoch: 11 [0/4336 (0%)]\tLoss: 1.192713\n",
      "Train Epoch: 11 [640/4336 (15%)]\tLoss: 1.153825\n",
      "Train Epoch: 11 [1280/4336 (29%)]\tLoss: 1.177587\n",
      "Train Epoch: 11 [1920/4336 (44%)]\tLoss: 1.165968\n",
      "Train Epoch: 11 [2560/4336 (59%)]\tLoss: 1.232391\n",
      "Train Epoch: 11 [3200/4336 (74%)]\tLoss: 1.381958\n",
      "Train Epoch: 11 [3840/4336 (88%)]\tLoss: 1.316623\n",
      "Train Epoch: 12 [0/4336 (0%)]\tLoss: 1.216322\n",
      "Train Epoch: 12 [640/4336 (15%)]\tLoss: 1.135755\n",
      "Train Epoch: 12 [1280/4336 (29%)]\tLoss: 1.085182\n",
      "Train Epoch: 12 [1920/4336 (44%)]\tLoss: 1.054139\n",
      "Train Epoch: 12 [2560/4336 (59%)]\tLoss: 1.185444\n",
      "Train Epoch: 12 [3200/4336 (74%)]\tLoss: 1.531548\n",
      "Train Epoch: 12 [3840/4336 (88%)]\tLoss: 0.978218\n",
      "Train Epoch: 13 [0/4336 (0%)]\tLoss: 1.182055\n",
      "Train Epoch: 13 [640/4336 (15%)]\tLoss: 1.200122\n",
      "Train Epoch: 13 [1280/4336 (29%)]\tLoss: 1.208864\n",
      "Train Epoch: 13 [1920/4336 (44%)]\tLoss: 1.159122\n",
      "Train Epoch: 13 [2560/4336 (59%)]\tLoss: 1.090783\n",
      "Train Epoch: 13 [3200/4336 (74%)]\tLoss: 1.388126\n",
      "Train Epoch: 13 [3840/4336 (88%)]\tLoss: 1.244848\n",
      "Train Epoch: 14 [0/4336 (0%)]\tLoss: 1.232188\n",
      "Train Epoch: 14 [640/4336 (15%)]\tLoss: 1.269238\n",
      "Train Epoch: 14 [1280/4336 (29%)]\tLoss: 1.284133\n",
      "Train Epoch: 14 [1920/4336 (44%)]\tLoss: 1.201400\n",
      "Train Epoch: 14 [2560/4336 (59%)]\tLoss: 1.113620\n",
      "Train Epoch: 14 [3200/4336 (74%)]\tLoss: 0.975595\n",
      "Train Epoch: 14 [3840/4336 (88%)]\tLoss: 1.180463\n",
      "Train Epoch: 15 [0/4336 (0%)]\tLoss: 1.047934\n",
      "Train Epoch: 15 [640/4336 (15%)]\tLoss: 1.128019\n",
      "Train Epoch: 15 [1280/4336 (29%)]\tLoss: 1.475713\n",
      "Train Epoch: 15 [1920/4336 (44%)]\tLoss: 1.323331\n",
      "Train Epoch: 15 [2560/4336 (59%)]\tLoss: 1.217336\n",
      "Train Epoch: 15 [3200/4336 (74%)]\tLoss: 1.107023\n",
      "Train Epoch: 15 [3840/4336 (88%)]\tLoss: 1.215449\n",
      "Train Epoch: 16 [0/4336 (0%)]\tLoss: 1.092601\n",
      "Train Epoch: 16 [640/4336 (15%)]\tLoss: 1.275322\n",
      "Train Epoch: 16 [1280/4336 (29%)]\tLoss: 1.097210\n",
      "Train Epoch: 16 [1920/4336 (44%)]\tLoss: 1.162691\n",
      "Train Epoch: 16 [2560/4336 (59%)]\tLoss: 1.058140\n",
      "Train Epoch: 16 [3200/4336 (74%)]\tLoss: 1.164240\n",
      "Train Epoch: 16 [3840/4336 (88%)]\tLoss: 1.132433\n",
      "Train Epoch: 17 [0/4336 (0%)]\tLoss: 1.467679\n",
      "Train Epoch: 17 [640/4336 (15%)]\tLoss: 1.175990\n",
      "Train Epoch: 17 [1280/4336 (29%)]\tLoss: 1.384975\n",
      "Train Epoch: 17 [1920/4336 (44%)]\tLoss: 1.175617\n",
      "Train Epoch: 17 [2560/4336 (59%)]\tLoss: 1.080472\n",
      "Train Epoch: 17 [3200/4336 (74%)]\tLoss: 1.287055\n",
      "Train Epoch: 17 [3840/4336 (88%)]\tLoss: 1.076312\n",
      "Training client 6\n",
      "Train Epoch: 1 [0/4045 (0%)]\tLoss: 1.304197\n",
      "Train Epoch: 1 [640/4045 (16%)]\tLoss: 1.098930\n",
      "Train Epoch: 1 [1280/4045 (31%)]\tLoss: 1.316672\n",
      "Train Epoch: 1 [1920/4045 (47%)]\tLoss: 1.294837\n",
      "Train Epoch: 1 [2560/4045 (62%)]\tLoss: 1.413956\n",
      "Train Epoch: 1 [3200/4045 (78%)]\tLoss: 1.167750\n",
      "Train Epoch: 1 [3840/4045 (94%)]\tLoss: 1.210193\n",
      "Train Epoch: 2 [0/4045 (0%)]\tLoss: 1.467995\n",
      "Train Epoch: 2 [640/4045 (16%)]\tLoss: 1.292490\n",
      "Train Epoch: 2 [1280/4045 (31%)]\tLoss: 1.427716\n",
      "Train Epoch: 2 [1920/4045 (47%)]\tLoss: 1.404217\n",
      "Train Epoch: 2 [2560/4045 (62%)]\tLoss: 0.970881\n",
      "Train Epoch: 2 [3200/4045 (78%)]\tLoss: 1.351363\n",
      "Train Epoch: 2 [3840/4045 (94%)]\tLoss: 1.166004\n",
      "Train Epoch: 3 [0/4045 (0%)]\tLoss: 1.262230\n",
      "Train Epoch: 3 [640/4045 (16%)]\tLoss: 1.352648\n",
      "Train Epoch: 3 [1280/4045 (31%)]\tLoss: 1.063319\n",
      "Train Epoch: 3 [1920/4045 (47%)]\tLoss: 1.381412\n",
      "Train Epoch: 3 [2560/4045 (62%)]\tLoss: 1.285727\n",
      "Train Epoch: 3 [3200/4045 (78%)]\tLoss: 1.341308\n",
      "Train Epoch: 3 [3840/4045 (94%)]\tLoss: 1.246122\n",
      "Train Epoch: 4 [0/4045 (0%)]\tLoss: 1.318416\n",
      "Train Epoch: 4 [640/4045 (16%)]\tLoss: 1.376117\n",
      "Train Epoch: 4 [1280/4045 (31%)]\tLoss: 1.125209\n",
      "Train Epoch: 4 [1920/4045 (47%)]\tLoss: 0.992330\n",
      "Train Epoch: 4 [2560/4045 (62%)]\tLoss: 1.211248\n",
      "Train Epoch: 4 [3200/4045 (78%)]\tLoss: 1.187828\n",
      "Train Epoch: 4 [3840/4045 (94%)]\tLoss: 1.471545\n",
      "Train Epoch: 5 [0/4045 (0%)]\tLoss: 1.605325\n",
      "Train Epoch: 5 [640/4045 (16%)]\tLoss: 1.176955\n",
      "Train Epoch: 5 [1280/4045 (31%)]\tLoss: 1.137963\n",
      "Train Epoch: 5 [1920/4045 (47%)]\tLoss: 1.355354\n",
      "Train Epoch: 5 [2560/4045 (62%)]\tLoss: 1.239170\n",
      "Train Epoch: 5 [3200/4045 (78%)]\tLoss: 1.182202\n",
      "Train Epoch: 5 [3840/4045 (94%)]\tLoss: 1.310184\n",
      "Train Epoch: 6 [0/4045 (0%)]\tLoss: 1.072933\n",
      "Train Epoch: 6 [640/4045 (16%)]\tLoss: 1.186669\n",
      "Train Epoch: 6 [1280/4045 (31%)]\tLoss: 1.150480\n",
      "Train Epoch: 6 [1920/4045 (47%)]\tLoss: 1.377222\n",
      "Train Epoch: 6 [2560/4045 (62%)]\tLoss: 1.367770\n",
      "Train Epoch: 6 [3200/4045 (78%)]\tLoss: 1.308782\n",
      "Train Epoch: 6 [3840/4045 (94%)]\tLoss: 1.135071\n",
      "Train Epoch: 7 [0/4045 (0%)]\tLoss: 1.111971\n",
      "Train Epoch: 7 [640/4045 (16%)]\tLoss: 1.395865\n",
      "Train Epoch: 7 [1280/4045 (31%)]\tLoss: 1.287927\n",
      "Train Epoch: 7 [1920/4045 (47%)]\tLoss: 1.224867\n",
      "Train Epoch: 7 [2560/4045 (62%)]\tLoss: 1.111368\n",
      "Train Epoch: 7 [3200/4045 (78%)]\tLoss: 1.291263\n",
      "Train Epoch: 7 [3840/4045 (94%)]\tLoss: 1.238229\n",
      "Train Epoch: 8 [0/4045 (0%)]\tLoss: 1.279460\n",
      "Train Epoch: 8 [640/4045 (16%)]\tLoss: 1.338094\n",
      "Train Epoch: 8 [1280/4045 (31%)]\tLoss: 1.401456\n",
      "Train Epoch: 8 [1920/4045 (47%)]\tLoss: 1.101043\n",
      "Train Epoch: 8 [2560/4045 (62%)]\tLoss: 1.280341\n",
      "Train Epoch: 8 [3200/4045 (78%)]\tLoss: 1.228855\n",
      "Train Epoch: 8 [3840/4045 (94%)]\tLoss: 1.223267\n",
      "Train Epoch: 9 [0/4045 (0%)]\tLoss: 1.068257\n",
      "Train Epoch: 9 [640/4045 (16%)]\tLoss: 1.287818\n",
      "Train Epoch: 9 [1280/4045 (31%)]\tLoss: 1.240660\n",
      "Train Epoch: 9 [1920/4045 (47%)]\tLoss: 1.203238\n",
      "Train Epoch: 9 [2560/4045 (62%)]\tLoss: 1.199782\n",
      "Train Epoch: 9 [3200/4045 (78%)]\tLoss: 1.305755\n",
      "Train Epoch: 9 [3840/4045 (94%)]\tLoss: 1.099091\n",
      "Train Epoch: 10 [0/4045 (0%)]\tLoss: 1.222726\n",
      "Train Epoch: 10 [640/4045 (16%)]\tLoss: 1.090136\n",
      "Train Epoch: 10 [1280/4045 (31%)]\tLoss: 1.196801\n",
      "Train Epoch: 10 [1920/4045 (47%)]\tLoss: 1.222453\n",
      "Train Epoch: 10 [2560/4045 (62%)]\tLoss: 1.236060\n",
      "Train Epoch: 10 [3200/4045 (78%)]\tLoss: 1.117163\n",
      "Train Epoch: 10 [3840/4045 (94%)]\tLoss: 1.438828\n",
      "Train Epoch: 11 [0/4045 (0%)]\tLoss: 1.459638\n",
      "Train Epoch: 11 [640/4045 (16%)]\tLoss: 1.235311\n",
      "Train Epoch: 11 [1280/4045 (31%)]\tLoss: 1.390926\n",
      "Train Epoch: 11 [1920/4045 (47%)]\tLoss: 1.288622\n",
      "Train Epoch: 11 [2560/4045 (62%)]\tLoss: 1.088055\n",
      "Train Epoch: 11 [3200/4045 (78%)]\tLoss: 1.052248\n",
      "Train Epoch: 11 [3840/4045 (94%)]\tLoss: 1.345521\n",
      "Train Epoch: 12 [0/4045 (0%)]\tLoss: 1.475564\n",
      "Train Epoch: 12 [640/4045 (16%)]\tLoss: 1.264414\n",
      "Train Epoch: 12 [1280/4045 (31%)]\tLoss: 1.198219\n",
      "Train Epoch: 12 [1920/4045 (47%)]\tLoss: 1.027192\n",
      "Train Epoch: 12 [2560/4045 (62%)]\tLoss: 1.213356\n",
      "Train Epoch: 12 [3200/4045 (78%)]\tLoss: 1.082855\n",
      "Train Epoch: 12 [3840/4045 (94%)]\tLoss: 1.334770\n",
      "Train Epoch: 13 [0/4045 (0%)]\tLoss: 1.730950\n",
      "Train Epoch: 13 [640/4045 (16%)]\tLoss: 1.351474\n",
      "Train Epoch: 13 [1280/4045 (31%)]\tLoss: 1.433504\n",
      "Train Epoch: 13 [1920/4045 (47%)]\tLoss: 1.439732\n",
      "Train Epoch: 13 [2560/4045 (62%)]\tLoss: 1.158404\n",
      "Train Epoch: 13 [3200/4045 (78%)]\tLoss: 1.110515\n",
      "Train Epoch: 13 [3840/4045 (94%)]\tLoss: 1.481419\n",
      "Train Epoch: 14 [0/4045 (0%)]\tLoss: 1.283363\n",
      "Train Epoch: 14 [640/4045 (16%)]\tLoss: 1.324865\n",
      "Train Epoch: 14 [1280/4045 (31%)]\tLoss: 1.363954\n",
      "Train Epoch: 14 [1920/4045 (47%)]\tLoss: 1.162880\n",
      "Train Epoch: 14 [2560/4045 (62%)]\tLoss: 1.203190\n",
      "Train Epoch: 14 [3200/4045 (78%)]\tLoss: 1.141349\n",
      "Train Epoch: 14 [3840/4045 (94%)]\tLoss: 1.314166\n",
      "Train Epoch: 15 [0/4045 (0%)]\tLoss: 1.177505\n",
      "Train Epoch: 15 [640/4045 (16%)]\tLoss: 1.188257\n",
      "Train Epoch: 15 [1280/4045 (31%)]\tLoss: 1.249604\n",
      "Train Epoch: 15 [1920/4045 (47%)]\tLoss: 1.007906\n",
      "Train Epoch: 15 [2560/4045 (62%)]\tLoss: 1.289324\n",
      "Train Epoch: 15 [3200/4045 (78%)]\tLoss: 1.327865\n",
      "Train Epoch: 15 [3840/4045 (94%)]\tLoss: 1.254208\n",
      "Train Epoch: 16 [0/4045 (0%)]\tLoss: 1.334303\n",
      "Train Epoch: 16 [640/4045 (16%)]\tLoss: 1.034330\n",
      "Train Epoch: 16 [1280/4045 (31%)]\tLoss: 1.189594\n",
      "Train Epoch: 16 [1920/4045 (47%)]\tLoss: 1.182663\n",
      "Train Epoch: 16 [2560/4045 (62%)]\tLoss: 1.259646\n",
      "Train Epoch: 16 [3200/4045 (78%)]\tLoss: 1.421648\n",
      "Train Epoch: 16 [3840/4045 (94%)]\tLoss: 1.449069\n",
      "Train Epoch: 17 [0/4045 (0%)]\tLoss: 1.187797\n",
      "Train Epoch: 17 [640/4045 (16%)]\tLoss: 1.173643\n",
      "Train Epoch: 17 [1280/4045 (31%)]\tLoss: 1.352702\n",
      "Train Epoch: 17 [1920/4045 (47%)]\tLoss: 1.466177\n",
      "Train Epoch: 17 [2560/4045 (62%)]\tLoss: 1.312869\n",
      "Train Epoch: 17 [3200/4045 (78%)]\tLoss: 1.348901\n",
      "Train Epoch: 17 [3840/4045 (94%)]\tLoss: 1.185479\n",
      "Training client 7\n",
      "Train Epoch: 1 [0/5496 (0%)]\tLoss: 1.432463\n",
      "Train Epoch: 1 [640/5496 (12%)]\tLoss: 1.577963\n",
      "Train Epoch: 1 [1280/5496 (23%)]\tLoss: 1.177029\n",
      "Train Epoch: 1 [1920/5496 (35%)]\tLoss: 1.449375\n",
      "Train Epoch: 1 [2560/5496 (47%)]\tLoss: 1.655648\n",
      "Train Epoch: 1 [3200/5496 (58%)]\tLoss: 1.511945\n",
      "Train Epoch: 1 [3840/5496 (70%)]\tLoss: 1.349983\n",
      "Train Epoch: 1 [4480/5496 (81%)]\tLoss: 1.439368\n",
      "Train Epoch: 1 [5120/5496 (93%)]\tLoss: 1.448597\n",
      "Train Epoch: 2 [0/5496 (0%)]\tLoss: 1.258090\n",
      "Train Epoch: 2 [640/5496 (12%)]\tLoss: 1.408298\n",
      "Train Epoch: 2 [1280/5496 (23%)]\tLoss: 1.470501\n",
      "Train Epoch: 2 [1920/5496 (35%)]\tLoss: 1.419137\n",
      "Train Epoch: 2 [2560/5496 (47%)]\tLoss: 1.296024\n",
      "Train Epoch: 2 [3200/5496 (58%)]\tLoss: 1.520717\n",
      "Train Epoch: 2 [3840/5496 (70%)]\tLoss: 1.116638\n",
      "Train Epoch: 2 [4480/5496 (81%)]\tLoss: 1.216114\n",
      "Train Epoch: 2 [5120/5496 (93%)]\tLoss: 1.415113\n",
      "Train Epoch: 3 [0/5496 (0%)]\tLoss: 1.403345\n",
      "Train Epoch: 3 [640/5496 (12%)]\tLoss: 1.426024\n",
      "Train Epoch: 3 [1280/5496 (23%)]\tLoss: 1.278160\n",
      "Train Epoch: 3 [1920/5496 (35%)]\tLoss: 1.415623\n",
      "Train Epoch: 3 [2560/5496 (47%)]\tLoss: 1.189192\n",
      "Train Epoch: 3 [3200/5496 (58%)]\tLoss: 1.452840\n",
      "Train Epoch: 3 [3840/5496 (70%)]\tLoss: 1.485919\n",
      "Train Epoch: 3 [4480/5496 (81%)]\tLoss: 1.383422\n",
      "Train Epoch: 3 [5120/5496 (93%)]\tLoss: 1.453134\n",
      "Train Epoch: 4 [0/5496 (0%)]\tLoss: 0.911343\n",
      "Train Epoch: 4 [640/5496 (12%)]\tLoss: 1.489235\n",
      "Train Epoch: 4 [1280/5496 (23%)]\tLoss: 1.304674\n",
      "Train Epoch: 4 [1920/5496 (35%)]\tLoss: 1.643231\n",
      "Train Epoch: 4 [2560/5496 (47%)]\tLoss: 1.384335\n",
      "Train Epoch: 4 [3200/5496 (58%)]\tLoss: 1.253053\n",
      "Train Epoch: 4 [3840/5496 (70%)]\tLoss: 1.475055\n",
      "Train Epoch: 4 [4480/5496 (81%)]\tLoss: 1.400667\n",
      "Train Epoch: 4 [5120/5496 (93%)]\tLoss: 1.397066\n",
      "Train Epoch: 5 [0/5496 (0%)]\tLoss: 1.628450\n",
      "Train Epoch: 5 [640/5496 (12%)]\tLoss: 1.346454\n",
      "Train Epoch: 5 [1280/5496 (23%)]\tLoss: 1.114427\n",
      "Train Epoch: 5 [1920/5496 (35%)]\tLoss: 1.418815\n",
      "Train Epoch: 5 [2560/5496 (47%)]\tLoss: 1.487259\n",
      "Train Epoch: 5 [3200/5496 (58%)]\tLoss: 1.306240\n",
      "Train Epoch: 5 [3840/5496 (70%)]\tLoss: 1.091136\n",
      "Train Epoch: 5 [4480/5496 (81%)]\tLoss: 1.209928\n",
      "Train Epoch: 5 [5120/5496 (93%)]\tLoss: 1.243867\n",
      "Train Epoch: 6 [0/5496 (0%)]\tLoss: 1.093171\n",
      "Train Epoch: 6 [640/5496 (12%)]\tLoss: 1.597050\n",
      "Train Epoch: 6 [1280/5496 (23%)]\tLoss: 1.535207\n",
      "Train Epoch: 6 [1920/5496 (35%)]\tLoss: 1.210765\n",
      "Train Epoch: 6 [2560/5496 (47%)]\tLoss: 1.298365\n",
      "Train Epoch: 6 [3200/5496 (58%)]\tLoss: 1.554607\n",
      "Train Epoch: 6 [3840/5496 (70%)]\tLoss: 1.379051\n",
      "Train Epoch: 6 [4480/5496 (81%)]\tLoss: 1.301560\n",
      "Train Epoch: 6 [5120/5496 (93%)]\tLoss: 1.388885\n",
      "Train Epoch: 7 [0/5496 (0%)]\tLoss: 1.321844\n",
      "Train Epoch: 7 [640/5496 (12%)]\tLoss: 1.375091\n",
      "Train Epoch: 7 [1280/5496 (23%)]\tLoss: 1.144038\n",
      "Train Epoch: 7 [1920/5496 (35%)]\tLoss: 1.197322\n",
      "Train Epoch: 7 [2560/5496 (47%)]\tLoss: 1.311760\n",
      "Train Epoch: 7 [3200/5496 (58%)]\tLoss: 1.329095\n",
      "Train Epoch: 7 [3840/5496 (70%)]\tLoss: 1.107684\n",
      "Train Epoch: 7 [4480/5496 (81%)]\tLoss: 1.603171\n",
      "Train Epoch: 7 [5120/5496 (93%)]\tLoss: 1.383953\n",
      "Train Epoch: 8 [0/5496 (0%)]\tLoss: 1.415927\n",
      "Train Epoch: 8 [640/5496 (12%)]\tLoss: 1.329220\n",
      "Train Epoch: 8 [1280/5496 (23%)]\tLoss: 1.227508\n",
      "Train Epoch: 8 [1920/5496 (35%)]\tLoss: 1.264125\n",
      "Train Epoch: 8 [2560/5496 (47%)]\tLoss: 1.308674\n",
      "Train Epoch: 8 [3200/5496 (58%)]\tLoss: 1.407515\n",
      "Train Epoch: 8 [3840/5496 (70%)]\tLoss: 1.224993\n",
      "Train Epoch: 8 [4480/5496 (81%)]\tLoss: 1.377298\n",
      "Train Epoch: 8 [5120/5496 (93%)]\tLoss: 1.276103\n",
      "Train Epoch: 9 [0/5496 (0%)]\tLoss: 1.353146\n",
      "Train Epoch: 9 [640/5496 (12%)]\tLoss: 1.255984\n",
      "Train Epoch: 9 [1280/5496 (23%)]\tLoss: 1.153324\n",
      "Train Epoch: 9 [1920/5496 (35%)]\tLoss: 1.363820\n"
     ]
    }
   ],
   "source": [
    "for alpha in alpha_values:\n",
    "    print(f\"Running experiment with alpha: {alpha} \")\n",
    "\n",
    "    partitioned_data_classic = partition.balanced_dirichlet_partition(trainingset,partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "    classic_client_loaders = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic.values()\n",
    "    ]\n",
    "\n",
    "    local_models_classic = [copy.deepcopy(gloabl_model_classic) for _ in range(num_clients)]\n",
    "\n",
    "    rounds_classic = 4\n",
    "    \n",
    "    for round_idx in range(rounds_classic):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "    \n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "            \n",
    "        print(f\"after training{local_models_classic}\")\n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "        print(f\"after fedaveraging{local_models_classic}\")\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,local_models_classic,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,gloabl_model_classic,single=True)\n",
    "        test_losses = []\n",
    "        test(gloabl_model_classic,cifar10_test_loader,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in cifar10_test_loader:\n",
    "                output = global_model_classic(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(cifar10_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for non-clustered classic\n",
    "        if alpha not in results[\"classic\"]:\n",
    "            results[\"classic\"][alpha] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        results[\"classic\"][alpha][\"losses\"].extend(test_losses)\n",
    "        results[\"classic\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "    ######################\n",
    "    import cluster\n",
    "    \n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_classic, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_classic = clustered_data\n",
    "\n",
    "    classic_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic.values()\n",
    "    ]\n",
    "\n",
    "    for round_idx in range(rounds_classic):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "    \n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "            \n",
    "        print(f\"after training{local_models_classic}\")\n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "        print(f\"after fedaveraging{local_models_classic}\")\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,local_models_classic,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,gloabl_model_classic,single=True)\n",
    "        test_losses = []\n",
    "        test(gloabl_model_classic,cifar10_test_loader,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in cifar10_test_loader:\n",
    "                output = global_model_classic(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(cifar10_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if alpha not in clusteredResults[\"classic\"]:\n",
    "            clusteredResults[\"classic\"][alpha] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        clusteredResults[\"classic\"][alpha][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"classic\"][alpha][\"accuracy\"].extend(test_accuracies_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA \n",
    "trainingset_pca = train_loader_reduced_pca.dataset\n",
    "global_model_pca = cifar_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alpha_values:\n",
    "    print(f\"Running experiment with alpha: {alpha} \")\n",
    "\n",
    "    partitioned_data_pca = partition.balanced_dirichlet_partition(trainingset_pca, partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "    pca_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_pca, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_pca.values()\n",
    "    ]\n",
    "\n",
    "  \n",
    "    local_models_pca = [copy.deepcopy(global_model_pca) for _ in range(num_clients)]\n",
    "\n",
    "    rounds_pca = 4\n",
    "    \n",
    "    for round_idx in range(rounds_pca):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "    \n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "        print(f\"after training{local_models_pca}\")\n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "        print(f\"after fedaveraging{local_models_pca}\")\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,local_models_pca,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,global_model_pca,single=True)\n",
    "        test_losses = []\n",
    "        test(global_model_pca,test_loader_pca,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                output = global_model_pca(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for non-clustered classic\n",
    "        if alpha not in results[\"pca\"]:\n",
    "            results[\"pca\"][alpha] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        results[\"pca\"][alpha][\"losses\"].extend(test_losses)\n",
    "        results[\"pca\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "\n",
    "    ######################\n",
    "    import cluster\n",
    "    \n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset_pca.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_pca, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_pca = clustered_data\n",
    "\n",
    "    pca_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_pca, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_pca.values()\n",
    "    ]\n",
    "\n",
    "    for round_idx in range(rounds_pca):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "    \n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "        print(f\"after training{local_models_pca}\")\n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "        print(f\"after fedaveraging{local_models_pca}\")\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,local_models_pca,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,global_model_pca,single=True)\n",
    "        test_losses = []\n",
    "        test(global_model_pca,test_loader_pca,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in cifar10_test_loader:\n",
    "                output = global_model_pca(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(cifar10_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if alpha not in clusteredResults[\"pca\"]:\n",
    "            clusteredResults[\"pca\"][alpha] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        clusteredResults[\"pca\"][alpha][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"pca\"][alpha][\"accuracy\"].extend(test_accuracies_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Auto\n",
    "trainingset_auto = reduced_train_loader_auto.dataset\n",
    "global_model_auto = cifar_classification_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alpha_values:\n",
    "    print(f\"Running experiment with alpha: {alpha} \")\n",
    "\n",
    "    partitioned_data_auto = partition.balanced_dirichlet_partition(trainingset_auto, partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "    auto_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto.values()\n",
    "    ]\n",
    "\n",
    "    local_model_autoencoder = [copy.deepcopy(global_model_pca) for _ in range(num_clients)]\n",
    "\n",
    "    rounds_auto = 4\n",
    "    \n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "    \n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, auto_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "            \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "    \n",
    "        distribute_global_model(global_weights_auto,local_model_autoencoder,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_auto, global_model_auto,single=True)\n",
    "        test_losses = []\n",
    "        test(global_model_auto,test_loader_auto,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                output = global_model_auto(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for non-clustered classic\n",
    "        if alpha not in results[\"autoencoder\"]:\n",
    "            results[\"autoencoder\"][alpha] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        results[\"autoencoder\"][alpha][\"losses\"].extend(test_losses)\n",
    "        results[\"autoencoder\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "\n",
    "    ######################\n",
    "    import cluster\n",
    "    \n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset_pca.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_pca, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_pca = clustered_data\n",
    "\n",
    "    auto_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto.values()\n",
    "    ]\n",
    "\n",
    "    \n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "    \n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train(epoch, client_model, auto_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "            \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "    \n",
    "        distribute_global_model(global_weights_auto,local_model_autoencoder,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_auto, global_model_auto,single=True)\n",
    "        test_losses = []\n",
    "        test(global_model_auto,test_loader_auto,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_auto:\n",
    "                output = global_model_auto(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        accuracy = 100. * correct / len(test_loader_auto.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if alpha not in clusteredResults[\"autoencoder\"]:\n",
    "            clusteredResults[\"autoencoder\"][alpha] = {\"losses\": [], \"accuracy\": []}\n",
    "\n",
    "        clusteredResults[\"autoencoder\"][alpha][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"autoencoder\"][alpha][\"accuracy\"].extend(test_accuracies_classic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1400x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_client_averages(data):\n",
    "    client_averages = {}\n",
    "    for client, client_data in data.items():  \n",
    "        client_averages[client] = {}\n",
    "        for partition_number, metrics in client_data.items():  # Iterate over partitions\n",
    "            avg_loss = sum(metrics['losses']) / len(metrics['losses']) if metrics['losses'] else 0\n",
    "            avg_accuracy = sum(metrics['accuracy']) / len(metrics['accuracy']) if metrics['accuracy'] else 0\n",
    "            client_averages[client][partition_number] = {\"average_loss\": avg_loss, \"average_accuracy\": avg_accuracy}\n",
    "    return client_averages\n",
    "\n",
    "\n",
    "non_clustered_averages = calculate_client_averages(results)\n",
    "clustered_averages = calculate_client_averages(clusteredResults)\n",
    "\n",
    "\n",
    "def plot_averages(non_clustered, clustered):\n",
    "    for client_type in non_clustered.keys(): \n",
    "        partitions_non_clustered = list(non_clustered[client_type].keys())\n",
    "        losses_non_clustered = [\n",
    "            non_clustered[client_type][partition][\"average_loss\"] for partition in partitions_non_clustered\n",
    "        ]\n",
    "        accuracies_non_clustered = [\n",
    "            non_clustered[client_type][partition][\"average_accuracy\"] for partition in partitions_non_clustered\n",
    "        ]\n",
    "\n",
    "        partitions_clustered = list(clustered[client_type].keys())\n",
    "        losses_clustered = [\n",
    "            clustered[client_type][partition][\"average_loss\"] for partition in partitions_clustered\n",
    "        ]\n",
    "        accuracies_clustered = [\n",
    "            clustered[client_type][partition][\"average_accuracy\"] for partition in partitions_clustered\n",
    "        ]\n",
    "\n",
    "        plt.figure(figsize=(14, 6))\n",
    "\n",
    " \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(partitions_non_clustered, losses_non_clustered, label=\"Non-Clustered\", marker=\"o\")\n",
    "        plt.plot(partitions_clustered, losses_clustered, label=\"Clustered\", marker=\"o\")\n",
    "        plt.xlabel(\"Number of Partitions\")\n",
    "        plt.ylabel(\"Average Loss\")\n",
    "        plt.title(f\"Average Loss per Partition ({client_type.capitalize()})\")\n",
    "        plt.legend()\n",
    "\n",
    " \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.plot(partitions_non_clustered, accuracies_non_clustered, label=\"Non-Clustered\", marker=\"o\")\n",
    "        plt.plot(partitions_clustered, accuracies_clustered, label=\"Clustered\", marker=\"o\")\n",
    "        plt.xlabel(\"Number of Partitions\")\n",
    "        plt.ylabel(\"Average Accuracy (%)\")\n",
    "        plt.title(f\"Average Accuracy per Partition ({client_type.capitalize()})\")\n",
    "        plt.legend()\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Plot averages for all client types\n",
    "plot_averages(non_clustered_averages, clustered_averages)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
