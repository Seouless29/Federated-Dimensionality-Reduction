{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader, Subset,TensorDataset\n",
    "from autoencoder2 import Autoencoder2, reduce_dimensions2\n",
    "import torchvision\n",
    "from model2 import classification_model\n",
    "import copy\n",
    "import partition\n",
    "from pca import PCADigitReducer\n",
    "from training import train,test, train_fashion,test_fashion\n",
    "from federated_learning import distribute_global_model, federated_averaging\n",
    "from model4 import MultilayerPerceptron\n",
    "import cluster2\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_epochs = 10\n",
    "batch_size_train = 64\n",
    "batch_size_test = 1000\n",
    "learning_rate = 0.01 # ev. 0.001\n",
    "momentum = 0.5\n",
    "log_interval = 10\n",
    "\n",
    "random_seed = 1\n",
    "torch.backends.cudnn.enabled = False\n",
    "torch.manual_seed(random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fashion_mnist_transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.0,), (1,))  \n",
    "])\n",
    "\n",
    "fashion_mnist_train_loader = DataLoader(\n",
    "    datasets.FashionMNIST('/files/', train=True, download=True, transform=fashion_mnist_transform),\n",
    "    batch_size=batch_size_train, shuffle=True\n",
    ")\n",
    "\n",
    "fashion_mnist_test_loader = DataLoader(\n",
    "    datasets.FashionMNIST('/files/', train=False, download=True, transform=fashion_mnist_transform),\n",
    "    batch_size=batch_size_test, shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader_pca = copy.copy(fashion_mnist_train_loader)\n",
    "test_loader_pca = copy.copy(fashion_mnist_test_loader)\n",
    "\n",
    "train_loader_auto = copy.copy(fashion_mnist_train_loader)\n",
    "test_loader_auto = copy.copy(fashion_mnist_test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTensorDataset(TensorDataset):\n",
    "    def __init__(self, *tensors):\n",
    "        super().__init__(*tensors)\n",
    "        self.data = tensors[0]\n",
    "        self.targets = tensors[1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "#latent_dim = 100  \n",
    "autoencoder = Autoencoder2()\n",
    "auto_criterion = nn.MSELoss()\n",
    "auto_optimizer = optim.Adam(autoencoder.parameters(), lr=1e-3)\n",
    "auto_num_epochs = 5\n",
    "for epoch in range(auto_num_epochs): \n",
    "    for images, _ in train_loader_auto:\n",
    "        auto_optimizer.zero_grad()\n",
    "        reconstructed = autoencoder(images)\n",
    "        loss = auto_criterion(reconstructed, images)  \n",
    "        loss.backward()\n",
    "        auto_optimizer.step()\n",
    "        \n",
    "    print(f\"Epoch [{epoch+1}/5], Loss: {loss.item()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster: 6, CLient: 10\n",
    "alpha_values = [0.1, 0.5, 1, 5, 10, 20]\n",
    "num_clients = 10\n",
    "num_clusters = 6\n",
    "results = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}\n",
    "clusteredResults = {\"classic\": {}, \"pca\": {}, \"autoencoder\": {}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset = fashion_mnist_train_loader.dataset\n",
    "trial_model_strong = MultilayerPerceptron()\n",
    "global_model_classic_strong = MultilayerPerceptron()\n",
    "rounds_classic = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alpha_values:\n",
    "    print(f\"Running experiment with alpha: {alpha} \")\n",
    "\n",
    "    partitioned_data_classic = partition.balanced_dirichlet_partition(\n",
    "        trainingset, partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "    classic_client_loaders = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic.values()\n",
    "    ]\n",
    "\n",
    "    local_models_classic_strong = [copy.deepcopy(global_model_classic_strong) for _ in range(num_clients)]\n",
    "\n",
    "    \n",
    "    for round_idx in range(rounds_classic):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "    \n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic_strong):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "    \n",
    "    \n",
    "        distribute_global_model(global_weights_classic,local_models_classic_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,global_model_classic_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_classic_strong,fashion_mnist_test_loader,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in fashion_mnist_test_loader:\n",
    "                data = data.view(data.shape[0], -1)\n",
    "                output = global_model_classic_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / len(fashion_mnist_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "\n",
    "        # Save results for non-clustered classic\n",
    "        if alpha not in results[\"classic\"]:\n",
    "            results[\"classic\"][alpha] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "        results[\"classic\"][alpha][\"losses\"].extend(test_losses)\n",
    "        results[\"classic\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "        results[\"classic\"][alpha][\"precision\"].append(precision)\n",
    "        results[\"classic\"][alpha][\"recall\"].append(recall)\n",
    "        results[\"classic\"][alpha][\"f1\"].append(f1)\n",
    "\n",
    "    ######################\n",
    "    import cluster2\n",
    "\n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clients = [cluster2.FederatedClient(cid, indices, targets, num_classes) for cid, indices in partitioned_data_classic.items()]\n",
    "    client_distributions = [client.compute_label_distribution() for client in clients]\n",
    "    server = cluster2.FederatedClusterServer(num_clusters)\n",
    "    aggregated_data = server.aggregate_client_data(client_distributions)\n",
    "    clustered_data = server.perform_greedy_clustering(aggregated_data, partitioned_data_classic)\n",
    "    \n",
    "    partitioned_data_classic_clustered = clustered_data\n",
    "    \"\"\" \n",
    "    import cluster\n",
    "    cluster = cluster.Cluster(num_clusters=num_clusters)\n",
    "    \n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clustered_data = cluster.apply_clustering(partitioned_data_classic, targets, num_classes)\n",
    "    \n",
    "    partitioned_data_classic_clustered = clustered_data\"\n",
    "    \"\"\"\n",
    "\n",
    "    classic_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_classic_clustered.values()\n",
    "    ]\n",
    "\n",
    "    for round_idx in range(rounds_classic):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "        local_weights_classic = []\n",
    "        for client_idx, client_model in enumerate(local_models_classic_strong[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, classic_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_classic.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_classic = federated_averaging(local_weights_classic)\n",
    "    \n",
    "    \n",
    "        distribute_global_model(global_weights_classic,local_models_classic_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_classic,global_model_classic_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_classic_strong,fashion_mnist_test_loader,test_losses)\n",
    "\n",
    "        test_accuracies_classic = []\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in fashion_mnist_test_loader:\n",
    "                data = data.view(data.shape[0], -1)\n",
    "                output = global_model_classic_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "\n",
    "        accuracy = 100. * correct / len(fashion_mnist_test_loader.dataset)\n",
    "        test_accuracies_classic.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if alpha not in clusteredResults[\"classic\"]:\n",
    "            clusteredResults[\"classic\"][alpha] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "        clusteredResults[\"classic\"][alpha][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"classic\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "        clusteredResults[\"classic\"][alpha][\"precision\"].append(precision)\n",
    "        clusteredResults[\"classic\"][alpha][\"recall\"].append(recall)\n",
    "        clusteredResults[\"classic\"][alpha][\"f1\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingset_pca = train_loader_pca.dataset\n",
    "trial_model_pca_strong = MultilayerPerceptron()\n",
    "global_model_pca_strong = MultilayerPerceptron()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alpha_values:\n",
    "    print(f\"Running experiment with alpha {alpha} \")\n",
    "\n",
    "    partitioned_data_pca = partition.balanced_dirichlet_partition(trainingset_pca, partitions_number=num_clients, alpha=alpha)\n",
    "\n",
    "\n",
    "    pca_client_loaders = []\n",
    "    local_models_pca_strong = [copy.deepcopy(global_model_pca_strong) for _ in range(num_clients)]\n",
    "\n",
    "    for client_idx, client_indices in partitioned_data_pca.items():\n",
    "        client_data = [trainingset_pca[i][0] for i in client_indices]  \n",
    "        client_labels = [trainingset_pca[i][1] for i in client_indices]  \n",
    "        \n",
    "        client_data = torch.stack(client_data, dim=0)\n",
    "        client_data = client_data.view(client_data.size(0), -1)  \n",
    "        \n",
    "        pca = PCADigitReducer(100)\n",
    "        client_data_reduced = pca.fit_transform(client_data.numpy())  \n",
    "        \n",
    "        client_data_reconstructed_np = pca.inverse_transform(client_data_reduced) \n",
    "        client_data_reconstructed = torch.tensor(client_data_reconstructed_np, dtype=torch.float32)\n",
    "        \n",
    "        client_data_reconstructed = client_data_reconstructed.view(-1, 1, 28, 28)\n",
    "        client_data_reconstructed = (client_data_reconstructed - 0.2860) / 0.3204  \n",
    "        \n",
    "        client_dataset_pca = CustomTensorDataset(client_data_reconstructed, torch.tensor(client_labels))\n",
    "        pca_client_loaders.append(DataLoader(client_dataset_pca, batch_size=batch_size_train, shuffle=True))\n",
    "\n",
    "    \n",
    "    rounds_pca = 8\n",
    "    for round_idx in range(rounds_pca):\n",
    "        \n",
    "        print(f\"Round {round_idx + 1}/{rounds_pca}\")\n",
    "    \n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca_strong):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,local_models_pca_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,global_model_pca_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_pca_strong,test_loader_pca,test_losses)\n",
    "\n",
    "        test_accuracies_pca = []\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                data = data.view(data.shape[0], -1)\n",
    "                output = global_model_pca_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        \n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_pca.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "        \n",
    "\n",
    "        # Save results for non-clustered classic\n",
    "        if alpha not in results[\"pca\"]:\n",
    "            results[\"pca\"][alpha] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "    \n",
    "        results[\"pca\"][alpha][\"losses\"].extend(test_losses)\n",
    "        results[\"pca\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "        results[\"pca\"][alpha][\"precision\"].append(precision)\n",
    "        results[\"pca\"][alpha][\"recall\"].append(recall)\n",
    "        results[\"pca\"][alpha][\"f1\"].append(f1)\n",
    "\n",
    "    ######################\n",
    "    import cluster2\n",
    "\n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clients = [cluster2.FederatedClient(cid, indices, targets, num_classes) for cid, indices in partitioned_data_pca.items()]\n",
    "    client_distributions = [client.compute_label_distribution() for client in clients]\n",
    "    server = cluster2.FederatedClusterServer(num_clusters)\n",
    "    aggregated_data = server.aggregate_client_data(client_distributions)\n",
    "    clustered_data = server.perform_greedy_clustering(aggregated_data, partitioned_data_pca)\n",
    "    \n",
    "    partitioned_data_pca_clustered = clustered_data\n",
    "\n",
    "\n",
    "    pca_client_loaders_clustered = []\n",
    "\n",
    "    # Apply PCA after clustering\n",
    "    for client_idx, client_indices in partitioned_data_pca_clustered.items():\n",
    "        client_data = [trainingset_pca[i][0] for i in client_indices]  \n",
    "        client_labels = [trainingset_pca[i][1] for i in client_indices]  \n",
    "\n",
    "        client_data = torch.stack(client_data, dim=0)\n",
    "        client_data = client_data.view(client_data.size(0), -1)  \n",
    "        \n",
    "        pca = PCADigitReducer(100)\n",
    "        client_data_reduced = pca.fit_transform(client_data.numpy())  \n",
    "\n",
    "        client_data_reconstructed_np = pca.inverse_transform(client_data_reduced)  \n",
    "        client_data_reconstructed = torch.tensor(client_data_reconstructed_np, dtype=torch.float32)\n",
    "        \n",
    "        client_data_reconstructed = client_data_reconstructed.view(-1, 1, 28, 28)\n",
    "        client_data_reconstructed = (client_data_reconstructed - 0.2860) / 0.3204  \n",
    "\n",
    "        client_dataset_pca_clustered = CustomTensorDataset(client_data_reconstructed, torch.tensor(client_labels))\n",
    "        pca_client_loaders_clustered.append(DataLoader(client_dataset_pca_clustered, batch_size=batch_size_train, shuffle=True))\n",
    "\n",
    "\n",
    "    for round_idx in range(rounds_classic):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_classic}\")\n",
    "\n",
    "        local_weights_pca = []\n",
    "        for client_idx, client_model in enumerate(local_models_pca_strong[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate,\n",
    "                          momentum=momentum)\n",
    "    \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "    \n",
    "    \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, pca_client_loaders[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_pca.append(client_weights)\n",
    "            \n",
    "    \n",
    "        global_weights_pca = federated_averaging(local_weights_pca)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,local_models_pca_strong,single=False)\n",
    "    \n",
    "        distribute_global_model(global_weights_pca,global_model_pca_strong,single=True)\n",
    "        test_losses = []\n",
    "        test_fashion(global_model_pca_strong,test_loader_pca,test_losses)\n",
    "\n",
    "        test_accuracies_pca = []\n",
    "        correct = 0\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_pca:\n",
    "                data = data.view(data.shape[0], -1)\n",
    "                output = global_model_pca_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "                correct += pred.eq(target.data.view_as(pred)).sum().item()\n",
    "        \n",
    "        accuracy = 100. * correct / len(test_loader_pca.dataset)\n",
    "        test_accuracies_pca.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "\n",
    "        # Save results for clustered classic\n",
    "        if alpha not in clusteredResults[\"pca\"]:\n",
    "            clusteredResults[\"pca\"][alpha] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "\n",
    "        clusteredResults[\"pca\"][alpha][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"pca\"][alpha][\"accuracy\"].extend(test_accuracies_classic)\n",
    "        clusteredResults[\"pca\"][alpha][\"precision\"].append(precision)\n",
    "        clusteredResults[\"pca\"][alpha][\"recall\"].append(recall)\n",
    "        clusteredResults[\"pca\"][alpha][\"f1\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder\n",
    "trainingset_auto = train_loader_pca.dataset\n",
    "trial_model_auto_strong = MultilayerPerceptron()\n",
    "global_model_auto_strong = MultilayerPerceptron()\n",
    "autoencoder.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for alpha in alpha_values:\n",
    "    print(f\"Running experiment with alpha {alpha} ...\")\n",
    "    \n",
    "    partitioned_data_auto = partition.balanced_dirichlet_partition(trainingset_auto, partitions_number=num_clients, alpha=alpha)\n",
    "    auto_client_loaders = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto.values()\n",
    "    ]\n",
    "\n",
    "    auto_client_loader_reduced = []\n",
    "\n",
    "    for i,client in enumerate(auto_client_loaders):\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        latent_features, labels = reduce_dimensions2(auto_client_loaders[i], autoencoder.encoder, device)\n",
    "        latent_features = latent_features.detach()\n",
    "\n",
    "        reconstructed_images = autoencoder.decoder(latent_features.to(device))  \n",
    "        reconstructed_images = reconstructed_images.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "        reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "        reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "        auto_client_loader_reduced.append(reduced_train_loader_auto)\n",
    "\n",
    "\n",
    "    local_model_autoencoder_strong = [copy.deepcopy(global_model_auto_strong) for _ in range(num_clients)]\n",
    "    \n",
    "\n",
    "    rounds_auto = 8\n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "        \n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder_strong):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "        \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, auto_client_loader_reduced[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "        \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "        \n",
    "        distribute_global_model(global_weights_auto, local_model_autoencoder_strong, single=False)\n",
    "        distribute_global_model(global_weights_auto, global_model_auto_strong, single=True)\n",
    "        \n",
    "        test_losses = []\n",
    "        test_fashion(global_model_auto_strong, test_loader_auto, test_losses)\n",
    "\n",
    "        test_accuracies_auto = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_auto:\n",
    "                data = data.view(data.shape[0], -1)\n",
    "                output = global_model_auto_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]  # Get the predicted class\n",
    "                \n",
    "                all_preds.extend(pred.cpu().numpy().flatten())  # Store predictions\n",
    "                all_targets.extend(target.cpu().numpy().flatten())  # Store true labels\n",
    "\n",
    "        # Compute accuracy\n",
    "        correct = sum(p == t for p, t in zip(all_preds, all_targets))\n",
    "        accuracy = 100. * correct / len(all_targets)\n",
    "        test_accuracies_auto.append(accuracy)\n",
    "         # Compute precision, recall, and F1-score\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')  # or 'weighted' for imbalance\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "        \n",
    "        \n",
    "        # Save results for non-clustered classic\n",
    "        if alpha not in results[\"autoencoder\"]:\n",
    "            results[\"autoencoder\"][alpha] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "        \n",
    "        results[\"autoencoder\"][alpha][\"losses\"].extend(test_losses)\n",
    "        results[\"autoencoder\"][alpha][\"accuracy\"].extend(test_accuracies_auto)\n",
    "        results[\"autoencoder\"][alpha][\"precision\"].append(precision)\n",
    "        results[\"autoencoder\"][alpha][\"recall\"].append(recall)\n",
    "        results[\"autoencoder\"][alpha][\"f1\"].append(f1)\n",
    "\n",
    "    ######################\n",
    "    # Clustering process\n",
    "    import cluster2\n",
    "\n",
    "    targets = trainingset.targets\n",
    "    num_classes = len(set(targets)) \n",
    "    clients = [cluster2.FederatedClient(cid, indices, targets, num_classes) for cid, indices in partitioned_data_auto.items()]\n",
    "    client_distributions = [client.compute_label_distribution() for client in clients]\n",
    "    server = cluster2.FederatedClusterServer(num_clusters)\n",
    "    aggregated_data = server.aggregate_client_data(client_distributions)\n",
    "    clustered_data = server.perform_greedy_clustering(aggregated_data, partitioned_data_auto)\n",
    "    \n",
    "    partitioned_data_auto_clustered = clustered_data\n",
    "    \n",
    "    auto_client_loaders_clustered = [\n",
    "        DataLoader(Subset(trainingset_auto, indices), batch_size=batch_size_train, shuffle=True)\n",
    "        for indices in partitioned_data_auto_clustered.values()\n",
    "    ]\n",
    "\n",
    "    auto_client_loader_reduced_clustered = []\n",
    "\n",
    "    for i,client in enumerate(auto_client_loaders_clustered):\n",
    "        \n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        latent_features, labels = reduce_dimensions2(auto_client_loaders_clustered[i], autoencoder.encoder, device)\n",
    "        latent_features = latent_features.detach()\n",
    "\n",
    "        reconstructed_images = autoencoder.decoder(latent_features.to(device))  \n",
    "        reconstructed_images = reconstructed_images.view(-1, 1, 28, 28)  # Reshape to [batch_size, channels, height, width]\n",
    "\n",
    "        reconstructed_dataset = CustomTensorDataset(reconstructed_images.cpu(), labels)  \n",
    "        reduced_train_loader_auto = DataLoader(reconstructed_dataset, batch_size=batch_size_train, shuffle=True)\n",
    "        auto_client_loader_reduced_clustered.append(reduced_train_loader_auto)\n",
    "    \n",
    "    for round_idx in range(rounds_auto):\n",
    "        print(f\"Round {round_idx + 1}/{rounds_auto}\")\n",
    "        \n",
    "        local_weights_auto = []\n",
    "        for client_idx, client_model in enumerate(local_model_autoencoder_strong[0:num_clusters]):\n",
    "            print(f\"Training client {client_idx + 1}\")\n",
    "            \n",
    "            optimizer = optim.SGD(client_model.parameters(), lr=learning_rate, momentum=momentum)\n",
    "            \n",
    "            train_losses = []\n",
    "            train_counter = []\n",
    "        \n",
    "            for epoch in range(1, n_epochs + 1):  \n",
    "                train_fashion(epoch, client_model, auto_client_loader_reduced_clustered[client_idx], optimizer, log_interval, train_losses, train_counter)\n",
    "            \n",
    "            client_weights = [param.data.numpy() for param in client_model.parameters()]\n",
    "            local_weights_auto.append(client_weights)\n",
    "        \n",
    "        global_weights_auto = federated_averaging(local_weights_auto)\n",
    "        \n",
    "        distribute_global_model(global_weights_auto, local_model_autoencoder_strong, single=False)\n",
    "        distribute_global_model(global_weights_auto, global_model_auto_strong, single=True)\n",
    "        \n",
    "        test_losses = []\n",
    "        test_fashion(global_model_auto_strong, test_loader_auto, test_losses)\n",
    "\n",
    "        test_accuracies_auto = []\n",
    "        all_preds = []\n",
    "        all_targets = []\n",
    "        with torch.no_grad():\n",
    "            for data, target in test_loader_auto:\n",
    "                data = data.view(data.shape[0], -1)\n",
    "                output = global_model_auto_strong(data)\n",
    "                pred = output.data.max(1, keepdim=True)[1]\n",
    "                all_preds.extend(pred.cpu().numpy().flatten())\n",
    "                all_targets.extend(target.cpu().numpy().flatten())\n",
    "        \n",
    "        correct = sum(p == t for p, t in zip(all_preds, all_targets))\n",
    "        accuracy = 100. * correct / len(all_targets)\n",
    "        test_accuracies_auto.append(accuracy)\n",
    "        precision = precision_score(all_targets, all_preds, average='macro')\n",
    "        recall = recall_score(all_targets, all_preds, average='macro')\n",
    "        f1 = f1_score(all_targets, all_preds, average='macro')\n",
    "    \n",
    "        \n",
    "        # Save results for clustered classic\n",
    "        if alpha not in clusteredResults[\"autoencoder\"]:\n",
    "            clusteredResults[\"autoencoder\"][alpha] = {\"losses\": [], \"accuracy\": [], \"precision\": [], \"recall\": [], \"f1\": []}\n",
    "        \n",
    "        clusteredResults[\"autoencoder\"][alpha][\"losses\"].extend(test_losses)\n",
    "        clusteredResults[\"autoencoder\"][alpha][\"accuracy\"].extend(test_accuracies_auto)\n",
    "        clusteredResults[\"autoencoder\"][alpha][\"precision\"].append(precision)\n",
    "        clusteredResults[\"autoencoder\"][alpha][\"recall\"].append(recall)\n",
    "        clusteredResults[\"autoencoder\"][alpha][\"f1\"].append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Final Results (Non-Clustered):\", results)\n",
    "print(\"Final Results (Clustered):\", clusteredResults)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def compute_final_metrics(metrics):\n",
    "    final_loss = {alpha: data['losses'][-1] for alpha, data in metrics.items()}\n",
    "    final_accuracy = {alpha: data['accuracy'][-1] for alpha, data in metrics.items()}\n",
    "    final_precision = {alpha: data['precision'][-1] for alpha, data in metrics.items()}\n",
    "    final_f1 = {alpha: data['f1'][-1] for alpha, data in metrics.items()}\n",
    "    final_recall = {alpha: data['recall'][-1] for alpha, data in metrics.items()}\n",
    "\n",
    "    return final_loss, final_accuracy, final_precision, final_f1, final_recall\n",
    "\n",
    "def print_results(results, title):\n",
    "    print(f\"\\n{title}:\")\n",
    "    for method in results.keys():\n",
    "        final_loss, final_accuracy, final_precision, final_f1, final_recall = compute_final_metrics(results[method])\n",
    "        print(f\"{method}:\")\n",
    "        print(\"  Final Loss:\", final_loss)\n",
    "        print(\"  Final Accuracy:\", final_accuracy)\n",
    "        print(\"  Final Precision:\", final_precision)\n",
    "        print(\"  Final F1 Score:\", final_f1)\n",
    "        print(\"  Final Recall:\", final_recall)\n",
    "\n",
    "def plot_results(results, title, filename):\n",
    "    plt.figure(figsize=(15, 10))\n",
    "\n",
    "    metrics_names = [\"Loss\", \"Accuracy\", \"Precision\", \"F1 Score\", \"Recall\"]\n",
    "    for i, metric in enumerate(metrics_names):\n",
    "        plt.subplot(2, 3, i + 1)\n",
    "\n",
    "        for method in results.keys():\n",
    "            final_loss, final_accuracy, final_precision, final_f1, final_recall, total_time = compute_final_metrics(results[method])\n",
    "\n",
    "            metric_values = {\n",
    "                \"Loss\": final_loss,\n",
    "                \"Accuracy\": final_accuracy,\n",
    "                \"Precision\": final_precision,\n",
    "                \"F1 Score\": final_f1,\n",
    "                \"Recall\": final_recall\n",
    "            }[metric]\n",
    "\n",
    "            plt.plot(metric_values.keys(), metric_values.values(), marker='o', label=method)\n",
    "\n",
    "        plt.xlabel('Alpha')\n",
    "        plt.ylabel(metric)\n",
    "        plt.title(f'{title} - Alpha vs {metric}')\n",
    "        plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(filename)\n",
    "    plt.show()\n",
    "\n",
    "print_results(results, \"Non-Clustered Results\")\n",
    "plot_results(results, \"Non-Clustered Results\", \"non_clustered_results.png\")\n",
    "\n",
    "print_results(clusteredResults, \"Clustered Results\")\n",
    "plot_results(clusteredResults, \"Clustered Results\", \"clustered_results.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
